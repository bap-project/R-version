{"content":{"0":"A major retrenchment is occurring in the artificial intelligence industry, dashing the hopes of many companies that thought they would prosper by providing the technology to make computers ''think.''\nSome of the setback stems from the failure of artificial intelligence to quickly live up to its promise of making machines that can understand English, recognize objects or reason like a human expert - to be used for such purposes as diagnosing machinery breakdowns or deciding whether to authorize a loan. Despite this, the technology is making slow but steady progress, and now is being subtly incorporated into more conventional computer programs.\u00a0Weak Business Moves\nFar more of the artificial intelligence industry's problems result from poor business decisions by companies that were heavily weighted with technologists rather than business minds. Their main mistake was trying to use special computers for artificial intelligence; the machines were too expensive and did not mesh well with those used by potential customers.\u00a0\n''People believed their own hype,'' said S. Jerrold Kaplan, co-founder of one leading artificial intelligence company, Teknowledge Inc., and now president of the Go Corporation, a startup software company in San Francisco. ''Everyone was planning on growth that was unsustainable.''\nAlthough computers often appear to be intelligent in their everyday applications, they generally perform repetitive tasks following rigid rules set down by programs. They do not learn or make cognitive decisions, as humans do.\nArtificial intelligence aims to make computers do tasks that are said to require intelligence when people do them. One goal has been to make computers understand English and other human languages, so that people do not have to use special computer commands. Another goal has been to make computer vision systems that can recognize objects - to allow robots, for instance, to find objects on conveyer belts and allow military tanks to steer by themselves and choose their own targets.\nA third area, so-called expert systems, involves allowing computers to reason like experts, to give investment advice or to analyze seismic data to decide where to drill for oil. Developing such a program generally involves conducting lengthy interviews with experts and trying to incorporate their thought processes into a computer program.\nHardest hit in the retrenchment so far have been companies that sell special computers for use in artificial intelligence. Symbolics Inc. of Cambridge, Mass., the leader in that market, has had several quarters of heavy losses and recently ousted its chairman. Its archrival, Lisp Machine Inc., declared bankruptcy last year. Texas Instruments Inc. and the Xerox Corporation, the two other vendors in that market, are also experiencing sluggish sales. Meanwhile, Texas Instruments and Apple Computer Inc. announced yesterday that Texas Instruments would sell a version of the Macintosh containing the favored artificial intelligence programming language, Lisp.\nAlso being hurt are companies that sell software tools that allow users to develop their own expert systems. Teknowledge, based in Palo Alto, Calif., halted sales of its product and laid off more than one third of its employees last month. Intellicorp of Mountain View, Calif., has had six quarters of losses, and its chief operating officer resigned in January. The two other major companies in that market, Carnegie Group of Pittsburgh and the Inference Corporation of Los Angeles, have also had problems.\nStill, while companies are suffering, artificial intelligence as a technology continues to make progress. Some companies that have adapted are still doing well. The Aion Corporation, a small, privately held company in Palo Alto, saw its sales triple last year, though from a tiny base. It sells tools that include artificial intelligence technology to help data processing employees develop programs for I.B.M. mainframes. The AI Corporation of Waltham, Mass., has successfully sold programs that allow corporate computer users to use English commands to retrieve information from data bases on I.B.M. mainframes.\u00a0Use in General Software\nArtificial intelligence techniques are also being incorporated into general computer programs without necessarily being labeled as artificial intelligence. Symantec Inc. of Cupertino, Calif., sells a data base program for personal computers called Q & A that responds to English queries. A new class of data bases, such as Agenda by the Lotus Development Corporation, also use some artificial intelligence techniques to organize data.\n''You're beginning to see the embedding of A.I., sub rosa,'' said Louis G. Robinson, publisher of the Spang Robinson Report, an artificial intelligence newsletter.\nThere are also probably more than 1,000 expert systems now being used by companies, according to Paul Harmon, editor of Expert Systems Strategies in San Francisco. Popular applications include diagnosing failures in equipment and scheduling factory operations.\nFor instance, when a merchant calls American Express to verify whether to allow a customer to make a large purchase using an American Express card, an expert system helps make the decision. Du Pont has more than 100 expert systems doing things like scheduling medical examinations and diagnosing computer failures. United Airlines uses a computer program to schedule the docking of airplanes at its terminal in Chicago.\u00a0A Narrow Customer Base\nA major problem for the industry, some analysts say, is that artificial intelligence has long been the preserve of a relatively small number of researchers at a few leading colleges, mainly the Massachusetts Institute of Technology, Stanford University and Carnegie-Mellon University.\nThese researchers flocked to start companies in the early 1980's, when it looked as if artificial intelligence was about to boom. But good academicians often do not make good business people.\nThe academic researchers believed in the special programming language for artificial intelligence called Lisp, and they used computers tailored to run it. At first, sales were strong because machines were being sold to research and development arms of large corporations, which were setting up divisions to explore artificial intelligence.\nBut soon that market became saturated, and artificial intelligence had trouble migrating to the mainstream of corporate America. Corporate customers did not want to spend $50,000 to $100,000 for a special machine used by one person. They wanted artificial intelligence programs to run on their existing computers, such as I.B.M. mainframes and Digital Equipment minicomputers, to be shared by many users. Preferably, they wanted to develop artificial intelligence programs without requiring their own programmers to learn Lisp.\u00a0New Source of Competition\nYet another factor has been that engineering work stations and personal computers have now become powerful enough to run Lisp at a fraction of the price of special artificial intelligence computers.\nAn acknowledgment of that came today when Texas Instruments said it would sell a version of Apple's Macintosh II personal computer for use in artificial intelligence. The machine, with a Lisp processing circuit board made by Texas Instruments, will sell for about half the price of Texas Instruments' special artificial intelligence computers. Lisp processing is already available on powerful I.B.M. and compatible personal computers as well, using software sold by companies like Gold Hill Computers of Cambridge, Mass.\nThe result has been that sales of special-purpose artificial intelligence computers have dried up. And the expert-system tools that were designed to run on such machines have also suffered. So have the expert systems that were designed to run on those machines rather than on general-purpose computers.\nAt the same time, many corporations that invested heavily in developing expert systems have not been able to put their systems into use. ''Most corporate programs have failed to fulfill their promise,'' said Ed Mahler, program manager for artificial intelligence at Du Pont.\u00a0The Sale of Expert Systems\nA few companies are trying to sell expert systems as a product. These include Palladian Software and Applied Expert Systems, both of Cambridge, Mass., and Syntelligence of Sunnyvale, Calif. These companies, which are selling products to financial services companies, have also suffered setbacks.\nIn some cases vendors and companies have chosen the wrong problem to attack, said Sheldon Breiner, chairman of Syntelligence. For problems like evaluating home loans or life insurance policies, expert systems are not really needed. Statistical techniques exist, expertise is widespread and the consequences of a mistake are not severe. A good candidate for an expert system is one that many people can use, in a field in which actual experts are scarce and in which using the computer can save the company millions of dollars, he said.\nSyntelligence ran into its own hurdle when it came to choosing a problem to solve with artificial intelligence. It began work on a product that would give expert investment advice to financial firms. The idea was to incorporate the expertise of Wall Street luminaries like Henry Kaufman, the former Salomon Brothers chief economist. But the company discarded the project because it determined that there were not any real experts when it comes to investment decisions.\u00a0TROUBLE FOR MANY OF THE ARTIFICIAL INTELLIGENCE COMPANIES\u00a0As the industry realigns, the companies that relied on special purpose machines are languishing.\u00a0EXPERT SYSTEM DEVELOPMENT TOOLS\u00a0Company\/Headquarters: Teknowledge, Palo Alto, Calif.\u00a0\u00a0Description: Four quarters of losses. 60 workers of 220 laid off. 1987 sales: $20 million.\u00a0Company\/Headquarters: Intellicorp, Mountain View, Calif.\u00a0\u00a0Description: Six quarters of losses. 30 workers of 200 laid off. 1987 sales: $20 million.\u00a0Company\/Headquarters: Carnegie Group*, Pittsburgh\u00a0\u00a0Description: Losses. 20-40 workers of 200 laid off. 1987 sales: $12 million.\u00a0Company\/Headquarters: Inference*, Los Angeles\u00a0\u00a0Description: Losses. 20 workers of 130 laid off. 1987 sales: $12 million.\u00a0MACHINE MANUFACTURERS\u00a0Company\/Headquarters: Symbolics, Cambridge, Mass.\u00a0\u00a0Description: Continuing losses. Third round of layoffs last fall. Ousted chairman and founder. 1987 sales: $104 million.\u00a0Company\/Headquarters: Lisp Machine, Andover, Mass.\u00a0\u00a0Description: Filed for bankruptcy last year. 1986 sales: $12 million.\u00a0Company\/Headquarters: Xerox, Stamford, Conn.\u00a0\u00a0Description: Sluggish sales; recent reorganization of its artificial intelligence business.\u00a0Company\/Headquarters: Texas Instruments, Dallas\u00a0\u00a0Description: Big push in artificial intelligence. Announced a chip containing Lisp to go into Macintosh.\u00a0EXPERT SYSTEM APPLICATIONS\u00a0Company\/Headquarters: Syntelligence*, Sunnyvale, Calif.\u00a0\u00a0Description: No layoffs. 1987 sales: $9 million.\u00a0Company\/Headquarters: Applied Expert Systems*, Cambridge, Mass.\u00a0\u00a0Description: Layoffs. 1987 sales: $4 million.\u00a0Company\/Headquarters: Palladian*, Cambridge, Mass.\u00a0\u00a0Description: Layoffs. Ousted chairman and founder. 1987 sales: $6 million.\u00a0*Private company, estimates.\u00a0(Source: DM Data, Harvey Newquist)\n","1":"AUSTIN -\u00a0Arguably the most alarming part of concerns over artificial intelligence's potential to end human civilization is the voices that\u00a0are speaking out. Proven technology visionaries -\u00a0from Bill Gates to Elon Musk and Stephen Hawking -\u00a0have warned about the dangers of artificial intelligence. Given humans' generally poor track record for predicting the future, it stands to reason that the forward thinkers could identify huge technology risks while the rest of us live in ignorance.\u00a0\nBut Monday at SXSW, a tech thinker with a long\u00a0r\u00e9sum\u00e9, Google Chairman Eric Schmidt, offered reassurance that we don't need to be so worried right now.\n\"I think that this technology will ultimately be one of the greatest forces for good in mankind's history simply because it makes people smarter,\" Schmidt said during a keynote address with author Walter Isaacson and Megan Smith, U.S. chief technology officer.\n\"I'm certainly not worried in the next 10 to 20 years about that. We're still in the baby step of understanding things,\" Schmidt said. \"We've made tremendous progress in respect to [artificial intelligence].\"\nHe highlighted benevolent uses of artificial intelligence, such as Google Voice and Google's translation services. Indeed, from antilock brakes to your iPhone's autocorrect function, artificial intelligence already surrounds us, with favorable results.\n\"Stuff beyond that is, at this point, really speculation,\" Schmidt said. \"I'm not a dystopian. I'm a utopian, if you phrase it that way.\"\nAs chairman of Google, Schmidt has a unique vantage point on how artificial intelligence is impacting our world and how it will continue to do so.\u00a0Google is a leader in the space. In February, DeepMind, a Google acquisition, devised an algorithm that taught itself to beat Atari video games. While there's reason to see Schmidt's views as reassuring, it's also worth noting that as the chairman of a leader in artificial intelligence, he has an incentive to underplay the downside of artificial intelligence. If the general public or regulators move to hamper artificial intelligence, Google's businesses could suffer.\nLater in the talk, Schmidt singled out machine learning, a subset of artificial intelligence, as having huge potential to reshape our world for the better.\n\"I think the biggest trend is going to be the use of machine intelligence of large data sets\u00a0to solve every problem,\" Schmidt said. \"I can't think of a field of study, a field of research -\u00a0whether it's English, soft sciences, hard sciences or any corporation - that can't become far more efficient, far more powerful, far more clever.\"\n              Related:\u00a0 The 12 threats to human civilization, ranked \n","2":"In October, Elon Musk called artificial intelligence \"our greatest existential threat,\" and equated making machines that think with \"summoning the demon.\" In December, Stephen Hawking said \"full artificial intelligence could spell the end of the human race.\" And this year, Bill Gates said he was \"concerned about super intelligence,\" which he appeared to think was just a few decades away.\nBut if the human race is at peril from killer robots, the problem is probably not artificial intelligence. It is more likely to be artificial stupidity. The difference between those two ideas says much about how we think about computers.\u00a0\nIn the kind of artificial intelligence, or A.I., that most people seem to worry about, computers decide people are a bad idea, so they kill them. That is undeniably bad for the human race, but it is potentially a smart move by the computers.\nBut the real worry, say specialists in the field, is a computer program rapidly overdoing a single task, with no context. A machine that makes paper clips proceeds untrammeled, one example goes, and becomes so proficient that overnight we're drowning in paper clips. \nIn other words, something really dumb happens, at a global scale. As for those Terminator robots you tend to see on scary news stories about an A.I. apocalypse, forget it.\n\"What you should fear is a computer that is competent in one very narrow area, to a bad degree,\" said Max Tegmark, a professor of physics at the Massachusetts Institute of Technology and the president of the Future of Life Institute, a group dedicated to limiting the risks from A.I. \nWhen in late June a worker in Germany was killed by an assembly line robot, Mr. Tegmark said, \"it was an example of a machine being stupid, not doing something mean but treating a person like a piece of metal.\" \nHis institute recently disbursed much of the $10 million that Mr. Musk, the founder of Tesla and SpaceX, gave it to think of ways of preventing autonomous programs from going rogue. Yet even Mr. Musk, along with other luminaries in science and tech, like Mr. Hawking and Mr. Gates, seem to be focused on the wrong potential threat. \nThere is little sense among practitioners in the field of artificial intelligence that machines are anywhere close to acquiring the kind of consciousness where they form lethal opinions about their makers.\n\"These doomsday scenarios confuse the science with remote philosophical problems about the mind and consciousness,\" Oren Etzioni, chief executive of the Allen Institute for Artificial Intelligence, a nonprofit that explores artificial intelligence. \"If more people learned how to write software, they'd see how literal-minded these overgrown pencils we call computers actually are.\"\nWhat accounts for the confusion? One big reason is the way computer scientists work. \"The term 'A.I.' came about in the 1950s, when people thought machines that think were around the corner,\" said Mr. Etzioni. \"Now we're stuck with it.\" \nIt is still a hallmark of the business. Google's advanced A.I. work is at a company it acquired called DeepMind. A pioneering company in the field was called Thinking Machines. Researchers are pursuing something called Deep Learning, another suggestion we are birthing intelligence. \nDeep Learning relies on a hierarchical reasoning technique called neural networks, suggesting the neurons of a brain. Comparing a node in a neural network to a neuron, though, is at best like comparing a toaster to the space shuttle.\nIn fairness, the kind of work DeepMind is doing, along with much other work in the burgeoning field of machine learning, does involve spotting patterns, suggesting actions, and making predictions. That is akin to the mental stuff people do. \nIt is among the most exciting fields in tech. There is a pattern-finding race among Google, Facebook, and Amazon. Companies including Uber and General Electric are staking much of their future on machine learning. \nBut machine learning is automation, a better version of what computers have always done. The \"learning\" is not stored and generalized in the ways that makes people smart. \nDeepMind made a program that mastered simple video games, but it never took the learning from one game into another. The 22 rungs of a neural net it climbs to figure out what is in a picture do not much operate like human image recognition, and are still easily defeated. \nMoving out of that stupidity to a broader human-like capability is called \"transfer learning.\" It is at best in the research phase. \n\"People in A.I. know that a chess-playing computer still doesn't yearn to capture a queen,\" said Stuart Russell, a professor of Computer Science at the University of California, Berkeley. He is also on the Future of Life's board, and as a recipient of some of Mr. Musk's grant. He seeks mathematical ways to ensure dumb programs don't conflict with our complex human values.\n\"What the paper clip program lacks is a background value structure,\" he said. \"The misunderstanding is thinking that there is only a threat if there is consciousness.\"\n","3":"In ''The Age of Intelligent Machines,'' Mr. Kurzweil valiantly attempts to provide an overview of artificial intelligence - the field devoted to the theory, design and construction of computing machines that think, or at least appear to. He discusses its philosophical, mathematical, psychological and technical roots; the outstanding problems in contemporary research on artificial intelligence; the history and current state of the industry; the impact of artificial intelligence on the arts, and the future of the discipline. The volume is an innovative blend of monograph and anthology, for it includes not only Mr. Kurzweil's own writing but a number of essays by philosophers and researchers in artificial intelligence. This structure would have been more successful, however, if the author had included views divergent from his own alongside those that buttress his position.\u00a0\nOn his own turf Mr. Kurzweil is clear, current and informative. He understands how artificial intelligence can be used in pattern recognition, reading, speech and music synthesis. But when he strays into philosophy, logic and psychology, he is often sloppy and vague. He relies on dated and secondary sources, ignores or misunderstands many of the most important thinkers and distorts the relevant history of philosophy and logic. Mr. Kurzweil also is too quick to draw inferences about human psychology from artificial intelligence models, and he is superficial when he discusses one of the most exciting developments in current artificial intelligence theory - the emergence of connectionism, which explains intelligence as arising from a web of interconnected, individually very simple, processes.\nMr. Kurzweil's enthusiasm for the artificial intelligence industry also leads him to disregard his own advice against overly optimistic projection as well as his own remarks on the deep differences between human and machine intelligence. While noting that ''understanding language can . . . require a knowledge of history, myths, literary allusions and references, and many other categories of shared human experience,'' he says on the same page that Terry Winograd's 1970 Shrdlu program (an early attempt, later repudiated by its designer, to model language comprehension) represents progress toward a genuine understanding of natural language, and he asserts that ''we can expect translating telephones with reasonable levels of performance . . . early in the first decade of the next century.'' There is no suggestion of the mammoth difficulties that confront anyone who tries to accomplish such a task.\nChess is a leitmotif in the history of artificial intelligence, and in this book. Mr. Kurzweil writes that ''when a computer does become the chess champion . . . before the end of the century, we will either think more of computers, less of ourselves, or less of chess.'' But then he undermines this odd speculation by observing correctly that the methods by which computers play chess differ dramatically from those by which humans play. This difference is evidenced by the fact that even though humans can play go (a Japanese board game) with no greater difficulty than chess, computers, which can play chess extremely well, are embarrassingly weak at go. In other words, the fact that planes and birds fly makes us think neither more nor less of birds, planes or flight. Bird flight and plane flight are simply two quite different feats.\nArtificial intelligence raises pressing philosophical and social questions about the concentration of wealth and power, the nature of war, the character of work, personal privacy and what it is to be human. To ignore these is to succumb to what Langdon Winner has aptly called ''technological somnambulism.'' In all fairness, Mr. Kurzweil is aware of some of these issues. ''Computer technology,'' he writes, ''is already a powerful ally of the totalitarian.'' And in another passage, he notes that computer identification technology is ''capable of helping Big Brother track and control individual transactions and movements.'' But none of these issues make Mr. Kurzweil question the unbridled development of this technology.\nArtificial intelligence boosters and critics are often puzzled by one another, and often talk past one another. Mr. Kurzweil has heard the voices on the other side, but he has not quite understood the force of their arguments or the problems they pose. Even sympathetic critics of artificial intelligence look like Luddites to him. But, after all, judicious Luddism may not be such a bad thing.\n","4":"JUST what constitutes artificial intelligence has always been a matter of some dispute. And the terms of the argument change with each new advance in computer science.\nSeen one way, as the effort to produce machines whose output cannot be distinguished from that of a human, artificial intelligence, or A.I., is still very far away.\u00a0\n But from another perspective, it is all around us.\nThirty years ago, for instance, speech recognition was an artificial-intelligence problem of the first order. Today it is commonplace, a fact that is evident to anyone who has called the United Airlines flight information line or has used speech transcription software.\n\"These things are considered A.I. before you do them,\" said Dr. Danny Hillis, who has been working in the field for years. \"And after you do it, they're considered engineering.\"\nOther fruits of artificial intelligence research abound as well. Whether you are struggling to beat your Palm organizer at chess, watching your word processing program correct your spelling or playing a video game, you are witnessing the ways in which artificial intelligence has insinuated itself into daily life.\n\"A.I. is becoming more important as it has become less conspicuous, and it's less conspicuous because it's everywhere, but often under the surface,\" said Dr. Patrick Winston, a professor at the Massachusetts Institute of Technology who was the director of the Artificial Intelligence Lab there for 25 years.\nSince the time when the first work was being done by Dr. Marvin Minsky, Dr. John McCarthy, Dr. Winston and others at M.I.T., in the 1950's and 1960's, computer scientists have generally agreed that artificial intelligence would arrive incrementally.\n\"We're engineering A.I. one piece at a time,\" said Dr. Hillis, a former student of Dr. Minsky's and chairman of Applied Minds, a start-up in Glendale, Calif.\nDr. Hillis and others said that the machine intelligence currently in evidence fell along a spectrum.\nAt the less intelligent end are things like smart washing machines and coffeepots -- appliances that can figure out how dirty a load of clothes is or when to turn off a coffee warmer. Experts generally agree that such appliances are the product of rather sophisticated microprocessors and sensors, not evidence of artificial intelligence.\nAt the other end are machines whose output is genuinely difficult to distinguish from a human's, like I.B.M.'s chess-playing computer, Deep Blue, and Aaron, a robotic artist that produces paintings that could easily pass for human work.\nAnd somewhere in the middle are speech recognition programs, used in lieu of word processors; collaborative filtering software, like that used by Amazon.com to make purchase recommendations; and search engines that respond to questions phrased in full sentences, not just search terms.\nOne reason for the proliferation of machine intelligence in the commercial world is the seeding of the computer industry with artificial-intelligence researchers who have moved beyond academia and taken jobs at high-tech companies.\nThe Microsoft Corporation, for instance, employs about 80 artificial-intelligence researchers, many of whom came from universities. For several years, Microsoft has sold its Office software with various embedded intelligence features, like the automatic correction of frequently misspelled words and the Answer Wizard, which anticipates the needs of users who look up topics in the electronic documentation.\nAn infamous piece of Microsoft software that includes components of artificial intelligence is the Paper Clip help wizard, which pops up on the screen to offer advice. Many people say Paper Clip pops up too often with unwanted suggestions.\nIn defense of Paper Clip, Dr. Winston said: \"It's less annoying than it would have been without A.I. It does try to zero in on what kinds of information you're most interested in, and that sort of thing will get better and better as time goes by.\"\nMicrosoft's next big step into the marketplace with a product that incorporates artificial intelligence will be its Outlook Mobile Manager, a system that scrutinizes each incoming e-mail message, does an automatic synopsis, throws away extraneous words and abbreviates others, then sends the message to the user's mobile device. The product is scheduled to be released next year.\n\"It's what a great secretary would do,\" said Craig Mundie, Microsoft's senior vice president for advanced strategies.\nResearchers in artificial intelligence at Microsoft are also working on a more general effort called the Attentional User Interfaces and Systems Project, which includes a project for continually monitoring streams of data like e-mail, voice mail, Internet news alerts and instant messages.  The system will gauge what the computer user is doing, assign priorities to the messages and decide whether and when to interrupt.\nOther graduates of university-based artificial intelligence programs have started companies of their own. In 1983, Dr. Hillis co-founded the Thinking Machines Corporation, a supercomputer company that was bought by other companies in the 1990's. In 1986, Dr. Winston and three colleagues at M.I.T. started Ascent Technology in Cambridge, Mass., to apply the research they had been doing to help airports solve scheduling and allocation problems like gate assignments for aircraft.\nDr. Winston said the first commercialization efforts of artificial intelligence, in the 1980's, had made an obvious mistake. \"We blundered about what we thought A.I. was going to be good for, which was replacing people,\" he said. \"What we discovered was that's not the commercial appeal of A.I. It's about making things possible that weren't possible with people alone.\"\nAs examples, Dr. Winston pointed to a project at the Artificial Intelligence Lab for giving brain surgeons a kind of X-ray vision by coupling video images with M.R.I. images. He also pointed to the Mars Rover, which navigates terrain autonomously.\nAnother M.I.T. spinoff is the iRobot Corporation, started 10 years ago by Dr. Rodney Brooks, the current director of the Artificial Intelligence Lab. The company developed an interactive doll with Hasbro called My Real Baby and in February will begin selling the iRobot-LE, a self-navigating home robot that will be equipped with sonar and a camera and will be controlled via the Web.\nOf course, there are those who disagree that pieces of the artificial intelligence puzzle are falling into place incrementally. \"It's very much like a country that's declaring a war that it's losing to be won and then withdrawing,\" said Dr. Douglas Lenat, an artificial intelligence researcher who is president of Cycorp in Austin, Tex.\nBy way of example, Dr. Lenat described the shortcomings of speech recognition programs currently on the market. \"They are just the palest shadows of what we can, should and soon will have with real A.I.,\" he said. \"You have to speak the punctuation marks, and that's pretty pathetic. And they don't recognize the simplest inflections for things like italics and commas.\n\"There is still this tremendously important problem, which is to get computers to know enough about the world that they can do the final few percentage points of speech recognition.\"\nDr. Lenat's criticism of speech recognition raises the larger question of what constitutes intelligence.\nDr. Hillis addressed that question this way: \"Intelligence is just a whole lot of little things, thousands of them. And what will happen is we'll learn about each one one at a time, and as we do it, machines will be more and more like people. It will be a gradual process, and that's been happening.\"\nDr. Ray Kurzweil, an artificial intelligence researcher who created the Kurzweil VoiceReport, a speech recognition program, agreed that \"machines still do not have the subtlety, depth, range and richness of human intelligence because it is still a million times simpler than the human brain.\"\n\"That gap,\" Dr. Kurzweil continued, \"is going to go away, and when it does, then machines can combine the subtlety and pattern-recognition strengths with the other natural advantages they already have, and that will be a very formidable combination.\"\nPerhaps the flight schedule information line that understands words like \"Chicago\" and \"today\" helps take machines a step closer to duplicating the outward signs of a person's intelligence. But the artificial intelligence field remains far short of modeling human consciousness and the inner mind.\n\"A.I. has done a lot of little things that are very powerful,\" Dr. Winston said. \"On the other hand, on the science side, where we try to understand what makes humans work, we're still a long way from that prize, and we need to work hard on it if we want to understand our intelligence the same way molecular biologists understand our genes.\"\nThe world of artificial intelligence would not be the same without a robotic lawnmower, available for about $800 from Friendly Robotics, which has its United States headquarters in Irving, Tex.\nDr. Brooks, of M.I.T., has used the device, called the Robomower, on his own yard in suburban Boston. He said that although it did a respectable job on a patch of lawn, the family gardener's reaction reinforced Dr.  Winston's point. \"He looked at that third of the lawn and said, 'I guess I'm not out of a job soon,' \" Dr. Brooks said.\u00a0\u00a0\nhttp:\/\/www.nytimes.com\n","5":"Tesla chief executive Elon Musk has warned about artificial intelligence\u00a0before, tweeting that\u00a0it could be more dangerous than nuclear weapons. Speaking Friday at the\u00a0MIT Aeronautics and Astronautics department's Centennial Symposium, Musk called it our biggest existential threat:\u00a0\n I think we should be very careful about artificial intelligence. If I were\u00a0to guess like what our biggest existential threat\u00a0is, it's probably\u00a0that. So we\u00a0need to be very careful with the artificial\u00a0intelligence. Increasingly scientists think there should be some regulatory oversight maybe at the national and international level, just to make sure that we don't do something very foolish. With artificial intelligence we are summoning the demon. In all those stories where there's the guy with the pentagram and the holy water, it's like yeah he's sure he can control the demon. Didn't work out.\u00a0\u00a0\nMusk was so caught up on artificial\u00a0intelligence that he missed the audience's next question. \"Sorry can you repeat the question, I was just sort of thinking about the AI thing for a second,\" he said.\nMusk spoke expansively for over an hour, at one point\u00a0even asking a MIT student what his favorite sci-fi books were. He left to a standing ovation. You can watch the entire interview here.\n","6":"After years of relying on natural sales savvy for its profitability, International Business Machines Corp. is now counting on its artificial intelligence.\nThe world's largest computer company is expected to outline much of its artificial intelligence software strategy and ambition Wednesday when IMB executive Herbert Schorr delivers the keynote address at the American Association for Artificial Intelligence conference here.\nArtificial intelligence \"is becoming a part of all our product lines,\" Schorr, IBM's artificial intelligence advocate and group director of the company's information systems and storage group, said in an interview. \"From a businessman's point of view, it's got exceedingly great potential. By our own internal applications, we know it has a great deal of value -- and we're really ready now for deployment.\"\u00a0\nIBM probably will begin shipping \"expert systems\" software -- computer programs that embody rules of thumb that experts use to solve problems -- sometime next year, Schorr said, though he declined to be more specific.\nIBM already has more than 70 expert systems under development within the company, and several are potential candidates to be sold (\"exported\" is the company's term for it) in the marketplace. One such system, designed to diagnose problems in IBM's large-computer disc drives, already has saved the company nearly $6 million, Schorr said.\nAs IBM's internal activities indicate, expert systems have moved from the theoretical to the practical. Literally hundreds of companies, from General Motors Corp. to Aetna Life & Casualty Co., are exploring how to use this type of software. Indeed, expert-systems applications have emerged as a theme of this conference: More than 75 percent of the extimated 6,000 people attending the meeting are from business, compared with less than 25 percent two years ago.\n\"The exciting thing about this technology is that it creates a new class of computer users -- the knowledge workers,\" Schorr said. \"Knowledge becomes a corporate asset. Data grows old and obsolete; a knowledge base gets more valuable with age and maintenance. Artificial intelligence means it doesn't walk out the door at 5 o'clock or retire at 65 years old.\"\nConsequently IBM, which has seen its earnings slump over the last two years, is hoping that customer interest in expert systems and other AI techniques will boost hardware and software sales. Schorr said he believes that companies will have to invest in artificial-intelligence technology to remain competitive.\n\"I don't believe they have a choice,\" he said, adding, \"the absolute amount spent on computer technologies is going to increase.\"\nIBM believes that its vast installed base of large computers gives it an edge over the competition in the race to bring expert systems to the office.\n\"Customers want to embed AI around existing data systems,\" said Charles Buchheit, IBM director for knowledge-based-systems marketing.\n\"We need to integrate AI with standard data processing methods,\" agreed Schorr.\nBut a large and vocal contingent of conference attendees here question IBM's ability to bring artificial intelligence successfully to market.\n\"IBM has to de-feature the software,\" to make it able to run with existing systems, asserted artificial intelligence consultant Beau Shiel. \"It's just nonsense\" to believe otherwise. Shiel contended that the power of artificial intelligence software must be significantly diluted to make it compatible with existing systems and that, as a consequence, the AI solution no longer will become the cost-effective solution customers hoped it would be.\n","7":"Even as the computer industry endures its worst slump in over a decade, optimism reigns supreme among the artificial intelligentsia here at the American Association for Artificial Intelligence conference.\n\"Artificial intelligence is explosive,\" said Curt Monash, who follows the software industry for Paine Webber.\n\"Insofar as there is technological change in software, the significant majority comes from AI,\" he said.\u00a0\nWith 110 exhibitors and an anticipated 5,000 participants this week, the conference offers numerous signs that artificial intelligence has evolved from a computer science discipline to a cutting edge of the information industry.\nPerhaps the clearest signal that artificial intelligence has joined the computer establishment -- or that the establishment has accepted artificial intelligence -- is that, for the first time, a representative from IBM will deliver the keynote conference address.\n\"IBM wants into AI,\" says industry analyst Esther Dyson. \"They're finally being serious about it.\"\nComputer artificial intelligence research seeks to replicate the knowledge, reasoning and linquistic skills of people into computer hardware and software. Doing that requires different kinds of programming tools and techniques from the numeric calculations performed by traditional computers.\nWhile this does not lead to \"thinking machines,\" these efforts yield computer programs that more closely approximate how humans solve certain types of problems.\nThe artificial intelligence programs that have attracted the most business interest are \"expert systems.\" Expert systems are programs that can give advice: They draw upon specific knowledge and rules from human experts on how to apply that knowledge. Numerous expert systems have been developed but are not yet widely in use, including programs that would help financial analysts invest money, engineers design products and doctors make diagnoses.\n\"The easiest sell is expert systems,\" said Howard Cannon, director of special programs for Symbolics Inc., a Concord, Mass., company that's a leader in AI work stations. Texas Instruments and Xerox also are in that field.\nCannon points out that expert systems development is such a hot topic that other computer companies such as Digital Equipment Corp. and Sun Microsystems also are trying to make their machines better able to handle symbolic processing.\n\"More traditional systems have to play a role,\" says Cannon, who believes that personal computing and symbolic processing technologies are converging. He hints that Symbolics may offer hardware attachments that will turn IBM Personal Computers into articifical intelligence work stations. The personal computer of the future could be able to more easily run advanced expert systems and other artificial intelligence-related software.\nWhile the market for AI work stations is growing at 40 percent a year, the bulk of those purchases are by the scientific and research communities, say Dyson and Cannon. The \"expert system on a desk\" has yet to penetrate the general management market, they say.\nPart of the problem is that corporate data processing departments are uncertain of how to cope with the new technology. Just as these departments did not initiate the role of personal computers in companies, they are not taking the lead in introducing artificial intelligence into the workplace.\n\"There's more concern from top management,\" says Symbolic's Cannon, but there isn't nearly as much concern from the data processing department as there should be.\"\nAbout 40 percent of the Fortune 500 companies are pursuing artificial intelligence work, according to AI Services Co., a New Haven consultant.\n","8":"The International Business Machines Corporation today announced a three-year multimillion-dollar project in artificial intelligence with Carnegie-Mellon University, as part of what the company called ''a major new I.B.M. initiative'' in advanced software.\nUnder the terms of the agreement, I.B.M. will provide equipment and financing for Carnegie-Mellon researchers developing ''expert systems'' that attempt to simulate human reasoning and draw conclusions, along with longer-range work in speech recognition and robotics.\nWhile I.B.M. said it would not have exclusive access to the results of the research, company executives attending the annual meeting of the American Association for Artificial Intelligence here said they expected those results would become part of I.B.M.'s future software products.\u00a0\nThe move appears to be part of a major shift within I.B.M., which has traditionally been weak in applications software, programs that perform specific tasks for computer users. Artifical intelligence, a set of programming techniques that generally make computer systems easier to use and capable of performing many jobs without human intervention, are considered essential to that effort. They also use an extraordinary amount of computing power and memory, meaning the prospect of additional hardware sales.\n''There is something of an awakening under way at I.B.M.,'' said Raj Reddy, head of Carnegie-Mellon's robotics institute and a leading authority in artificial intelligence. ''Four or five years ago, when you mentioned artificial intelligence, I.B.M. shrugged its shoulders.''\nI.B.M. executives here said that within the last year the company had created an artificial-intelligence project office that reports directly to I.B.M.'s management committee, with unusually broad responsibility to integrate new techniques in I.B.M. products. In coming months, the company is expected to bring out its first commercially available expert systems, primarily programs designed for banks, insurance companies and manufacturers.\n''Strategically, this is now a very high priority for us,'' said Herbert Schorr, who is directing I.B.M.'s artificial-intelligence initiative. ''It should enable us to attract a new set of users - like loan officers or insurance underwriters - who can retrieve facts off their computers, but get little analysis or instruction about how to apply rules.''\nMuch of the most promising artificial-intelligence technology is still in university laboratories, notably at Carnegie-Mellon, the Massachusetts Institute of Technology and Stanford University. Thus, companies are forming alliances with those institutions.\nUnder the agreement announced today, I.B.M. will provide Carnegie-Mellon with about $5.5 million in computer equipment and will negotiate contracts for individual studies in artificial intelligence. The equipment is primarily the PC\/RT, an engineeering and scientific computer introduced earlier this year.\n","9":"The technology entrepreneur Elon Musk recently urged the nation's governors to regulate artificial intelligence ''before it's too late.'' Mr. Musk insists that artificial intelligence represents an ''existential threat to humanity,'' an alarmist view that confuses A.I. science with science fiction. Nevertheless, even A.I. researchers like me recognize that there are valid concerns about its impact on weapons, jobs and privacy. It's natural to ask whether we should develop A.I. at all.\nI believe the answer is yes. But shouldn't we take steps to at least slow down progress on A.I., in the interest of caution? The problem is that if we do so, then nations like China will overtake us. The A.I. horse has left the barn, and our best bet is to attempt to steer it. A.I. should not be weaponized, and any A.I. must have an impregnable ''off switch.'' Beyond that, we should regulate the tangible impact of A.I. systems (for example, the safety of autonomous vehicles) rather than trying to define and rein in the amorphous and rapidly developing field of A.I. \u00a0\n  I propose three rules for artificial intelligence systems that are inspired by, yet develop further, the ''three laws of robotics'' that the writer Isaac Asimov introduced in 1942: A robot may not injure a human being or, through inaction, allow a human being to come to harm; a robot must obey the orders given it by human beings, except when such orders would conflict with the previous law; and a robot must protect its own existence as long as such protection does not conflict with the previous two laws.\n  These three laws are elegant but ambiguous: What, exactly, constitutes harm when it comes to A.I.? I suggest a more concrete basis for avoiding A.I. harm, based on three rules of my own.\n  First, an A.I. system must be subject to the full gamut of laws that apply to its human operator. This rule would cover private, corporate and government systems. We don't want A.I. to engage in cyberbullying, stock manipulation or terrorist threats; we don't want the F.B.I. to release A.I. systems that entrap people into committing crimes. We don't want autonomous vehicles that drive through red lights, or worse, A.I. weapons that violate international treaties.\n  Our common law should be amended so that we can't claim that our A.I. system did something that we couldn't understand or anticipate. Simply put, ''My A.I. did it'' should not excuse illegal behavior.\n  My second rule is that an A.I. system must clearly disclose that it is not human. As we have seen in the case of bots -- computer programs that can engage in increasingly sophisticated dialogue with real people -- society needs assurances that A.I. systems are clearly labeled as such. In 2016, a bot known as Jill Watson, which served as a teaching assistant for an online course at Georgia Tech, fooled students into thinking it was human. A more serious example is the widespread use of pro-Trump political bots on social media in the days leading up to the 2016 elections, according to researchers at Oxford.\n  My rule would ensure that people know when a bot is impersonating someone. We have already seen, for example, @DeepDrumpf -- a bot that humorously impersonated Donald Trump on Twitter. A.I. systems don't just produce fake tweets; they also produce fake news videos. Researchers at the University of Washington recently released a fake video of former President Barack Obama in which he convincingly appeared to be speaking words that had been grafted onto video of him talking about something entirely different.\n  My third rule is that an A.I. system cannot retain or disclose confidential information without explicit approval from the source of that information. Because of their exceptional ability to automatically elicit, record and analyze information, A.I. systems are in a prime position to acquire confidential information. Think of all the conversations that Amazon Echo -- a ''smart speaker'' present in an increasing number of homes -- is privy to, or the information that your child may inadvertently divulge to a toy such as an A.I. Barbie. Even seemingly innocuous housecleaning robots create maps of your home. That is information you want to make sure you control.\n  My three A.I. rules are, I believe, sound but far from complete. I introduce them here as a starting point for discussion. Whether or not you agree with Mr. Musk's view about A.I.'s rate of progress and its ultimate impact on humanity (I don't), it is clear that A.I. is coming. Society needs to get ready.\n  Follow The New York Times Opinion section on Facebook and Twitter (@NYTopinion), and sign up for the Opinion Today newsletter. \n\n\n\n","10":"Scientists have begun what they say will be a century-long study of the effects of artificial intelligence on society, including on the economy, war and crime, officials at Stanford University announced Monday.\nThe project, hosted by the university, is unusual not just because of its duration but because it seeks to track the effects of these technologies as they reshape the roles played by human beings in a broad range of endeavors.\n''My take is that A.I. is taking over,'' said Sebastian Thrun, a well-known roboticist who led the development of Google's self-driving car. ''A few humans might still be 'in charge,' but less and less so.''\u00a0\nArtificial intelligence describes computer systems that perform tasks traditionally requiring human intelligence and perception. In 2009, the president of the Association for the Advancement of Artificial Intelligence, Eric Horvitz, organized a meeting of computer scientists in California to discuss the possible ramifications of A.I. advances. The group concluded that the advances were largely positive and lauded the ''relatively graceful'' progress.\nBut now, in the wake of recent technological advances in computer vision, speech recognition and robotics, scientists say they are increasingly concerned that artificial intelligence technologies may permanently displace human workers, roboticize warfare and make of Orwellian surveillance techniques easier to develop, among other disastrous effects.\nDr. Horvitz, now the managing director of the Redmond, Wash., campus of Microsoft Research, last year approached John Hennessy, a computer scientist and president of Stanford University, about the idea of a long-term study that would chart the progress of artificial intelligence and its effect on society. Dr. Horvitz and his wife, Mary Horvitz, agreed to fund the initiative, called the ''One Hundred Year Study on Artificial Intelligence.''\nIn an interview, Dr. Horvitz said he was unconvinced by recent warnings that superintelligent machines were poised to outstrip human control and abilities. Instead, he believes these technologies will have positive and negative effects on society.\n''Loss of control of A.I. systems has become a big concern,'' he said. ''It scares people.'' Rather than simply dismiss these dystopian claims, he said, scientists instead must monitor and continually evaluate the technologies.\n''Even if the anxieties are unwarranted, they need to be addressed,'' Dr. Horvitz said.\nHe declined to divulge the size of his gift to Stanford, but said it was sufficient to fund the study for a century and suggested the amount might be increased in the future.\nDr. Horvitz will lead a committee with Russ Altman, a Stanford professor of bioengineering and computer science. The committee will include Barbara J. Grosz, a Harvard University computer scientist; Deirdre K. Mulligan, a lawyer and a professor in the School of Information at the University of California, Berkeley; Yoav Shoham, a professor of computer science at Stanford; Tom Mitchell, the chairman of the machine learning department at Carnegie Mellon University; and Alan Mackworth, a professor of computer science at the University of British Columbia.\nThe committee will choose a panel of specialists who will produce a report on artificial intelligence and its effects that is to be published late in 2015.In a white paper outlining the project, Dr. Horvitz described 18 areas that might be considered, including law, ethics, the economy, war and crime. Future reports will be produced at regular intervals.\nDr. Horvitz said that progress in the field of artificial intelligence had consistently been overestimated.\nIndeed, news accounts in 1958 described a neural network circuit designed by Frank Rosenblatt, a psychologist at Cornell University. The Navy enthusiastically announced plans to build a ''thinking machine'' based on the circuits within a year for $100,000. It never happened.\nStill, Dr. Horvitz acknowledged, the pace of technological change has accelerated, as has the reach of artificial intelligence. He cited Stuxnet, the malicious program developed by intelligence agencies to attack Iranian nuclear facilities, as an example.\n''My grandmother would tell me stories about people running outside when they saw a plane fly over, it was so unusual,'' he said. ''Now, in a relatively few decades, our worry is about whether we are getting a salt-free meal when we take off from J.F.K. in a jumbo jet.''\n","11":"Artificial intelligence, the science of making computers ''think,'' has long been the preserve of theoreticians who were little concerned with practical applications.\n''When they said 'real things,' they meant computers that can play chess,'' said Dr. Roger Schank, chairman of the computer science department at Yale University. ''They were not going to talk to Wall Street, let alone own a suit.''\nNow, however, business is taking an interest in artificial intelligence, or A.I., and some professors, such as Dr. Schank, are forming or joining companies to capitalize on the expected boom. But the new move toward commercialization is disrupting the academic community and provoking fears that university research will be hurt.\u00a0\nSome researchers welcome the business interest. Others, however, complain that corporations are outbidding the campus for scarce personnel, and that work is being diverted from long-term research to short-term problems with immediate application. They also say that scientists are becoming more reluctant to share research results.\u00a0Effects on Research\n''We perceive there's a real potential for the existing quality of A.I. research to diminish,'' said Ron Olander, who coordinates such research for the Defense Department's Advanced Research Projects Agency. He made the remark during a panel discussion at the National Conference on Artificial Intelligence, held in Pittsburgh last month.\nArtificial intelligence is concerned with making computers do things that are said to require intelligence when people do them.  Commercial interest is centered on four areas:\n- Vision systems, which would allow computers to interpret satellite photographs and allow industrial robots to identify objects coming down the assembly line.\n- Natural language systems, which allow people who do not know computer languages to get information out of computer storage by asking for it in plain English.\n- Expert systems, computer programs that mimic the behavior of human experts and that can do such things as diagnose diseases and interpret geological data in exploring for minerals.\n- Equipment and programs used by the artificial intelligence researchers themselves. Xerox and two start-up companies, Symbolics Inc. and Lisp Machine Inc., sell computers specially designed to handle Lisp, the programming language used by artificial intelligence researchers.\nSeveral large companies such as Schlumberger, Hewlett-Packard, Digital Equipment and Texas Instruments have formed artificial intelligence groups, to design products for internal use and perhaps for outside sale.\nSchlumberger, for instance, hopes to have expert systems interpreting data from logs of oil wells. Digital uses an expert system to help package computer systems and is developing a program to diagnose broken computers. RCA Government Systems and Lockheed's Emsco division, meanwhile, advertised at the Pittsburgh conference for people to form artificial intelligence groups.\u00a0Wall Street Notices\nWall Street is starting to take notice. F. Eberstadt & Company, a brokerage firm, has formed a special unit to analyze and possibly invest in companies in artificial intelligence.\nMore companies are being started, many of them drawing people from university research programs in a phenomenon similar to what occurred when genetic engineering was commercialized a few years ago.\nYale's Dr. Schank, for instance, formed Cognitive Systems, which will sell natural language systems. It is designing a system for oil companies that will retrieve information on oil wells using plain English commands. Dr. Schank plans to develop computer programs that can do such things as give expert advice on taxes or wills.\nEdward A. Feigenbaum, a computer science professor at Stanford University, has co-founded two companies - Intelligenetics, which aims to apply artificial intelligence to genetic engineering, and Teknowledge Inc., which designs expert systems for other companies.  Teknowledge is designing a system for Elf Aquitaine, the French national oil company, to diagnose why a drilling bit gets stuck during drilling.\u00a0Exodus From M.I.T.\nAlready, such university spinoffs have led to strains. The staff of the artificial intelligence laboratory at the Massachusetts Institute of Technology was decimated in 1980 when more than a dozen researchers left to form Symbolics. The company sells computers designed for artificial intelligence that the researchers developed while at M.I.T. About the only two staff research people who did not join Symbolics left M.I.T. to form Lisp Machine, a competing company.\n''We took so many that it's going to take years for M.I.T. to build back up,'' conceded Russell Noftsker, president of Symbolics and former director of the artificial intelligence lab.\nMarvin Minsky, an M.I.T. professor who is considered a founding father of artificial intelligence, agrees. ''Most A.I. labs cannot buy the machines they had a hand in designing,'' he lamented. He also fears that universities will lack resources to develop the next generation of machines.\u00a0Where Universities May Benefit\nThe commercial activity might have some benefit for universities, however. If artificial intelligence is considered commercially important, corporations might finance university research. The Carnegie-Mellon University has signed on several corporate sponsors for its robotics laboratory.\nAlso, the rise of the companies might make it easier for people who want to concentrate on basic research, because pressure from Government sponsors for practical results would be eased.\nSome of the uneasiness in the university community stems from a difference in cultures. Academic researchers consider products coming out on the market unsophisticated and oversold.\n''I don't think they have anything to do with artificial intelligence - they have to do with artificial intelligence of 10 years ago,'' said David Waltz, professor of electrical engineering at the University of Illinois.\u00a0Wrong Twice\nAn example often given is that of expert systems, the programs that can diagnose diseases or help explore for oil. Although the computer programs are fairly adept at making analyses, they cannot learn from experience. Given the same set of symptoms, for instance, an expert system will make the same diagnosis twice, even if the first one proves wrong.\n''If you don't have an expert that can learn and have memory, you get a little anxious,'' said Dr. Schank, adding that expert systems are going on the market prematurely. Yet his own company is often cited by others as an example of one that overly promotes its products. Cognitive Systems' literature advertises that the company develops systems that offer ''all the benefits of having a human expert on your staff, but it never takes a lunch hour or goes on vacation.''\nThose entering the business say that it is impossible to wait indefinitely for technology to be perfected before introducing it commercially. Lee Hecht, president and chief executive officer of Teknowledge, said there are many applications - from electronic circuit design to diagnosing nuclear power plant accidents - in which existing expert systems could save companies millions of dollars.\u00a0Challenge for Management\nBesides the question of how sophisticated their products are, start-up companies may face a bigger stumbling block - a lack of skill in managing a company and in focusing on specific market areas. They must also hang on until the market develops further and then face competition from the more established companies. ''What's going to delay A.I. is that there isn't an infrastructure for developing applied work,'' said John H. Clippinger, president of the Brattle Research Corporation, a consulting and market research firm in Boston.\nThose same problems afflicted genetic engineering companies formed by professors a few years ago. For lack of management talent, money or products, many of those companies have fallen on hard times. Some predict a similar shakeout in artificial intelligence.\n''This field is even more university-bound than genetic engineering,'' said one analyst who asked not to be identified.  ''Some of those guys can't manage their way out of a paper bag.''\n","12":"In its first steps toward commercialization, IBM's Watson took on grand, science-laden challenges like helping doctors diagnose cancer. But that is changing as IBM strives to build its artificial intelligence technology into a multibillion-dollar business.\nToday, companies including Geico, Staples and Macy's are adding the Watson technology to answer customer questions or to improve mobile apps that guide shoppers through stores. \u00a0\n  Now in its broadest deployment so far, Watson will be assisting H&R Block's 70,000 tax professionals this filing season at 10,000 branch offices across the country, where 11 million people file taxes.\n  The H&R Block partnership with Watson, announced on Wednesday, is being presented to a wider audience with a 60-second television ad during the Super Bowl on Sunday.\n  For IBM, the collaboration with H&R Block underlines its strategy in the emerging market for artificial intelligence technology. Watson will touch consumers, but through IBM's corporate clients.\n  ''Watson will become a really smart, virtual assistant,'' said David Kenny, senior vice president of IBM's Watson business.\n  The embedded-in-business formula is different from the path other technology companies are taking with digital assistants powered by artificial intelligence. Others are pursuing the broad consumer market directly with artificial intelligence software helpers like Siri from Apple, Cortana from Microsoft, Alexa from Amazon and Assistant from Google.\n  In the corporate technology market, the major companies are racing to get on the artificial intelligence bandwagon, though they lack the early lead and powerful branding of Watson.\n  Data-fueled artificial intelligence will be an ingredient in all kinds of software used in corporations, to streamline work, identify new customers, spot savings and guide product development, analysts say. By 2018, the research company IDC predicts that 75 percent of new business software will include artificial intelligence features.\n  ''A.I. is becoming part of the mix, part of the infrastructure in consumer and business applications,'' said David Schubmehl, an IDC analyst.\n  IBM is also banking on Watson as an engine of its corporate transformation. The company needs its new businesses like Watson, data analysis and cloud computing to more than make up for the erosion in its traditional hardware and software products.\n  That has not happened yet. Two weeks ago, IBM reported its 19th consecutive quarter of declining revenue.\n  But there were encouraging signs in the last quarter of 2016.\n  IBM executives noted that the company's newer businesses, including Watson, grew by 14 percent to represent 41 percent of total revenue. That was ahead of IBM's previous forecast that the 40 percent of revenue threshold would not be reached until 2018.\n  The H&R Block collaboration suggests that IBM is getting better at working with partners. Some of the early Watson projects stretched out for years partly because of poor communications between IBM and its clients.\n  But when Bill Cobb, chief executive of H&R Block, contacted IBM last summer, saying he wanted to use Watson to help ''reinvent the retail experience for taxpayers,'' he heard what he called a candid assessment, not a sales pitch.\n  ''This is not magic,'' Mr. Cobb said the Watson team told him. ''You have to teach Watson over time.''\n  Watson proved to be a quick learner. Its core skill is its ability to digest and classify vast amounts of text, using what is known as natural language processing. So, among other things, it was fed the 74,000 pages of the federal tax code and thousands of tax-related questions culled from H&R Block's data, accumulated over six decades of preparing tax returns.\n  Then, H&R Block tax professionals were brought in to ''train'' Watson. They approved when Watson suggested a smart question for a particular tax filer and corrected it when a proposed question was off base. The tax professionals were not told they were working with Watson, just a software program.\n  The technology was tested in about 100 H&R Block offices in January, and it will be available throughout the company's retail network next week. One goal, Mr. Cobb said, is to assist H&R Block's tax professionals and improve their chances of fairly increasing refunds and reducing tax liabilities for clients. Seventy-five percent of Americans who file taxes get money back. For H&R Block clients, the figure is about 85 percent, Mr. Cobb said.\n  But Watson's other task was to make visiting H&R Block a more ''engaging and interactive'' experience, he said. Clients will be able to watch suggestions and questions on a separate screen, as the Watson-assisted tax professional works.\n  Mr. Cobb said he was ''very pleased'' with Watson's performance in the 100-office trial. But the test will come after this tax season, when H&R Block sees what percentage of customers return next year.\n  The retention rate is crucial in the tax-preparation business. Sixty percent of the 140 million Americans who file taxes seek help instead of doing them themselves. Kartik Mehta, an analyst at Northcoast Research, estimates that H&R Block's retention rate is 75 percent. ''If they can get that 75 percent up to 80 percent, it's a big deal for H&R Block,'' Mr. Mehta said.\n  The Watson collaboration is a ''long-term partnership,'' Mr. Cobb said. Mr. Kenny of IBM described this tax season as ''phase one'' of the partnership.\n  Over time, Watson's suggestions get better and better, more individually tailored to specific occupations, household finances and personal circumstances. ''It's crawl, walk, run learning,'' Mr. Kenny said. ''But the pace of the Watson learning is so much faster than humans.''\n\n\n\n","13":"Just how worried should we be about artificial intelligence? Earlier this year the Global Challenges Foundation released a thorough report on the greatest threats to human civilization. While the odds of most things were minuscule - about a hundredth of a percent - artificial intelligence was in a class of its own, given a zero to 10 percent chance.\nIn the past year, leading technology thinkers such as Elon Musk, Bill Gates and Stephen Hawking have warned of the perils of artificial intelligence. Last week the Future of Life Institute gave out 37 grants to ensure that artificial intelligence remains beneficial. The fear is that some super-intelligent being will shape the world to its preferences, and humans might be expendable as it pursues its goals.\u00a0\nOf course, the debate remains theoretical and full of what-ifs. In past decades, researchers have suggested we're on the precipice of massive breakthroughs in artificial intelligence, only to see those predictions fall flat. The future is notoriously hard to predict.\nNick Bostrom, author of the well-regarded book on the dangers of artificial intelligence, \"Superintelligence,\" admits in its opening pages that \"many of the points made in this book are probably wrong.\" He warns that some of his conclusions could be invalidated because of key considerations he didn't make.\n[What the debacle of climate change can teach us about the dangers of artificial intelligence]\nMeanwhile\u00a0fears and talk of killer robots run through pop culture. In past four months, three movies about artificial intelligence - \"Ex Machina,\" \"Chappie\" and \"Terminator Genisys\"\u00a0- have arrived in theaters.\nBut not everyone is buying it. One skeptic is John Underkoffler, who was the technology adviser to \"Minority Report,\" the Steven Spielberg film in which an intelligent system predicts who will commit future crimes. Underkoffler's work at the MIT Media Lab inspired the memorable hand gestures and interface that Tom Cruise's character used to run the system.\n\"I'm actually really bemused by this sudden furor over the dangers of AI,\" Underkoffler told me. \"It's a pretty simple reaction. We don't have AI and we're nowhere close to it.\"\nMost in the artificial intelligence community expect we're decades away from the powerful type\u00a0of artificial intelligence that could prove troublesome. While Google's DeepMind and IBM's Watson system have shown progress in the field, they still have plenty of limitations.\n\"For something to suddenly become sentient and Skynet, or Proteus or any of these other sci-fi things to suddenly to become malevolent and have enough resources at its disposal to go stomping around and squashing us - the thing is, it doesn't emerge overnight,\" Underkoffler said. \"Wouldn't you notice that stuff was getting smart?\"\nAfter \"Minority Report,\" Underkoffler went on to found Oblong Industries, which brings some of the gesture and interface technology that Cruise used in \"Minority Report\" to companies today. Underkoffler is creating a collaborative environment where employees in different cities can interact on shared surfaces. He wants to move past the current model where one person works on one computer.\nUnderkoffler sees artificial intelligence as something that should be researched and developed for its potential to improve lives today, not potentially destroy them years from now.\n\"For billionaires to be donating millions of dollars to foundations to worry about making sure that AI doesn't get away seems analogous to me to saying 'I'm going to donate $15 million right now to an institute to make sure that teleportation doesn't enable thieves to grab my wallet,' \" Underkoffler said. \"Okay, yeah, I don't want that to happen, but wouldn't we have to have teleportation first? I'd rather have my $15 million go to inventing teleportation.\"\nHe envisions that the development of the potentially dangerous artificial systems will mirror biological\u00a0evolution. A lone researcher at an office supply company is unlikely to suddenly hatch an algorithm that endlessly replicates itself, grabs control of human systems like the Internet and covers most of the earth's surface with paper clips.\n\"You would get an AI that would basically do what a marmot does first. And then maybe a really smart crow,\" Underkoffler said. \"And then you might get to a monkey or something and eventually you get a dolphin or human and beyond.\"\nWhen I spoke with Underkoffler, he did caution\u00a0about what he calls nuisance artificial intelligence. If the world's electric grids are turned over to a machine-learning system, a bug or something might cause the system to shut down in a way that we can never get it operational again.\n\"It's not impossible to imagine a world-changing event like that,\" Underkoffler said. \"And it's definitely worth building checks and balances. You would do that anyway. If you're building a roller-coaster, you build multiple layers of checking and fail-safes and so forth.\"\n","14":"HONG KONG -- S\u00f6ren Schwertfeger finished his postdoctorate research on autonomous robots in Germany, and seemed set to go to Europe or the United States, where artificial intelligence was pioneered and established.\nInstead, he went to China. \n  ''You couldn't have started a lab like mine elsewhere,'' Mr. Schwertfeger said.\n  The balance of power in technology is shifting. China, which for years watched enviously as the West invented the software and the chips powering today's digital age, has become a major player in artificial intelligence, what some think may be the most important technology of the future. Experts widely believe China is only a step behind the United States.\n  China's ambitions mingle the most far-out sci-fi ideas with the needs of an authoritarian state: Philip K. Dick meets George Orwell. There are plans to use it to predict crimes, lend money, track people on the country's ubiquitous closed-circuit cameras, alleviate traffic jams, create self-guided missiles and censor the internet.\u00a0\n  Beijing is backing its artificial intelligence push with vast sums of money. Having already spent billions on research programs, China is readying a new multibillion-dollar initiative to fund moonshot projects, start-ups and academic research, all with the aim of growing China's A.I. capabilities, according to two professors who consulted with the government on the plan.\n  China's private companies are pushing deeply into the field as well, though the line between government and private in China sometimes blurs. Baidu -- often called the Google of China and a pioneer in artificial-intelligence-related fields, like speech recognition -- this year opened a joint company-government laboratory partly run by academics who once worked on research into Chinese military robots.\n  China is spending more just as the United States cuts back. This past week, the Trump administration released a proposed budget that would slash funding for a variety of government agencies that have traditionally backed artificial intelligence research.\n  ''It's a race in the new generation of computing,'' said James Lewis, a senior fellow at the Center for Strategic and International Studies. ''The difference is that China seems to think it's a race and America doesn't.''\n  For Mr. Schwertfeger, the money mattered. He received a grant six times larger than what he might have gotten in Europe or America. That enabled him to set up a full artificial intelligence lab, with an assistant, a technician and a group of Ph.D. students.\n  ''It's almost impossible for assistant professors to get this much money,'' he said. ''The research funding is shrinking in the U.S. and Europe. But it is definitely expanding in China.''\n  Mr. Schwertfeger's lab, which is part of ShanghaiTech University, works on ways for machines, without any aid from humans, to avoid obstacles. Decked out with wheeled robots, drones and sensors, the lab works on ways for computers to make their own maps and to improve the performance of robots with tasks like finding objects -- specifically, people -- during search-and-rescue operations.\n  Much of China's artificial intelligence push is similarly peaceful. Still, its prowess and dedication have set off alarms within the United States' defense establishment. The Defense Department found that Chinese money has been pouring into American artificial intelligence companies -- some of the same ones it had been looking to for future weapons systems.\n  Quantifying China's spending push is difficult, because authorities there disclose little. But experts say it looks to be considerable. Numerous provinces and cities are spending billions on developing robotics, and a part of that funding is likely to go to artificial intelligence research. For example, the relatively unknown city of Xiangtan, in China's Hunan province, has pledged $2 billion toward developing robots and artificial intelligence. Other places have direct incentives for the A.I. industry. In Suzhou, leading artificial intelligence companies can get about $800,000 in subsidies for setting up shop locally, while Shenzhen, in southern China, is offering $1 million to support any A.I. project established there.\n  On a national level, China is working on a system to predict events like terrorist attacks or labor strikes based on possible precursors like labor strife. A paper funded by the National Natural Science Foundation of China showed how facial recognition software can be simplified so that it can be more easily integrated with cameras across the country.\n  China is preparing a concerted nationwide push, according to the two professors who advised on the effort but declined to be identified, because the effort has not yet been made public. While the size wasn't clear, they said, it would most likely result in billions of dollars in spending.\n  President Trump's proposed budget, meanwhile, would reduce the National Science Foundation's spending on so-called intelligent systems by 10 percent, to about $175 million. Research and development in other areas would also be cut, though the proposed budget does call for more spending on defense research and some supercomputing. The cuts would essentially shift more research and development to private American companies like Google and Facebook.\n  ''The previous administration was preparing for a future with artificial intelligence,'' said Subbarao Kambhampati, president of the Association for the Advancement of Artificial intelligence. ''They were talking about increasing basic research for artificial intelligence. Instead of increases, we are now being significantly affected.''\n  China's money won't necessarily translate into dominance. The government's top-down approach, closed-mouth bureaucracy and hoarding of information can hobble research. It threw a tremendous amount of resources toward curing severe acute respiratory syndrome, the deadly virus known as SARS, when it swept through the country 15 years ago. Yet the virus was eventually sequenced and tamed by a small Canadian lab, said Clay Shirky, a professor at N.Y.U. Shanghai and a technology writer.\n  ''It wasn't that anyone was trying to stop the development of a SARS vaccine,'' Mr. Shirky said. ''It's the habit that yes is more risky than no.''\n  Authorities in China are now bringing top-down attention to fixing the problem of too much top-down control. While that may not sound promising, Wang Shengjin, a professor of electronic engineering at China's Tsinghua University, said he had noticed some improvement, such as professional groups sharing information, and authorities who are rolling back limits on professors claiming ownership of their discoveries for commercial purposes.\n  ''The lack of open sources and sharing of information, this has been the reality,'' Mr. Wang said. ''But it has started to change.''\n  At the moment, cooperation and exchanges in artificial intelligence between the United States and China are largely open, at least from the American side. Chinese and American scholars widely publish their findings in journals accessible to all, and researchers from China are major players in America's research institutions.\n  Chinese tech giants like Baidu, Tencent and Didi Chuxing have opened artificial intelligence labs in America, as have some Chinese start-ups. Over the past six years, Chinese investors helped finance 51 American artificial intelligence companies, contributing to the $700 million raised, according to the recent Pentagon report.\n  It's unclear how long the cooperation will continue. The Pentagon report urged more controls. And while there are government and private pushes out of China, it is difficult to tell which is which, as Baidu shows.\n  Baidu is a leader in China's artificial intelligence efforts. It is working on driverless cars. It has turned an app that started as a visual dictionary -- take a picture of an object, and your cellphone will tell you what it is -- into a site that uses facial recognition to find missing people, a major problem in a country where child kidnapping has been persistent. In one stunning example, it helped a family find a child kidnapped 27 years earlier. DNA testing confirmed the family connection.\n  Baidu's speech-recognition software -- which can accomplish the difficult task of hearing tonal differences in Chinese dialects -- is considered top of the class. When Microsoft announced last October that its speech recognition software had surpassed human-level language recognition, Baidu's head of research at the time playfully reminded the American company that his team had accomplished a similar feat a year earlier.\n  We had surpassed human-level Chinese recognition in 2015; happy to see Microsoft also get there for English less than a year later. -- Andrew Ng (@AndrewYNg) October 20, 2016\n  In an apparent effort to harness Baidu's breakthroughs, China said this year that it would open a lab that would cooperate with the company on A.I. research. The facility will be headed by two professors with long experience working for government programs designed to catch up to and replace foreign technology. Both professors also worked on a program called the Tsinghua Mobile Robot, according to multiple academic papers published on the topic. Research behind the robot, which in one award is described as a ''military-use intelligent ground robot,'' was sponsored by funding to improve Chinese military capabilities.\n  Li Wei, a professor involved in the Baidu cooperative effort, spent much of his career at Beihang University, one of China's seven schools of national defense.\n  A company spokeswoman said: ''Baidu develops products and services that improve people's lives. Through its partnership with the A.I. research community, Baidu aims to make a complicated world simpler through technology.''\n  Still, there are advantages in China's developing cutting-edge A.I. on its own. National efforts are aided by access to enormous amounts of data held by Chinese companies and universities, the large number of Chinese engineers being trained on either side of the Pacific and from government backing, said Mr. Wang, of Tsinghua.\n  Driving that attention is a breakthrough from an American company largely banned in China: Google. In March 2016, a Google artificial intelligence system, AlphaGo, beat a South Korean player at the complicated strategy game Go, which originated in China. This past week, AlphaGo beat the best player in the world, a Chinese national, at a tournament in Wuzhen, China.\n  The Google event changed the tenor of government discussions about funding, according to several Chinese professors.\n  ''After AlphaGo came out and had such a big impact on the industry,'' said Zha Hongbin, a professor of machine learning at Peking University, ''the content of government discussions got much wider and more concrete.'' Shortly afterward, the government created a new project on brain-inspired computing, he added.\n  For all the government support, advances in the field could ultimately backfire, Mr. Shirky said. Artificial intelligence may help China better censor the internet, a task that often blocks Chinese researchers from finding vital information. At the same time, better A.I. could make it easier for Chinese readers to translate articles and other information.\n  ''The fact is,'' Mr. Shirky said, ''unlike automobile engineering, artificial intelligence will lead to surprises. That will make the world considerably less predictable, and that's never been Beijing's favorite characteristic.''\n  Follow Paul Mozur on Twitter @paulmozur and John Markoff @markoff.\n\n\n\n","15":"Demis Hassabis is an impressive guy. A former child prodigy, a chess master at 13 and the founder of DeepMind Technologies, a British artificial intelligence company that Google acquired last year. Now 38, he's at the forefront of an emerging technology with an unmatched potential for good and bad.\u00a0\nHassabis and his researchers published a landmark paper this week, creating an algorithm that learns in a human-like manner. Observers of artificial intelligence have warned that advances like this are a step toward potentially destroying civilization.\nElon Musk, a DeepMind investor - he says the better to keep an eye on them - has led the charge, calling artificial intelligence mankind's greatest threat. Stephen Hawking and Bill Gates have also issued warnings.\nAt a news conference Tuesday Hassabis addressed Musk's concerns:\n\"We're many, many decades away from anything, any kind of technology that we need to worry about. But it's good to start the conversation now and be aware of as with any new powerful technology it can be used for good or bad,\" Hassabis said.\nHe was also quick to downplay any rift with DeepMind and Musk.\n\"We're good friends with Elon and he's been a big supporter of ours for a number of years,\" Hassabis said. \"And he's fascinated, loves the potential of artificial intelligence.\"\nElon Musk loves artificial intelligence? Never would've guessed that.\n              Related: Google's breakthrough in artificial intelligence, and what it means for self-driving cars \n","16":"Correction Appended\nSAN FRANCISCO - Uber envisions a future in which a fleet of vehicles can make the most complex maneuvers while carting passengers around without the help of a driver. To achieve that, cars will need to get a whole lot smarter.\nEnter Gary Marcus and Zoubin Ghahramani. The two men are being appointed as co-directors of Uber's new in-house research arm on artificial intelligence, which the ride-hailing company unveiled on Monday. The research arm's aim is to apply A.I. in areas like self-driving vehicles, along with solving other technological challenges through machine learning.\u00a0\nThe two are joining Uber through an acquisition of their start-up, Geometric Intelligence. Unlike most A.I. start-ups that generally follow one method of study of artificial intelligence, Geometric Intelligence takes a multidisciplinary approach to the field.\nAll 15 people from the start-up will be absorbed by Uber. Terms of the deal were not disclosed.\nThe acquisition and new research arm, which will be called Uber's A.I. Labs, exemplifies how seriously Silicon Valley tech companies are betting on artificial intelligence. Google, Facebook and others have also pushed into artificial intelligence, which underlies voice recognition software, digital assistants like Amazon's Alexa and Apple's Siri, and technologies like self-driving cars. Many companies are racing to bring on new A.I. talent to compete against one another.\n\"Every major company realizes how essential A.I. is to what they're doing,\" Dr. Marcus said in an interview. \"Because of the scale of data people are operating on, even the smallest gains in efficiency can turn out enormous changes at these companies, especially in terms of profit.\"\nWith the Geometric Intelligence deal, Uber, which is now valued at close to $70 billion, said it hoped that Dr. Marcus's team could harness the wealth of data it collects from the millions of daily Uber rides. The company wants to use the data to make major advances in how computers behind self-driving vehicles think and make decisions on the road.\nMany of Silicon Valley's biggest tech companies, such as Google and Facebook, have tried to commercialize artificial intelligence through the application of algorithms modeled largely on how the human brain functions. This method, called deep learning, leans heavily on the vast data sets that private technology companies own and that are used to train computers to do simple tasks, such as match patterns or recognize faces in photographs.\nPart of what drew Uber to Dr. Marcus's team is that his start-up is tackling artificial intelligence in a different way. Rather than taking just one approach like deep learning, Geometric Intelligence combines data scientists who use varying techniques to study artificial intelligence, including the Bayesian and \"evolutionary\" methods.\nDr. Marcus, who helped found Geometric Intelligence in late 2014, said the gist of his philosophy goes something like this: Instead of training machines by feeding them enormous amounts of data, what if computers were capable of learning more like humans by extrapolating a system of rules from just a few or even a single example?\nIn recent years, researchers at institutions such as the Massachusetts Institute of Technology, New York University and the University of Toronto have also worked on similar theories. Using this approach, some reported a breakthrough in \"one-shot\" machine learning last December, in which artificial intelligence advances surpassed human capabilities for a narrow set of vision-related tasks.\nBesides autonomous vehicles, Uber said it expected its A.I. Labs to apply its method to other tasks, including combating fraud, extracting information from street signs and learning to improve its mapping research and capabilities.\n\"When step function changes in this field occur, we're going to see very significant differences in how businesses run themselves,\" Jeff Holden, Uber's chief product officer, said in an interview. He said the A.I. Labs and Uber's Advanced Technologies Center, which is home to the company's self-driving car research, will work in tandem with each other.\n\"It's going to be a very long time before a self-driving car will be able to make all the kinds of trips that Uber does every single day,\" Mr. Holden said. \"But the answers to this are all going to come in the form of artificial intelligence.\"\nMr. Holden said recruiting A.I. research talent is highly competitive, especially as Google, Apple and Tesla are also developing self-driving cars or related projects.\n\"From a defensive perspective, if someone else develops them first, we're in trouble,\" he said.\nCorrection: December 5, 2016, Monday\n Because of an editing error, an earlier version of this article misstated the surname of a new co-director of Uber's in-house research arm on artificial intelligence. He is Zoubin Ghahramani, not Ghahraman.\nPHOTO: A driver using Uber in Washington. Uber bought the start-up Geometric Intelligence to help develop artificial intelligence. (PHOTOGRAPH BY ANDREW HARRER\/BLOOMBERG)Related Articles\n\n","17":"For those wondering when artificial intelligence will truly take root, here's a bulletin: it already has.\n     Artificial intelligence is now a regular academic discipline. It is already embedded in many everyday products. And it helps businesses sort through and make sense of huge databases. \n Even so, a controversy erupts with each step toward the day when machines might be said to surpass humans in intelligence -- a day that some say will trumpet progress for humanity, but that others say will court disaster.\u00a0\n\"The concept scares people,\" said Jordan B. Pollack, a Brandeis University researcher who found himself in the limelight last year as co-creator of the first machine to design and manufacture other machines with virtually no human help. \nBut Mr. Pollack knows better than most that although yesterday's release of the futuristic film \"A.I.\" is likely to stir up such fears, artificial intelligence has already seeped into many corners of daily life.\nLast year, Mr. Pollack branched out from his research roots to found Thinmail, a venture that provides users of wireless devices with an intelligent electronic assistant capable of tasks like translating documents into simple text and diverting bulky attachments to fax machines.\nThinmail is just one of countless businesses that use machine intelligence, whether to guide missiles, detect credit card fraud, diagnose medical problems, or make toys more entertaining. Automated money management and trading systems manage an estimated $1 trillion in pension funds and other investments. While few people see any connection of all this to the machines Mr. Pollack is researching in his lab, much less the robots of science fiction, their variety and the pace of their development is clearly shaping social and economic life.\n\"An A.T.M. is not very intelligent, but it puts bank tellers out of work,\" Mr. Pollack said. \"It earns a living.\" \nJust how much of a living is anybody's guess. \"I haven't seen an estimate of the market value of A.I. products in years because it's become part of the landscape,\" said Curt Hall, a software analyst with the Cutter Consortium, a research firm in Arlington, Mass., who has followed the technology since the 1980's.\nThe numbers are elusive in part because artificial intelligence is spread across a variety of disciplines that overlap and, in some cases, start from conflicting premises about how humans think. They include the ability to understand and manipulate language; make sense of what can be seen, heard or felt; find useful patterns in data; and draw conclusions based on rules and experience. Other attributes include an ability to respond to environmental changes without human intervention or evolve through selecting the best results from random mutation.\nNo product puts everything in one package the way science fiction repeatedly envisions it, but researchers are increasingly stacking several of them together, said Joseph Sirosh, executive director of the Advanced Technology Solutions Group at HNC Software Inc. HNC, a publicly traded company based in San Diego, is best known for its Falcon software, which is used by banks to scan more than 12 million credit card transactions a day for unusual patterns that might signal fraud. The software is estimated to save them $500 million annually. \nMost computer experts expect public fascination with artificial intelligence to surge with the fanfare surrounding the release of \"A.I.: Artificial Intelligence,\" the sci-fi film vision of Stanley Kubrick and Steven Spielberg that some have described as Pinocchio meets robotics. Some suspect the impact will be comparable to 1968, when Kubrick's \"2001: A Space Odyssey\" made HAL the world-famous symbol of deadly thinking machines. \nToday's theatergoers will emerge with a reality check all around them, in the form of numerous university programs that teach artificial intelligence and the many businesses that embed such concepts in their products. But the artificial-intelligence world is so fragmented that some experts fear the film will leave many people with mistaken notions that could slow development.\n\"My fear is that the movie will make the subject too cute,\" said Ronald R. Yager, director of the Machine Intelligence Institute at Iona College.  \"Serious people would become afraid to associate themselves with the technology.\" \nSimilarly, some vendors of intelligent software fear that an explosion of interest could produce a flood of shaky business schemes and products dressed up as artificial intelligence. \"When you get the circus, you get the clowns,\" said Konrad Feldman, head of American operations of Searchspace, a British vendor of multimillion-dollar software agents that continually examine databases and online activity.\nOthers are more hopeful, suggesting that \"A.I.\" might awaken venture capitalists to the commercial potential of research projects in controversial areas like the emotional dimensions of machine intelligence. The film asks what would become of a childlike robot programmed to love a human mother. As unnerving as the results depicted are for both the robots and humans, researchers said \"A.I.\" could build support for today's more mundane goals of using programmed emotional capabilities to make Web sites, tutorial software and products like cars more responsive and engaging.\n\"The movie could propel what I'm doing at an exponential rate,\" said Cindy Mason, a researcher at the University of California at Berkeley who has been developing programming techniques to represent attitudes, moods, temperament and other emotional states.\nMany researchers and entrepreneurs with artificial-intelligence products say they hope the movie will be the occasion for a national crash course on how far the technology has come. The technology had a notably rocky commercial debut in the 1970's and 1980's. Fortunes were invested and lost in robotics, in machine vision systems and in software known as expert systems that tried to reduce human expertise to collections of rules that machines could follow.\nIn some cases, like Digital Equipment's software package XCON (for expert configurer), which helped customers choose among many options for computer systems, programs initially hailed as great successes proved to be embarrassingly limited and expensive as the number of rules they juggled swelled. Automakers were dismayed to discover that expert systems they developed to help manufacture cars had to go through extensive overhauls every time the models changed. \nThe Nobel Prize-winning economist Herbert Simon predicted in 1965 that by 1985 \"machines will be capable of doing any work man can do.\" When that year rolled around, though, a period of diminished expectations known as \"A.I.'s winter\" had set in. Several start-ups failed, and some big corporations reduced their programs. \nSome technology managers still associate artificial intelligence with the hype of that era, said Steven A. Ward, founder and chief executive of the Ward Systems Group in Frederick, Md. Ward Systems began marketing a form of artificial intelligence software known as neural networks in 1988. Such products, which try to mimic human learning, make projections about, say, how markets will move or when a manufacturing process will break down. \n\"When engineers who want to buy the product tell us that terms like A.I. and neural network are raising red flags with their superiors, we tell them to call it multivariable nonlinear modeling,\" Mr. Ward said. \"Their superiors won't have the foggiest notion of what it is, but it sounds traditional.\"\nOn the other hand, a growing number of businesses, led by the video game industry, appear to view the use of artificial intelligence as a selling point. After all, their mostly young customers have no memories of the disappointments in the 1980's. Richard Stottler, whose San Mateo, Calif., company, Stottler Henke Associates, helps clients add artificial intelligence to their products, said that a marriage counseling firm, a company that reviews highway designs and a trouble-shooting service for computer network operators all plan to stress this feature in their marketing.\nVeterans of the long push to expand and commercialize machine intelligence say that efforts to market it have often been confounded by the tendency of experts continually to raise the threshold of what they consider true artificial intelligence.\nRaymond Kurzweil, a researcher and entrepreneur whose involvement began when he was a teenager in the 1960's, said, \"Once a technique works, it's no longer considered A.I.\"\n","18":"Once the sole province of arcane computer science researchers, artificial intelligence is quickly emerging as a mainstream product for some of the biggest computer companies in the world, including International Business Machines Corp., Digital Equipment Corp. and Hewlett-Packard Co.\nInterest in computers that emulate human intelligence is surging, despite an industrywide sales slump in computers and software. Attendance at the International Joint Conference on Artificial Intelligence this week in Los Angeles has more than doubled from last year's attendance of 3,000, and the number of exhibitors also doubled to nearly 60.\u00a0\n\"People are still very bullish on AI,\" said Woody Bledsoe, president of the American Association on Artificial Intelligence and director of artificial intelligence programming efforts at the Microelectronics and Computer Technology research consortium in Austin, Tex. \"We'll probably have a downturn at some point but, right now, we seem to be headed in the right direction.\"\nFor the first time, major companies such as IBM and DEC are offering products using artificial intelligence for sale rather than simply presenting research papers for academic consumption.\nThe big computer companies are now selling artificial intelligence tools that enable computer programmers to design their own artificial intelligence systems. Most notable are computer programmer workstations that run the LISP computer language. LISP -- which stands for LISt Processing -- is the most common language used for artificial intelligence programming. Initially, young companies like Symbolics Inc. pioneered the LISP workstation market, but now, the larger companies are moving in.\nDEC introduced a $50,000 AI workstation for its Microvax II computer that runs LISP. Hewlett-Packard introduced LISP workstations in the $20,000 to $50,000 price range, and Xerox Corp. revealed a new $10,000 artificial intelligence applications workstation.\nBusiness computer specialists who attended the conference, however, were particularly interested in the slew of new \"expert systems\" software development programs now being offered by both venture firms and more established companies. Expert systems are computer programs designed to replicate the decision-making process of experts using special decision rules. For example, a medical diagnosis program might offer thousands of computer-based \"if . . . then\" rules that could aid a physician in making a diagnosis.\nIBM, which last year demonstrated an internally developed expert system, introduced an expert system in Los Angeles designed to enable people to customize their own expert systems on IBM computers. IBM's Expert System Environment\/VM will cost roughly $35,000.\nTexas Instruments announced that it would buy a 10 percent stake in the Pittsburgh-based Carnegie Group, a major expert systems developer, as well as acquire a Carnegie license for AI software products and training. There appears to be a major emphasis in selling the tools to make expert systems for such applications as engineering design and financial analysis, rather than design the expert systems themselves.\nOne important aspect of the mainstream computer industry's interest in AI is that it may be easier for established companies to sell AI hardware and software to customers than the less established venture companies. Data processing managers in major companies may feel more comfortable in exploring artificial intelligence products if they are backed by well-known suppliers.\n","19":"Marvin Minsky, who combined a scientist's thirst for knowledge with a philosopher's quest for truth as a pioneering explorer of artificial intelligence, work that helped inspire the creation of the personal computer and the Internet, died on Sunday night in Boston. He was 88.\nHis family said the cause was a cerebral hemorrhage. \n  Well before the advent of the microprocessor and the supercomputer, Professor Minsky, a revered computer science educator at M.I.T., laid the foundation for the field of artificial intelligence by demonstrating the possibilities of imparting common-sense reasoning to computers.\n  ''Marvin was one of the very few people in computing whose visions and perspectives liberated the computer from being a glorified adding machine to start to realize its destiny as one of the most powerful amplifiers for human endeavors in history,'' said Alan Kay, a computer scientist and a friend and colleague of Professor Minsky's.\u00a0\n  Fascinated since his undergraduate days at Harvard by the mysteries of human intelligence and thinking, Professor Minsky saw no difference between the thinking processes of humans and those of machines. Beginning in the early 1950s, he worked on computational ideas to characterize human psychological processes and produced theories on how to endow machines with intelligence.\n  Professor Minsky, in 1959, co-founded the M.I.T. Artificial Intelligence Project (later the Artificial Intelligence Laboratory) with his colleague John McCarthy, who is credited with coining the term ''artificial intelligence.''\n  Beyond its artificial intelligence charter, however, the lab would have a profound impact on the modern computing industry, helping to impassion a culture of computer and software design. It planted the seed for the idea that digital information should be shared freely, a notion that would shape the so-called open-source software movement, and it was a part of the original ARPAnet, the forerunner to the Internet.\n  Professor Minsky's scientific accomplishments spanned a variety of disciplines. He designed and built some of the first visual scanners and mechanical hands with tactile sensors, advances that influenced modern robotics. In 1951 he built the first randomly wired neural network learning machine, which he called Snarc. And in 1956, while at Harvard, he invented and built the first confocal scanning microscope, an optical instrument with superior resolution and image quality still in wide use in the biological sciences.\n  His own intellect was wide-ranging and his interests were eclectic. While earning a degree in mathematics at Harvard he also studied music, and as an accomplished pianist, he would later delight in sitting down at one and improvising complex baroque fugues.\n  Professor Minsky was lavished with many honors, notably, in 1969, the Turing Award, computer science's highest prize.\n  He went on to collaborate, in the early '70s, with Seymour Papert, the renowned educator and computer scientist, on a theory they called ''The Society of Mind,'' which combined insights from developmental child psychology and artificial intelligence research.\n  Professor Minsky's book ''The Society of Mind,'' a seminal work published in the mid-1980s, proposed ''that intelligence is not the product of any singular mechanism but comes from the managed interaction of a diverse variety of resourceful agents,'' as he wrote on his website.\n  Underlying that hypothesis was his and Professor Papert's belief that there is no real difference between humans and machines. Humans, they maintained, are actually machines of a kind whose brains are made up of many semiautonomous but unintelligent ''agents.'' And different tasks, they said, ''require fundamentally different mechanisms.''\n  Their theory revolutionized thinking about how the brain works and how people learn.\n  ''Marvin was one of the people who defined what computing and computing research is all about,'' Dr. Kay said. ''There were four or five supremely talented characters from back then who were early and comprehensive and put their personality and stamp on the field, and Marvin was among them.''\n  Marvin Lee Minsky was born on Aug. 9, 1927, in New York City. The precocious son of Dr. Henry Minsky, an eye surgeon who was chief of ophthalmology at Mount Sinai Hospital, and Fannie Reiser, a social activist and Zionist.\n  Fascinated by electronics and science, the young Mr. Minsky attended the Ethical Culture School in Manhattan, a progressive private school from which J. Robert Oppenheimer, who oversaw the creation of the first atomic bomb, had graduated. (Mr. Minsky later attended the affiliated Fieldston School in Riverdale.) He went on to attend the Bronx High School of Science and later Phillips Academy in Andover, Mass.\n  After a stint in the Navy during World War II, he studied mathematics at Harvard and received a Ph.D. in math from Princeton, where he met John McCarthy, a fellow graduate student.\n  Intellectually restless throughout his life, Professor Minsky sought to move on from mathematics once he had earned his doctorate. After ruling out genetics as interesting but not profound, and physics as mildly enticing, he chose to focus on intelligence itself.\n  ''The problem of intelligence seemed hopelessly profound,'' he told The New Yorker magazine when it profiled him in 1981. ''I can't remember considering anything else worth doing.''\n  To further those studies he reunited with Professor McCarthy, who had been awarded a fellowship to M.I.T. in 1956. Professor Minsky, who had been at Harvard by then, arrived at M.I.T. in 1958, joining the staff at its Lincoln Laboratory. A year later, he and Professor McCarthy founded M.I.T.'s AI Project, later to be known as the AI Lab. (Professor McCarthy left for Stanford in 1962.)\n  Professor Minsky's courses at M.I.T. -- he insisted on holding them in the evenings -- became a magnet for several generations of graduate students, many of whom went on to become computer science superstars themselves.\n  Among them were Ray Kurzweil, the inventor and futurist; Gerald Sussman, a prominent A.I. researcher and professor of electrical engineering at M.I.T.; and Patrick Winston, who went on to run the AI Lab after Professor Minsky stepped aside.\n  Another of his students, Danny Hillis, an inventor and entrepreneur, co-founded Thinking Machines, a supercomputer maker in the early 1990s.\n  Mr. Hillis said he had so been taken by Professor Minsky's intellect and charisma that he found a way to insinuate himself into the AI Lab and get a job there. He ended up living in the Minsky family basement in Brookline, Mass.\n  ''Marvin taught me how to think,'' Mr. Hillis said in an interview. ''He had a style and a playful curiosity that was a huge influence on me. He always challenged you to question the status quo. He loved it when you argued with him.''\n  Professor Minsky's prominence extended well beyond M.I.T. While preparing to make the 1968 science-fiction epic ''2001: A Space Odyssey,'' the director Stanley Kubrick visited him seeking to learn about the state of computer graphics and whether Professor Minsky believed it would be plausible for computers to be able to speak articulately by 2001.\n  Professor Minsky is survived by his wife, Gloria Rudisch, a physician; two daughters, Margaret and Juliana Minsky; a son, Henry; a sister, Ruth Amster; and four grandchildren.\n  ''In some ways, he treated his children like his students,'' Mr. Hillis recalled. ''They called him Marvin, and he challenged them and engaged them just as he did with his students.''\n  In 1989, Professor Minsky joined M.I.T.'s fledgling Media Lab. ''He was an icon who attracted the best people,'' said Nicholas Negroponte, the Media Lab's founder and former director.\n  For Dr. Kay, Professor Minsky's legacy was his insatiable curiosity. ''He used to say, 'You don't really understand something if you only understand it one way,''' Dr. Kay said. ''He never thought he had anything completely done.''\n\n\n\n","20":"Bill Gates is a passionate technology advocate (big surprise), but his predictions about the future of computing aren't uniformly positive.\nDuring a wide-ranging Reddit \"Ask me Anything\" session on Wednesday -- one that touched upon everything from Gate's biggest regrets to his favorite spread to lather on bread -- the Microsoft co-founder and billionaire philanthropist outlined a future that is equal parts promising and ominous.\nMidway through the discussion, Gates was asked what personal computing will look like in 2045. Gates responded by asserting that the next 30 years will be a time of rapid progress.\u00a0\n\"Even in the next 10 problems like vision and speech understanding and translation will be very good,\" he wrote. \"Mechanical robot tasks like picking fruit or moving a hospital patient will be solved. Once computers\/robots get to a level of capability where seeing and moving is easy for them then they will be used very extensively.\"\nHe went on to highlight a Microsoft project known as the \"Personal Agent,\" which is being designed to help people manage their memory, attention and focus. \"The idea that you have to find applications and pick them and they each are trying to tell you what is new is just not the efficient model - the agent will help solve this,\" he said. \"It will work across all your devices.\"\nThe response from Reddit users was mixed, with some making light of Gate's revelation and others sounding the alarm.\n\"Clippy 2.0?,\" wrote one user.\n\"Please...more like Clippy 2020,\" another replied.\n\"This technology you are developing sounds at its essence like the centralization of knowledge intake,\" a third user wrote. \"Ergo, whomever controls this will control what information people make their own. Even today, we see the daily consequences of people who live in an environment that essentially tunnel-visions their knowledge.\"\nShortly after, a Reddit user asked Gates how much of an existential threat superintelligent machines pose to humans. The question has been at the forefront of several recent discussions among prominent futurists. Last month, theoretical physicist Stephen Hawking said artificial intelligence \"could spell the end of the human race.\"\nSpeaking at the\u00a0MIT Aeronautics and Astronautics department's Centennial Symposium in October,\u00a0Tesla boss Elon Musk referred to artificial intelligence as \"summoning the demon.\"\nI think we should be very careful about artificial intelligence. If I were\u00a0to guess like what our biggest existential threat\u00a0is, it's probably\u00a0that. So we\u00a0need to be very careful with the artificial\u00a0intelligence. Increasingly scientists think there should be some regulatory oversight maybe at the national and international level, just to make sure that we don't do something very foolish. With artificial intelligence we are summoning the demon. In all those stories where there's the guy with the pentagram and the holy water, it's like yeah he's sure he can control the demon. Didn't work out.\nAfter gushing about the immediate future of technology on Reddit, Gates aligned himself with Musk and struck a more cautious tone.\n\"I am in the camp that is concerned about super intelligence,\" Gates wrote. \"First the machines will do a lot of jobs for us and not be super intelligent. That should be positive if we manage it well. A few decades after that though the intelligence is strong enough to be a concern. I agree with Elon Musk and some others on this and don't understand why some people are not concerned.\"\nOnce he finished addressing the potential demise of humankind, Gates got back to more immediate questions, like revealing his favorite spread to put on bread.\n\"Butter? Peanut butter? Cheese spread?\" he wrote. \"Any of these.\"\n             RELATED:\u00a0             Why the world's most intelligent people shouldn't be so afraid of artificial intelligence]\n               Elon Musk, Stephen Hawking, Google researchers join forces to avoid 'pitfalls' of artificial intelligence            \n               Stephen Hawking just got an artificial intelligence upgrade, but still thinks AI could bring an end to mankind            \n               Elon Musk: 'With artificial intelligence we are summoning the demon.'            \n","21":"Ebola sounds like the stuff of nightmares. Bird flu and SARS also send shivers down my spine. But I'll tell you what scares me most: artificial intelligence.\nThe first three, with enough resources, humans can stop. The last, which humans are creating, could soon become unstoppable.\nBefore we get into what could possibly go wrong, let me first explain what artificial intelligence is. Actually, skip that. I'll let someone else explain it: Grab an iPhone and ask Siri about the weather or stocks. Or tell her ''I'm drunk.'' Her answers are artificially intelligent.\u00a0\nRight now these artificially intelligent machines are pretty cute and innocent, but as they are given more power in society, these machines may not take long to spiral out of control.\nIn the beginning, the glitches will be small but eventful. Maybe a rogue computer momentarily derails the stock market, causing billions in damage. Or a driverless car freezes on the highway because a software update goes awry.\nBut the upheavals can escalate quickly and become scarier and even cataclysmic. Imagine how a medical robot, originally programmed to rid cancer, could conclude that the best way to obliterate cancer is to exterminate humans who are genetically prone to the disease.\nNick Bostrom, author of the book ''Superintelligence,'' lays out a number of petrifying doomsday settings. One envisions self-replicating nanobots, which are microscopic robots designed to make copies of themselves. In a positive situation, these bots could fight diseases in the human body or eat radioactive material on the planet. But, Mr. Bostrom says, a ''person of malicious intent in possession of this technology might cause the extinction of intelligent life on Earth.''\nArtificial-intelligence proponents argue that these things would never happen and that programmers are going to build safeguards. But let's be realistic: It took nearly a half-century for programmers to stop computers from crashing every time you wanted to check your email. What makes them think they can manage armies of quasi-intelligent robots?\nI'm not alone in my fear. Silicon Valley's resident futurist, Elon Musk, recently said artificial intelligence is ''potentially more dangerous than nukes.'' And Stephen Hawking, one of the smartest people on earth, wrote that successful A. I. ''would be the biggest event in human history. Unfortunately, it might also be the last.'' There is a long list of computer experts and science fiction writers also fearful of a rogue robot-infested future.\nTwo main problems with artificial intelligence lead people like Mr. Musk and Mr. Hawking to worry. The first, more near-future fear, is that we are starting to create machines that can make decisions like humans, but these machines don't have morality and likely never will.\nThe second, which is a longer way off, is that once we build systems that are as intelligent as humans, these intelligent machines will be able to build smarter machines, often referred to as superintelligence. That, experts say, is when things could really spiral out of control as the rate of growth and expansion of machines would increase exponentially. We can't build safeguards into something that we haven't built ourselves.\n''We humans steer the future not because we're the strongest beings on the planet, or the fastest, but because we are the smartest,'' said James Barrat, author of ''Our Final Invention: Artificial Intelligence and the End of the Human Era.'' ''So when there is something smarter than us on the planet, it will rule over us on the planet.''\nWhat makes it harder to comprehend is that we don't actually know what superintelligent machines will look or act like. ''Can a submarine swim? Yes, but it doesn't swim like a fish,'' Mr. Barrat said. ''Does an airplane fly? Yes, but not like a bird. Artificial intelligence won't be like us, but it will be the ultimate intellectual version of us.''\nPerhaps the scariest setting is how these technologies will be used by the military. It's not hard to imagine countries engaged in an arms race to build machines that can kill.\nBonnie Docherty, a lecturer on law at Harvard University and a senior researcher at Human Rights Watch, said that the race to build autonomous weapons with artificial intelligence -- which is already underway -- is reminiscent of the early days of the race to build nuclear weapons, and that treaties should be put in place now before we get to a point where machines are killing people on the battlefield.\n''If this type of technology is not stopped now, it will lead to an arms race,'' said Ms. Docherty, who has written several reports on the dangers of killer robots. ''If one state develops it, then another state will develop it. And machines that lack morality and mortally should not be given power to kill.''\nSo how do we ensure that all these doomsday situations don't come to fruition? In some instances, we likely won't be able to stop them.\nBut we can hinder some of the potential chaos by following the lead of Google. Earlier this year when the search-engine giant acquired DeepMind, a neuroscience-inspired, artificial intelligence company based in London, the two companies put together an artificial intelligence safety and ethics board that aims to ensure these technologies are developed safely.\nDemis Hassabis, founder and chief executive of DeepMind, said in a video interview that anyone building artificial intelligence, including governments and companies, should do the same thing. ''They should definitely be thinking about the ethical consequences of what they do,'' Dr. Hassabis said. ''Way ahead of time.''\n","22":"To the degree there was a human face of Watson, the \"Jeopardy!\" computer champion, it was David Ferrucci. He was the I.B.M. researcher who led the development of Watson, an artificial intelligence engine. The goateed computer scientist was always articulate and at ease in front of a camera or a microphone.\nDr. Ferrucci has left I.B.M. to join the giant hedge fund  Bridgewater Associates. And the weight of the Watson-related fame, it seems, played a role. \"I was so linked to the Watson achievement, and where I.B.M. was taking it, that I felt I was almost losing my identity,\" he said in a recent interview.\u00a0\nAfter Watson beat the best human Jeopardy champions in 2011, its artificial intelligence technology was directed toward new challenges, like assisting doctors in making diagnoses in a research project at the Cleveland Clinic.\nDr. Ferrucci led that next-generation Watson research as well. But he went to Bridgewater at the end of last year. Bridgewater, a private company, made no announcement of its new hire. Yet word of Dr. Ferrucci's departure from I.B.M. has been circulating among scientists in the artificial intelligence field. And I caught up with him recently for an interview, supplemented by a lengthy e-mail he titled, \"My Reflections.\"\nDr. Ferrucci, 51, said he had \"a great, great career\" at I.B.M., spanning 20 years, and \"they paid me very well.\"He said he \"never imagined myself at a hedge fund,\" but eventually the appeal of working in a smaller environment in an entirely new field for him - applying artificial intelligence to macroeconomic modeling - won him over.\nDr. Ferrucci said the more recent work he was doing at I.B.M., called WatsonPaths, was the direction he thought artificial intelligence research needed to go to make further advances, and it was the approach he saw Bridgewater pursuing to economic modeling.\nMuch of artificial intelligence today, he said, focuses on mining vast amounts of data to make predictions. Those predictions are based on statistical probabilities and patterns - a certain symptom is highly correlated with a certain disease, for example.\n\"But in a purely data-driven approach, I can't explain my decisions,\" Dr. Ferrucci said. \"People are so enamored with the data-driven approach that they believe correlation is sufficient.\"\nThe Big Data formula, he noted, has proved to be \"incredibly powerful\" for tasks like natural-language processing - a central technology behind Google search, for instance.\nWatsonPaths, by contrast, builds step-by-step graphs, or paths, that trace possible causes rather than mere statistical correlations. In the case of medicine at the Cleveland Clinic project, for example, the paths go from an observation of symptoms to a conclusion about the diagnosis of a disease and treatment.\nThat approach is a hybrid of the Big Data tools, which sift through troves of medical literature, and logic tools to identify likely chains of inference - what humans see as logical explanations for the \"why\" of things. The approach is also a step in the direction of classic artificial intelligence, which relied on knowledge rules and relationships, to create so-called expert systems. The blend combines elements of what Dr. Ferrucci termed \"my 30-year journey in A.I.\"\nAt Bridgewater, Dr. Ferrucci sees a similar path to modeling the economy and markets. \"Their approach to investment,\" he wrote in his e-mail, \"is based on a fundamental understanding of how the global 'economic machine' works.\"\nIts models, he added, are \"informed by but not blindly driven by the data.\" The opportunity, Dr. Ferrucci wrote, is to build \"predictive systems that fit perfectly with my interests. How cool is it to imagine a machine that can combine deductive and inductive processes to develop, apply, refine and explain a fundamental economic theory?\"\nCool, indeed. If successful, perhaps Bridgewater could share its findings with the Fed and the Treasury rather than merely trading for its own account. In the realm of policy, economic modeling and forecasting could use some help.\n\n","23":"MACHINES OF LOVING GRACEThe Quest for Common Ground Between Humans and Robots By John MarkoffIllustrated. 378 pp. Ecco\/HarperCollins Publishers. $26.99.  Technologists rarely question technology in public. Yet last fall Tesla's chief executive, Elon Musk, suggested we might need to regulate the development of artificial intelligence ''just to make sure that we don't do something very foolish.''\n  Surprisingly, Musk found support from some prominent technology leaders. Bill Gates said he didn't ''understand why some people are not concerned'' about what he called ''super intelligence.'' Stephen Hawking claimed that ''the development of full artificial intelligence could spell the end of the human race.''\u00a0\n  Over the past decade or so, we have seen some impressive demonstrations of artificial intelligence, including Watson, IBM's ''Jeopardy'' champion; Siri, Apple's personal assistant; and Google's self-driving car. We have also seen some clear evidence that smart technology is restructuring the industrial economy by doing certain kinds of work that require skill and judgment we traditionally associate with flesh and blood. \n  We may not need to follow Musk's call for regulation, but we probably need to assess the state of artificial intelligence and robotics, a task that John Markoff describes as looking for ''common ground between humans and robots'' in ''Machines of Loving Grace.''\n  The development and deployment of any technology is a complex process that involves a host of people with different interests, including researchers, engineers, regulators, bankers, business leaders and others. Markoff, a science and technology reporter for The New York Times, tries to find his common ground by focusing on the researchers who create the basic technology and how, he writes, they ''have grappled with questions about the deepening relationship between human and machine.'' Markoff concedes that designers and engineers are removed from the ultimate application of their work. They ''grow uncomfortable when asked about the potential consequences of their inventions and frequently deflect questions with gallows humor.'' \n  To help us understand these researchers, Markoff divides the field into two categories. The first consists of work that is trying to duplicate human behavior with computing systems: artificial intelligence. The second category, intelligence augmentation, consists of work that attempts to expand human abilities. \n  Markoff developed these two categories in ''What the Dormouse Said,'' his 2005 book on the influence of 1960s counterculture on technology and computing.   In ''Dormouse,'' he laid the foundation for ''Machines'' by telling the stories of the pioneering artificial intelligence researcher John McCarthy and Doug Engelbart, the engineer who invented the computer mouse. In his new book, Markoff updates his narrative with recent stories of artificial intelligence and intelligence augmentation, including the Google autonomous vehicle program, which began at Stanford University, and Apple's Siri, which started as a project at S.R.I. International, formerly known as the Stanford Research Institute. Along the way, he retells the stories of McCarthy, Engelbart and others.\n  Markoff mentions ''Dormouse'' in his preface to ''Machines,'' writing that his research for the earlier book gave rise to a paradox that he wanted to explore in this new one: ''The same technologies that extend the intellectual power of humans can displace them as well.'' But ''Machines of Loving Grace'' often returns to ideas from the foundational years of computer science, and the repeated stories sometimes make this new book seem as if it's looking backward rather than forward. As Markoff himself points out, the line between artificial intelligence and intelligence augmentation is sometimes fuzzy, ''with machines that will simultaneously augment and displace humans.'' Artificial intelligence technology from self-driving vehicles can be used as intelligence augmentation to help a driver avoid dangerous situations. Similarly, devices that are intended to augment human performance can easily allow one person to do the work of two.\n  The book ends with a too brief treatment of the problem that may ultimately bring large numbers of robots into our homes and lives: the need to care for the aging members of the baby boom generation. We may need to deploy smart machines to augment failing skills or to expand the population of nurses and companions. Personal care is one of the most human of activities. A full discussion of this subject would involve not only interviews with researchers but also conversations with nurses, business people and regulators -- only then might we be better equipped to determine whether a new machine can be made humane enough to care for us. \n              Like to be first? Get The New York Times Book Review before it appears online every Friday.                           Sign up for the email newsletter here.                              \n\n\n\n","25":"SEATTLE -- Never mind Terminator-like killer robots. Artificial intelligence researchers are grappling with more realistic questions like whether their creations will take too many jobs from humans.\nEight years after leading artificial intelligence scientists said their field did not need to be regulated, the question of government oversight has re-emerged as the technology has rapidly progressed. \n  On Tuesday, at an event sponsored by the White House Office of Science and Technology Policy, legal specialists and technologists explored questions about autonomous systems that would increasingly make decisions without human input in areas like warfare, transportation and health.\u00a0\n  Still, despite improvement in areas like machine vision and speech understanding, A.I. research is still far from matching the flexibility and learning capability of the human mind, researchers at the conference said.\n  ''The A.I. community keeps climbing one mountain after another, and as it gets to the top of each mountain, it sees ahead still more mountains,'' said Ed Felten, a computer scientist who is a deputy chief technology officer in the Office of Science and Technology Policy.\n  The half-day program, co-sponsored with the University of Washington School of Law and the university's Tech Policy Lab, was the first of four events the White House has planned that will focus on law and policy, as well as the social and economic implications of autonomous machine research.\n  ''These issues of A.I. and machines learning were popping up all over the government, and there was an opportunity to get more coordinated in how we address them,'' Dr. Felten said.\n  After 25 artificial intelligence researchers met in 2009, the group, sponsored by the Association for the Advancement of Artificial Intelligence, reported that there was no imminent danger from a technology that had prompted fears of Hollywood-style weapons and advanced economies that were devoid of human workers.\n  Leading technologists and scientists have also pondered the possibility that artificial intelligence, like genetic engineering, might soon constitute an existential threat to the human race.\n  In recent years, the debate has spread to the broader social impact of autonomous programs that will perform tasks like driving cars and offering medical and financial advice.\n  One challenge for A.I., according to a number of the researchers who spoke, is that the public perception about the threat of A.I. has largely been shaped by Hollywood.\n  ''Certainly, Hollywood has played a tremendous role with vision like Skynet,'' the computer network that turns on humans in the ''Terminator'' movies, said Oren Etzioni, chief executive of the Allen Institute for Artificial Intelligence, a nonprofit research group funded by the Microsoft co-founder Paul Allen. ''It's pretty much always the case in science fiction that A.I. is this monolithic entity that is scheming to take over.''\n  He cautioned that attention-getting feats like Google's AlphaGo program, which defeated a human champion in the board game Go, had plenty of humans behind the machine doing the work.\n  At the same time, despite the consensus about the limits of today's technology, many of the panelists wrestled with new challenges presented by A.I. systems that were appearing in virtually all walks of life, including services delivered by smartphones and algorithms that guide missiles.\n  Kate Crawford, a principal researcher at Microsoft Research, called on the industry to add ethics to the professional training of engineers. ''We need to start changing the skill set of the people who are going to be the data scientists of the future and the A.I. creators of the future,'' she said.\n  A.I. systems are pervasive, Ms. Crawford said, pointing to a doll like Hello Barbie, which speaks and listens.\n  ''You might think that's a fantastic toy, that's really wonderful,'' she said. ''What you don't realize is that it is the front to this huge data ingestion machine that is taking all of those statements by that child and then using them for a whole range of purposes.''\n\n\n\n","26":"Misconception: Computers will outstrip human capabilities within many of our lifetimes.\nActually: You won't be obsolete for a long time, if ever, most researchers say.\nIn March when Alphago, the Go-playing software program designed by Google's DeepMind subsidiary defeated Lee Se-dol, the human Go champion, some in Silicon Valley proclaimed the event as a precursor of the imminent arrival of genuine thinking machines.\nThe achievement was rooted in recent advances in pattern recognition technologies that have also yielded impressive results in speech recognition, computer vision and machine learning. The progress in artificial intelligence has become a flash point for converging fears that we feel about the smart machines that are increasingly surrounding us.\u00a0\nHowever, most artificial intelligence researchers still discount the idea of an \"intelligence explosion.\"\nThe idea was formally described as the \"Singularity\" in 1993 by Vernor Vinge, a computer scientist and science fiction writer, who posited that accelerating technological change would inevitably lead to machine intelligence that would match and then surpass human intelligence. In his original essay, Dr. Vinge suggested that the point in time at which machines attained superhuman intelligence would happen sometime between 2005 and 2030.\nRay Kurzweil, an artificial intelligence researcher, extended the idea in his 2006 book \"The Singularity Is Near: When Humans Transcend Biology,\" where he argues that machines will outstrip human capabilities in 2045. The idea was popularized in movies such as \"Transcendence\" and \"Her.\"\nRecently several well-known technologists and scientists, including Stephen Hawking, Elon Musk and Bill Gates, have issued warnings about runaway technological progress leading to superintelligent machines that might not be favorably disposed to humanity.\nWhat has not been shown, however, is scientific evidence for such an event. Indeed, the idea has been treated more skeptically by neuroscientists and a vast majority of artificial intelligence researchers.\nFor starters, biologists acknowledge that the basic mechanisms for biological intelligence are still not completely understood, and as a result there is not a good model of human intelligence for computers to simulate.\nIndeed, the field of artificial intelligence has a long history of over-promising and under-delivering. John McCarthy, the mathematician and computer scientist who coined the term artificial intelligence, told his Pentagon funders in the early 1960s that building a machine with human levels of intelligence would take just a decade. Even earlier, in 1958 The New York Times reported that the Navy was planning to build a \"thinking machine\" based on the neural network research of the psychologist Frank Rosenblatt. The article forecast that it would take about a year to build the machine and cost about $100,000.\nThe notion of the Singularity is predicated on Moore's Law, the 1965 observation by the Intel co-founder Gordon Moore, that the number of transistors that can be etched onto a sliver of silicon doubles at roughly two year intervals. This has fostered the notion of exponential change, in which technology advances slowly at first and then with increasing rapidity with each succeeding technological generation.\nAt this stage Moore's Law seems to be on the verge of stalling. Transistors will soon reach fundamental physical limits when they are made from just handfuls of atoms. It's further evidence that there will be no quick path to thinking machines.\nPHOTO:  (PHOTOGRAPH BY Zohar Lazar for The New York Times FOR THE NEW YORK TIMES)Related Articles\n\n","27":"Stephen Hawking, Elon Musk and a number of other tech luminaries from MIT, IBM and Harvard recently signed off on an open letter from the nonprofit Future of Life Institute warning about the perils of artificial intelligence. Without the appropriate safety measures built in, they argue, the rapid growth of artificial intelligence could end in disaster for humanity.\nOf course, it's easy to understand why AI has been giving rise to dystopian fears about the future from the world's most intelligent people. That's because the problem at the heart of AI is something that the supporters of the Future of Life\u00a0letter refer to as \"existential risk\" - the risk that very bad things can happen in the near future to wipe out the human race as a result of technology gone bad.\u00a0\n\"Existential risk\" is precisely what makes Hollywood sci-fi movies so scary. In last year's dystopian thriller \"Transcendence,\" for example, Johnny Depp morphs into a super-brain with the ability to wipe the human race off the planet. At about the same time the movie hit cinemas, Hawking bluntly warned about the risks of super-intelligence: \"I think the development of full artificial intelligence could spell the end of the human race.\"\nThe reason, Hawking told the BBC in an interview, is that, \"Once humans develop artificial intelligence, it will take off on its own and redesign itself at an ever-increasing rate. Humans, who are limited by slow biological evolution, couldn't compete and would be superseded.\" In short, if computers get too smart, it's game over for humans.\nIn the Future of Life letter, Musk and Hawking hint at a dystopian future in which humans have lost control of self-driving cars, drones, lethal weapons \u00a0and the right to privacy. Even worse, computers would become smarter than humans and at some point, would decide that humans really aren't so necessary after all. And they would do so not because they are inherently evil, but because they are so inherently rational - humans tend to make a mess of things.\nBut how likely is it, really, that an AI super-mind could wreak that kind of havoc and decide that humans are expendable?\nThe flip side of \"existential risk\" is \"existential reward\" - the possibility that very good things can happen in the near future as a result of exponential leaps in technology. For every Stephen Hawking and Elon Musk, there's a Ray Kurzweil or Peter Diamandis. People who focus on \"existential reward\" claim that AI will bring forth a utopian future, in which the human brain's full potential will be opened up, giving us the ability to discover new cures, new sources of energy, and new solutions to all of humanity's problems. Even the supporters of Future of Life's dystopian AI premise concede that there are a lot of positives out there, including the \"eradication of disease and poverty.\"\nIf you think about what AI has already accomplished, well, there's a lot more that can be done when super-intelligence is applied to the pressing humanitarian issues of the day. The Future of Life letter notes how far, how fast, we've already come. AI has given us\u00a0speech recognition, image classification, autonomous vehicles and machine translation. Thinking in terms of \"existential reward\" is what leads one to think about\u00a0the future as one of abundance, in which\u00a0AI helps - not hurts - humanity.\nThe types of AI safeguards alluded to by Hawking and Musk in the Future of Life open letter could make a difference in ensuring \"reward\" wins out over \"risk.\" In short, these safeguards could tilt the playing field in favor of humans, by ensuring that \"our AI systems must do what we want them to do.\"\nHowever, to view the debate over AI purely in terms of humans vs. the machines misses the point. It's not us vs. them in a race for mastery of planet Earth, with human intelligence evolving linearly and digital intelligence evolving exponentially. What's more likely is some form of hybrid evolution in which humans remain in charge but develop\u00a0augmented capabilities as a result of technology. One popular\u00a0scenario for sci-fi fans is one in which humans and computers ultimately merge into some sort of interstellar species, figure out how to leave planet Earth behind on a new mission to colonize the galaxy and live happily ever after.\nWhen a technology is so obviously dangerous - like nuclear energy or synthetic biology - humanity has an imperative to consider dystopian predictions of the future. But it also has an imperative to push on, to reach its full potential. While it's scary, sure, that humans may no longer be the smartest life forms in the room a generation from now, should we really be that concerned? Seems like we've already done a pretty good job of finishing off the planet anyway. If anything, we should be welcoming our AI masters to arrive sooner rather than later.\n","28":"Before reading the article: \nLook at the Daily 360 Times video \"Can Artificial Intelligence Fly a Plane?\" below.\nHow would you answer the narrator's question about birds and flying, \"What would it take for us to reproduce such kind of intelligence in machines?\"\nCan you think of any examples of artificial intelligence in action? If so, what are they and how did you learn about them?\nNow, read the article, \"Tech Giants Are Paying Huge Salaries for Scarce A.I. Talent,\" and answer the following questions: \n1. Why are tech companies paying large salaries to artificial intelligence specialists?\n2. How many people in the world have the skills necessary to tackle serious artificial intelligence research, according to Element AI, an independent lab in Montreal?\n3. What are some examples of the A.I. projects that companies are working on?\n4. What are deep neural networks and what sorts of things can they accomplish?\n5. What is happening in academia in regard to A.I. professors -- for example, at Carnegie Mellon, Stanford University and the University of Washington?\n6. How has Luke Zettlemoyer of the University of Washington found a way to continue teaching and also capitalize on an offer from private industry?\nFinally, tell us more about what you think:\nDo you think you would find a career in artificial intelligence interesting? Why or why not? Would you be interested in classes that aim to teach \"deep learning\" and related techniques, like the one described here\nWhat creative ideas do you have to help solve the shortage of people with top-notch artificial intelligence skills?\n","29":"SEATTLE -- Microsoft said on Thursday that it was reorganizing part of the company to better position itself as one of the significant players in the emerging field of artificial intelligence.\u00a0\nThe company has created a new organization that combines its research group, one of the largest in the technology industry, and a number of products that rely on artificial intelligence, including its Bing search engine and Cortana virtual assistant. The new artificial intelligence and research group at Microsoft will have more than 5,000 employees. \n  Microsoft also said that one of its top executives, Qi Lu, has left the company to recuperate from a serious bicycling accident that occurred several months ago. Once he recovers, Mr. Lu will continue to act as an adviser to Satya Nadella, Microsoft's chief executive, and Bill Gates, its co-founder, Mr. Nadella said in an email to company employees Thursday.\n  The creation of a new group at Microsoft with a focus on artificial intelligence was already planned, but the departure of Mr. Lu -- a respected computer scientist who spent a decade at Yahoo -- affected the shape of the new organization. In addition to Microsoft's Office products and other applications, Mr. Lu oversaw Bing, which is part of the new artificial intelligence and research group.\n  ''Microsoft is really betting the company on A.I.,'' Harry Shum, the Microsoft executive vice president who will oversee the new group, said in a phone interview.\n  Technology companies are making deep investments in artificial intelligence, seeing it as a key ingredient in the emergence of everything from self-driving vehicles to devices that can identify faces. Microsoft has been particularly vocal in the last year about the importance of artificial intelligence to its future.\n  It is investing heavily to improve virtual assistants like Cortana that can answer verbal questions and proactively perform tasks and it plans to weave more intelligent functions into all of its products, including Skype and Office.\n  The company is sometimes criticized by investors for not producing enough commercial successes from its vaunted research organization. The structure of the new organization will tighten the link between its researchers and products already on the market today.\n  ''We see need to accelerate the cycle from developing research technology to eventually shipping technology to our customers,'' Mr. Shum said.\n\n\n\n","30":"The five robots that successfully navigated a 132-mile course in the Nevada desert last weekend demonstrated the re-emergence of artificial intelligence, a technology field that for decades has overpromised and underdelivered.\n  At its low point, some computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.\n But the work of a small team of researchers at the Stanford Artificial Intelligence Laboratory is helping to restore credibility to the field. The team's winning robotic Volkswagen, named Stanley, covered the unpaved course in just 6 hours and 53 minutes without human intervention and guided only by global positioning satellite waypoints. \u00a0\n  The feat, which won a $2 million prize from the Pentagon Defense Advanced Research Project Agency, was compared by exuberant Darpa officials to the Wright brothers' accomplishment at Kitty Hawk, because it was clear that it was not a fluke. Twenty-two of the 23 vehicles that started this year did as well or better than the seven miles completed by the best vehicle last year.\n  The ability of the vehicles to complete a complex everyday task -- driving -- underscores how artificial intelligence may at last be moving beyond the research laboratory.\n  While artificial intelligence technology is already in use in telephone answering systems with speech recognition and in popular household gadgets like the iRobot vacuum cleaner, none of the existing systems have been as ambitious as Darpa's Grand Challenge road race.\n  This leap was possible, in large part, because researchers are moving from an approach that relied principally on logic and rule-based systems to more probability or statistics-oriented software technologies.\n  ''In the past A.I. has been dominated by symbolic systems and now the world is gray,'' said Terrence J. Sejnowski, head of the computational neurobiology laboratory at the Salk Institute in La Jolla, Calif. ''That's what it's like to deal with the real world.'' \n  This crucial shift, Mr. Sejnowski said, ''grew out of the recognition that the human brain is very good at this, why not have machines do the same thing?''\n  New artificial intelligence systems -- like that embodied in Stanley -- are now capable of evaluating a huge amount of data from sensors and then making probabilistic decisions.\n  ''The prior opinion of many informed observers, based on decades of disappointing experimental results, was that the problems were so hard that they would remain unsolved for many decades yet,'' said Hans Moravec, a Carnegie Mellon University robotics researcher who was one of the nation's first developers of autonomous vehicles during the 1970's. ''But now everyone knows differently,'' he said. ''The interest, effort and investment in the broader field is sure to skyrocket.''\n  The Stanford lab has long been at the forefront of A.I. research. The first autonomous vehicle, based on a vehicle salvaged from the NASA lunar landing program, was created at the lab and took its first baby steps in 1975. By the late 1970's, the robotic vehicle was capable of moving about two feet at a time in one-second spurts, pausing for half a minute to compute between attempting the next movement. \n  Until recently, progress in artificial intelligence lagged so far behind computing technology that some in the field talked about an ''A.I. winter,'' after commercial and government funding evaporated in the mid-1980's.\n  Now there is talk about an A.I. spring among researchers like Sebastian Thrun, the director of the Stanford lab. \n  ''The amount of journalistic interest and investor interest has fluctuated wildly,'' said John McCarthy, a pioneer in the field and now professor emeritus in the computer science department at Stanford University. ''A.I. has continued all along, thanks to the interest among researchers and the continued support of government agencies, especially Darpa.''\n  The enthusiasm is already spreading. Researchers point out that an obvious and powerful application for A.I. technology is in automobile safety systems. \n  ''Any time you create a technology that has the potential of saving 20,000 to 30,000 lives in a year, one has to sit up and take notice,'' said Raj Reddy, a professor of computer science and robotics at Carnegie Mellon University. ''If you look at automotive accidents in the United States, the repair bill is about $55 billion each year.''\n  The potential of the application is directly relevant to Volkswagen, the German car manufacturer who was one of the research sponsors of the Stanford team.\n  The company has put a high priority on what it refers to as driver-assistance systems, which are now capable of providing intelligent cruise control and lane ''departure'' warnings, two systems that will be crucial for driver safety in coming years.\n  ''We can take a lot of the approaches used in Stanley and adapt them,'' Sven Strohband, senior research engineer at the Volkswagen Electronics Research Laboratory in Palo Alto, Calif. ''It's a nice fresh wind of ideas.''\n  The public visibility of the Grand Challenge is a big boost for Darpa, but may also show that the agency's current funding approach is a poor strategy. It has shifted money away from universities and experimental projects and toward work that is classified or done through military contractors. The victory of the Stanford team is proof of what can be done by a motivated team of scientific researchers on a relatively small budget.\n  ''This is consistent with the history of our field,'' said David A. Patterson, a computer scientist at the University of California, Berkeley, who is president of the Association for Computing Machinery. ''This demonstrates the importance of the participation of government-funded academics.''\n","31":"Yann LeCun is director of artificial intelligence research at Facebook, and founding director of New York University's Center for Data Science. \nHe is the author of more than 180 academic papers, and has created character recognition technology widely used by banks to verify checks.\nHe is also one of the preeminent developers of so-called deep learning, a dramatic advance in computer-based understanding. \nIn the long run, he said in this condensed and edited conversation, advanced computing techniques will create digital partners that will accompany us throughout life.\nBelow, a conversation with Mr. LeCun:\u00a0\n Q. \nWhat is the significance of artificial intelligence?\n A. \nA.I. is how we will make sense of all of the information that will be out there in the digital world. A lot of interaction with each other and with the digital world will come from what you could call \"digital companions,\" that will help us work through things.\n Q. \nWhat does that mean for Facebook?\n A. \nFacebook is in the business of connecting people, giving them the information that is informative, entertaining, necessary even if painful, to help them reach their goals. Based on the amount of posts, pictures and news items someone typically gets, we could show you 2,000 things a day. But people's time is precious, and we can only show about 100 to 150 things a day. They should be the most useful ones.\nTo do that efficiently we have to know what is in the content. We do that by labeling images, recognizing faces or classifying text. And we have to know your interests, what you want to do, who your friends are in different situations.\n Q. \nWhat will that look like in the future?\n A. \nI'm not saying this is a future product, but a way to think about this if there is an intelligent digital companion that allows you to think about things in a new way, the way you interact with friends, expand your thinking. There will be a single point of control that knows and respects your private information.\nThis kind of A.I. will come progressively. In some ways, parts of this are already there in the Facebook News Feed, in Apple's Siri, or MicrosoftCortana. They are shallow now, in the kind of interactions you are having. They are somewhat scripted. \n Q. \nHow does deep learning, or the ability for a machine to figure out things on its own, work?\n A. \nDeep learning enables complex interactions. The machine has to go back through different levels of problem solving and think of the step it took when things became different. When you are playing chess, at some point you make a mistake, you may go back several steps that were \"correct\" to find the one that was wrong. When you fall off a bicycle, you think of when you lost your balance. Deep learning does that. The credit assignment in a deep learning exercise can be tens, even hundreds, of levels deep. \n Q. \nYou make it sound so easy.\n A. \nWe have a way of representing objects by what we call vectors, which are long strings of numbers. The vector for \"cat\" is similar to the vector for \"dog,\" so that is a close relationship. It would be further apart for a symbolic object. The system seeks associations to determine meaning.\n Q. \nHow do you create a system that has good vectors, that is, can chose to represent a word differently, depending on the context? How do you make it learn rules of language, for example? \n A. \nTomas Mikolov, who did graduate work and was at Google before he came here, has done a lot of work on vector-based language relations, what we call \"word to vec.\" You show a sequence of words, one leading logically to another over, say, 11 words. You can ask it what the word in the middle is, and it will predict it. In doing so the system is learning to represent individual words to overall meanings.\nWe are working on a vector representation of a language that you can use for another language. That involves figuring out how to make a text meaningful for another person. \n Q. \nYou could work anywhere. Why Facebook?\n A. \nSolving A.I. will require contributions from the tech industry, academics and the government. And it has to be done in the open. There are very few companies that can do this work. \nApple is completely secretive, so it's not a good place to try and do this. Google is partly secretive. They have a culture of hybrid research, alongside engineers and don't say much. Google X is a secretive research modell you can't expect a breakthrough from there. Deep Mind, Google's A.I. work in the U.K., is more open, so I'm more optimistic about the model there.\nFacebook has a culture of openness in its DNA. A lot of our software and hardware are open. It sees itself as a fundamentally open company, and in the business of connecting people. We release a lot of code on open source, publish a lot on what we do. \nHow we will deploy A.I. commercially, we won't talk about that. It's not important to our research, and we have to keep a competitive advantage.\n Q. \nWhat is the long-term goal? \n A. \nYou can't have intelligence without motivations or emotions.\n Q. \nYou can't render emotions in software.\n A. \nO.K., lets talk about emotions. People have lots of mental representations of the world that they learn. We are prediction machines, and we change the world to be in a state that we like. What are emotions, but registers of things we like or don't like? You could assign values to these.\nRight now, you eat, reproduce, avoid pain, to have or avoid outcomes. You think, \"If I don't go to school, then life will be painful,\" so you go. We make these predictions, and yes, a lot of the time we are conflicted. There is no reason to think we can't encode this in a machine. \n Q. \nWhat is the big challenge to proving this is possible? \n A. \nThere are major conceptual advances that we still don't know how to make. One of the biggest is how we do unsupervised learning.\nSupervised learning is like when you train a computer to recognize images of dogs or cars. Reinforcement learning is where you don't tell the machine the correct answer. Instead, you just score performance. The machine figures out the rules by figuring out where it made a mistake. \nUnsupervised learning is what humans do a lot: learn about how the world works by having an interest in things. A baby learns that when you put a toy behind a box, the toy is still in the world. Humans and animals have that capacity. Contrast that with machines, where most learning is still supervised. We don't have a good grand model yet. \n\"Vec to word\" is perhaps a kind of unsupervised learning. We are still missing a basic principle of what unsupervised learning should be built upon. \n","32":"Allen Newell, a founder of the field of artificial intelligence and a leader in the study of human thinking, died yesterday at Montefiore Hospital in Pittsburgh. He was 65 years old and lived in Pittsburgh.\nHe died of cancer, a spokeswoman for the family said.\u00a0\n Dr. Newell, a professor at Carnegie Mellon University in Pittsburgh, was considered one of the four fathers of artificial intelligence and an expert on how people think. He earned an international reputation for developing programs for complex information processing. He was the founding president of the American Association for Artificial Intelligence and had also headed the Cognitive Science Society.\nLast month, President Bush awarded him the National Medal of Science.\nDr. Newell's interest in artificial intelligence grew from his work in the early 1950's on a Rand Corporation project for the Air Force to simulate an early warning monitoring station with radar screens and a crew. In trying to predict how the crew members would react, he became fascinated with the puzzle of how people think and make decisions.\nAlong with Dr. Newell and Dr. Simon, Marvin Minsky from the Massachusetts Institute of Technology and John McCarthy from Stanford University are considered the pioneers in artificial intelligence by experts.\nIn the 1980's, Dr. Newell developed Soar, a sophisticated software system that learns and solves problems in a manner similar to human mental processes. He was the author or co-author of 10 books and more than 200 academic articles. His last book was \"Unified Theories of Cognition\" (Harvard, 1990).\nDr. Newell was born in San Francisco. He served two years in the Navy at the end of World War II. He graduated from Stanford University in 1949 and earned a doctorate in industrial administration at Carnegie in 1957.\nHe worked for Rand from 1950 to 1961, when he joined Carnegie. There he helped create the School of Computer Science and continued as a professor until his death.\nHis survivors include his wife of 45 years, the former Noel McKenna; a son, Paul, of Manhattan Beach, Calif., and a sister, Ann, of Pennsylvania.\n\n","33":"The technologies that now fly under the banner of Big Data are undeniably advancing across the economy, well beyond their early stronghold in consumer Internet companies like Google, Amazon and Facebook. Exploring that evolution is the main theme of the articles in a special section of The New York Times on Thursday.\u00a0\nBut what about the term Big Data itself? What is its likely life span?\nI've written about the origins of the term before. The first person to use Big Data in its current meaning, it seems, was John Mashey, chief scientist of Silicon Graphics in the 1990s. But that is a probability, not a certainty - appropriately enough, since Big Data is all about probabilities and correlations.\nAs to its longevity, Big Data, I'm betting, will cycle out of general use over time. Not because it is a catchall marketing term, which it is, among other things. The best marketing and sales language is distilled communication.\nMy bet against Big Data, I suppose, boils down to a linguistic bias. It is too straightforward. It lacks the whiff of poetry, the tension that tightens the bond between words in a phrase. By way of contrast, look at \"artificial intelligence.\" Its inspiration was a sales pitch of sorts.\nIn 1955, John McCarthy, a mathematician and computer scientist, was seeking funds from the Rockefeller Foundation for a conference the following year that would explore the growing excitement that computers might become more than big number-crunching calculators - that they might, in their way, actually be able to mimic human thought.\nIn an interview 45 years later, Mr. McCarthy explained that he \"just cooked up the phrase\" when drafting the grant proposal. As Mr. McCarthy recalled, \"My idea was to nail the flag to the mast, as it were.\"\nMr. McCarthy, who died two years ago,certainly did that. Artificial intelligence was a deft turn of phrase in the 1950s, and one that took on greater meaning over the years, evoking both the inspiring ambition of science and unnerving qualms about machine intelligence.\nBut the history of artificial intelligence is instructive in another way. The enthusiasm for the technologies of artificial intelligence, or A.I., has gone through up and down cycles, and at times they were very down. There were two long stretches when investment and interest in artificial intelligence fell sharply - roughly 1974-80 and 1987-93. Those years were known as \"A.I. winters.\" Researchers scrambled to find other names for projects, anything but artificial intelligence.\nA similar pattern seems likely for the technologies of Big Data. There may be up and down cycles, the term Big Data may fall from favor, but the technology itself will keep progressing. Indeed, the tools of artificial intelligence, like machine learning, are behind the promise of Big Data, which is to find useful insights in an ever-growing universe of digital data.\n\n","34":" In \"Scientists See Promise in Deep-Learning Programs,\" John Markoff writes about advances in artificial intelligence. \n WHAT advances have recently been made by artificial intelligence? WHAT is deep learning? WHAT are some examples mentioned in the article? WHAT are \"neural nets\"?\u00a0\n WHO is Yann LeCun?\n WHEN did deep-learning programs gain a lot of speed and accuracy? WHEN did scientists first predict that an artificial intelligence system would be working? WHEN was the so-called \"A.I. winter\"?\n HOW was deep-learning software used by the winners of the top prize in a contest sponsored by Merck ?\n WHY was their entry \"particularly impressive\"?\n WHERE did Microsoft's Richard F. Rashid demonstrate a deep-learning program that \"led to stunned applause\"?\n Related: Our lesson plan \"What Lies Ahead? Predicting the Future of Computing\" and collection of resources on teaching with technology\n\n","36":"Like a good gambler, Daphne Koller, a researcher at Stanford whose work has led to advances in artificial intelligence, sees the world as a web of probabilities.\n  There is, however, nothing uncertain about her impact. \n  A mathematical theoretician, she has made contributions in areas like robotics and biology. Her biggest accomplishment -- and at age 39, she is expected to make more -- is creating a set of computational tools for artificial intelligence that can be used by scientists and engineers to do things like predict traffic jams, improve machine vision and understand the way cancer spreads.\n  Ms. Koller's work, building on an 18th-century theorem about probability, has already had an important commercial impact, and her colleagues say that will grow in the coming decade. Her techniques have been used to improve computer vision systems and in understanding natural language, and in the future they are expected to lead to an improved generation of Web search. \u00a0\n  ''She's on the bleeding edge of the leading edge,'' said Gary Bradski, a machine vision researcher at Willow Garage, a robotics start-up firm in Menlo Park, Calif.\n  Ms. Koller was honored last week with a new computer sciences award sponsored by the Association for Computing Machinery and the Infosys Foundation, the philanthropic arm of the Indian computer services firm Infosys. \n  The award to Ms. Koller, with a prize of $150,000, is viewed by scientists and industry executives as validating her research, which has helped transform artificial intelligence from science fiction and speculation into an engineering discipline that is creating an array of intelligent machines and systems. It is not the first such recognition; in 2004, Ms. Koller received a $500,000 MacArthur Fellowship.\n  Ms. Koller is part of a revival of interest in artificial intelligence. After three decades of disappointments, artificial intelligence researchers are making progress. Recent developments made possible spam filters, Microsoft's new ClearFlow traffic maps and the driverless robotic cars that Stanford teams have built for competitions sponsored by the Defense Advanced Research Projects Agency. \n  Since arriving at Stanford as a professor in 1995, Ms. Koller has led a group of researchers who have reinvented the discipline of artificial intelligence. Pioneered during the 1960s, the field was originally dominated by efforts to build reasoning systems from logic and rules. Judea Pearl, a computer scientist at the University of California, Los Angeles, had a decade earlier advanced statistical techniques that relied on repeated measurements of real-world phenomena.\n  Called the Bayesian approach, it centers on a formula for updating the probabilities of events based on repeated observations. The Bayes rule, named for the 18th-century mathematician Thomas Bayes, describes how to transform a current assumption about an event into a revised, more accurate assumption after observing further evidence. \n  Ms. Koller has led research that has greatly increased the scope of existing Bayesian-related software. ''When I started in the mid- to late 1980s, there was a sense that numbers didn't belong in A.I.,'' she said in a recent interview. ''People didn't think in numbers, so why should computers use numbers?''\n  Ms. Koller is beginning to apply her algorithms more generally to help scientists discern patterns in vast collections of data.\n  ''The world is noisy and messy,'' Ms. Koller said. ''You need to deal with the noise and uncertainty.''\n  That philosophy has led her to do research in game theory and artificial intelligence, and more recently in molecular biology.\n  Her tools led to a new type of cancer gene map based on examining the behavior of a large number of genes that are active in a variety of tumors. From the research, scientists were able to develop a new explanation of how breast tumors spread into bone. \n  One potentially promising area to apply Ms. Koller's theoretical work will be the emerging field of information extraction, which could be applied to Web searches. Web pages would be read by software systems that could organize the information and effectively understand unstructured text.\n  ''Daphne is one of the most passionate researchers in the A.I. community,'' said Eric Horvitz, a Microsoft researcher and president of the Association for the Advancement of Artificial Intelligence. ''After being immersed for a few years with the computational challenges of decoding regulatory genomics, she confided her excitement to me, saying something like, 'I think I've become a biologist -- I mean a real biologist -- and it's fabulous.' ''\n  To that end, Ms. Koller is spending a sabbatical doing research with biologists at the University of California, San Francisco. Because biology is increasingly computational, her expertise is vital in gaining deeper understanding of cellular processes.\n  Ms. Koller grew up in an academic family in Israel, the daughter of a botanist and an English professor. While her father spent a year at Stanford in 1981 when she was 12, she began programming on a Radio Shack PC that she shared with another student. \n  When her family returned to Israel the next year, she told her father, the botanist, that she was bored with high school and wanted to pursue something more stimulating in college. After half a year, she persuaded him to let her enter Hebrew University, where she studied computer science and mathematics. \n  By 17, she was teaching a database course at the university. The next year she received her master's degree and then joined the Israeli Army before coming to the United States to study for a Ph.D. at Stanford.\n  She didn't spend her time looking at a computer monitor. ''I find it distressing that the view of the field is that you sit in your office by yourself surrounded by old pizza boxes and cans of Coke, hacking away at the bowels of the Windows operating system,'' she said. ''I spend most of my time thinking about things like how does a cell work or how do we understand images in the world around us?''\n  In recent years, many of her graduate students have gone to work at Google. However she tries to persuade undergraduates to stay in academia and not rush off to become software engineers at start-up companies.\n  She acknowledges that the allure of Silicon Valley riches can be seductive. ''My husband still berates me for not having jumped on the Google bandwagon at the beginning,'' she said. Still, she insists she does not regret her decision to stay in academia. ''I like the freedom to explore the things I care about,'' she said.\n","37":"Could artificial intelligence be the new climate change? Is it a dangerous threat that human will overlook until it is too late?\nStuart Russell, a computer science professor at the University of California\u00a0at\u00a0Berkeley, and leading thinker on the perils of artificial intelligence, noted the parallels Tuesday at an Information Technology and Innovation Foundation event on whether superintelligent computers are a threat to humanity.\n\"If you went back to the late 19th century and said to people look this development of the internal combustion engine and coal fire electrical generation. This combination is going to lead to something that in a few year's time we're going to call global warming and you'll be sorry. So maybe you should start thinking about how to prevent a rise in carbon dioxide and how to generate alternative methods such as solar and wind power,\" Russell said. \"If we had started in the late 19th century I think we would've had a chance at preventing it.\"\u00a0\nThe Industrial Revolution brought a lot of good, but also created the problem of climate change. Experts have warned we won't be able to reverse its effects. So what huge problems will the Digital Revolution bring, and can we stop them?\nArtificial\u00a0intelligence has become a hot field with the interest of top tech companies such as Google and Facebook, which are hoarding top talent. DeepMind, a London start-up Google purchased last year, has shown remarkable progress, developing an algorithm that can teach itself to beat Atari video games. The next challenge is racing games from the 1990s, then one day maybe it can teach itself to drive our cars.\nFacebook chief executive Mark Zuckerberg said Tuesday in an online chat that Facebook's \"goal is to build [artificial intelligence] systems that are better than humans at our primary senses: vision, listening, etc.\"\nThe question is where does all of this lead us?\nZuckerberg and Google executives are quick to emphasize the positives. An artificial intelligence system could drive a blind person's car, increasing their mobility. A system could scan a blind person's Facebook Newsfeed and describe photos to them.\nYes, those are great examples of positives uses of technology. Of course, inventors of technologies tend to be much better at identifying the upsides of their creations, and don't dwell on the negatives.\nResearchers at the Global Challenges Foundation called artificial intelligence one of the greatest threats to humanity, with a zero-to-10 percent chance of wiping out human civilization. We've heard warnings from Elon Musk, Stephen Hawking and Bill Gates.\nGeorgia Tech professor Ronald Arkin, who also spoke at Tuesday's event, warned against trusting engineers to protect us from the hazards of artificial intelligence.\n\"It took me a long time, years - decades perhaps - to realize that,\" Arkin said. \"Not all our colleagues are concerned with safety. The important thing is you can not leave this up to the AI researchers. You can not leave this up to the roboticists. We are an arrogant crew, and we think we know what's best and the right way to do it, but we need help.\"\nOne valuable aspect of Russell's perspective is his ability to simplify the subject matter in a way that most of us can understand. When we hear about self-replicating bots learning at a Moore's Law pace, we may be hearing an accurate description of one of mankind's greatest risks. But given that most people aren't familiar with terms such as algorithms and Moore's Law, the warning might as well be told to us in Greek. And if no one understands the next climate change, there will be little\u00a0motivation to work to prevent it.\nOn Tuesday, Russell brought up King Midas, the character in Greek mythology who thought it would be wonderful if everything he touched turn to gold. It sounds like a great superpower before he had it. After the fact he was full of regret - but there was no going back - and that's the risk with artificial intelligence, according to Russell.\nArtificial\u00a0intelligence systems will be programmed to carry out goals, and with levels of intelligence that exceed humans, they'll likely act to prevent anyone from shutting them down. Because they have capabilities that exceed ours, it will be extremely difficult to defeat them.\n\"It's already outthought you,\" Russell noted. \"The system has spread itself out onto the Web. It now exists in tens of millions of copies in hundreds of millions of machines \u00a0and it's already out-thought you. So it's not easy to shut it down.\"\nRussell said that research currently isn't being done to prevent such situations, but that he's reasonably optimistic that things will work out. He pointed to interest from the Defense Department and National Science Foundation to fund such research, and the increased warnings coming from observers.\n","38":"Machine learningScholars to study artificial intelligence\u00a0\nStanford University is anchoring a study to examine the long-term effects of artificial intelligence. \nLed and funded by Eric Horvitz, managing director of Microsoft research and a Stanford University alumnus, the 100-year study will be overseen by a committee with rotating members who will track progress at five-year intervals. \u00a0\nThey plan to pay close attention to focus on how artificial intelligence affects national security, psychology, ethics, law, privacy and democracy, among other topics.\nSo far, professors from Stanford, Harvard University, Carnegie Mellon University, the University of California at Berkeley and the University of British Columbia are joining. \n\"[W]e feel obliged and qualified to host a conversation about how artificial intelligence will affect our children and our children's children,\" Stanford President John Hennessy said in a statement. \n- Mohana RavindranathMobile devicesGAO: Agencies make progress in mobile \nAll 24 federal agencies required to comply with provisions from the Digital Government Strategy - a 2012 plan for modernizing services - have made progress on that goal, according to a recent report from the Government Accountability Office.\nThe report examined efforts within agencies to make their site and services more easily accessible via smartphones and tablets. For instance, the GAO found that the number of visitors using smartphones or tablets to access the Interior Department's sites increased from 57,428 visitors in 2011 to 1,206,959 in 2013, the report said.  The report also analyzed digital efforts by the Federal Emergency Management Agency and the Transportation Department, among others.\n- Mohana Ravindranath\n","39":"The vast majority of Americans expect artificial intelligence to lead to job losses in the coming decade, but few see it coming for their own position.\nAnd despite the expected decline in employment, the public widely embraces artificial intelligence in attitude and in practice. About five in six Americans already use a product or service that features it, according to a survey that was conducted last fall and from which new findings were released on Tuesday. \u00a0\n  ''Whether they know it or not, A.I. has moved into a big percent of Americans' lives in one way or another already,'' said Frank Newport, the editor in chief of Gallup, which conducted the survey with Northeastern University.\n  The study defines artificial intelligence as any technology that can perform a task as humans do. But even if that definition feels overly broad, the findings reveal just how ubiquitous certain products have become in American life.\n  About 84 percent of Americans, for example, use navigation apps like Google Maps, Waze and Apple Maps, the study found. About 72 percent of respondents said they streamed music or video with services like Netflix and Pandora.\n  Nearly half said they used personal assistants on their smartphones, while about 32 percent use ride-sharing apps. Some 22 percent used intelligent home personal assistants, like Google Home or Amazon's Alexa. Twenty percent said they use a smart home device, such as a self-learning light or thermostat.\n  The technologies were used most widely among younger and more educated Americans: More than 90 percent of adults with at least a bachelor's degree or between the ages of 18 and 35 used navigation apps, for example.\n  The report that was released on Tuesday reflects only some of the findings of a large survey of nearly 3,300 American adults conducted in September and October.\n  The other findings, released in January, show that more than three in four Americans believe that artificial intelligence will fundamentally change how the public works and lives in the coming decade.\n  About the same share expect artificial intelligence to destroy more jobs than it creates, though only about one in four were worried about losing their own job.\n\n\n\n","40":"Ray Solomonoff, a physicist who was one of the founders of the field of artificial intelligence, died on Dec. 7 in Boston. He was 83 and had homes in New Ipswich, N.H., and Cambridge, Mass.\n  The cause was a ruptured brain aneurysm, said his wife, Grace.\n  As a child Mr. Solomonoff developed what would become a lifelong passion for mathematical theorems, and as a teenager he became captivated with idea of creating machines that could learn and ultimately think.\u00a0\n  In 1952 he met Marvin Minsky, a cognitive scientist who was also exploring the idea of machine learning, and John McCarthy, a young mathematician. In 1956 he became one of the 10 scientists who took part in the original Dartmouth Summer Research Project, whose organizers included Mr. Minsky and Mr. McCarthy, and which coined the term ''artificial intelligence'' and was instrumental in creating the field. \n  In 1960 Mr. Solomonoff developed the idea of algorithmic probability, which emerged from his effort to grapple with a problem of induction: Given a long sequence of symbols describing real-world events, how can you extrapolate the sequence? The idea gave rise to a new approach to probability theory. \n  Mr. Solomonoff went on to pioneer the application of probability theory to solving artificial intelligence problems. But in the 1960s and 1970s he was ahead of his time, and the approach initially had little impact on the field. More recently, probability theory has caught on among artificial intelligence researchers; it is now the dominant approach.\n  ''Ray did early work on the theoretical foundations of learning systems, focused on understanding how to generate and assign probabilities to sequences of symbols -- which could be mapped to the challenge of predicting what comes next, given what you've seen so far,'' said Eric Horvitz, a Microsoft computer scientist and a former president of the Association for the Advancement of Artificial Intelligence. ''Beyond his core technical work, he was an omnipresent and passionate proponent of the probabilistic approach to A.I., on the promise of building intelligent computing systems that could learn and reason under uncertainty.'' \n  His work in the early 1960s predated the research of the Russian mathematician Andrei Kolmogorov, who also did pioneering research in information theory and later acknowledged Mr. Solomonoff's prior contributions.\n  Mr. Solomonoff would later turn his attention to the consequences of artificial intelligence and in 1985 wrote a paper that speculated on the cost and the time it would take to develop a machine with many times the intelligence of a group of humans. He called this the ''infinity point.'' The idea predated the prediction of the computer scientist Vernor Vinge, who in 1993 speculated on a similar evolution in machine intelligence, which he called ''the singularity.''\n  Born in Cleveland on July 25, 1926, Mr. Solomonoff was the son of Russian immigrants, Julius and Sarah Solomonoff. He studied physics at the University of Chicago and graduated with a master's degree in 1951.\n  Fiercely independent, he would remain self-employed for much of his life, taking a variety of visiting scholar positions. In 2001 he was a visiting professor at the Dalle Molle Institute for Artificial Intelligence in Lugano, Switzerland, and more recently he was a visiting professor at the Computer Learning Research Center at Royal Holloway, University of London.\n  He is survived by his wife.\n  Mr. Solomonoff enjoyed building things, and in the 1960s he built himself a home in New Hampshire. It was heated by two rows of light bulbs in the ceiling, a feat made possible by thick insulation and inserts to cover the windows.\n","41":"During a 1950s encounter at MIT, Marvin Minsky, one of the fathers of research on artificial intelligence, declared: \"We're going to make machines intelligent. We are going to make them conscious!\" To which Douglas Engelbart, another early Information Age icon, reportedly replied: \"You're going to do all that for the machines? What are you going to do for the people?\"\nThe Minsky-Engelbart exchange captures the tension that has continued to dog development of artificial intelligence. What should be the goal of engineers who create thinking machines? Engelbart  proposed building smart devices that could augment human capabilities (intelligence augmentation, or IA) as an alternative to the pursuit of autonomous machines with  thinking powers that might equal or  exceed human capabilities (artificial intelligence, or AI).  Both research trajectories have made significant strides. In 1997, IBM's Deep Blue defeated Garry Kasparov, the reigning world chess champion, and in 2011, Watson beat   \"Jeopardy\" whizzes Ken Jennings and Brad Rutter, marking significant steps on the road toward artificial intelligence.\u00a0\nFor nearly three decades,  New York Times reporter John Markoff has tracked advances in technology. In \"Machines of Loving Grace,\" he captures the history of artificial intelligence and robotics.  He introduces us to a large cast of computer geeks and colorful personalities, and  explores their many failures and successes. \"Machines of Loving Grace\" and Walter Isaacson's bestseller \"The Innovators\" tell a few of the same stories, but the two books can be read as complementary. Isaacson began with the prehistory of hardware and software in the early 1800s, while Markoff focuses on   the pursuit of artificial intelligence once it seriously got underway at a 1956 summer conference on the subject at Dartmouth College.\nMarkoff explores the pros and cons of advanced technology and robotics. He examines, for example, whether robots could one day serve a useful role in caring for the elderly and concludes that they could, augmenting the work of humans without elimininating caregivers' jobs. \"The development of robots that will act as companions and caregiver,\" Markoff writes, \"is a way of using artificial intelligence to ward off one of the greatest hazards of old age - loneliness and isolation.\"\nIn his final chapter, he asks whether developments in AI and IA will create intelligent machines that are \"Masters, Slaves, or Partners?\" He wonders if AI systems can be designed so that they are unequivocally beneficial and controllable.  Noting the dual nature of smart machines, he explains that on the one hand, they can eliminate human drudgery, but on the other they can subjugate humanity. Over the decades, he writes, the dichotomy has \"only sharpened.\" But he knows where responsibility lies. \"This is about us, about humans and the kind of world we will create,\" he argues. \"It's not about the machines.\"\n In \"Our Robots, Ourselves,\" David A. Mindell offers a more in-depth and tightly woven discussion of whether robots will replace or complement human skills. Mindell, an MIT professor, has played a leading role in developing deep-sea submersibles and autonomous aircraft. The history of robots designed to perform tasks in extreme environments provides Mindell with plenty of fodder to challenge the conventional argument that tasks performed by people inevitably migrate first to remotely controlled robots and then on to autonomous systems. \nDiscussing the Mars exploration rovers, Mindell shows how the efforts of people and robots can be complementary and evolve together. He points out that transporting an astronaut to and from Mars is dramatically more expensive than launching an unmanned mission like the rovers. But without humans involved, either on board or in nearby  orbit around Mars, exploration of the planet is more difficult and time-consuming. A 20-minute delay in getting a signal from Earth to Mars meant that the remote control of the rovers would be ridiculously slow. Therefore   the rovers were designed to do many things on their own while they waited for instructions from Earth.\nBut autonomous tasks also often take a lot of time. \"The rover can autonomously plan a route around a series of rocks or obstacles using imagery it gathers from its camera,\" Mindell writes. \"But to do that it stops every ten seconds to look at the terrain for twenty seconds. Thus autonomy is costly in time.\"   As a result, even with the most advanced forms of machine autonomy, some researchers believe that a human presence on Mars would be more efficient.\nMindell clearly demonstrates that the efforts of people and robots can be complementary and inextricably entangled, and can evolve together. He acknowledges that each step forward, however, improves the prospect of assembling fully autonomous machines. \"The challenges of robotics in the twenty-first century,\" he writes, \"are those of situating machines within human and social systems. They are challenges of relationship.\"\nIn \"We, Robots,\" Curtis White offers witty and insightful rants against the culture being created by the alliance between innovative technologies and capitalism.  He joins Evgeny Morozov and Jaron Lanier on the front line of critics challenging assumptions about the benefits of technology. White wants us to create a new narrative that will compete with what he sees as the dominant techno-cultural interpretation of modern life and human destiny: that humans are constantly diminished by the march of technological possibilities. However, he provides little guidance as to what that narrative might be. Machines of Loving GraceThe Quest for Common Ground Between Humans and Robots\nBy John Markoff \nEcco. 378 pp. $26.99OUr Robots, OurselvesRobotics and the Myths of Autonomy\nBy David A. Mindell\nViking. 260 pp. $27.95We, RobotsStaying Human in the Age of Big Data\nBy Curtis White\nMelville House. 284 pp. $25.95\n","42":"Robert O. Work, the veteran defense official retained as deputy secretary by President Trump, calls them his ''A.I. dudes.'' The breezy moniker belies their serious task: The dudes have been a kitchen cabinet of sorts, and have advised Mr. Work as he has sought to reshape warfare by bringing artificial intelligence to the battlefield.\nLast spring, he asked, ''O.K., you guys are the smartest guys in A.I., right?'' \n  No, the dudes told him, ''the smartest guys are at Facebook and Google,'' Mr. Work recalled in an interview.\u00a0\n  Now, increasingly, they're also in China. The United States no longer has a strategic monopoly on the technology, which is widely seen as the key factor in the next generation of warfare.\n  The Pentagon's plan to bring A.I. to the military is taking shape as Chinese researchers assert themselves in the nascent technology field. And that shift is reflected in surprising commercial advances in artificial intelligence among Chinese companies.\n  Last year, for example, Microsoft researchers proclaimed that the company had created software capable of matching human skills in understanding speech.\n  Although they boasted that they had outperformed their United States competitors, a well-known A.I. researcher who leads a Silicon Valley laboratory for the Chinese web services company Baidu gently taunted Microsoft, noting that Baidu had achieved similar accuracy with the Chinese language two years earlier.\n  That, in a nutshell, is the challenge the United States faces as it embarks on a new military strategy founded on the assumption of its continued superiority in technologies such as robotics and artificial intelligence.\n  First announced last year by Ashton B. Carter, President Barack Obama's defense secretary, the ''Third Offset'' strategy provides a formula for maintaining a military advantage in the face of a renewed rivalry with China and Russia.\n  Well into the 1960s, the United States held a military advantage based on technological leadership in nuclear weapons. In the 1970s, that perceived lead shifted to smart weapons, based on brand-new Silicon Valley technologies like computer chips. Now, the nation's leaders plan on retaining that military advantage with a significant commitment to artificial intelligence and robotic weapons.\n  But the global technology balance of power is shifting. From the 1950s through the 1980s, the United States carefully guarded its advantage. It led the world in computer and material science technology, and it jealously hoarded its leadership with military secrecy and export controls.\n  In the late 1980s, the emergence of the inexpensive and universally available microchip upended the Pentagon's ability to control technological progress. Now, rather than trickling down from military and advanced corporate laboratories, today's new technologies increasingly come from consumer electronics firms. Put simply, the companies that make the fastest computers are the same ones that put things under our Christmas trees.\n  As consumer electronics manufacturing has moved to Asia, both Chinese companies and the nation's government laboratories are making major investments in artificial intelligence.\n  The advance of the Chinese was underscored last month when Qi Lu, a veteran Microsoft artificial intelligence specialist, left the company  to become chief operating officer at Baidu, where he will oversee the company's ambitious plan to become a global leader in A.I.\n  And last year, Tencent, developer of the mobile app WeChat, a Facebook competitor, created an artificial intelligence research laboratory and began investing in United States-based A.I. companies.\n  Rapid Chinese progress has touched off a debate in the United States between military strategists and technologists over whether the Chinese are merely imitating advances or are engaged in independent innovation that will soon overtake the United States in the field.\n  ''The Chinese leadership is increasingly thinking about how to ensure they are competitive in the next wave of technologies,'' said Adam Segal, a specialist in emerging technologies and national security at the Council on Foreign Relations.\n  In August, the state-run China Daily reported that the country had embarked on the development of a cruise missile system with a ''high level'' of artificial intelligence. The new system appears to be a response to a missile the United States Navy is expected to deploy in 2018 to counter growing Chinese military influence in the Pacific.\n  Known as the Long Range Anti-Ship Missile, or L.R.A.S.M., it is described as a ''semiautonomous'' weapon. According to the Pentagon, this means that though targets are chosen by human soldiers, the missile uses artificial intelligence technology to avoid defenses and make final targeting decisions.\n  The new Chinese weapon typifies a strategy known as ''remote warfare,'' said John Arquilla, a military strategist at the Naval Post Graduate School in Monterey, Calif. The idea is to build large fleets of small ships that deploy missiles, to attack an enemy with larger ships, like aircraft carriers.\n  ''They are making their machines more creative,'' he said. ''A little bit of automation gives the machines a tremendous boost.''\n  Whether or not the Chinese will quickly catch the United States in artificial intelligence and robotics technologies is a matter of intense discussion and disagreement in the United States.\n  Andrew Ng, chief scientist at Baidu, said the United States may be too myopic and self-confident to understand the speed of the Chinese competition.\n  ''There are many occasions of something being simultaneously invented in China and elsewhere, or being invented first in China and then later making it overseas,'' he said. ''But then U.S. media reports only on the U.S. version. This leads to a misperception of those ideas having been first invented in the U.S.''\n  A key example of Chinese progress that goes largely unreported in the United States is Iflytek, an artificial intelligence company that has focused on speech recognition and understanding natural language. The company has won international competitions both in speech synthesis and in translation between Chinese- and English-language texts.\n  The company, which Chinese technologists said has a close relationship with the government for development of surveillance technology, said it is working with the Ministry of Science and Technology on a ''Humanoid Answering Robot.''\n  ''Our goal is to send the machine to attend the college entrance examination, and to be admitted by key national universities in the near future,'' said Qingfeng Liu, Iflytek's chief executive.\n  The speed of the Chinese technologists, compared to United States and European artificial intelligence developers, is noteworthy. Last April, Gansha Wu, then the director of Intel's laboratory in China, left his post and began assembling a team of researchers from Intel and Google to build a self-driving car company. Last month, the company, Uisee Technology, met its goal -- taking a demonstration to the International Consumer Electronics Show in Las Vegas -- after just nine months of work.\n  ''The A.I. technologies, including machine vision, sensor fusion, planning and control, on our car are completely home-brewed,'' Mr. Wu said. ''We wrote every line by ourselves.''\n  Their first vehicle is intended for controlled environments like college and corporate campuses, with the ultimate goal of designing a shared fleet of autonomous taxis.\n  The United States' view of China's advance may be starting to change. Last October, a White House report on artificial intelligence included several footnotes suggesting that China is now publishing more research than scholars here.\n  Still, some scientists say the quantity of academic papers does not tell us much about innovation. And there are indications that China has only recently begun to make A.I. a priority in its military systems.\n  ''I think while China is definitely making progress in A.I. systems, it is nowhere close to matching the U.S.,'' said Abhijit Singh, a former Indian military officer who is now a naval weapons analyst at the Observer Research Foundation in New Delhi.\n  Chinese researchers who are directly involved in artificial intelligence work in China have a very different view.\n  ''It is indisputable that Chinese authors are a significant force in A.I., and their position has been increasing drastically in the past five years,'' said Kai-Fu Lee, a Taiwanese-born artificial intelligence researcher who played a key role in establishing both Microsoft's and Google's China-based research laboratories.\n  Mr. Lee, now a venture capitalist who invests in both China and the United States, acknowledged that the United States is still the global leader but believes that the gap has drastically narrowed. His firm, Sinovation Ventures, has recently raised $675 million to invest in A.I. both in the United States and in China.\n  ''Using a chess analogy,'' he said, ''we might say that grandmasters are still largely North American, but Chinese occupy increasingly greater portions of the master-level A.I. scientists.''\n  What is not in dispute is that the close ties between Silicon Valley and China both in terms of investment and research, and the open nature of much of the American A.I. research community, has made the most advanced technology easily available to China.\n  In addition to setting up research outposts such as Baidu's Silicon Valley A.I. Laboratory, Chinese citizens, including government employees, routinely audit Stanford University artificial intelligence courses.\n  One Stanford professor, Richard Socher, said it was easy to spot the Chinese nationals because after the first few weeks, his students would often skip class, choosing instead to view videos of the lectures. The Chinese auditors, on the other hand, would continue to attend, taking their seats at the front of the classroom.\n  Artificial intelligence is only one part of the tech frontier where China is advancing rapidly.\n  Last year, China also brought the world's fastest supercomputer, the Sunway TaihuLight, online, supplanting another Chinese model that had been the world's fastest. The new supercomputer is thought to be part of a broader Chinese push to begin driving innovation, a shift from its role as a manufacturing hub for components and devices designed in the United States and elsewhere.\n  In a reflection of the desire to become a center of innovation, the processors in the new computer are of a native Chinese design. The earlier supercomputer, the Tianhe 2, was powered by Intel's Xeon processors; after it came online, the United States banned further export of the chips to China, in hopes of limiting the Chinese push into supercomputing.\n  The new supercomputer, like similar machines anywhere in the world, has a variety of uses, and does not by itself represent a direct military challenge. It can be used to model climate change situations, for instance, or to perform analysis of large data sets.\n  But similar advances in high-performance computing being made by the Chinese could be used to push ahead with machine-learning research, which would have military applications, along with more typical defense functions, such as simulating nuclear weapons tests or breaking the encryption used by adversaries.\n  Moreover, while there appear to be relatively cozy relationships between the Chinese government and commercial technology efforts, the same cannot be said about the United States. The Pentagon recently restarted its beachhead in Silicon Valley, known as the Defense Innovation Unit Experimental facility, or DIUx. It is an attempt to rethink bureaucratic United States government contracting practices in terms of the faster and more fluid style of Silicon Valley.\n  The government has not yet undone the damage to its relationship with the Valley brought about by Edward J. Snowden's revelations about the National Security Agency's surveillance practices. Many Silicon Valley firms remain hesitant to be seen as working too closely with the Pentagon out of fear of losing access to China's market.\n  ''There are smaller companies, the companies who sort of decided that they're going to be in the defense business, like a Palantir,'' said Peter W. Singer, an expert in the future of war at New America, a think tank in Washington, referring to the Palo Alto, Calif., start-up founded in part by the venture capitalist Peter Thiel. ''But if you're thinking about the big, iconic tech companies, they can't become defense contractors and still expect to get access to the Chinese market.''\n  Those concerns are real for Silicon Valley.\n  ''No one sort of overtly says that, because the Pentagon can't say it's about China, and the tech companies can't,'' Mr. Singer said. ''But it's there in the background.''\n\n\n\n","43":"WHEN the computer scientist John McCarthy coined the term artificial intelligence in the late 1950's, he did not mean to imply that there would be anything second rate about mechanical minds. However, three decades later computers still do not think. Is this because of the technological failings of the computer industry? Or is artificial intelligence theoretically impossible? Finally, since most of the research is financed by the Pentagon, will smarter computers lead to more efficient ways of killing people? Three reports follow.\u00a0\nDURING the early 1980's, scientists at Teknowledge, Intellicorp and the other ambitiously named companies in the fledgling artificial-intelligence industry boasted of a bright future in which computers would match people in their ability to make important business decisions.\u00a0\nIn the last few years, such optimism has gradually faded. Bringing the visionary technologies of artificial intelligence to the market has proved far more difficult than had been anticipated. Many of the original artificial-intelligence companies - including Teknowledge, Intellicorp, the Carnegie Group and the Inference Corporation - have suffered losses. Several others have gone out of business.\nWhile the industry is far from developing machines that bring to problem-solving the kind of creativity and flexibility humans use, many of the techniques developed in the early stages of the quest have begun to filter into the mainstream computer industry. Rather than trying to develop computers with rudimentary reasoning abilities, a new generation of companies is concentrating on ways of making conventional computers a little bit ''smarter'' and easier to use than they were before.\n''It's a Darwinian process - the first generation is dying because of complete lack of fitness,'' said Alain Rappaport, president of Neuron Data, a four-year-old Silicon Valley firm that made a profit of more than $2 million in 1987.\nArtificial intelligence began during the late 1950's as an academic discipline dedicated to the possibility that computers could be programmed to think like people. Financed largely by the Defense Department, scientists pursued a variety of ''blue sky'' possibilities: machines that could recognize objects or understand written - and even spoken - English. By the late 1970's a few entrepreneurs began turning their attention to making commercial products such as ''expert systems,'' programs that would diagnose diseases, for example, or give investment advice.\nIt was a captivating vision, and companies as diverse as General Motors and Procter & Gamble experimented with programs that would help executives make decisions or control various industrial processes. But even large companies, which could afford to dabble in the art, found that development costs often outweighed potential rewards. Many expert systems were written in exotic programming languages and would run only on specialized computers costing as much as $100,000 each.\nIn addition, developing expert systems required a cadre of ''knowledge engineers,'' highly paid computer scientists who could translate the expertise of a human specialist into a set of rules that could be programmed into a machine. This task was so daunting that some scientists talked about developing computerized knowledge engineers - expert systems whose expertise was developing expert systems.\nBut nothing nearly so sophisticated has emerged. So far, expert systems have been useful only to solve the most narrowly defined problems, such as diagnosing malfunctioning electronic equipment.\nRecently, however, there has been new enthusiasm about the promise of intelligent machines. With the advent of the 32-bit microprocessor, a computer-on-a-chip that is as fast and powerful as room-sized machines were five years ago, many corporations are routinely buying far more advanced computers. To cater to this market, software sellers are rewriting their expert systems to run on this new machinery.\u00a0A Sense of Modesty\nArtificial intelligence also is increasingly being folded into mainstream programs, such as word processors, making them easier to use and amplifying their computing power. For example, Q&A, a popular personal computer database manager - a kind of filing system for the computer illiterate - uses artificial-intelligence technology to allow users to retrieve information by typing in English sentences, not cryptic computer commands.\nThe second generation of artificial-intelligence companies has scaled back the overly optimistic claims of its predecessors, which often sounded as though they were about to deliver the equivalent of a brain in a box.\n''We don't make artificially intelligent machines in much the same way that the Boeing Company doesn't make artifical birds,'' said Harry Reinstein, president of Aion Corporation, a Palo Alto, Calif., company that sells expert systems designed for I.B.M. computers. Rather than trying to re-create human intelligence, the companies are taking cues from how people think and using them to design better software.\nProgress in building intelligent machines is also coming from a new group of researchers who are attempting to merge neurobiology and semiconductor manufacturing technology. In the past most developments in artificial intelligence have been in the software rather than in the hardware; whatever intelligence there was existed in the programs not in the machines. Researchers are now trying to use recent theories about how the brain works to make complex, neuron-like chips that might be used for machine vision and speech recognition.\nThese fresh approaches suggest that fundamental breakthroughs in machine intelligence may yet be possible. Last year computer scientists, biologists and mathematicians met in Los Alamos, N.M., to discuss the possibility of ''artificial life,'' machines that would evolve over seconds rather than eons, to become ever more intelligent.\nThe problem in the past, says Apple computer scientist Alan Kay, is that researchers have spent their time designing systems that attempt to imitate adult thought processes.\nTime could be better spent, he argues, trying to recreate the manner in which children learn.\n","44":"DR. MARVIN MINSKY, 71, a rumpled-looking man who wears shirts mended with masking tape, is Toshiba professor of media arts and sciences and professor of electrical engineering and computer sciences at the Massachusetts Institute of Technology, and one of the world's leading theorists of artificial intelligence. In the late 1950's, Dr. Minsky and John McCarthy, a professor of computer science at Stanford University, founded a research program that would evolve into the MIT Artificial Intelligence Laboratory. In addition to inventing and building thinking machines, Dr. Minsky wrote the classic \"The Society of Mind,\" (Simon & Schuster, 1986) in which he tries to show how intelligence works \"by the particular way the agents in the brain have evolved to interact.\" \"The Turing Option,\" (Warner Books, 1992) a novel by Dr. Minsky and Harry Harrison, is about superintelligence in not-too-far-off 2023.\u00a0\nQ. In the 1960's, science students, particularly those at M.I.T., talked of artificial intelligence, or A.I., as if it would create world revolution. Were they too optimistic?\u00a0A. Well, it got stuck. A.I. was able to produce all kinds of wonderful things . . . programs that did better than the average stockbroker or portfolio manager, programs that could fix some piece of equipment. Around 1980, progress stopped in some ways and people went off in a number of other directions to try to find some way to get back. It stopped because we'd done the easy things. In the eye of eternity, it got stuck for a moment.\nA good example is, in 1964 or 1965, one of our students, Daniel Bobrow (now a vice president at the Xerox Corporation) wrote a program that could read a question from a high school algebra book, and sometimes, solve the problem. So it could figure out a little bit of language and algebra. It didn't get most of the problems because it couldn't understand the words. What people tried to do then is get a program that would read a story from a first- or second-grade children's book. But what happened was this: For any particular story, we could build into the program the knowledge necessary to read that story. We didn't have much trouble with the grammar. As soon as something was mentioned that the program didn't know about . . . (the system broke down). One M.I.T. student had a story where some person's daughter was kidnapped by the Mafia and they demanded a ransom.  So he asked the program \"What should we do?\" The program couldn't understand. Finally, it asked, \"Why would he pay MONEY to get his daughter back?\"\nIt could figure out a little bit of language, a little bit of algebra. It didn't get most of the problems because it couldn't understand the words. As far as I know, nobody has been able to get a machine to solve real problems that are informally expressed, the way somebody would normally express them.\u00a0Q. How do you define common sense?\u00a0A. Common sense is knowing maybe 30 or 50 million things about the world and having them represented so that when something happens, you can make analogies with others. If you have common sense, you don't classify the things literally; you store them by what they are useful for or what they remind us of. For instance, I can see that suitcase (over there in a corner) as something to stand on to change a light bulb as opposed to something to carry things in.\u00a0Q. Could you get machines to the point where they can deal with the intangibles of humanness?\u00a0A. It's very tangible, what I'm talking about. For example, you can push something with a stick, but you can't pull it. You can pull something with a string, but you can't push it. That's common sense. And no computer knows it. Right now, I'm writing a book, a sequel to \"The Society of Mind,\" and I am looking at some of this. What is pain? What is common sense? What is falling in love?\u00a0Q. What is love?\u00a0A. Well, what are emotions? Emotions are big switches, and there are hundreds of these. . . . If you look at a book about the brain, the brain just looks like switches. . . . You can think of the brain as a big supermarket of goodies that you can use for different purposes. Falling in love is turning on some 20 or 30 or these and turning a lot of the others off. It's some particular arrangement. To understand it, one has to get some theory of what are the resources in the brain, what kind of arrangements are compatible and what happens when you turn several on and they get into conflict. Being angry is another collection of switches. In this book, I'm trying to give examples of how these things work.\u00a0Q. In the 1968 Stanley Kubrick film \"2001: A Space Odyssey,\" a computer named Hal developed a lethal jealousy of his space companion, a human astronaut. How far are we away from a jealous machine?\u00a0A. We could be five minutes from it, but it would be so stupid that we couldn't tell. Though Hal is fiction, why shouldn't he be jealous? There's an argument between my friend John McCarthy and me because he thinks you could make smart machines that don't have any humanlike emotions. But I think you're going to have to go to great lengths to prevent them from having some acquisitiveness and the need to control things. Because to solve a problem, you have to have the resources and if there are limited resources . . .\u00a0Q. Where were Stanley Kubrick and his co-author, Arthur C. Clarke, right with their \"2001: Space Odyssey\" predictions?\u00a0A. On just about everything except for the date. It's quite a remarkable piece.\u00a0Q. Do you believe the National Aeronautics and Space Administration wastes money by insisting on humans for space exploration?\u00a0A. It's not that they waste money. It's that they waste ALL the money.\u00a0Q. If you were heading NASA, how would you run it?\u00a0A. I would have a space station, but it would be unmanned. And we would throw some robots up there that are not intelligent, but just controlled through teleoperators and you could sort of feel what's doing. Then, we could build telescopes and all sorts of things and perhaps explore the moon and Mars by remote control. Nobody's thought of much use for space. The clearest use is building enormous telescopes to see the rest of the universe.\u00a0Q. Why are manned shots a NASA priority?\u00a0A. Because NASA's people are basically oriented toward keeping themselves alive. They are a big organization. And the biggest part of it is Houston and that has to be fed, and what Houston is good at is putting men in space. The Jet Propulsion Lab is much smaller and has a smaller staff and is good at doing everything else. So, I think, in order to support that, they get into this vicious circle where you have to convince yourself that's what the public wants. Now, I think, the public is more excited by Sojourner than by astronauts.\u00a0Q. When you go to the movies, what do you see?\u00a0A. \"Terminator,\" \"Total Recall,\" which had ideas about implanted memory. Pretty clumsy, but I loved the engineering. I don't like movies exactly. One of my rules is not to think of the whole thing as having any unity. The idea of liking a whole movie is . . . People have this idea that they have to like something or not.\u00a0Q. What do you read?\u00a0A. Science fiction.\u00a0Q. Do you read science fiction in the same way spies read spy novels -- for ideas?\u00a0A. Yes. There are a dozen very, very rich sources of ideas out there. Gregory Benford of U.C. Irvine, David Brin, Larry Niven are the best writers of our period. When they write a book, there's some big new idea about something. I've also gotten a lot of good ideas from old-timers like Robert Heimlein and the late Isaac Asimov.\u00a0Q. Where was Mary Shelley right and where was she wrong with her \"Frankenstein\" last century?\u00a0A. She certainly was right in predicting how people would not understand the poor thing. That's SUCH a sad story! By the way, I've gone through that book very carefully to see if she left any hints explaining how the robot worked. But alas, no clues and the funny part is when you read it, you don't mind.\n","45":"Artificial intelligence has waxed and waned as a topic of interest in Silicon Valley over the decades. Now there is a fever pitch over it, with behemoths including Apple, Alphabet's Google and Facebook increasingly playing in the space. Numerous start-ups have also sprung up around machine learning.\u00a0\nThe latest example of the heightened interest came on Friday, when Toyota said it was making a $1 billion investment over the next five years for a research and development effort around artificial intelligence in Silicon Valley. The effort is notable because it is reminiscent of earlier research initiatives in the area, such as Xerox's Palo Alto Research Center, and signals how nontraditional companies are filling the gap in basic technology research, writes John Markoff. \nToyota told Mr. Markoff that the new center, which will be called Toyota Research Institute Inc., will initially focus on artificial intelligence and robotics technologies. It was a no-brainer to locate the institute in Silicon Valley, the company said.\n\"The density of people doing this kind of work in Silicon Valley is higher than any other place in the world,\" said Dr. Gill Pratt, a roboticist who will lead the new Toyota effort.\n","46":"The Ford Motor Company, following the example of the General Motors Corporation, announced today that it had purchased minority interests in two companies that specialize in the development of artificial intelligence computer systems.\nFord said it would invest up to $28 million for up to 10 percent equity in each of two companies - the Inference Corporation of Los Angeles and the Carnegie Group Inc. of Pittsburgh. Both companies are privately owned. The investment also includes contracts for systems development.\u00a0\nLast year General Motors similarly made minority investments in five companies involved in robotics, vision controls and artificial intelligence. G.M. said it wanted to be first with advanced technology and also wanted to provide the companies with financing to develop products and processes tailored to G.M.'s needs.\nArtificial intelligence, while relatively new in the commercial world, has been under development for about 25 years.\u00a0'Expert Systems'\nFord's initial interest in the field appears to be restricted to a specific software technology called ''expert systems.'' Such programs enable a computer to replicate the technical knowledge of one or more human experts, such as a doctor or a mechanic. The computer system is programmed to ask a series of questions to help determine what is wrong, and it suggests solutions. Such systems are increasingly used in hospitals, for example, to help doctors diagnose illness from seemingly ambiguous symptoms.\nMost of G.M.'s work in this area is being done by Teknowledge Inc., a Palo Alto, Calif., company in which the auto giant invested $3 million last year.\nFord's immediate plans for artificial intelligence include the development of diagnostic systems for its electronics, engine and parts and service divisions. It willl also be applied to financial services. Ford has already used such systems in its Aerospace division.\n''Given the volume of production and the number of parts and processes that the automobile industry has to contend with,'' artificial intelligence ''becomes more than just a candidate for alternative solutions,'' said Larry Geisel, chief executive of the Carnegie Group. ''This technology constitutes an increasingly critical factor in the ability of manufacturing operations the size of Ford to realize long-range objectives.''\u00a0Faculty Group\nThe Carnegie Group was formed in 1983 by four computer scientists from the faculty of Carnegie-Mellon University. One of the founders, Dr. Mark Fox, said artificial intelligence would benefit Ford in two ways. ''First, by creating an extremely detailed model of what's taking place on the factory floor, our systems will greatly increase Ford's ability to respond quickly and cost-effectively to changing market and vehicle schedule conditions,'' he said. ''Second, by capturing the critical know-how of highly skilled personnel, we will produce expert systems that make this rare and expensive knowledge resource available throughout Ford's entire network of operations.''\nThe Inference Corporation was formed in 1979 by Alexander D. Jacobson, who serves as chairman. Its system applications have been used by the National Aeronautics and Space Administration, and the Lockheed Corporation and other commercial companies.\nMr. Jacobson said the equity interest by Ford would enable the company to make advancements in technology quickly available to the car company. ''They are much more than an arm's-length customer,'' he said.\n","48":"A group of prominent Silicon Valley investors and technology companies said on Friday that they would establish an artificial-intelligence research center to develop ''digital intelligence'' that will benefit humanity.\nThe investors -- including Elon Musk, Peter Thiel and Reid Hoffman -- said they planned to commit $1 billion to the project long term, but would initially spend only a small fraction of that amount in the first few years of the project. But, Mr. Musk said, ''Everyone who is listed as a contributor has made a substantial commitment and this should be viewed as at least a billion-dollar project.'' \n  The organization, to be named OpenAI, will be established as a nonprofit, and will be based in San Francisco.\u00a0\n  Its long-range goal will be to create an ''artificial general intelligence,'' a machine capable of performing any intellectual task that a human being can, according to Mr. Musk. He also stressed that the focus was on building technologies that augment rather than replace humans.\n  Mr. Musk, who is deploying A.I.-based technologies in some of his products like the Tesla automobile, said that he has had longstanding concerns about the possibility that artificial intelligence could be used to create machines that might turn on humanity.\n  He began discussing the issue this year with Mr. Hoffman, Mr. Thiel and Sam Altman, president of the Y Combinator investment group.\n  ''We discussed what is the best thing we can do to ensure the future is good?'' he said. ''We could sit on the sidelines or we can encourage regulatory oversight, or we could participate with the right structure with people who care deeply about developing A.I. in a way that is safe and is beneficial to humanity.''\n  ''Artificial\u00a0intelligence is one of the great opportunities for improving the world today,'' Mr. Hoffman said in an email. ''The specific applications range from self-driving cars, to medical diagnosis and precision personalized medicine, to many other areas of data, analysis, decisioning across industries.''\n  Other backers of the project include Jessica Livingston of Y Combinator; Greg Brockman, the former chief technology officer of Stripe, as well as Amazon Web Services, Amazon's Cloud Services subsidiary; and Infosys, an Indian software consulting and consulting firm. The research effort has also attracted a group of young artificial intelligence researchers.\n  The founders said they were not yet ready to provide details on who had donated how much and the rate at which the project money would be spent. They will fund the development of the project on a year-by-year basis. They also said they were not yet ready to describe how quickly the project would grow in terms of funding or staffing.\n  The announcement occurs in the same week that one of the main academic gatherings focusing on artificial intelligence, the Conference on Neural Information Processing Systems, is being held in Montreal.\n  In recent years the event has grown as major technology corporations like Apple, Facebook, Google, IBM and Microsoft have started competing to hire the most talented researchers in the field. Salaries and hiring incentives have soared.\n  The research director of OpenAI will be Ilya Sutskever, a Google expert on machine learning. Mr. Brockman will be the chief technology officer. The group will begin with seven researchers, including graduate researchers who have been standouts at universities like Stanford, the University of California, Berkeley, and New York University.\n  ''The people on the team have all been offered substantially more to work at other places,'' Mr. Musk said.\n  Mr. Altman added, ''It is lucky for us the best people in any field generally care about what is best for the world.''\n  In October 2014, Mr. Musk stirred controversy when, in an interview at M.I.T., he described artificial intelligence as our ''biggest existential threat.'' He also said, ''With artificial intelligence we're summoning the demon.''\n  In October, he donated $10 million to the Future of Life Institute, a Cambridge, Mass., organization focused on developing positive ways for humanity to respond to challenges posed by advanced technologies. \n  He said the new organization would be separate from the Future of Life Institute, and that while the new organization did have a broad research plan, it was not yet ready to offer a specific road map.\n  In a statement, the group sounded an open-source theme -- open-source software can be freely shared without intellectual property restrictions -- and said it was committed to ensuring that advanced artificial tools remained publicly available. ''Since our research is free from financial obligations, we can better focus on a positive human impact,'' the group said. ''We believe A.I. should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible.''\n  Mr. Musk said he remained concerned that A.I. advances might work against, rather than benefit, humanity.\n  ''There is always some risk that in actually trying to advance A.I. we may create the thing we are concerned about,'' he said.\n  In the last two years there has been a race to set up research facilities focused both on advancing A.I. and in assessing its impact.\n  In 2014, Paul Allen, Microsoft's co-founder, established the nonprofit Allen Institute for Artificial Intelligence, which says its mission is ''to contribute to humanity through high-impact A.I. research and engineering.''\n  Also in 2014, the Microsoft A.I. researcher Eric Horvitz gave an undisclosed amount as a gift to Stanford to study the impact of the technology over the next century.\n  Last month, the Toyota Corporation said that it would invest $1 billion in a five-year research effort in artificial intelligence and robotics technologies to be based in a laboratory near Stanford.\n\n\n\n","49":"Jay Baer, a digital marketing consultant in Bloomington, Ind., spends half his time traveling on business. That means he also has to spend hours each week coordinating that travel.\nHelp has arrived with the Pana app, which employs artificial intelligence to aid customers. \n  Virtual travel assistant services -- some from established companies like Facebook, IBM and Expedia, and others from new entrants like Pana and HelloGbye -- are now popping up worldwide, just as major hotel chains like Starwood and Hilton are incorporating robots into their everyday operations.\u00a0\n  Many of the virtual assistant services use artificial intelligence, a branch of computer science that simulates intelligent human behavior. Some respond to questions posed by travelers, either in live speech or digitally, while some, like Pana, rely on additional input by humans to provide answers.\n  Although many services are now in their infancy, they are expected to change the way travel is planned in the not-too-distant future.\n  The Pana app lets users chat, in conversational language, about booking travel and helps if they encounter a delay or cancellation. Responses are provided by a team of travel agents who are on duty round the clock and have access to artificial intelligence to gather information customized for the traveler.\n  Mr. Baer, who has been a Pana devotee since last year and recently signed a contract for his staff of 11 to use it, relies on it primarily for tips on places he visits and to rebook when his schedule changes or he encounters an unexpected delay.\n  Like many in the travel industry, Miriam Moscovici, director of emerging technologies for BCD Travel, a travel management company, predicts that within a year ''lower-priority tasks will be handled by self-service artificial intelligence, which will free up human travel agents to do more of the intense work required.''\n  In 2011, artificial intelligence came into the limelight in the United States when the Watson computer, made by IBM, beat human competitors in a ''Jeopardy'' match. IBM's artificial intelligence efforts are also at the forefront of travel-related initiatives. Two years ago, it invested in and began working with WayBlazer, a company whose co-founder, Terrell Jones, helped start the travel websites Travelocity and Kayak.\n  Using Watson's natural-language capabilities and other cognitive computing technology, WayBlazer makes customized recommendations for travelers planning trips to Austin, Tex. The Leading Hotels of the World, a luxury-hotel marketing organization, is using WayBlazer to help customers choose accommodations based on their tastes, through its website.\n  WayBlazer and Watson's cognitive technology are the brains behind Connie, a robot being tested at the Hilton in McLean, Va., outside Washington. Connie answers guests' questions about amenities and services, and also suggests local attractions.\n  Norm Rose, an analyst for Phocuswright, a travel research company, predicted that artificial intelligence would help simplify and automate travel planning by providing quicker responses and more relevant content.\n  But noting that artificial intelligence is not yet capable of completely replacing human interactions, he warned it could also alienate travelers if a travel company used it but was unable to answer their questions.\n  He said artificial intelligence systems need to understand the vocabulary used in travel to be successful, but still may have difficulty helping travelers if their requests are complex. He also said some emerging systems are arranging complex itineraries that contain multiple flights or involve flights and hotels, and predicted many will eventually be able to handle both.\n  Expedia is among a few dozen companies and organizations that Facebook recently said would use a new chatbot on the social network's Messenger platform. This lets users pose questions in digital chats and employs artificial intelligence-powered systems to provide answers. Expedia is testing responding to hotel booking requests via Messenger.\n  Passengers of KLM Royal Dutch Airlines, an earlier partner on Messenger, can now use the platform to confirm bookings and get flight status updates. The airline also plans to offer bookings through Messenger. And since 2011, the airline has been testing a 6-foot-3 robot, Spencer, at Amsterdam Schiphol Airport, its home base. In its most recent trial, which ended in March, Spencer escorted travelers to their gates.\n  Robots are also being used at select Starwood, InterContinental and Marriott hotels in the United States and are being tested in Europe by SNCF, the French railway, and on cruise ships operated by Costa and Aida.\n  Skyscanner, a travel search engine, this year developed a voice-enabled app for Alexa, the cloud-based voice service for the Echo from Amazon, to search for flights. Alexa also recently began working with Kayak to help users search for flights, hotels, rental cars and destinations, though no booking is done through it.\n  HelloGBye will begin testing an app this summer that lets users type or speak into their phone and create an itinerary with flights and hotel accommodations. The app can book travel for as many as nine people at once.\n  In November, Hipmunk, which has helped travelers research and book travel online since 2010, began testing a free, personalized virtual travel-planning assistant, Hello Hipmunk, powered by artificial intelligence. It provides help via email. Users can also give Hello Hipmunk permission to access their Google calendar, which it then periodically scans for events requiring travel and proactively provides booking options.\n  The Pana app answers many users' questions in less than a minute, said Devon Tivona, the company's chief executive, and agents notify customers if more time is required. Pana is geared toward people who frequently travel for business. Its monthly fees range from $19 for one person to $449 for companies with fewer than 200 employees. Introduced in a beta test last summer, it formally began operating in April and works with thousands of individual travelers and fewer than 100 businesses.\n  Mr. Baer, the digital marketing consultant, said Pana is ''so reasonably priced and so fast, if you travel any more than once a month, it's literally a no-brainer to buy this.'' His company pays $250 a month for its services.\n  Henry Harteveldt, a travel analyst for Atmosphere Research, predicted that artificial intelligence would make shopping for travel ''more efficient, less time-consuming and more useful.''\n  Mr. Harteveldt said this would occur, in part, because travel-planning tools that employ artificial intelligence would make use of travelers' relevant data, such as their home city and age, and would not require them to use technical terms, like airport codes, as many sellers now do. The result, he added, would be tools that come up with suggestions ''at lightning speed that are contextually relevant.''\n\n\n\n","51":"Standing outside the apartment on Thursday, Walter could hear the barbs and retorts of a failed marriage's final throes. \n Walter's friends, Grace and Trip, had invited him over. Now, though only every third word seeped through the door, Walter could hardly mistake the bickering.\n  At Walter's knock the voices stopped. The couple adopted brittle masks of happiness. But as their banter moved from Trip's new bartender set to recent Italian vacations to Grace's latest apartment makeover, the couple gradually returned to the needling exchanges of domestic strife.\u00a0\n As Grace and Trip retreated to opposite sides of the living room, sniping about old grievances, Walter appealed to the couple's loyalties, trying valiantly to reconcile his friends. \n This is the future of video games. In their modern riff on ''Who's Afraid of Virginia Woolf?'' Walter was the only human. Grace and Trip were virtual characters powered by advanced artificial intelligence techniques, which allowed them to change their emotional state in fairly complicated ways in response to the conversational English being typed in by the human player. \n It was one version of the future here this past week at the first Artificial Intelligence and Interactive Digital Entertainment conference. It is a future where games are driven as strongly by characters as combat, where games are as much soap opera as shooting gallery and as much free-form construction set as destruction arena. The apartment drama, a 15-minute interactive story called ''Facade'' that is scheduled to be released free next month (interactivestory.net), was one of the demonstrations offered to the roughly 120 game makers and academic computer experts who attended.\n ''As we try to create more immersive experiences, these artificial intelligence techniques are helping drive games forward and this is one of the areas that could really explode,'' Bing Gordon, chief creative officer at Electronic Arts, the No. 1 video game company, said after his talk Wednesday night. ''We hope that the folks here start thinking about artificial intelligence as a feature, like graphics is a feature or sound is a feature.''\n While the adaptability and behavioral subtlety in recent classics like ''Black & White,'' ''Sid Meier's Alpha Centauri'' and ''The Sims'' have impressed gamers with their seeming-intelligence, those titles have been but an early step.\n ''For a long time, games have been judged largely on their graphics,'' said Ian Lane Davis, a conference organizer and chief executive of Mad Doc Software, which recently created the well-received Empire Earth II, a real-time strategy game. ''The graphics hardware is now getting powerful enough that basically everything looks good now. So what is starting to differentiate games is what is happening inside the characters, how the opponents behave and make plans, how comprehensively and realistically the worlds respond to what the players want to do.''\n ''At the same time,'' he added, ''players are demanding a lot more freedom. Often they don't want to be put on a roller coaster track that just takes them along one path, no matter how entertaining that one path may be. They want a range of choices and they want those choices to matter in creating the overall experience. You put together all of these demands, and that's why you're seeing all of this attention now on artificial intelligence in games.'' \n Outside the game world, the term artificial intelligence is used to label technologies as disparate as air traffic control systems and automated vacuum cleaners. At the conference, much of the discussion was about specific game activities that, to a human, would seem more intuitive than rational, like using conversational language. \n But one of the broadest and most powerful approaches to artificial intelligence may be one that does not focus on determining specific behaviors. (''Does the computer general know that it should use tanks and artillery together?'')\n Rather, it is a move to structure programs so that they absorb available information and then generate their own strategies to achieve sometimes-contradictory goals (''protect the hostages'' versus ''kill the enemy,'' for instance). \n Traditionally, game programmers have created activity through explicit if-then statements: if the player attacks the castle, then send pikemen to defend it; if the player corners the market on wheat, then invest in corn. That process is known as scripting. But what should the computer do if the player takes an action that is not in a script? \n ''The problem now is that the worlds are so complex and the variety of potential actions so vast that trying to direct the environments and the behaviors of computer-controlled agents through traditional scripting can become unmanageable,'' Jeff Orkin, an artificial intelligence programmer at Monolith Productions, said between sessions.\n Three years ago, Mr. Orkin worked on Monolith's campy ''No One Lives Forever 2,'' set in the 1960's. Now he is working on ''F.E.A.R.,'' a game scheduled for later this year.\n ''We used to manually lay out all of the steps that an agent would take: do this, then do that, and if this other thing happens then try this,'' Mr. Orkin said. ''Now we tell the agent: here are your goals, here are your basic tools, you figure out how to accomplish it.''\n ''For example, let's say you the player are running down a hall and an enemy is pursuing you,'' Mr. Orkin said. ''You get to a door and slam it behind you. The enemy replans and tries to kick it in, but if you hold it closed with your weight he will replan again and maybe come around and dive through a window. In the past, the programmer would have had to explicitly code each of these steps. Now, you put the character in the building and it figures out a plan on its own.''\n As put by Chris Crawford, a legendary game designer of the 1980's who now focuses on interactive storytelling technology: ''As a game designer you are an absolute god. One kind of god says, 'O.K., now this leaf will fall a little bit here, and then this wind will blow a bit over there.' The other kind of god says, 'Here are the laws of physics. Go for it.''' \n That conceptual leap from designer-as-determinist to designer-as-prime mover is what has made both the ''Grand Theft Auto'' and ''The Sims'' series so popular. The challenge is that even as gamers have come to expect more freedom in their virtual environments, they have also come to expect more explicitly directed cinematic moments, like the D-Day invasion scenario in ''Medal of Honor,'' where players can feel as if they are living a movie.\n ''There is a real tension between wanting to handcraft the experience to generate a specific emotional response and wanting to allow a more open-ended environment so the player feels they are in control,'' said Doug Church, one of the designers behind the highly regarded ''Thief'' and ''System Shock'' series. ''Artificial intelligence will help us bridge the two.''\n But perhaps that bridge will run in unexpected directions. Until now, artificial intelligence has often involved making computers accessible to humans. With his new project, ''Spore,'' Will Wright of ''The Sims'' fame means to invert that concept.\n ''Until now, artificial intelligence has usually meant that the human creates or perceives a model of how the computer makes decisions,'' Mr. Wright said. ''But what if the computer is instead analyzing the player, and the program is customizing the experience based on the internal model it has created of the human?'' \n ''Spore'' is meant to tailor a species' entire evolutionary experience -- from amoebalike gene pattern to intergalactic emperor -- to each user's individual play style. In that sense, future generations of games may process humans just as intensively as humans are playing the software. But not to worry, Mr. Church said: ''We have a long way to go before we get there.'' \n","52":"Elon Musk and other big tech names are worried that humanity will develop artificial intelligence so smart that it becomes a threat to humanity itself - think self-aware killer robots that realize their best course of action is to wipe out mankind.\u00a0\nOr, basically, terminators.\nMusk has been raising the alarm about this possibility for a while now. At an MIT event last year, he called artificial intelligence humanity's \"biggest existential threat\" and compared it to \"summoning the demon.\" \nMusk was also one of a slew of scientists and tech leaders, including Apple co-founder Steve Wozniak and famed theoretical physicist Stephen Hawking, who signed onto a letter calling for artificial intelligence research to be aimed at creating systems that \"do what we want them to do,\" rather than, say, killing us all.\nEven Bill Gates said in a reddit chat in January that he doesn't \"understand why some people are not concerned\" about the possibility of a rogue, self-aware robot.\nThis anti-Skynet contingent is increasingly putting their money where their dystopian fear is. Musk and a group of other big tech names, including fellow PayPal co-founder and investor Peter Thiel as well as Amazon Web Services, just committed $1 billion to a new group called OpenAI. It's a nonprofit company specifically aimed at responsible artificial intelligence development. \"It's hard to fathom how much human-level AI could benefit society, and it's equally hard to imagine how much it could damage society if built or used incorrectly,\" a blog post announcing the venture said.\nIts chief technology officer is Greg Brockman, who formerly had the same role at digital payments company Stripe. Musk and Y Combinator President Sam Altman are serving as co-chairs.\nThe group says researchers will be \"strongly encouraged to publish their work,\" and any patents it obtains will be \"shared with the world.\" And it doesn't plan to burn through the $1 billion right away - it expects to \"only spend a tiny fraction\" of the mega-fund in the next few years.\nBut OpenAI isn't the first nonprofit with big tech ties aimed at finding positive uses for artificial intelligence. In 2014, Microsoft co-founder Paul Allen launched the Allen Institute for Artificial Intelligence, which uses the tagline \"AI for the common good.\"\nThe big takeaway from these apparently altruistic investments is that Silicon Valley thinks a future where terminators could exist is basically inevitable. But they want to do everything they can to ensure that the tech that could support those killing machines gets used for good instead.\nandrea.peterson@washpost.com\n","53":"Last year, a Google computer program known as DeepMind's AlphaGo beat one of the world's top players in the game of Go, a two-player board game that originated in China more than 2,000 years earlier. The event made headlines the world over, including in The New York Times, as a sign of the increasing strength of artificial intelligence versus human brainpower.\u00a0\nOn Tuesday, AlphaGo was at it again. \n  This time, the Google artificial intelligence program took on the top-ranked Chinese Go player, Ke Jie, in Wuzhen, China, as part of a three-game match. AlphaGo was victorious in the first game, writes Paul Mozur, a New York Times technology reporter, causing Mr. Ke to call the program ''like a god of Go.'' The next two games of the series are scheduled for Thursday and Saturday.\n  At this stage, artificial intelligence in all its forms is inevitable. It's evident in speech recognition and image recognition software. It's tangible in Amazon's Echo speakers when the smart assistant Alexa speaks to you. It's on some streets in the form of self-driving cars. All use A.I. as part of their underlying technologies.\n  Think of AlphaGo's wins as a marker of this march of A.I. The program's Go matches are a reminder to many of us who don't follow all the advances day to day of just how much progress artificial intelligence is making each year. AlphaGo forced human Go players to change how they play the game. Even so, the humans have not been able to keep up.\n  ''AlphaGo is improving too fast,'' Mr. Ke said in a news conference after the first game. ''AlphaGo is like a different player this year compared to last year.''\n\n\n\n","54":"For a fleeting moment, the humans thought they had a chance.\nFour professional\u00a0poker players were convinced they found a flaw in the sophisticated artificial intelligence software that was beating them in a tournament of no-limit Texas hold 'em. If they bet in odd sizes, it seemed to trip up the computer. Within a day or two, though, that weakness vanished - and hope of beating the machine went with it.\n\"It became very demoralizing showing up every day and losing this hard,\" said Jason Les, who has played professional poker for a decade.\u00a0\nAnd a hard loss it was. When the 20-day tournament was done, the artificial intelligence, called Libratus, won a princely $1,766,250. All four professional players - Dong Kim, Daniel McAulay, Jimmy Chou and Les - finished in the negative.\nThe win demonstrates the increasing sophistication of artificial intelligence software as computer scientists work to digitally replicate, and ultimately improve upon, the human thought process. In this case, scientists demonstrated that AI can outwit the human brain in situations where at least some of the information\u00a0needed to make smart decisions is unknown.\nArtificial intelligence systems have mastered and beaten humans at other strategy games, such as Go and\u00a0chess, in which\u00a0both players have a full view of the game board. But poker is tricky: The computer doesn't know the hands that opponents have been dealt, or what decisions other players might make as a result.\nIn this way, poker is more reflective of the decisions people face everyday - using just the information available to them and their experience to make their decisions while also trying to account for the decisions that others will make in response. In economics and social science, it's a discipline called game theory.\nGames with imperfect information have made it hard for artificial intelligence to outwit the human brain. AI has defeated\u00a0players in\u00a0limit Texas hold 'em, which has more rules and structure, but not the more freewheeling no-limit version.\u00a0That changed Monday night with Libratus's win at the Rivers Casino in Pittsburgh.\nFortunately for the professional players, no money will change hands. The tournament was conducted for research purposes by the computer science department at Carnegie Mellon University.\u00a0Professor Tuomas Sandholm and doctoral student Noam Brown\u00a0hope Libratus can ultimately be used in a number of game theory scenarios, such as business negotiations, cybersecurity attacks or military operations. Basically, any situation in which two parties with different objectives and incomplete information need to find a resolution.\n\"There is usually hidden information that one side has or the other side has and they want to keep hidden,\" Brown said. \"We see this as research that is fundamental to dealing with uncertainty in the real world.\"\n\"This is not necessarily replacing humans, but it's taking their negotiation and strategic reasoning ability to another level as a support tool,\" Sandholm said.\nIn particular, the artificial intelligence developed at Carnegie Mellon was able to assess and revise its strategy each time an opponent made a move so that its decisions were optimized based on the most recent information. Past AI software did not adapt as quickly or as often while a hand was being played.\nEven still, real-world decision-making is more complex than a game of heads-up Texas hold 'em poker.\nThe tournament proved artificial intelligence can be successful in one-on-one competitions - each of the professionals played solo\u00a0against the machine - but it\u00a0becomes less effective when there are multiple actors to take into consideration.\u00a0What's more, poker is a zero-sum game with a defined hierarchy of outcomes - a royal flush always beats a full house. Real-world decisions, on the other hand, often require compromise, and whether one outcome is more or less desirable than another may depend entirely on the player's perspective, said Vincent Conitzer, a computer science professor at Duke University.\n\"Applying it to the real world does require a few more steps typically,\" Conitzer said.\nSandholm and Brown say the tournament's outcome will help determine those next steps for their research.\nParticipants were dealt 120,000 hands over the course of 20 days. Each played poker for 10 to 11 hours per day, then the four human competitors convened in the evenings to trade strategy tips and study\u00a0the computer's performance.\nBut while the humans plotted, Libratus was also preparing. Each night, a super computer would analyze all of the day's plays, compare them to past data, and learn lessons to make the algorithm smarter the next day. As an opponent, Libratus got stronger, not weaker, with each hit the professionals managed to land.\n\"If anyone had any doubt about the quality of this technology, I can tell you from our experience: We tried everything we could, and it was just too strong,\" Les said\n              Read more from The Washington Post's Innovations section.\u00a0           \n","55":"Seattle -- Inside the Allen Institute for Artificial Intelligence, known as AI2, everything is a gleaming architectural white. The walls are white, the furniture is white, the counters are white. It might as well have been a set for the space station in ''2001: A Space Odyssey.''\n''The brilliant white was a conscious choice meant to evoke experimental science -- think 'white lab coat,' '' said Oren Etzioni, a computer scientist and director of the new institute, which the Microsoft co-founder Paul Allen launched this year as a sibling of the Allen Institute for Brain Science, his effort to map the human brain.\nYet for the 30 (soon to be 50) artificial-intelligence researchers who can look out on a striking view of downtown Seattle, the futuristic surroundings offer a paradoxical note: AI2 is an effort to advance artificial intelligence while simultaneously reaching back into the field's past.\u00a0\nWhile Silicon Valley looks to fashionable techniques like neural networks and machine learning that have rapidly advanced the state of the art, Dr. Etzioni remains a practitioner of a modern version of what used to be known as Gofai, for good old-fashioned artificial intelligence.\nThe reference goes back to the earliest days of the field in the 1950s and '60s, when artificial-intelligence researchers were confident they could model human intelligence using symbolic systems -- logic embedded in software programs, running on powerful computers.\nThen in the late 1980s, an early wave of commercial artificial-intelligence companies failed, bringing on what became known as the ''A.I. winter.'' The field was seen as a failure and went into eclipse.\nIn recent years, however, A.I. has come roaring back as speech recognition, machine vision and self-driving cars have made progress with powerful computers, cheap sensors and machine-learning techniques. That has started a Silicon Valley gold rush led by Google, Facebook and Apple, drawing outsiders like Alibaba and Baidu in China, all caught up in a frantic race to hire the world's best machine-learning talent.\nBut the debate over how to reach genuine artificial intelligence has not ended, and Dr. Etzioni and Mr. Allen are betting that their path is more pragmatic. The power of the new techniques is not disputed, but there is a growing debate over whether they can take the field to human-level capabilities by themselves.\n''Think of it as Sherlock Holmes versus Spider-Man,'' said Jerry Kaplan, a visiting lecturer at Stanford who teaches a course on the history and philosophy of artificial intelligence, comparing Holmes's deductive powers with the irrational ''spider sense'' that tingles at the base of Spider-Man's skull and alerts him to danger.\nMr. Allen, who noted that he came from a family of librarians, said his decision to fund an artificial-intelligence research lab was inspired by the question of how books and other knowledge might be encoded to become the basis for computer interactions in which human questions might be answered more fully.\n''AI2 was born from a desire to create a system that could truly reason about knowledge, rather than just offer up what had been written on a subject before,'' he wrote in an email interview.\nDr. Etzioni says that the artificial-intelligence field has made incremental advances in areas like vision and speech, but that we have gotten no closer to the larger goal of true human-level systems.\n''Driverless cars are a great thing,'' he said, but added that the field had given rise to ''bad A.I., like the N.S.A. is using it or Facebook is using it to track you.''\n''We want to be the good guys,'' he went on, ''and it's up to us to deliver on that.''\nMoreover, he says, both he and Mr. Allen believe that technology cannot be separated from its social and economic consequences. They have added a social mission to the project that they call ''artificial intelligence for the common good.''\nThe success or failure of the project, however, will ultimately hinge on whether Dr. Etzioni can create a new synthesis of artificial intelligence, weaving together powerful machine-learning tools with traditional logic-oriented software.\nThe current fad for big data, of which machine learning is a major component, has significant limits. ''If you step back a little and say we want to do A.I., then you will realize that A.I. needs knowledge, reasoning and explanation,'' he said. ''My argument is that big data has made great progress in limited areas.''\nEven Watson, the brainy IBM computer whose intelligence the company wants to apply in complex applications like medical diagnoses and automated call centers with interactive speech recognition, will soon reach fundamental limits, he argues.\n''I really don't want a system that can't explain itself to be my doctor,'' he said. ''I can just imagine sitting there with Dr. Watson and the program saying, 'Well, we need to remove a kidney, Mr. Etzioni,' and I'm like, 'What?!' and they respond, 'Well, we have a lot of variables and a lot of data, and that's just what the model says.' ''\nDr. Etzioni, 50, was already known for innovative web projects, including MetaCrawler, an early search engine, and an array of successful start-up companies; one of them, Farecast, was acquired by Microsoft and became the basis for its Bing Travel service. (The first student to major in computer science at Harvard, he is a son of the well-known sociologist Amitai Etzioni.)\nAt AI2 he is motivated by Mr. Allen's view that ''in order to be truly intelligent, computers must understand -- that is probably the critical word,'' as the Microsoft co-founder put it in a 1977 interview.\nSome technology experts argue that self-aware computing machines are now on the horizon. ''As for A.I. progress, we're mostly haggling about a few decades,'' said Hans Moravec, a leading roboticist who is the chief scientist of Seegrid Corporation, a maker of autonomous vehicles for warehouse applications. ''I'm content to simply watch it play out, trying to do my part. I do want fully autonomous robots as soon as possible, to begin visiting the rest of the universe.''\nMr. Allen and Dr. Etzioni are not so optimistic. Both are skeptical of claims that we may be only years away from machines that think in any human sense.\n''Full A.I., in the sense of something like HAL in '2001,' '' Mr. Allen wrote in an email interview, ''is probably a hundred years away (or more). In reality, we are only beginning to grasp how deep intelligence works.''\nDr. Etzioni wants AI2 to set measurable goals to help get a new class of learning systems off the ground. During its first year, the researchers have focused on three projects -- one in computer vision (in which computers learn to recognize images), one to build a reasoning system capable of taking standardized school tests, and a third to help scholars deal with the fire hose of information that is inundating every scientific field.\nThe school-test effort, Project Aristo, seeks to create a learning program that can collect and organize a wide range of information, and then use that database to reason and to answer questions, even discussing and explaining its answers with human users.\nTo chart Aristo's progress, researchers plan to test it on increasingly difficult standardized science exams, moving from the fourth grade through the 12th.\n''We're not planning on putting 10th graders out of work,'' Dr. Etzioni said. But he does believe that a program that can converse with humans and answer questions would serve as a foundation for many other achievements, going far beyond the most powerful search engines and systems like Watson.\nIn September, the researchers celebrated their first milestone -- 60 percent correct answers in the language portion of New York State's fourth-grade science test. Many of the questions in the actual test include diagrams and illustrations, which will ultimately require advances in computer vision.\nThat challenge is considered far more difficult than recognizing human speech. It calls for a computer system with ''scene understanding,'' the human ability to extract meaning from animate and inanimate objects that interact.\nWhether AI2's research leads to a new generation of thinking machine or just more incremental advances, the project is a clear indication that artificial intelligence has once again become the defining force in the software world.\n''The narrative has changed,'' said Peter Norvig, Google's director of research. ''It has switched from, 'Isn't it terrible that artificial intelligence is a failure?' to 'Isn't it terrible that A.I. is a success?' ''\n","56":"SAN FRANCISCO -- For years, science-fiction moviemakers have been making us fear the bad things that artificially intelligent machines might do to their human creators. But for the next decade or two, our biggest concern is more likely to be that robots will take away our jobs or bump into us on the highway.\nNow five of the world's largest tech companies are trying to create a standard of ethics around the creation of artificial intelligence. While science fiction has focused on the existential threat of A.I. to humans, researchers at Google's parent company, Alphabet, and those from Amazon, Facebook, IBM and Microsoft have been meeting to discuss more tangible issues, such as the impact of A.I. on jobs, transportation and even warfare. \u00a0\n  Tech companies have long overpromised what artificially intelligent machines can do. In recent years, however, the A.I. field has made rapid advances in a range of areas, from self-driving cars and machines that understand speech, like Amazon's Echo device, to a new generation of weapons systems that threaten to automate combat.\n  The specifics of what the industry group will do or say -- even its name -- have yet to be hashed out. But the basic intention is clear: to ensure that A.I. research is focused on benefiting people, not hurting them, according to four people involved in the creation of the industry partnership who are not authorized to speak about it publicly.\n  The importance of the industry effort is underscored in a report issued on Thursday by a Stanford University group funded by Eric Horvitz, a Microsoft researcher who is one of the executives in the industry discussions. The Stanford project, called the One Hundred Year Study on Artificial Intelligence, lays out a plan to produce a detailed report on the impact of A.I. on society every five years for the next century.\n  One main concern for people in the tech industry would be if regulators jumped in to create rules around their A.I. work. So they are trying to create a framework for a self-policing organization, though it is not clear yet how that will function.\n  ''We're not saying that there should be no regulation,'' said Peter Stone, a computer scientist at the University of Texas at Austin and one of the authors of the Stanford report. ''We're saying that there is a right way and a wrong way.''\n  While the tech industry is known for being competitive, there have been instances when companies have worked together when it was in their best interests. In the 1990s, for example, tech companies agreed on a standard method for encrypting e-commerce transactions, laying the groundwork for two decades of growth in internet business.\n  The authors of the Stanford report, which is titled ''Artificial Intelligence and Life in 2030,'' argue that it will be impossible to regulate A.I. ''The study panel's consensus is that attempts to regulate A.I. in general would be misguided, since there is no clear definition of A.I. (it isn't any one thing), and the risks and considerations are very different in different domains,'' the report says.\n  One recommendation in the report is to raise the awareness of and expertise about artificial intelligence at all levels of government, Dr. Stone said. It also calls for increased public and private spending on A.I.\n  ''There is a role for government and we respect that,'' said David Kenny, general manager for IBM's Watson artificial intelligence division. The challenge, he said, is ''a lot of times policies lag the technologies.''\n  A memorandum is being circulated among the five companies with a tentative plan to announce the new organization in the middle of September. One of the unresolved issues is that Google DeepMind, an Alphabet subsidiary, has asked to participate separately, according to a person involved in the negotiations.\n  The A.I. industry group is modeled on a similar human rights effort known as the Global Network Initiative, in which corporations and nongovernmental organizations are focused on freedom of expression and privacy rights, according to someone briefed by the industry organizers but not authorized to speak about it publicly.\n  Separately, Reid Hoffman, a founder of LinkedIn who has a background in artificial intelligence, is in discussions with the Massachusetts Institute of Technology Media Lab to fund a project exploring the social and economic effects of artificial intelligence.\n  Both the M.I.T. effort and the industry partnership are trying to link technology advances more closely to social and economic policy issues. The M.I.T. group has been discussing the idea of designing new A.I. and robotic systems with ''society in the loop.''\n  The phrase is a reference to the long-running debate about designing computer and robotic systems that still require interaction with humans. For example, the Pentagon has recently begun articulating a military strategy that calls for using A.I. in which humans continue to control killing decisions, rather than delegating that responsibility to machines.\n  ''The key thing that I would point out is computer scientists have not been good at interacting with the social scientists and the philosophers,'' said Joichi Ito, the director of the MIT Media Lab and a member of the board of directors of The New York Times. ''What we want to do is support and reinforce the social scientists who are doing research which will play a role in setting policies.''\n  The Stanford report attempts to define the issues that citizens of a typical North American city will face in computers and robotic systems that mimic human capabilities. The authors explore eight aspects of modern life, including health care, education, entertainment and employment, but specifically do not look at the issue of warfare. They said that military A.I. applications were outside their current scope and expertise, but they did not rule out focusing on weapons in the future.\n  The report also does not consider the belief of some computer specialists about the possibility of a ''singularity'' that might lead to machines that are more intelligent and possibly threaten humans.\n  ''It was a conscious decision not to give credence to this in the report,'' Dr. Stone said.\n\n\n\n","57":" Louis Hodes Scientist \nLouis Hodes, 74, a mathematician and scientist who conducted groundbreaking work in artificial intelligence, computer programming language and cancer research, died June 30 of pulmonary failure at Suburban Hospital. He lived in Rockville.\nDr. Hodes was at the forefront of computer-related research at the Massachusetts Institute of Technology and the National Institutes of Health.\nWhile working toward his doctorate in mathematic logic at MIT from 1957 to 1962, he studied under two founders of theoretical computer science and artificial intelligence, Marvin Minsky and John McCarthy.\u00a0\nDr. Hodes was a member of the artificial intelligence group of the MIT Research Laboratory of Electronics and did pioneering work in the development of the computer programming language LISP, which was used in artificial intelligence research.\nHe also is credited with being one of the first people to recognize that logic could be used as a programming language.\nIn 1966, Dr. Hodes joined NIH and worked in the artificial intelligence laboratory before moving to the National Cancer Institute. There he worked on computer tools for biomedical applications, including developing a software for online analysis of biomedical images.\nProminent among his contributions to cancer research was the computer system he designed that compared chemicals compounds. His work helped the NCI remove potential anti-tumor chemicals that were being tested on animals, resulting in fewer animals being used in scientific testing.\n\"We're saving twice as many animals as we used to,\" Dr. Hodes said in a 1982 article in the Globe and Mail of Toronto.\nHe was born in New York City and graduated summa cum laude with a degree in electrical engineering from what was then the Polytechnic Institute of Brooklyn. He worked full time at the post office to put himself through college.\nHis PhD thesis at MIT was titled, \"Hyperarithmetical Real Numbers and Hyperarithmetical Analysis,\" and noted mathematician Hartley Rogers Jr. was his adviser.\nAll his life, Dr. Hodes had peripheral neuropathy, a nerve condition that made walking and other activities difficult.\nHe began painting pastel portraits 10 years ago, had two one-man shows in Rockville and won prizes for his paintings. He belonged to the Rockville Art League, Strathmore Artists and the Senior Artists Alliance.\nSurvivors include his wife of 40 years, Susan Hodes of Rockville, and a brother.\n -- Yvonne Shinhoster Lamb   Albert W. Stoffel Foreign Service Officer \nAlbert W. Stoffel, 92, a retired Foreign Service officer specializing in economics and aviation who retired in 1966 and spent nearly 20 years as director of international affairs for Boeing, died June 19 of respiratory and congestive heart failure at the Washington Home hospice.\nMr. Stoffel became a Foreign Service officer in 1946 and during a 20-year career served in Saigon (Ho Chi Minh City), Toronto, Berlin and Paris.\nIn Berlin from 1950 to 1954, he was responsible for reporting on economic developments in the German Democratic Republic (East Germany).\nFrom 1962 until his retirement in 1966, he was the regional civil air attache to Bonn, Germany, and was responsible for administering aviation relationships between the United States and the Federal Republic of Germany, Austria and Yugoslavia.\nMr. Stoffel, a District resident, was a native of Rochester, N.Y., and a 1938 economics graduate of the University of Rochester.\nHe became a civilian radar technician with the British air force before the United States entered the war. Later, he became an Army Air Forces communications officer and served in North Africa, the Mediterranean and Europe.\nHis wife, Helena Gilda Stoffel, died in 2004. A son, Dr. Robert John Stoffel, died in 2005.\nSurvivors include two children, A. William Stoffel of Mitchellville and Elizabeth Jean Stoffel of Scientists Cliffs; six grandchildren; and two great-grandchildren.\n-- Joe Holley Jean E. O'Riordan Business Founder \nJean E. O'Riordan, 77, who helped found and direct several Washington area businesses with her daughter, died July 25 at the Washington Home hospice of complications from pancreatic cancer. She lived in the District.\nMs. O'Riordan moved to Washington in 1985 and was director of operations for the O'Riordan Bethel Law Firm from 1995 until she died.\nShe had managed a property management business since 2001 and, for the past two years, was a director of Washington Consulting Corp., an international consulting business.\nMs. O'Riordan was born in New York and began her career in the 1950s as a secretary to a vice president of IBM. She later developed reorganization plans and business systems for small businesses and nonprofit corporations.\nFrom 1976 to 1984, she was president of the Honeybunch Inc., a flower shop in Connecticut.\nHer marriage to Daniel J. O'Riordan ended in divorce.\nSurvivors include a daughter, Carol L. O'Riordan of Washington.\n-- Matt Schudel \n","59":"When is a supercomputer not a supercomputer? When it is the goal of the Defense Department's new Supercomputer Project, which is devoted to another type of advanced computing technology, artificial intelligence. The project is one of a panoply of American responses to the Japanese computer challenge. The name is unfortunate, for it confuses two technologies whose proponents face stiff competition from Japan and who are vying with each other for national priority.\nForthcoming Federal reports suggest the priority battle has been settled: Programs in artificial intelligence and supercomputers will be undertaken by separate Government agencies, although artificial intelligence will receive a more ambitious development effort.\u00a0\nSupercomputers are machines tailored to high-speed multiplications and additions. The fastest of these ''number crunchers'' can perform more than 100 million calculations a second. Only a few dozen supercomputers are being used today and only for complex tasks, such as designing nuclear weapons and forecasting weather.\nComputers with artificial intelligence would ''think.'' They would understand spoken and written English, recognize objects and reason as an expert in, say, medicine or engineering. Like their number-crunching cousins, artificial intelligence computers require super-fast calculations. That's where the similarities end, for these machines must be built to react to varying demands and to process symbols, not numbers.\nThe Japanese have mounted two national programs: the Fifth Generation Project, which is trying to capture a large hunk of the multibillion dollar computer market with artificial intelligence machines that are smart and easy to use, and the Superspeed Project, which is concentrating on supercomputer research and development.  In the United States, the Defense Department's Advanced Research Projects Agency has focused on artificial intelligence, with a projected start-up budget of $50 million in the current fiscal year. Robert Cooper, the agency's director, said the program is an answer not to the Japanese computer challenge, but to the nation's defense needs. Program goals include the development of planes, submarines and land vehicles that can undertake unmanned missions; the creation of ''fire and forget'' weapons that, fired from afar, could seek their own targets; and the evolution of military expert systems that could recommend alternative maneuvers to battlefield commanders and pilots.  Nevertheless, DARPA research should aid America's computer industry. ''It's our best bet,'' said Edward Feigenbaum, a Stanford professor and co-author of ''The Fifth Generation,'' which warns of the Japanese threat.\u00a0Limited Defense Needs\nDr. Cooper said supercomputers merit less attention since they have more limited defense uses. He noted, too, that artificial intelligence is the more experimental field requiring longer-term research; the supercomputer business is an existing market with two established manufacturers - Cray Research and Control Data Corporation, both of Minneapolis. ''Only a few such processors are needed by Defense,'' Dr. Cooper said, ''and it is our judgment that industry will be able to supply those.'' (Just last week, Japan's state-owned telephone company ordered a supercomputer from Cray, suggesting that, for the moment, Americans still lead the field.) Dr. Cooper's assessment does not sit well with critics who believe the industry will not develop the radical approaches needed for great leaps in computing speed. ''Supercomputers are certainly as important as artificial intelligence,'' said Peter D. Lax, a New York University mathematician who led an expert Government panel studying the issue. The panel's report, released about a year ago, said American manufacturers were likely to fall behind in supercomputer development, which would force the United States to depend on the Japanese for machines used in weapons design.\nProponents of an increased supercomputer effort say it is also vital to industrial innovation. Supercomputers, they contend, can help in the design of airplanes and other commercial products, and are needed by researchers in fields such as astronomy and subatomic physics. Nobel laureate Kenneth G. Wilson, a physics professor at Cornell, said some scientists are going to Europe or Japan, where computer time is more accessible.\nThe White House Office of Science and Technology formed three interagency committees in May to examine the issue. The reports have not been released, but two studies have been submitted to the White House. One recommends that the Government promise to purchase a certain number of supercomputers that achieve 200 times the speed of existing models, thus giving manufacturers a market incentive. Panelists also suggested setting up a users network that would allow university researchers to tie into supercomputers at national laboratories. This would benefit the scientists directly and expand the supercomputer market as well.\n''We don't feel it is necessary to provide direct research and development support for supercomputer vendors,'' said James Decker of the Department of Energy, who headed two of the committees. Sidney Fernbach, a computer consultant, disagreed. The committee proposals, he said, are ''not strong enough.'' Professor Feigenbaum insists both technologies merit major efforts. ''This is not a zero-sum game,'' he said. ''The Japanese have two programs and so should we.''\n","60":"SAN FRANCISCO -- In July, China unveiled a plan to become the world leader in artificial intelligence and create an industry worth $150 billion to its economy by 2030.\nTo technologists working on A.I. in the United States, the statement, which was 28 pages long in its English translation, was a direct challenge to America's lead in arguably the most important tech research to come along in decades. It outlined the Chinese government's aggressive plan to treat A.I. like the country's own version of the Apollo 11 lunar mission -- an all-in effort that could stoke national pride and spark agenda-setting technology breakthroughs. \n  The manifesto was also remarkably similar to several reports on the future of artificial intelligence released by the Obama administration at the end of 2016.\n  ''It is remarkable to see how A.I. has emerged as a top priority for the Chinese leadership and how quickly things have been set into motion,'' said Elsa Kania, an adjunct fellow at the Center for a New American Security who helped translate the manifesto and follows China's work on artificial intelligence. ''The U.S. plans and policies released in 2016 were seemingly the impetus for the formulation of China's national A.I. strategy.''\u00a0\n  But six months after China seemed to mimic that Obama-era road map, A.I. experts in industry and academia in the United States say that the Trump White House has done little to follow through on the previous administration's economic call to arms.\n  ''We are still waiting on the White House to provide some direction'' on how to respond to the competition, said Tim Hwang, who worked on A.I. policy at Google and is now the director of the Ethics and Governance of AI Initiative, a new organization created by the LinkedIn founder Reid Hoffman and others to fund ethical research in artificial intelligence.\n  China's embrace of A.I. comes at a crucial time in the development of the technology and just as the lead long enjoyed by the United States has started to dwindle.\n  For decades, artificial intelligence was more fiction than science. In the past few years, however, dramatic improvements have prompted some of the biggest companies in Silicon Valley and Detroit -- and China -- to invest billions on everything from self-driving cars to home appliances that can have a conversation with a human.\n  A.I. has also become a significant part of national defense policy as military leaders and ethicists debate how much autonomy we should give to weapons that can think for themselves.\n  American companies like Amazon and Google have done more than anyone to turn A.I. concepts into real products. But for a number of reasons, including concerns that the Trump administration will limit the number of immigrant engineers allowed into the United States, much of the critical research being done on artificial intelligence is already migrating to other countries, including tech hot spots like Toronto, London and Beijing.\n  To China's growing tech community, driving the industry's next big thing -- a mantra of Silicon Valley -- is becoming a tantalizing possibility.\n  ''Thanks to the size of the market and the rapid experimentation, China is going to become one of the most powerful -- if not the most powerful -- A.I. countries in the world,'' said Kai Fu-Lee, a former Microsoft and Google executive who now runs a prominent Chinese venture capital firm dedicated to artificial intelligence.\n  The 2016 A.I. reports were shepherded by President Barack Obama's Office of Science and Technology Policy.\n  The O.S.T.P., which has overseen science and technology activities across the federal government for more than four decades, is now run by the deputy chief technology officer Michael Kratsios. He had worked as a Wall Street analyst before serving as chief of staff for an investment fund run by Peter Thiel, a venture capitalist who supported Mr. Trump's presidential run. The administration has yet to name an office director or fill four other assistant posts.\n  In a recent interview, Mr. Kratsios was adamant that any concerns over the administration's approach to A.I. were unfounded.\n  ''Artificial\u00a0intelligence has been a priority for the Trump administration since Day 1,'' he said. Mr. Kratsios added that the administration was particularly concerned with the development of A.I. in national security and as a way of encouraging economic prosperity.\n  Many staff members in Mr. Kratsios's office are exploring issues related to artificial intelligence, he said. Mr. Kratsios also meets with a committee, set up by the Obama administration, that coordinates A.I. policy across the government.\n  ''The key thing to remember is that the front line of A.I. policy is at the agencies,'' he said. ''The White House is a convener and a coordinator.''\n  In an echo of plans laid out by the Obama administration, China's government said it intended to significantly increase long-term funding for A.I. research and develop a much larger community of A.I. researchers.\n  There are several ways to do that, according to the Obama administration and China. First, educate more students in these technologies. Second, recruit experts from other countries.\n  At the same time, both policy statements urged companies to share more technology and data. Huge pools of data are need to ''train'' A.I. systems, and in the United States much of this is locked up inside companies like Facebook and Google. Mr. Lee said China already has an enormous advantage here because its large population will generate more data and its companies are more willing to share.\n  Artificial\u00a0intelligence has been a focus of Chinese technologists for some time. By 2013, China was already producing more research papers than the United States in the area of ''deep learning,'' the main technology driving the rise of A.I., according to the Obama reports. Deep learning, which allows machines to learn tasks by analyzing vast amounts of data, is one of the main technologies driving the rise of artificial intelligence.\n  It is unclear how much China as a whole is spending. But one Chinese state has promised to invest $5 billion in A.I., and the government of Beijing has committed $2 billion to an A.I. development park in the city. South Korea has set aside close to $1 billion of its own. Canada, already home to many of the top researchers in the field, has also committed $125 million to, in part, attract new talent from other countries.\n  It is also difficult to say just how much the government of the United States is spending. Government organizations like the Intelligence Advanced Research Projects Activity, the National Institute of Standards and Technology, and the National Science Foundation continue to fund new research in universities and the private sector. According to an O.S.T.P report, the federal government spent about $1 billion a year in 2015. The Trump administration says that spending jumped to $3 billion in 2017. But the current administration said that was not an apples-to-apples comparison to the 2015 tally, because it was not certain how the Obama administration made it calculations.\n  ''We may have a bunch of small initiatives inside the government that are doing good, but we don't have a central national strategy,'' said Jack Clark, a former journalist who now oversees policy efforts at OpenAI, the artificial intelligence lab co-founded by Elon Musk, Tesla's chief executive. ''It is confusing that we have this technology of such obvious power and merit and we are not hearing full-throated support, including financial support.''\n  The Trump administration's budget for 2018 aims to cut science and technology research funding across the government by 15 percent, according to a report from the American Association for the Advancement of Science.\n  ''They are headed in precisely the wrong direction,'' said Thomas Kalil, who led O.S.T.P's Technology and Innovation Division under President Obama. ''That is particularly concerning given that China has identified this as a strategic priority.''\n  Over the past five years, much of the progress in A.I. technology has been led by American companies like Google, Microsoft, Amazon and Facebook. But these companies don't need A.I. technologists to work in the United States in order to employ them.\n  Take Geoffrey Hinton, a major figure in the rise of A.I. at Google and across the tech industry. He recently moved back to Toronto, where he was a professor for many years. He now runs a new Google lab in that city. Last year, he took on an Iranian researcher who was denied a visa by the United States government.\n  Google operates another important lab in Montreal. Its London lab, DeepMind, may be home to more top-notch A.I. researchers than any other lab on earth. And Google recently unveiled new labs in both Paris and Beijing. Facebook, after creating its own lab in Canada, recently pumped 10 million euros, or more than $12 million, into its existing operation in Paris. And Amazon is opening a lab in Germany.\n  Inside these facilities, researchers still create technology for their American employers. As the labs grow and the products get better, some employees can be expected to leave to start their own companies and hire their own employees.\n  Google's and Microsoft's work in China has already led to Chinese start-ups like Malong, which is building image recognition systems, and a major A.I. investment fund run by Mr. Lee.\n  ''When it is close to you, something like Microsoft Research has real economic value,'' said Mr. Clark, of OpenAI.\n\n\n\n","61":"BOBROW--Daniel (Danny) G.,PhD. November 29, 1935 - March 20, 2017. Daniel (Danny) Bobrow passed away peacefully at home with his wife Toni and daughters Kimberly and Deborah in Palo Alto, California, on March 20, 2017, having bravely fought a five-month battle with cancer. Danny was born to Ruth Gureasko Bobrow and Jacob Bobrow on November 29, 1935, in the Bronx, New York City. \u00a0\nA gifted student, he attended Bronx High School of Science and went on to earn a BS from Rensselaer Polytechnic Institute, an MS from Harvard, and a PhD in Mathematics from Massachusetts Institute of Technology under the supervision of Marvin Minsky. His was one of the first MIT doctoral theses in Artificial Intelligence. A pioneer with a long and distinguished research career in Artificial Intelligence as a Research Fellow in the System Sciences Laboratory of the Palo Alto Research Center (PARC), he is remembered as a mentor, friend, and role model for many. One esteemed colleague wrote: \"Danny Bobrow's death is a huge loss for PARC\/Xerox and the research community at large. Danny was an influential researcher, true gentleman and a wonderful person.\" Danny served as president of the American Association for Artificial Intelligence (AAAI), president of the Cognitive Science Society, editor-in-chief of the Artificial Intelligence Journal, and also was a recipient of the Association of Computing Machinery (ACM) Software Systems Award and a fellow of both the ACM and AAAI. He was key to the development of many ground- breaking systems, starting with his doctoral dissertation, STUDENT, and followed by the widely used programming languages Logo and Interlisp; the (innovative) operating system TENEX; the knowledge-sharing system Eureka; and more recently, Powerset, the natural language search engine. Beyond his profound professional accomplishments, Danny was a truly caring husband, father, and friend to all who had the privilege to know him. He shared 39 joyful years with his beloved wife Toni Wagner Bobrow. Together, they enjoyed many world travel adventures and had a mutual passion for the arts and literature. Danny took great pleasure in fatherhood, enjoying a rich and special relationship with each of his daughters, Kimberly and Deborah, and his son Jordan. He shared a warm and close relationship with his brothers, Michael, Rusty and Eric, as well as many extended family members. Danny is survived by his wife, Toni Wagner Bobrow; his children, Kimberly Bobrow Jennery, Deborah Bobrow, and Jordan Bobrow; and his brothers, Michael Bobrow, Robert (Rusty) Bobrow, and Eric Bobrow. The family requests that in lieu of flowers, a donation in his name be given to one of Danny's favorite nonprofit organizations: KQED (or a local PBS\/NPR station), Planned Parenthood or the ACLU. 1\/3\n","62":"Google search, I.B.M.'s Watson Jeopardy-winning computer, credit-card fraud detection and automated speech recognition.\nThere seems not much in common on that list. But it is a representative sampling of the kinds of modern computing chores that use the ideas and technology developed by Judea Pearl, the winner of this year's Turing Award.\nThe award, often considered the computer science equivalent of a Nobel prize, was announced on Wednesday by the Association for Computing Machinery.\nDr. Pearl, 75, a professor at the University of California, Los Angeles, is being honored for his contributions to the development of artificial intelligence.\nIn the 1970s and 1980s, the dominant approach to artificial intelligence was to try to capture the process of human judgment in rules a computer could use. They were called rules-based expert systems.\nDr. Pearl championed a different approach of letting computers calculate probable outcomes and answers. It helped shift the pursuit of artificial intelligence onto more favorable terrain for computing.\n\"It allowed us to learn from the data rather than write down rules of logic,\" said Peter Norvig, an artificial intelligence expert and research director at Google. \"It really opened things up.\"\nDr. Pearl, with his work, he added, \"was influential in getting me, and many others, to adopt this probabilistic point of view.\"\nAnd Dr. Pearl had not only a conceptual insight, but also a technical framework to make it practical. \"He figured out how to do the calculation,\" Mr. Norvig said.\nDr. Pearl's work on Bayesian networks -- named for the 18th-century English mathematician Thomas Bayes -- provided \"a basic calculus for reasoning with uncertain information, which is everywhere in the real world,\" said Stuart Russell, a professor of computer science at the University of California, Berkeley. \"That was a very big step for artificial intelligence.\"\nDr. Pearl said he was not surprised that his ideas are seen in many computing applications. \"The applications are everywhere, because uncertainty is everywhere,\" Dr. Pearl said.\n\"But I didn't do the applications,\" he continued. \"I provided a way for thinking about your problem, and the formalism and framework for how to do it.\"\nThe Turing Award, named for the English mathematician Alan M. Turing, includes a cash prize of $250,000, with financial support from Intel and Google.\nDr. Pearl said he hadn't entirely decided what he would do with the money. But he said he would probably divide it in three parts, as he has with other scientific prizes he has received in recent years. One part, he said, would very likely go to support research at the Israel Institute of Technology, or Technion, where Dr. Pearl received his bachelor's degree; part would be set aside for his five grandchildren; and part would go to the Daniel Pearl Foundation, which promotes cross-cultural understanding through journalism and music programs.\nDaniel Pearl, a Wall Street Journal reporter who was kidnapped and murdered in Pakistan in 2002, was Dr. Pearl's son.\n.\n\n","64":"To the Editor:\nRe ''How to Regulate Artificial Intelligence,'' by Oren Etzioni (Op-Ed, Sept. 2): \u00a0\n  Last year, my lab at Georgia Tech created Jill Watson, an A.I.-powered virtual teaching assistant designed to help answer students' questions in the discussion forum of an online class on artificial intelligence. To assess Jill's performance properly, we chose not to reveal her identity until the conclusion of the class.\n  Mr. Etzioni characterized our experiment as an effort to ''fool'' students. The point of the experiment was to determine whether an A.I. agent could be indistinguishable from human teaching assistants on a limited task in a constrained environment. (It was.)\n  When we did tell the students about Jill, their response was uniformly positive.\n  We were aware of the ethical issues and obtained approval of Georgia Tech's Institutional Review Board, the office responsible for making sure that experiments with human subjects meet high ethical standards.\n  We believe that experiments like Jill are critical for deeply understanding the emerging ethics of artificial intelligence.\n  ASHOK GOEL, ATLANTA\n  The writer is a professor of computer science at Georgia Institute of Technology.\n\n\n\n","65":"SEOUL, South Korea -- Computer, one. Human, zero.\nA Google computer program stunned one of the world's top players on Wednesday in a round of Go, which is believed to be the most complex board game ever created. \n  The match -- between Google DeepMind's AlphaGo and the South Korean Go master Lee Se-dol -- was viewed as an important test of how far research into artificial intelligence has come in its quest to create machines smarter than humans.\u00a0\n  ''I am very surprised because I have never thought I would lose,'' Mr. Lee said at a news conference in Seoul. ''I didn't know that AlphaGo would play such a perfect Go.''\n  Mr. Lee acknowledged defeat after three and a half hours of play.\n  Demis Hassabis, the founder and chief executive of Google's artificial intelligence team DeepMind, the creator of AlphaGo, called the program's victory a ''historic moment.''\n  The match, the first of five scheduled through Tuesday, took place at a Seoul hotel amid intense news media attention. Hundreds of reporters, many of them from China, Japan and South Korea, where Go has been played for centuries, were there to cover it. Tens of thousands of people watched the contest live on YouTube.\n  Go is a two-player game of strategy said to have originated in China 3,000 years ago. Players compete to win more territory by placing black and white ''stones'' on a board made up of 19 lines by 19 lines.\n  The play is more complex than chess, with a far greater possible sequence of moves, and requires superlative instincts and evaluation skills. Because of that, many researchers believed that mastery of the game by a computer was still a decade away.\n  Before the match, Mr. Lee said he could win 5-0 or 4-1, predicting that computing power alone could not win a Go match. Victory takes ''human intuition,'' something AlphaGo has not yet mastered, he said.\n  But after reading more about the program he became less upbeat, saying that AlphaGo appeared able to imitate human intuition to a certain degree and predicting that artificial intelligence would eventually surpass humans in Go.\n  AlphaGo posed Mr. Lee a unique challenge. In a human-versus-human Go match, which typically lasts several hours, the players ''feel'' each other and evaluate styles and psychologies, he said.\n  ''This time, it's like playing the game alone,'' Mr. Lee said on the eve of the match. ''There are mistakes humans make because they are humans. If that happens to me, I can lose a match.''\n  To researchers who have been using games as platforms for testing artificial intelligence, Go has remained the great challenge since the I.B.M.-developed supercomputer Deep Blue beat the world chess champion Garry Kasparov in 1997.\n  ''Really, the only game left after chess is Go,'' Mr. Hassabis said on Wednesday.\n  AlphaGo made news when it routed the three-time European Go champion Fan Hui in October, 5-0.\n  But Mr. Lee, 33, is one of the world's most accomplished professional Go players, with 18 international titles under his belt. He has called the European champion's level in Go ''near the top among amateurs.''\n  AlphaGo has become much stronger since its matches with Mr. Fan, its developers said. It challenged Mr. Lee because it was ready to take on someone ''iconic,'' ''a legend of the game,'' Mr. Hassabis said. Google offered Mr. Lee $1 million if he wins the best-of-five series.\n  Mr. Hassabis said AlphaGo did not try to consider all the possible moves in a match, as a traditional artificial intelligence machine like Deep Blue does. Rather, it narrows its options based on what it has learned from millions of matches played against itself and in 100,000 Go games available online.\n  Mr. Hassabis said that a central advantage of AlphaGo was that ''it will never get tired, and it will not get intimidated either.''\n  Kim Sung-ryong, a South Korean Go master who provided commentary during Wednesday's match, said that AlphaGo made a clear mistake early on, but that unlike most human players, it did not lose its ''cool.''\n  ''It didn't play Go as a human does,'' he said. ''It was a Go match with human emotional elements carved out.''\n  Mr. Lee said he knew he had lost the match after AlphaGo made a move so unexpected and unconventional that he thought ''it was impossible to make such a move.''\n  Mr. Lee said he now thought his chances for victory in the five-match series were 50-50.\n  Some computer scientists said Wednesday that they had expected the outcome.\n  ''I'm not surprised at all,'' said Fei-Fei Li, a Stanford University computer scientist who is director of the Stanford Artificial Intelligence Laboratory. ''How come we are not surprised that a car runs faster than the fastest human?''\n  On Tuesday, before the match began, Oren Etzioni, the director of the Allen Institute for Artificial Intelligence, a nonprofit research organization in Seattle, conducted a survey of leading members of the Association for the Advancement of Artificial Intelligence.\n  Of 55 scientists, 69 percent believed that the program would win, and 31 percent believed that Mr. Lee would be victorious. Moreover, 60 percent believed that the achievement could be considered a milestone toward building human-level artificial intelligence software.\n  That question remains one of the most hotly debated within the field of artificial intelligence. Machines have had increasing success in the past half-decade at narrow humanlike capabilities, like understanding speech and vision.\n  However, the goal of ''strong A.I.'' -- defined as a machine with an intellectual capability equal to that of a human -- remains elusive.\n  Other artificial-intelligence scientists said that humans might still find refuge if the goal posts for the competition were moved.\n  ''I wonder what would happen if they played on a 29-by-29 grid?'' wondered Rodney Brooks, a pioneering artificial intelligence researcher. By enlarging the playing space, humans might once again escape the machine's computing power.\n\n\n\n","66":"TORONTO -- As an undergraduate at Cambridge University, Geoffrey Everest Hinton thought a lot about the brain. He wanted to better understand how it worked but was frustrated that no field of study -- from physiology and psychology to physics and chemistry -- offered real answers.\nSo he set about building his own computer models to mimic the brain's process. \n  ''People just thought I was crazy,'' said Dr. Hinton, now 69, a Google fellow who is also a professor emeritus of computer science at the University of Toronto.\n  He wasn't. He became one of the world's foremost authorities on artificial intelligence, designing software that imitates how the brain is believed to work. At the same time, Dr. Hinton, who left academia in the United States in part as a personal protest against military funding of research, has helped make Canada a high-tech hotbed.\u00a0\n  Dictate a text on your smartphone, search for a photo on Google or, in the not too distant future, ride in a self-driving car, and you will be using technology based partly on Dr. Hinton's ideas.\n  His impact on artificial intelligence research has been so deep that some people in the field talk about the ''six degrees of Geoffrey Hinton'' the way college students once referred to Kevin Bacon's uncanny connections to so many Hollywood movies.\n  Dr. Hinton's students and associates are now leading lights of artificial intelligence research at Apple, Facebook, Google and Uber, and run artificial intelligence programs at the University of Montreal and OpenAI, a nonprofit research company.\n  ''Geoff, at a time when A.I. was in the wilderness, toiled away at building the field and because of his personality, attracted people who then dispersed,'' said Ilse Treurnicht, chief executive of Toronto's MaRS Discovery District, an innovation center that will soon house the Vector Institute, Toronto's new public-private artificial intelligence research institute, where Dr. Hinton will be chief scientific adviser.\n  Dr. Hinton also recently set up a Toronto branch of Google Brain, the company's artificial intelligence research project. His tiny office there is not the grand space filled with gadgets and awards that one might expect for a man at the leading edge of the most transformative field of science today. There isn't even a chair. Because of damaged vertebrae, he stands up to work and lies down to ride in a car, stretched out on the back seat.\n  ''I sat down in 2005,'' said Dr. Hinton, a tall man, with uncombed silvering hair and hooded eyes the color of the North Sea.\n  Dr. Hinton started out under a constellation of brilliant scientific stars. He was born in Britain and grew up in Bristol, where his father was a professor of entomology and an authority on beetles. He is the great-great-grandson of George Boole, the father of Boolean logic.\n  His middle name comes from another illustrious relative, George Everest, who surveyed India and made it possible to calculate the height of the world's tallest mountain that now bears his name.\n  Dr. Hinton followed the family tradition by going to Cambridge in the late 1960s. But by the time he finished his undergraduate degree, he realized that no one had a clue how people think.\n  ''I got fed up with academia and decided I would rather be a carpenter,'' he recalled with evident delight, standing at a high table in Google's white-on-white cafe here. He was 22 and lasted a year in the trade, although carpentry remains his hobby today.\n  When artificial intelligence coalesced into a field of study from the fog of information science after World War II, scientists first thought that they could simulate a brain by building neural networks assembled from vast arrays of switches, which would mimic synapses.\n  But the approach fell out of favor because computers were not powerful enough then to produce meaningful results. Artificial\u00a0intelligence research turned instead to using logic to solve problems.\n  As he was having second thoughts about his carpentry skills, Dr. Hinton heard about an artificial intelligence program at the University of Edinburgh and moved there in 1972 to pursue a Ph.D. His adviser favored the logic-based approach, but Dr. Hinton focused on artificial neural networks, which he thought were a better model to simulate human thought.\n  His study didn't make him very employable in Britain, though. So, Ph.D. in hand, he turned to the United States to work as a postdoctoral researcher in San Diego with a group of cognitive psychologists who were also interested in neural networks.\n  They were soon making significant headway.\n  They began working with a formula called the back propagation algorithm, originally described in a 1974 Harvard Ph.D. thesis by Paul J. Werbos. That algorithm allowed neural networks to learn over time and has since become the workhorse of deep learning, the term now used to describe artificial intelligence based on those networks.\n  Dr. Hinton moved in 1982 to Carnegie Mellon University in Pittsburgh as a professor, where his work with the algorithm and neural networks allowed computers to produce some ''interesting internal representations,'' as he put it.\n  Here's an example of how the brain produces an internal representation. When you look at a cat -- for some reason cats are a favorite subject of artificial intelligence research -- light waves bouncing off it hit your retina, which converts the light into electrical impulses that travel along the optic nerve to the brain. Those impulses, of course, look nothing like a cat. The brain, however, reconstitutes those impulses into an internal representation of the cat, and if you close your eyes, you can see it in your mind.\n  ''In A.I., the holy grail was how do you generate internal representations,'' Dr. Hinton explained.\n  Interesting as those internal representations were from an academic point of view, computers were still too slow to recreate them in a way that mimicked the brain.\n  At that point, Dr. Hinton was becoming disillusioned with the politics of the United States in the Reagan era. He also didn't like that most artificial intelligence research was funded by the United States military.\n  Canada beckoned with a research position at the Canadian Institute For Advanced Research. He moved to Toronto and eventually set up a program at the institute that is now called Learning in Machines & Brains.\n  He also joined the University of Toronto as a professor of computer science, though he confesses that he has never taken a computer science course himself.\n  By 2012, computers had become fast enough to allow him and his researchers to create those internal representations as well as reproduce speech patterns that are part of the translation applications we all use today.\n  He formed a company specializing in speech and photo recognition with two of his students at the University of Toronto.  Google bought the business, so Dr. Hinton joined Google half time and continues to work there on creating artificial neural networks.\n  The deal made Dr. Hinton a wealthy man.\n  Now he is turning his attention to health care, thinking that artificial intelligence technology could be harnessed to scan lesions for cancer. The combination of the Vector Institute, a surrounding cluster of hospitals and government support, he added, makes Toronto ''one of the best places in the world to do it.''\n\n\n\n","68":"Once the territory of large corporations, research laboratories and the military, advanced artificial intelligence research is now increasingly driven by the consumer electronics and entertainment industries.\nMore than a dozen autonomous software programs capable of operating independently -- performing tasks ranging from electronic shopping to retrieving information over the Internet -- were demonstrated at the First International Conference on Autonomous Agents, which ended Saturday in Marina Del Rey, Calif.\u00a0\n The push to commercialization of artificial intelligence is just being renewed after numerous disappointments during the 1980's. Today a growing number of researchers and entrepreneurs believes that the explosion of the Internet is paving the way for new artificial intelligence applications.\nA number of executives from companies including the AT&T Corporation, the Microsoft Corporation and the International Business Machines Corporation were meeting in Marina Del Rey yesterday and today in an effort to develop industry standards for autonomous agents.\nDiscussions at the conference highlighted the fundamental shift that has occurred in the financing of artificial intelligence research.\n\"What is driving artificial intelligence now is the entertainment industry instead of the defense industry,\" said Danny Hillis, a vice president and research fellow at the Walt Disney Company. Mr. Hillis, a computer scientist who founded the supercomputer maker Thinking Machines Corporation, said that the shift had led to a sometimes rough cultural adjustment for computer researchers.\n\"The term 'agent' means something very different in Hollywood,\" he said.\nHowever, the technology shift in the artificial intelligence field parallels a similar transformation in other areas of the computer industry, where increasingly powerful computing technologies are showing up first in consumer applications.\nThe transition has taken place in part because simplifying computing -- with voice recognition software, for instance -- often requires tremendous increases in computer power. Moreover, with the end of the cold war, the resources to make substantial investments in new computer technologies tend to be found among those companies that make products designed to go under Christmas trees.\nThis year's conference included multiagent programs that created synthetic characters intended to act as \"greeters\" on World Wide Web sites, toy robots that act like pets, software \"agents\" intended to simplify computer tasks and special computer gateways for voice-controlled Internet browsing or retrieval of electronic mail from cellular telephones.\nMr. Hillis said that autonomous software had already made its way into films like Walt Disney's \"Hunchback of Notre Dame.\" In that movie, the \"extras\" in the crowd scenes were controlled by autonomous programs, not hand-drawn by animators. The interacting programs yielded a more realistic look than previous animated movies, he said.\nArtificial intelligence technology has already begun to make its way into personal computer software. Microsoft, for example, originally used software agent technology, developed in its research laboratory, in its Office 95 application. Microsoft has extended the agent capabilities to assist computer users with basic tasks in its recently released Office 97 software.\nDespite such advances, many computer researchers remain skeptical about the ability of artificial intelligence research to match human reasoning capabilities any time soon.\n\"It's very hard to design intelligent programs that are going to come anywhere near human intelligence,\" said Michael Dertouzous, director of the Laboratory for Computer Science at the Massachusetts Institute of Technology.\nHe said that most artificial intelligence programs fall into two broad categories: programs that follow simple if-then-else rules and other programs that try to mystify their activities, but which are also essentially trivial in their capabilities.\nThat fact has done little to check the enthusiasm of a new generation of entrepreneurs who are rushing to develop software agents. The newcomers play down recent disappointments like General Magic Inc., a heavily backed start-up founded by a group of former Apple Computer Inc. computer designers that promised agent-based systems.\nDespite ambitious claims of a new generation of agent programs to perform electronic commerce services, General Magic failed to win consumer support and is now trying to redesign its software for the Internet.\nThe emergence of the World Wide Web and programming languages, like Java and Microsoft's Active-X, are viewed as creating the basis for standards that will support the development of commercial intelligent software.\n\"I like to think of myself as staffing cyberspace,\" said Barbara Hayes-Roth, chief executive of Extempo Systems Inc., a company in Santa Clara, Calif., that is developing several lines of \"characters\" -- interactive text or graphical programs that can interact with computer users on the Internet in ways that are more realistic and entertaining than earlier programs.\nOne of the earliest such programs, Eliza, was written in 1966 by Joseph Weizenbaum, a computer scientist at the Massachusetts Institute of Technology, Eliza was a novelty because it appeared to give conversational answers to questions. However, the program could easily be fooled into giving nonsensical answers.\nMs. Hayes-Roth, who was an artificial intelligence researcher at Stanford University before leaving to found Extempo last year, said that her company's characters functioned much like improvisational actors.\n\"We start by making characters that understand their situation,\" she said. \"We want characters that are interesting and don't do the same thing over and over.\"\nMobile agents that move from computer to computer in the Internet to perform tasks are also being designed. The already crowded global Internet will soon be awash in a new generation of mobile programs flitting from computer to computer while automatically performing tasks as diverse as shopping and data base searching.\nA group of researchers at the University of Washington computer science department described a program called Shopbot which is designed to perform price comparisons at various Internet electronic shopping malls.\nOther software agent developers said that research was just beginning into the design of a network world in which programs were not cooperative and which might try to take unfair advantage of each other in commercial transactions.\n\"The way to think about this is to consider software agents that are capable of lying, cheating and stealing,\" said Tuomas Sandholm, an assistant professor in the computer science department at Washington University in Saint Louis.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n","69":"Features, similar \nto rivals', aim to cut fatigue for consumers\nApple's sweeping new artificial intelligence play takes a page from features introduced by rival tech companies in recent years - but adds the polished, user-friendly twist that consumers have come to expect from the electronics giant.\nAt the company's annual developers' conference in San Francisco on Monday, Apple executives announced a raft of features in the company's soon-to-be released desktop and mobile operating systems that are powered by artificial intelligence - the blend of powerful computing capabilities and software algorithms. Such technology can make the phone or other device appear smarter because it anticipates the types of activities people want to do. Apple also said it was opening up many applications to outside developers, including its messaging platform iMessage, Maps, and virtual assistant Siri - a departure for the company, which in the past has kept these systems tightly controlled.\u00a0\nFor Apple, more AI and more integrations with third party services will mean less fatigue for consumers, who are already overwhelmed with too many apps, too many devices and too much data. Ultimately, artificial intelligence behind the scenes could make it easier for users to organize their ever-growing photo collections, communicate and use online services more efficiently, and toggle less between devices. The moves also come at a time when tech giants and a wave of new start-ups are racing to create similar artificial-intelligence-based products.\nFor example, Apple will now scan your photos using facial recognition to cluster people together in your photo collection. If pictures of Grandpa are scattered across your photo collection, Apple will now find them using facial recognition and group them together so you can organize your memories without having to sift on your own. Facebook has had automatic facial recognition for several years. This stuff isn't simple: Behind the scenes, the software is doing 11 billion computations on each photo to make this happen, Apple said.\nApple's other AI announcements borrowed heavily from Cortana, Microsoft's voice-based virtual assistant, which works in Windows. Executives said that Siri is now coming to desktop computers - soon consumers will be able to talk to their Mac computers  the way that they talk to their phones. \nOverall, more consumers are talking to technology: Google recently said that roughly 20 percent of all queries are initiated by voice rather than typing. While Cortana has enabled users to talk to their desktops for over a year, Apple has the advantage of being able to integrate a popular tool on mobile onto desktop, making the experience of moving between devices more seamless. Like Cortana, Siri will also now scan people's communications and make suggestions. If the system sees two people discussing a meeting over text message, a calendar icon will pop up, enabling the users to schedule the meeting from within their texting thread.\nIn opening major applications to third parties, Apple is nodding to a growing view in Silicon Valley that consumers are seeking an alternative to toggling between the dizzying number of apps they store on their phones. \nIn addition to building Siri into the desktop, the changes will allow developers to supercharge their apps with Siri's voice. Users will soon be able to use Slack, Uber or Skype by talking directly to Siri. This widely anticipated move takes a page from Amazon. The company's Alexa voice service on its popular Echo smart home assistant device, has been allowing third parties to build services onto its platform - for some time now, consumers have been able to ask Alexa to read out the weather or connect to smart locks.\nApple is also trying to reduce app fatigue by enabling consumers to do more within a single application. Want to call an Uber without leaving your text message conversation? You can do that within iMessage. The company is also attempting to relieve some password fatigue by letting you log on to your computer with your Apple Watch, so you don't have to type another password. And Apple Pay will be expanded, too, so that it now works on your desktop on a number of e-commerce sites. That means consumers who prefer to use Apple can avoid painstakingly entering and storing their credit card information with a slew of vendors.\nOpening up its platforms to third parties has also historically been a point of discomfort for Apple, as the company's impulse to control the quality and integrity of its own products has come up against major trends in AI. The original Siri included integrations with many third parties that were dissolved after Apple bought the Siri start-up five years ago. Today, against a wave of outside pressures, Apple just about came full circle.\nelizabeth.dwoskin@washpost.com\n","71":"John McCarthy, 84, a computer scientist often credited with creating the very name of the futuristic field in which he was an honored pioneer - artificial intelligence - died Oct. 24 at his home in Stanford, Calif.\nThe death was announced by Stanford University, where he was a professor in mathematics and later computer science from 1962 until his retirement in 2001. He had pulmonary hypertension, his daughter Sarah McCarthy said.\nWhen Dr. McCarthy launched his career, in the years after World War II, there was great interest in exploring connections between the fields of computer science (which had been vital to military code-breaking), cognitive science (how the brain works) and mathematics. \u00a0\nHe described attending a 1948 symposium bringing together some of the leading minds in all three subjects - including mathematicians Alan Turing and Claude Shannon and psychologist Karl Lashley -  as a watershed moment in his life. It occurred to the young Dr. McCarthy, a Caltech graduate then working on his doctorate in mathematics from Princeton, that machines could be made to think like humans. \nThis blossomed into his infatuation with artificial intelligence, commonly known as AI. In addition to his rigorous professional pursuits in AI, Dr. McCarthy was credited with many contributions to the computing field. They included the invention in the late 1950s of list processing language, a computer programming language known as LISP that continues to be used in AI.\nHe also was reputed to have conceived the idea of computer time-sharing, which has been described as a contribution to the development of the Internet, and a precursor of cloud computing. Cloud computing is a method of storing data - which used to reside on a hard drive or in house servers - on a remote server accessible from the Internet or other networks.\nDr. McCarthy was not the first to work in the field now known as artificial intelligence, but many people have credited him with creating the name for it in connection with a landmark 1956 AI conference at Dartmouth College in New Hampshire, where he was then teaching mathematics.\nIn the late 1950s, he and Marvin Minsky, a friend and fellow AI specialist, helped start the AI lab at the Massachusetts Institute of Technology. Their views of the discipline began to diverge and, in 1962, Dr. McCarthy returned to Stanford, where he had briefly taught. He soon founded Stanford's artificial intelligence laboratory, known as SAIL.\nBy that time, he became associated with the development of computers that could play chess. But when gamesmanship and showmanship appeared to be supplanting science, he turned away. \nIn a 2007 article, Dr. McCarthy described artificial intelligence as \"the science and engineering of making intelligent machines\" and said intelligence was \"the computational part of the ability to achieve goals in the world.\" \nBut in creating intelligence, he said, AI research had been only partly successful. It was not, he wrote, for lack of computing power. \n\"My own opinion is that the computers of 30 years ago were fast enough,\" he wrote, \"if only we knew how to program them.\" \nJohn McCarthy was born in Boston on Sept. 4, 1927. His father was an Irish immigrant;  his mother was a Jewish immigrant from Lithuania. The family ended up during the Depression in Los Angeles, where his father was an organizer for a garment workers  union. \nSelf-taught in mathematics as a teenager, he received an undergraduate degree in that field from the California Institute of Technology in 1948 and a PhD three years later from Princeton. \nHe received several major honors, including the Association for Computing Machinery's prestigious A.M. Turing Award in 1971, the Kyoto Prize in 1988 and the National Medal of Science in 1990. \nThe National Medal of Science cited his \"fundamental contribution to computer science and artificial intelligence, including the development of the LISP programming language.\" It also noted his role in the \"the concept and development of time-sharing\" and \"the naming and thus definition of the field of artificial intelligence itself.\"\nHis first marriage, to Martha White, ended in divorce. His second wife, Vera Watson, a member of the American Women's Himalayan Expedition, died in 1978 in a mountain-climbing accident while attempting to scale Annapurna in Nepal.\nSurvivors include his third wife, Carolyn Talcott of Stanford; two daughters from his first marriage, Susan McCarthy of San Francisco and Sarah McCarthy of Nevada City, Calif.; a son from his third marriage, Timothy McCarthy of Stanford; a brother; and two grandchildren.\nDr. McCarthy demonstrated a rigor in professional pursuits that was counterbalanced in his personal life by what a Stanford release called self-mocking humor and a philosophy of \"radical optimism.\"\nAs quoted by Stanford, Susan McCarthy described her father as holding the view that \"everything will be okay even if people don't take my advice.\"\nweilm@washpost.com\n","73":"If you've ever played the electronic version of the game 20 Questions, either the hand-held game or online, you probably had a question of your own: \"How does it do that?\"\u00a0\nIf you haven't played, 20Q is a game in which a player thinks of a person, place or thing, then another player asks up to 20 yes-or-no questions before trying to guess what the player is thinking of. In the electronic versions of the game, a computer asks the questions and guesses the answer, usually correctly.\nThe computer does this using a type of technology called artificial intelligence, which, very simply, gives it the ability to think like a human. That doesn't mean it thinks of popsicles on a hot day, of course. But a computer using artificial intelligence can answer questions based not only on information it has been given but also on mistakes it has made in the past. \n\"Every time somebody plays, [20Q] learns from the answers that [players] give,\" explained Eric Levin, president of Techno Source, which is introducing a new, smarter version of the 20Q game this summer.\nSo, unlike a calculator, which does specific math calculations that have only one answer, artificial intelligence figures things out from experience. The more it works, the smarter it gets - just like you.\nA good example, Levin said, would be if a 20Q player thought of a dolphin. The game might ask the player if he was thinking of a fish. The answer should be \"no,\" because a dolphin is a mammal. But some players might accidentally say \"yes.\" \nIf that happens, the 20Q program figures out that the player made a mistake and that the answer probably still is a dolphin. \nArtificial-intelligence technology has become very sophisticated and has many applications. Google uses it to figure out what you're looking for on the Internet. Amazon uses it to suggest other products you might like to buy, based on ones you've bought or looked at. Recently a computer called Watson, which is based on artificial-intelligence technology, won a Jeopardy tournament against the game show's two best players. And many companies now use the technology to create computer programs that can answer the phone and \"talk\" to real people. \n\"When I call the phone company, I don't get a person on the line - it's a computerized system, and I can almost say anything I want and it will respond in a reasonable manner,\" said Robin Burgener, who created the electronic 20Q game in the 1980s. \"Most of the time, if it's done properly, you don't even know that it's there.\"\nBut Levin, being a human who learns from experience, knows that 20Q is more fun because it's right only about 85 percent of the time. After all, who wants to play an opponent that always wins? \"Kids like beating it,\" he said.\n- Margaret Webb Pressler\nkidspost@washpost.com\n","74":"As a child Ray Solomonoff developed what would become a lifelong passion for mathematical theorems, and as a teenager he became captivated with the idea of creating machines that could learn and ultimately think.\n  In 1952 he met Marvin Minsky, a cognitive scientist who was also exploring the idea of machine learning, and John McCarthy, a young mathematician. And within four years, they and seven other scientists, as part of the original Dartmouth Summer Research Project,  had founded a new field and given it a name: artificial intelligence.\n  The conference proved to be a watershed both for the field of artificial intelligence (Dr. McCarthy, a Dartmouth College mathematician at the time, coined the term) and for modern computing. It laid out a proposal for a program of study, stating, ''The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.'' \u00a0\n  The next summer Allen Newell, J.C. Shaw and Herbert Simon, researchers at the Carnegie Institute of Technology (now Carnegie Mellon University), devised a program to discover proofs of logical theorems. Simulated by hand in 1955, the program, called ''Logic Theorist,'' was demonstrated at the Dartmouth conference and is considered to be the first effort to create an artificial intelligence program.\n  Mr. Solomonoff, who died on Dec. 7 in Boston at the age of 83 but whose death was not widely reported, plunged further into the field in 1960, when he  developed the idea of algorithmic probability. \n  The notion emerged from his effort to grapple with a problem of induction: Given a long sequence of symbols describing real-world events, how can you extrapolate the sequence? The idea gave rise to a new approach to probability theory. \n  Mr. Solomonoff went on to pioneer the application of probability theory to solving artificial intelligence problems. But in the 1960s and 1970s he was ahead of his time, and the approach initially had little impact on the field. More recently, probability theory has caught on among artificial intelligence researchers and is now the dominant approach.\n  ''Ray did early work on the theoretical foundations of learning systems, focused on understanding how to generate and assign probabilities to sequences of symbols, which could be mapped to the challenge of predicting what comes next, given what you've seen so far,'' said Eric Horvitz, a Microsoft computer scientist and a former president of the Association for the Advancement of Artificial Intelligence. \n  ''Beyond his core technical work,'' Mr. Horvitz, added, Mr. Solomonoff  was a ''passionate proponent of the probabilistic approach to A.I., on the promise of building intelligent computing systems that could learn and reason under uncertainty.'' \n  His work in the early 1960s predated the work of the Russian mathematician Andrei Kolmogorov, who also did pioneering research in information theory and later acknowledged Mr. Solomonoff's earlier contributions.\n  Mr. Solomonoff later turned his attention to the consequences of artificial intelligence. In 1985 he wrote a paper that speculated on the cost and the time it would take to develop a machine with many times the intelligence of a group of humans. He called this the ''infinity point.'' \n  The idea predated the prediction of the computer scientist Vernor Vinge, who in 1993 speculated on a similar evolution in machine intelligence, which he called ''the singularity.''\n  Born in Cleveland on July 25, 1926, Mr. Solomonoff was the son of immigrants from Russia, Julius and Sarah Solomonoff. He studied physics at the University of Chicago and graduated with a master's degree in 1951.\n  Fiercely independent, he would remain self-employed for much of his life, taking a variety of visiting scholar positions. In 2001 he was a visiting professor at the Dalle Molle Institute for Artificial Intelligence in Lugano, Switzerland; more recently he was a visiting professor at the Computer Learning Research Center at Royal Holloway, University of London.\n  He is survived by his wife, Grace, who said the cause of death was a ruptured brain aneurysm. He lived in Cambridge, Mass., and also had a home in New Ipswich, N.H., which he built himself, heating it with two rows of light bulbs in the ceiling, a feat made possible by thick insulation and inserts to cover the windows.\n","75":"Since its triumph over human \"Jeopardy!\" champions two years ago, I.B.M.'s Watson has explored a number of frontiers for its artificial-intelligence technology, even inventing new recipes in the kitchen.\nTo date, medicine has been Watson's most focused foray. Its machine intelligence has read through vast troves of medical texts, research papers and patient records to suggest diagnosis and treatment options at institutions including the Cleveland Clinic, the Memorial Sloan-Kettering Cancer Center and the M.D. Anderson Cancer Center, and for the insurer WellPoint. Yet even its work in health care has had the feel of applied science projects - impressive, but seemingly custom jobs rather than the beginning of a big commercial business.\nI.B.M. hopes to take a big step toward mass-market commercialization of Watson on Tuesday, announcing that the technology will be applied to customer service, broadly defined. Its new offering, called IBM Watson Engagement Advisor, is being pitched as a smart assistant whose services can apply to almost any industry, but especially those that receive many customer service calls,  like retailing, banking, insurance and telecommunications.\u00a0\n\"Watson has a new job - assisting brands,\" said Manoj Saxena, the I.B.M. executive in charge of the Watson business unit.\nIn health care, Mr. Saxena noted, Watson has done \"high-value\" work for a handful of institutions. The customer-service business, by contrast, is a \"high-volume play for us.\"\nThis week's announcement is the next move in an ambitious, longer-term commercialization agenda for Watson. I.B.M. plans to offer the Watson software as the equivalent of an operating system for artificial intelligence applications - much as Microsoft's Windows was the operating system that dominated the personal computer era.\nSoftware developers and start-up companies, according to the I.B.M. game plan, will build software applications that run on the Watson artificial-intelligence engine - and with them, new businesses. In a speech last week at a meeting of the National Venture Capital Association in San Francisco, Virginia M. Rometty, I.B.M.'s chief executive, said, \"We'll launch an ecosystem where Watson is a service and you build applications around it.\"\nThe Watson technology can either assist human customer-service representatives or take over some of their tasks. An \"Ask Watson\" feature allows consumers to ask Watson questions directly from a smartphone or notebook computer and proceed, the company says, in a conversational style, either with typed text or spoken words. (Voice recognition technology would be supplied by third-party specialists, like Nuance, as is the case for Apple's Siri question-answering service.) \nThe two years of development since the \"Jeopardy!\" contest, Mr. Saxena said, have made Watson far faster and more compact, whether the cloud service resides on servers in an I.B.M. data center or on a corporate customer's servers.\nThe original Watson was a question-and-answer machine - with each interaction a separate question and answer. The current technology, according to Mr. Saxena, can engage a consumer in a dialogue. It can listen to and respond to, say, four or five follow-up questions and remember the previous questions a person had.\n\"That's the difference,\" he said, \"between a question-and-answer system and a conversational system.\"\nThose who have been shown demonstrations of Watson's new capabilities sound impressed. Joyce Phillips, chief executive of ANZ Bank's wealth management and private banking group, plans to use Watson to assist financial advisers in coming up with investment, insurance and pension offerings that are better tailored to the needs of individual clients.\nANZ, a major banking group in Australia and New Zealand, is still testing the Watson technology.  But Ms. Phillips said: \"We're pretty confident that this technology will help transform our business. It's a tool that will make our advisers smarter.\"\nAt the Nielsen Company, the media measurement and market research firm, Randall Beard, global head of advertiser solutions, wants to use Watson to instantly sift through and make sense of all the data advertisers receive about their Web, social media, print and television marketing campaigns.\n\"You want to optimize campaigns in real time across every marketing channel, to see what people are watching and what are they buying to accurately measure ad effectiveness,\" Mr. Beard said. \"We think there's a real opportunity here.\"\nBut the Watson opportunity has not yet been tested. Companies so far have signed agreements to develop projects to try out the I.B.M. technology. Out in the wild, when thousands of customers are asking questions all at once, will the promise of Watson's technology prove itself? Will its conversations with people seem smart, as if it is listening and learning?\nThat would appear to be a tall order. Still, Watson's introduction to the wider world will be far more controlled than Siri's introduction in October 2011. Amid high expectations, fueled by Apple's polished marketing campaign, Siri was instantly subjected to all kinds of messy human questions, on every imaginable subject. Inevitably, Siri often stumbled at first, becoming the subject of ridicule and jokes on late-night television.\nBut the bar will most likely be lower for Watson in business than it was for Siri in the consumer market. If the basis of comparison for Watson is the current state of voice-automated systems in call centers, the newcomer, analysts say, could well come out a winner.\n\"People are free-form, and they ask questions in different ways,\" said Mary Wardley, an analyst at IDC. \"If Watson can learn and offer more personalized answers, it could be a very useful tool in solving customer service problems.\"\n\n","76":"Major websites all over the world use a system called CAPTCHA to verify that someone is indeed a human and not a bot when entering data or signing into an account. CAPTCHA stands for the \"Completely Automated Public Turing test to tell Computers and Humans Apart.\" The squiggly letters and numbers, often posted against photographs or textured backgrounds, have been a good way to foil hackers. They are annoying but effective.\nThe days of CAPTCHA as a viable line of defense may, however, be numbered.\u00a0\nResearchers at Vicarious, a Californian artificial intelligence firm funded by Amazon founder (and Washington Post owner) Jeffrey P. Bezos and Facebook's Mark Zuckerberg, have just published a paper documenting how they were able to defeat CAPTCHA using new artificial-intelligence techniques. Whereas today's most advanced AI (artificial intelligence) technologies use neural networks that require massive amounts of data to learn from, sometimes millions of examples, the researchers said their system needed just five training steps to crack Google's reCAPTCHA technology. With this, they achieved a 67 percent success rate per character - reasonably close to the human accuracy rate of 87 percent. In answering PayPal and Yahoo CAPTCHAs, the system achieved an accuracy rate of greater than 50 percent.\nThe CAPTCHA breakthrough came hard on the heels of another major milestone from Google's DeepMind team, the people who built the world's best Go-playing system. DeepMind built a new artificial-intelligence system called AlphaGo Zero that taught itself to play the game at a world-beating level with minimal training data, mainly using trial and error - in a fashion similar to how humans learn.\nBoth playing Go and deciphering CAPTCHAs are clear examples of what we call narrow AI, which is different from Artificial General Intelligence (AGI) -  the stuff of science fiction. Remember R2-D2 of \"Star Wars,\" Ava from \"Ex Machina\" and Samantha from \"Her?\" They could do many things and learned everything they needed on their own.\nThe narrow AI technologies are systems that can only perform one specific type of task. For example, if you asked AlphaGo Zero to learn to play Monopoly, it could not, even though that is a far less sophisticated game than Go; if you asked the CAPTCHA cracker to learn to understand a spoken phrase, it would not even know where to start.\nTo date, though, even narrow AI has been difficult to build and perfect. To perform very elementary tasks such as determining whether an image is of a cat or a dog, the system requires the development of a model that details exactly what is being analyzed and massive amounts of data with labeled examples of both. The examples are used to train the AI systems, which are modeled on the neural networks in the brain, in which the connections between layers of neurons are adjusted based on what is observed. To put it simply, you tell an AI system exactly what to learn, and the more data you give it, the more accurate it becomes.\nThe methods that Vicarious and Google used were different; they allowed the systems to learn on their own, albeit in a narrow field. By making their own assumptions about what the training model should be and trying different permutations until they got the right results, they were able to teach themselves how to read the letters in a CAPTCHA or to play a game.\nThis blurs the line between narrow AI and AGI and has broader implications, in robotics and in virtually any other field in which machine learning in complex environments may be relevant.\nBeyond visual recognition, the Vicarious breakthrough and AlphaGo Zero success are encouraging scientists to think about how AIs can learn to do things from scratch. And this brings us one step closer to coexisting with classes of AIs and robots that can learn to perform new tasks that are slight variants on their previous tasks - and ultimately the AGI of science fiction.\nSo R2-D2 may be here sooner than we expected.\n","78":"AN industry of the future - artificial intelligence - needs philosophers.\nDr. Clark Glymour, professor of philosophy at Carnegie-Mellon University in Pittsburgh, speaks of an increased demand for philosophy majors, calling it a ''growing field.'' In fact, he talks of ''an incredible need for philosophers.''\n''It may seem odd,'' he said. ''What happened is that some years ago philosophy grew closely connected to logical theory, which, in turn, was the genesis of computer algorithms involved in the development of digital computers.''\u00a0\n''Programmers for computers are a dime a dozen, but what is needed are people who can take vaguely formed problems and find ways to make them precise enough to be programmed,'' Dr. Glymour added. ''This is what philosophers can do and they are planning a major role in artificial intelligence.''\n''For the past 40 years,'' he continued, ''American philosophy has been closely tied to the logical and mathematical study of knowledge and inference. The theories produced in these areas have been precise enough to be used in the design of artificial intelligence programs.''\nLike the human mind, a so-called intelligent machine, such as equipment for medical diagnoses, must be capable of applying the knowledge it acquires - intelligence that must be programmed into it by experts.\nThe connection between philosophy and high technology has recently been ''reaching the undergraduate level,'' Dr. Glymour said.\nAs a result, Carnegie-Mellon six months ago began a program for a major called ''Logic and Computation,'' which involves studying the technical and theoretical issues in artificial intelligence.''\n''It's a professional major,'' he said, adding that there are few programs in the area. ''Stanford University has one.''\nSome programs, such as those at Wellesley College and the University of Rochester, put more stress on psychology, he said.\nAll along, he said, there have been ''radical misconceptions'' about philosophy. Perhaps one reason is that in recent years philosophy programs have placed much emphasis on ethics - personal, societal and business.\nFrom a peak of 5,939 degrees awarded with a major in philosophy in 1972, the numbers dropped to 3,322 in 1983, the latest year reported by the National Center for Education Statistics. He expects the numbers to move up with the realization that philosophy suddenly is at the edge of high technology.\nHe pointed out that many leaders in the field of artificial intelligence have backgrounds in philosophy, with emphasis on logical thought. One is Dr. Herbert Simon of the Carnege Mellon faculty, who is a Nobel Prize winner. ''He took lots of philosophy,'' Dr. Glymour said last week.\nHe also pointed out that Dr. Bruce Buchanan, professor of computer science at Stanford University, had developed the Dendral program, which helps chemists identify the structure of molecules. In designing it, he called upon his background in philosophy courses taken at Michigan State University.\nDr. Glymour majored in chemistry and philosophy at the University of New Mexico and obtained a doctorate from Indiana University in history and philosophy of science.\nThe new major in philosophy and computation at Carnegie-Mellon has attracted 15 students so far, according to Dr. Glymour, who said that two of them will graduate next year. ''Maybe we will have 30 or 40 in the future, when the program is finally stabilized,'' he said.\n''The new major tends to interest those students who are good in mathematics, but that doesn't mean they have to take a lot of math.''\nCore courses include ''Logic and Computability,'' ''Probability and Artificial Intelligence,'' ''Fundamental Structures of Computer Science'' and ''Minds, Machines and Knowledge.''\nThe major also requires a few courses in mathematics, including calculus, statistics, philosophy, linguistics, and psychology. Specialties can be pursued in depth, such as in computer science, artificial intelligence and computational linguistics.\nProfessional career opportunities open for such majors in the new program include research programming, artificial intelligence, program development, industrial applications of computational linguistics, and teaching in the field.\nHe also said that a recent study indicated that a surprising number of the best new doctors had been philosophy majors as undergraduates -another indication of a career path for majors in this esoteric liberal arts subject.\nKevin Kelly, a colleague of Dr. Glymour, says in a soon-to-be published study that ''the last five years have seen a revival of interest in the logic of discovery among philosophers of science'' - an aspect that might interest all prospective inventors of products or processes.\n","82":"Elon Musk and Stephen Hawking, along with hundreds of artificial intelligence researchers and experts, are calling for a worldwide ban on so-called autonomous weapons, warning that they could set off a revolution in weaponry comparable to gunpowder and nuclear arms.\u00a0\nIn a letter unveiled as researchers gathered at the International Joint Conference on Artificial Intelligence in Buenos Aires on Monday, the signatories argued that the deployment of robots capable of killing while untethered to human operators is ''feasible within years, not decades.'' If development is not cut off, it is only a matter of time before the weapons end up in the hands of terrorists and warlords, they said. \n  Unlike drones, which require a person to remotely pilot the craft and make targeting decisions, the autonomous weapons would search for and engage targets on their own. Unlike nuclear weapons, they could be made with raw materials that all significant military powers could afford and obtain, making them easier to mass-produce, the authors argued.\n  The weapons could reduce military casualties by keeping human soldiers off battlefields, but they could also lower the threshold for going to battle, the letter said. ''If any major military power pushes ahead with A.I. weapon development, a global arms race is virtually inevitable, and the endpoint of this technological trajectory is obvious: autonomous weapons will become the Kalashnikovs of tomorrow,'' it said.\n  Mr. Musk, the head of SpaceX, has raised warnings about artificial intelligence before, calling it probably humanity's ''biggest existential threat.'' Mr. Hawking, the physicist, has written that while development of artificial intelligence could be the biggest event in human history, ''Unfortunately, it might also be the last.''\n  The letter said artificial intelligence ''has great potential to benefit humanity in many ways.'' Proponents have predicted applications in fighting disease, mitigating poverty and carrying out rescues. An association with weaponry, though, could set off a backlash that curtails its advancement, the authors said.\n  Other notable signatories to the letter included Steve Wozniak, the co-founder of Apple; Noam Chomsky, the linguist and political philosopher; and Demis Hassabis, the chief executive of the artificial intelligence company Google DeepMind.\n\n\n\n","83":"THE IMPROBABLE MACHINEWhat the Upheavals in Artificial Intelligence ResearchReveal About How the Mind Really Works.By Jeremy Campbell.334 pp. New York: Simon & Schuster. $19.95.\nUNTIL recently the world was more or less divided between those who believed in artificial intelligence and those who considered it a philosophical scam. People were either reductionists, who believed that the brain is a biological machine - a system of neurological gears and pulleys that can be mimicked by a computer - or holists, who insisted that there is some indescribable mind stuff that will slip through the cracks of any simulation. No matter how sophisticated its cogitations, these holists would say, a computer will never have intuition, wisdom or - some have even called it - soul. Like oil and water, the two positions are immiscible; holism and reductionism just don't mix.\u00a0\nLately the mind-machine argument has been muddied by the emergence of a very different kind of computer called a neural network, which Jeremy Campbell, the Washington correspondent of The Evening Standard of London, describes in ''The Improbable Machine: What the Upheavals in Artificial Intelligence Research Reveal About How the Mind Really Works.''\nBuilt from webs of randomly connected electronic neurons, these machines can be trained instead of programmed. As in the brain, the neurons send signals to one another through thousands of adjustable connections, or synapses. As the machine learns, the settings of these volume controls are automatically turned up and down; the chaos of connections evolves into a finely tuned machine, one that can read handwritten letters, for example, or recognize spoken sounds. In the past, a programmer who wanted to make a machine perform such brainlike feats would sit down and try to specify all the rules - and the exceptions and the exceptions to the exceptions - for recognizing, say, the letter a. With a neural net, a trainer simply shows the network a's and other letters, one after the other, and rewards it when it gives the right answer and punishes it when it is wrong.\nWith both artificial intelligence programs and neural networks the goal is to create intelligence outside the body and learn more about how the mind works. But in artificial intelligence it has been assumed that the brain's psychology can be simulated from the top down without paying much attention to the way neurons work. The neural network people take a bottom-up approach: they assemble a network of artificial neurons and hope that a mind will emerge.\nLike a lot of people, Mr. Campbell is fascinated that neural nets share some of the strengths and weaknesses of human brains. They may occasionally confuse an a and an o, but this fuzziness allows them to recognize a's written in a range of scribbles that would baffle a typical computer program.\nThe neural net enthusiasts are as far from their goal of making intelligent machines as are their competitors in artificial intelligence. But for some reason people who have reviled the artificial intelligentsia, as they like to call them, are suddenly embracing neural networks. The philosopher Hubert Dreyfus has made his career using obscure arguments from Heidegger and other philosophers to denounce artificial intelligence as theoretically impossible. But he was so impressed by neural nets that he rewrote parts of his 1985 book ''Mind Over Machine'' (written with his brother, Stuart Dreyfus), allowing that thinking machinery might not be so unthinkable after all.\nIt is surprising that as good a science writer as Mr. Campbell would fall into this same confusion, believing that neural nets constitute ''an approach that is radically different from much of the Western philosophical tradition.'' His first book, ''Grammatical Man,'' was an exhilarating meditation on information and entropy, order and chaos - the poles of the dynamo that generates life. ''The Improbable Machine'' elegantly describes the importance of neural networks in studying the brain-mind connection. But in trying to make neural nets seem like an upheaval rather than a variation on a theme, he turns artificial intelligence into a caricature that few of its adherents would recognize.\nSeduced by the pleasures of symbolic logic, he argues, the artificial intelligentsia decided that the brain was like a standard digital computer: step by step, a powerful central processor crunches information as it solves the problems posed by life. Intelligence, these benighted souls believed, consisted of solving syllogisms, and so they set out to program computers to duplicate this purely rational, objective mind.\nIn Mr. Campbell's mythology, the neural net people were clever enough to see through this ruse. Unlike standard digital computers, brains consist of billions of processors - the neurons - that operate in parallel. What is more, people are inherently illogical. We overgeneralize and jump to conclusions. But in our weakness lies our strength. We can tolerate ambiguity and deal with imperfect information. In the 1960's, the neural net researchers were bullied out of the laboratory by the artificial intelligentsia, he argues, but now, two decades later, artificial intelligence has failed and neural nets are making a comeback.\nThe problem with this scenario is that except in the most superficial sense, no one in artificial intelligence ever believed that the brain processed information in the same way as the I.B.M. tabulating your telephone bill. The genius of the prophets of artificial intelligence - people like Alan Turing, John McCarthy, Marvin Minsky and Herbert Simon - was in seeing that despite surface dissimilarities, brains and computers are species of the same genus: information processor. For those trying to understand intelligence, the computer was not a blueprint but an inspiration and a tool, the perfect device for modeling complex systems - the weather, the economy, so why not the mind?\nTrue, a few people in artificial intelligence have been preoccupied with using symbolic logic to simulate the mind. But the most interesting work has been in programming computers to exhibit the kind of unpredictable, spontaneous behavior that is anything but rigid and logical. A computer whose chips march to the dictates of digital logic can still be programmed to imitate a paranoid schizophrenic. The psychiatrist Kenneth Colby showed that in the early 1970's with a program called Parry. The artist Harold Cohen's program Aaron, which generates original freehand drawings, stands as a convincing demonstration that something as seemingly indescribable as artistic style can emerge from the interaction of a few hundred simple rules. Each of Aaron's pictures is unique and surprising, yet they are united by a regularity that can only be called Aaronesque.\nThe fundamental idea in artificial intelligence is that brains and computers, like such games as checkers and go, are what mathematicians call formal systems. They consist of tokens - black and red markers, ones and zeros, patterns of neural firings - that are manipulated according to a set of rules. Each token is meaningless by itself. But from the shuffling of empty tokens things like meaning and intelligence arise.\nIN fact, the neural nets themselves, with all their wonderful properties, are usually simulations run on digital computers. It's clear from his book that Mr. Campbell knows this, so it is baffling that he can approvingly quote the Dreyfus brothers, who seem to believe that neural networks are contributing to the downfall of the idea that the brain is a formal system. Stranger still is Mr. Campbell's contention that the representations inside both brains and artificial neural nets have a quality he calls ''aboutness,'' which the empty symbols of a digital computer supposedly cannot have. Where does this elusive quality go when a neural net is being simulated on a regular old computing machine? Artificial neural networks are formal systems. Deny that brains fall into the same category and you're in danger of becoming a holist, worshipping a ghost in the machine.\nIn trying to figure out how brains make minds, the people who fly the banner of artificial intelligence and those who champion neural networks disagree, sometimes vehemently, about the nature of both the symbols and the rules. But they have much more in common than they and some of their chroniclers seem ready to admit.\n","84":"BEMUSED, skeptical and even a little bit angry, scientists attending the American Association of Artificial Intelligence conference here last week heard a businessman declare that the central focus of their work -- creating computers with human-like intelligence -- is obscuring gains in the field that can be exploited now.\nThose same scientists have long debated the goals of their field. Should it be singly devoted to creating computers that think like people? Or does clinging to such an elusive goal only undermine the incremental achievements in making smarter software?\u00a0\n Now, people seeking commercial applications of artificial intelligence have entered the philosophical debate that was once limited to academic corridors or private meetings. If scientists could never agree, they didn't seem to mind. But business people are pressing to redefine success in artificial intelligence.\nThe idea that human intelligence can be replicated in computers was born in the mid-1950's. Scientists predicted that within a decade technology would lead to machines that could reason, solve problems, make decisions, understand spoken language and even talk back. The images fascinated the public. But they also set expectations that, as it turned out, science couldn't meet. Even today, computers cannot perform many of those tasks the way people do.\nBut even if computers still cannot think, artificial intelligence is making them smarter. Some large companies are using A.I. software that makes decisions by searching data for patterns, following rules of thumb and drawing on experience stored in memory. These programs allow American Express Company computers, for example, to analyze credit status and account activity and then decide whether to authorize a credit card charge. Another large user, Northwest Airlines, can audit millions of tickets and decide whether the right taxes, commissions and fares were paid.\nFor academics, these programs are only the first step on a long, uncertain road toward true thinking computers. But business people see these \"intermediates\" as valuable in themselves.\nMany share the belief of Joe Carter, a partner in Anderson Consulting, one of the nation's largest international management and technology consulting firms, that the development of artificial intelligence is overly dependent on the goal of matching human thinking. Saying as much in his speech, Mr. Carter set the room abuzz with vexation.\n\"As far as I can tell,\" he said, \"A.I. has one and only one illness keeping it from succeeding. But it's a big one. The illness is the gross misuse and abuse of something I call the human metaphor.\n\"It's time to recycle the A.I. acronym -- there's nothing artificial about the intelligence in the systems we build,\" he added. \"We don't need any more artificial humans, but we need vast improvements in alternatives to human intelligence.\"\nThat call annoyed some academics in the room -- psychologists and philosophers as well as computer scientists -- who had devoted careers to trying to determine precisely how the human mind works and now saw their goals questioned because they didn't make a neat, marketable pitch.\u00a0The Profit Motive\n \"Some of what you say is dangerous to the field of artificial intelligence,\" said Stuart Shapiro, a professor at the State University of New York at Buffalo, rising to answer Mr. Carter. \"A.I. is a science as well as an engineering and technical field. You can't tell people interested in science to pack up and go away because they're really irrelevant to making a business profit.\"\nBoth business and science are bucking a public whose expectations have been primed by visions of computers like R2D2 in \"Star Wars.\" And among this public are the executives and venture capitalists who might invest in the more limited achievements that the A.I. movement has pioneered.\n\"The use of humans as a model for computer systems is doing a disservice to A.I.,\" Mr. Carter said. \"The dictionary's definition of intelligence is the ability to capture and apply knowledge. It says nothing about capturing and applying human knowledge.\"\nTo those outside the industry, that might seem like splitting hairs. But those at the conference understood it as a distinction that defines their work.\n\"Where do you draw the line?\" asked Patrick Hayes, the association's president and a professor at the University of Illinois. \"Where do you stop saying you won't call it artificial intelligence yet, but next year you will call it artificial intelligence?\"\nAt a meeting early this year to formulate an official statement, all that emerged was a clear division between the two philosophies. \"The first one was known as Hal,\" after the malevolent computer in the movie \"2001,\" Mr. Hayes said. \"That is, the ultimate goal is making a computer with a human mind. That's the old goal. The second perspective was instead of making a human, we should make lots and lots of smaller systems with bits of human intelligence.\n\"Most of the applications we have today go beyond what humans can do,\" he added. \"Who can keep track of 10,000 topics like a computer? It's still A.I., but it takes one aspect of human ability and extends it.\"\nBut for a scientist trying to discover whether the human mind perceives the world through symbols, or how the brain processes language, that is not enough.\n\"Scientists are interested in solving unsolvable problems,\" said Philip Klahr, a vice president at the Inference Corporation, a leading producer of artificial intelligence software. Mr. Klahr began his career doing scientific research for the Rand Corporation.\n\"That creates a lofty, unreachable goal,\" he said. \"From the science point of view, that's great. People working on small successes have a major impact. But on the technical side, and to the public, only solving one small part seems like a failure.\"\nParadoxically, it is the tension between and the melding of business and science that has produced new technologies like neural networks and fuzzy logic. Both are ways of depicting and using partially true information, and are essential for enabling computers to deal with uncertainty and learn from mistakes.\n\"There is a lot of hype for what artificial intelligence can do, and we're still living with that legacy,\" Mr. Klahr said. \"Maybe it was naive, but it did set an agenda. And there have already been tremendous by-products of that challenge.\"\n","87":"Elon Musk, the PayPal co-founder behind SpaceX and Tesla Motors, is really worried about artificial intelligence -- so worried that he's donated $10 million to support research \"aimed at keeping AI beneficial to humanity.\"\u00a0\nEssentially, he doesn't want humanity to build Terminators. (In fact, Musk has discussed\u00a0the film series\u00a0an example of what he worries might happen if researchers aren't careful.)\nMusk's\u00a0$10 million donation will be administered by the non-profit Future of Life Institute,\u00a0which is \"working to mitigate existential risks facing humanity\" with a focus on artificial intelligence. It will be used to run a global research program carried out through\u00a0an open grants competition, according to a statement from the group.\u00a0\nThe eccentric inventor has been vocal about his fear of artificial intelligence, once tweeting that it was potentially more dangerous than nuclear weapons.\nWorth reading Superintelligence by Bostrom. We need to be super careful with AI. Potentially more dangerous than nukes.\nBut he isn't alone. Many academics as well as employees at major tech companies including Microsoft and Google have signed onto an open letter hosted by the Future of Life Institute that laid out priorities for keeping artificial intelligence research \"robust and beneficial.\"\nPresumably, avoiding the development of Terminators.\n","88":"Facebook is using artificial intelligence to address one of its darkest challenges: stopping suicide broadcasts.\nThe company said Monday that a tool that lets machines sift through posts or videos and flag when someone may be ready to commit suicide is now available to most of its 2 billion users (availability had been limited to certain users in the United States). The aim of the artificial-intelligence program is to find and review alarming posts sooner, because time is a key factor in preventing suicide.\u00a0\nFacebook said that it will use pattern recognition to scan all posts and comments for certain phrases to identify whether someone needs help. Its reviewers may call first responders. It will also apply artificial intelligence to prioritize user reports of a potential suicide. The company said phrases such as \"Are you ok?\" or \"Can I help?\" can be signals that a report needs to be addressed quickly.          \nIn the case of live video, users can report the video and contact a help line to seek aid. Facebook will also provide broadcasters with the option to contact a help line or another friend.\nUsers are also given information to contact law enforcement if necessary.\n\"We've found these accelerated reports - that we have signaled require immediate attention - are escalated to local authorities twice as quickly as other reports,\" Guy Rosen, Facebook vice president of product management, wrote in a company blog post.\nFacebook has been testing this program in the United States and will roll it out to most of the countries in which it operates, with the exception of those in the European Union. The company did not elaborate on why E.U. countries - which have privacy and Internet laws that are vastly different from laws in the United States - are not yet participating. But Facebook said it is speaking with authorities on the best ways to implement such a feature.\nThe social network focused new energy on identifying and stopping potential suicides after it experienced a cluster of live-streamed suicides in April, including one in which a father killed his baby daughter before taking his own life. The company said in May that it would hire 3,000 additional workers for its 4,500-employee \"community operations\" team, which reviews posts and other content reported as violent or otherwise troubling.\nFacebook chief executive Mark Zuckerberg said at that time that the company would use artificial intelligence to help identify problem posts across its network, but he acknowledged that this was a very difficult problem to address. \"No matter how many people we have on the team, we'll never be able to look at everything,\" he said in May.\nThe artificial-intelligence feature underscores Facebook's reliance on algorithms to monitor and police its network. In this case, the algorithm determines not only what posts should be reviewed but also in what order humans should review them.\nFacebook has been using artificial intelligence across its site to accomplish various tasks. It scans posts for instances of child pornography and other objectionable content that should be removed.  \nhayley.tsukayama@washpost.com\n","90":"Last Friday afternoon, a half-hour after he said his goodbyes at the Pentagon, where he was assistant secretary of defense for research and engineering, Zachary Lemnios was discussing why his new job made sense as the next step in his career. \"Exactly the right thing to do at the right time,\" he said.\nMr. Lemnios joined I.B.M. Research on Monday, as vice president for research strategy. Mr. Lemnios, 58, is leaving his post in the Obama administration, after nearly four years (political appointees typically last two to four years). Before that, he was chief technology officer at M.I.T.'s Lincoln Laboratory, a federally financed research center for advanced technology with national security applications, and previously a senior official at the Pentagon's futuristic research arm, the Defense Advanced Research Projects Agency.\u00a0\nHis own career charts a path from being a chip guy who would later champion and fund artificial intelligence research. Mr. Lemnios holds four patents on semiconductors that use gallium arsenide, an alternative to silicon.\nOf course, he noted, the years of progress in microprocessor design and sensors are essential to the recent advances in artificial intelligence. \"The hardware substrate is certainly part of it,\" Mr. Lemnios said. \"But it is the software for reasoning and learning that really pushes this forward.\"\nIn late March, for example, Mr. Lemnios announced $60 million in new Pentagon-supported Big Data research projects, as part of a $200 million administration initiative in the fast-growing field of trying to use smart technology to make sense of the explosion in data from the Web, sensors and streaming into traditional databases.\nToday's technological limitations, he said, are no longer the data collection tools but the technology for making sense of it. \"The real challenge is to handle, understand and use big sources of data,\" Mr. Lemnios said.\nAt Darpa, he promoted initiatives for \"cognitive systems,\" an approach to artificial intelligence that embraces a learning model of computing rather than more purely statistical methods. I.B.M.'s labs have been at the forefront of \"cognitive computing\" research in recent years, including projects financed by Darpa.\nMr. Lemnios had praise for I.B.M.'s best-known research project, the Watson question-answering computer. \"It moves toward what we think of as understanding information, which is the start of another revolution,\" he said.\nBut Mr. Lemnios said he had also been impressed with the company product and services offerings, which include large contributions from I.B.M.'s research labs, for traffic management, energy conservation and crime prevention. They are part of the company's Smarter Planet projects. \"There is a lot of deep technological meat on the bones of Smarter Planet,\" he said.\nHaving smart people is a crucial ingredient in a successful research operation, but so is research strategy and management, Mr. Lemnios noted. On strategy, he said, \"balance across a portfolio is important\" -- that is, balancing work on current products, research bets that may pay off in five to 10 years, and further out exploratory research.\nIn good times and bad, he observed, I.B.M. has had the managerial patience to continue financing long-range, exploratory research. That is a model, Mr. Lemnios suggests, that the federal government would do well to follow.\nGiven the need for budgetary belt-tightening in government, Mr. Lemnios said, \"There is great pressure to take funding for exploratory research to pay today's bills. That can prove to be a shortsighted mistake.\"\n\n","91":"PALO ALTO, Calif. -- A free online course at Stanford University on artificial intelligence, to be taught this fall by two leading experts from Silicon Valley, has attracted more than 58,000 students around the globe -- a class nearly four times the size of Stanford's entire student body.\nThe course is one of three being offered experimentally by the Stanford computer science department to extend technology knowledge and skills beyond this elite campus to the entire world, the university is announcing on Tuesday.\u00a0\nThe online students will not get Stanford grades or credit, but they will be ranked in comparison to the work of other online students and will receive a ''statement of accomplishment.''\nFor the artificial intelligence course, students may need some higher math, like linear algebra and probability theory, but there are no restrictions to online participation. So far, the age range is from high school to retirees, and the course has attracted interest from more than 175 countries.\nThe instructors are Sebastian Thrun and Peter Norvig, two of the world's best-known artificial intelligence experts. In 2005 Dr. Thrun led a team of Stanford students and professors in building a robotic car that won a Pentagon-sponsored challenge by driving 132 miles over unpaved roads in a California desert. More recently he has led a secret Google project to develop autonomous vehicles that have driven more than 100,000 miles on California public roads.\nDr. Norvig is a former NASA scientist who is now Google's director of research and the author of a leading textbook on artificial intelligence.\nThe computer scientists said they were uncertain about why the A.I. class had drawn such a large audience. Dr. Thrun said he had tried to advertise the course this summer by distributing notices at an academic conference in Spain, but had gotten only 80 registrants.\nThen, several weeks ago he e-mailed an announcement to Carol Hamilton, the executive director of the Association for the Advancement of Artificial Intelligence. She forwarded the e-mail widely, and the announcement spread virally.\nThe two scientists said they had been inspired by the recent work of Salman Khan, an M.I.T.-educated electrical engineer who in 2006 established a nonprofit organization to provide video tutorials to students around the world on a variety of subjects via YouTube.\n''The vision is: change the world by bringing education to places that can't be reached today,'' said Dr. Thrun.\nThe rapid increase in the availability of high-bandwidth Internet service, coupled with a wide array of interactive software, has touched off a new wave of experimentation in education.\nFor example, the Khan Academy, which focuses on high school and middle school, intentionally turns the relationship of the classroom and homework upside down. Students watch lectures at home, then work on problem sets in class, where the teacher can assist them one on one.\nThe Stanford scientists said they were focused on going beyond early Internet education efforts, which frequently involved uploading online videos of lectures given by professors and did little to motivate students to do the coursework required to master subjects.\nThe three online courses, which will employ both streaming Internet video and interactive technologies for quizzes and grading, have in the past been taught to smaller groups of Stanford students in campus lecture halls. Last year, for example, Introduction to Artificial Intelligence drew 177 students.\nThe two additional courses will be an introductory course on database software, taught by Jennifer Widom, chairwoman of the computer science department, and an introduction to machine learning, taught by Andrew Ng.\nDr. Widom said she had recorded her video lectures during the summer and would use classroom sessions to work with smaller groups of students on projects that might be competitive and to bring in people from the industry to give special lectures. Unlike the A.I. course, this one will compare online students with one another and not with the Stanford students.\nHow will the artificial intelligence instructors grade 58,000 students? The scientists said they would make extensive use of technology. ''We have a system running on the Amazon cloud, so we think it will hold up,'' Dr. Norvig said.\nIn place of office hours, they will use the Google moderator service, software that will allow students to vote on the best questions for the professors to respond to in an online chat and possibly video format. They are considering ways to personalize the exams to minimize cheating. Part of the instructional software was developed by Know Labs, a company Dr. Thrun helped start.\nAlthough the three courses are described as an experiment, the researchers say they expect university classes to be made more widely accessible via the Internet.\n''I personally would like to see the equivalent of a Stanford computer science degree on the Web,'' Dr. Ng said.\nDr. Widom said that having Stanford courses freely available could both assist and compete with other colleges and universities. A small college might not have the faculty members to offer a particular course, but could supplement its offerings with the Stanford lectures.\nThere has also been some discussion at Stanford about whether making the courses freely available would prove to be a threat to the university, which charges high fees for tuition. Dr. Thrun dismissed that idea.\n''I'm much more interested in bringing Stanford to the world,'' he said. ''I see the developing world having colossal educational needs.''\nHal Abelson, a computer scientist at M.I.T. who helped develop an earlier generation of educational offerings that began in 2002, said the Stanford course showed how rapidly the online world was evolving.\n''The idea that you could put up open content at all was risky 10 years ago, and we decided to be very conservative,'' he said. ''Now the question is how do you move into something that is more interactive and collaborative, and we will see lots and lots of models over the next four or five years.''\n","92":"Last winter, Artificial Intelligence, an audaciously imaginative 22-member troupe of comic actors led by Nancy Cassaro, invaded the Ballroom, a normally staid Chelsea cabaret to put on a mock TV special, ''Vicki's Valentine Thing,'' which satirically recreated the sound and style of a 1967 family variety show. Played by Ms. Cassaro, the Vicki of the title was a fictional character named Vicki Oberjeune, modelled loosely after the late Judy Garland.\u00a0Audiences saw not only the show but the studio technicians ''filming'' the event with all its backstage chaos and family squabbles. Last Christmas, Artificial Intelligence returned to the club with an equally zany holiday spoof, ''A Very Vicki Christmas,'' set in 1965 and featuring the same characters.\nThe troupe's brand of environmental theater isn't limited to clubs. The more it blends in with real life, the better it gets. And this Saturday, Artificial Intelligence will stage a two-part event, ''Tony and Tina's Wedding'' - an Italian lower-middle-class wedding and reception complete with food, drink and dancing - which lasts several hours. It begins with the wedding of Tina Vitale (Ms. Cassaro) and Tony Nunzio (Marc Nassar) at Washington Square Church (135 West Fourth Street) and is followed by a reception at Carmelita's, a dingy reception hall at 150 West 14th Street. Attending the performance last year, this critic was so drawn into the festivities that the line between theater and reality almost evaporated. Performances commence Saturday and continue Saturdays and Sundays. The ceremony begins promptly at 2 P.M. at Washington Square Church. Reservations: 279-4200.\n","93":"Five major technology companies said Wednesday that they had created an organization to set the ground rules for protecting humans -- and their jobs -- in the face of rapid advances in artificial intelligence.\u00a0\nThe Partnership on AI, unites Amazon, Facebook, Google, IBM and Microsoft in an effort to ease public fears of machines that are learning to think for themselves and perhaps ease corporate anxiety over the prospect of government regulation of this new technology. \n  The organization has been created at a time of significant public debate about artificial intelligence technologies that are built into a variety of robots and other intelligent systems, including self-driving cars and workplace automation.\n  The industry group introduced a set of basic ethical standards for engineering development and scientific research that its five members have agreed upon.\n  In a conference call on Wednesday, five artificial intelligence researchers representing the companies said they thought the technology would be a major force in the world for social and economic benefits, but they acknowledged the potential for misuse in a wide variety of ways.\n  They said their effort was not intended to be an enforcement organization to force technology companies into self-regulation. Rather, they want to foster ''public understanding'' and set ''best practices'' for work in artificial intelligence.\n  ''We passionately believe in the potential for it to transform in a positive way our world,'' said Mustafa Suleyman, head of applied A.I. for DeepMind, an artificial intelligence development company acquired by Google in 2014. ''We believe it's critical now to start to think about new models of engagement with the public, new models of collaboration across the industry and new models of transparency around the work that we do.''\n  The group released eight tenets that are evocative of Isaac Asimov's original ''Three Laws of Robotics,'' which appeared in a science fiction story in 1942. The new principles include high-level ideals such as, ''We will seek to ensure that A.I. technologies benefit and empower as many people as possible.''\n  Nevertheless, at least one of the tenets implies that the companies realize they could be drawn into sticky ethical situations, and it calls on engineers to oppose the use of artificial intelligence technology in weapons or other tools that could be used to violate human rights.\n  ''With the hyperbole about A.I. over the last two to four years, there have been concerns in an echo chamber of anxiety that the government itself will be misinformed,'' said Eric Horvitz, managing director for Microsoft Research.\n  The researchers said they were talking with other companies like Apple and research laboratories like the new nonprofit research group OpenAI about participating in their organization.\n\n\n\n","94":"SAN FRANCISCO -- Google's unveiling of new smartphones, smart speakers and other gadgets had all the makings of a typical technology product launch: a fawning crowd of superfans, skeptical journalists, slick product videos, not-so-subtle jabs at the competition, and overly romanticized descriptions of design choices, colors and materials.\nBut one nagging question lingered for Google, which makes nearly all of its money from selling online advertisements: Is it finally serious about making devices? \u00a0\n  On Wednesday, Google did its best to demonstrate its commitment. It introduced two new Pixel smartphones, Google Home speakers both small and large, a laptop running the company's Chrome software, a new virtual reality headset and wireless headphones.\n  But Google's pitch for why its hardware is different had little to do with the hardware itself.\n  Unlike the way an Apple event is conducted -- usually chock-full of talk about chip speeds and screen resolutions -- Google didn't spend much time on product specifications. Instead, its focus was on artificial intelligence. Sundar Pichai, Google's chief executive, spent the first 10 minutes explaining how artificial intelligence was helping Google Maps and its translations.\n  Mr. Pichai said that as an ''A.I. first'' company, this is a ''unique moment in time'' for Google to combine hardware, software and artificial intelligence. ''It's radically rethinking how computing should work,'' he said.\n  Google executives said it has been getting harder to find new hardware breakthroughs like bigger and better screens, but they believe significant improvements will come from artificial intelligence software that is developing at a faster clip than physical components.\n  Rick Osterloh, Google's senior vice president of hardware, compared the company's strategy for building devices to search and email. Google was not the first search engine and Gmail was hardly the first free web-based email provider -- but both services reimagined what those products should do.\n  Last year, the company started its ''Made By Google'' line of hardware products, headlined by the Pixel smartphone. The handset received positive reviews, but it did not threaten the premium smartphone dominance of Apple or Samsung.\n  On Wednesday, Google demonstrated how every hardware product had received an A.I. makeover. The Pixel smartphones come with an image-recognition app called Lens that can help users find information just by pointing a camera at a movie poster or an ad. The new ''smart speaker'' uses artificial intelligence to adjust its sound for the layout of a room. And new wireless headphones allow for instant translation of different languages.\n  The question of Google's commitment to hardware is a testament to the challenges of competing against devices made by Apple, Amazon and Samsung. Most other companies have found it hard to turn a profit in that product fight, and a flop can follow a company around for years -- both in money and reputation lost.\n  It is also a recognition of Google's history of fits and starts with devices. The company once acquired Motorola, only to sell it a few years later to Lenovo. It bought Nest and Dropcam, but the introduction of new products from those home device companies seemed to stagnate after they joined Google, now operating under the parent company, Alphabet.\n  Whether Google's device push sticks over the long haul remains to be seen, but its checkbook for hardware is still open.\n  Last month, Google said it had agreed to acquire a team of 2,000 engineers from the Taiwanese manufacturer HTC for $1.1 billion. The hardware-focused personnel came from an HTC research and development division that was already working with Google to create the Pixel phones. Google said the acquisition will allow it to move faster in its efforts to develop new features for smartphones.\n  The deal is expected to close, pending regulatory approval, early next year.\n\n\n\n","95":"John McCarthy, a computer scientist who helped design the foundation of today's Internet-based computing and who is widely credited with coining the term for a frontier of research he helped pioneer, Artificial Intelligence, or A.I., died on Monday at his home in Stanford, Calif. He was 84.\nThe cause was complications of heart disease, his daughter Sarah McCarthy said.\nDr. McCarthy's career followed the arc of modern computing. Trained as a mathematician, he was responsible for seminal advances in the field and was often called the father of computer time-sharing, a major development of the 1960s that enabled many people and organizations to draw simultaneously from a single computer source, like a mainframe, without having to own one.\u00a0\nBy lowering costs, it allowed more people to use computers and laid the groundwork for the interactive computing of today.\nThough he did not foresee the rise of the personal computer, Dr. McCarthy was prophetic in describing the implications of other technological advances decades before they gained currency.\n''In the early 1970s, he presented a paper in France on buying and selling by computer, what is now called electronic commerce,'' said Whitfield Diffie, an Internet security expert who worked as a researcher for Dr. McCarthy at the Stanford Artificial Intelligence Laboratory.\nAnd in the study of artificial intelligence, ''no one is more influential than John,'' Mr. Diffie said.\nWhile teaching mathematics at Dartmouth in 1956, Dr. McCarthy was the principal organizer of the first Dartmouth Conference on Artificial Intelligence.\nThe idea of simulating human intelligence had been discussed for decades, but the term ''artificial intelligence'' -- originally used to help raise funds to support the conference -- stuck.\nIn 1958, Dr. McCarthy moved to the Massachusetts Institute of Technology, where, with Marvin Minsky, he founded the Artificial Intelligence Laboratory. It was at M.I.T. that he began working on what he called List Processing Language, or Lisp, a computer language that became the standard tool for artificial intelligence research and design.\nAround the same time he came up with a technique called garbage collection, in which pieces of computer code that are not needed by a running computation are automatically removed from the computer's random access memory.\nHe developed the technique in 1959 and added it to Lisp. That technique is now routinely used in Java and other programming languages.\nHis M.I.T. work also led to fundamental advances in software and operating systems. In one, he was instrumental in developing the first time-sharing system for mainframe computers.\nThe power of that invention would come to shape Dr. McCarthy's worldview to such an extent that when the first personal computers emerged with local computing and storage in the 1970s, he belittled them as toys.\nRather, he predicted, wrongly, that in the future everyone would have a relatively simple and inexpensive computer terminal in the home linked to a shared, centralized mainframe and use it as an electronic portal to the worlds of commerce and news and entertainment media.\nDr. McCarthy, who taught briefly at Stanford in the early 1950s, returned there in 1962 and in 1964 became the founding directorof the Stanford Artificial Intelligence Laboratory, or SAIL. Its optimistic, space-age goal, with financial backing from the Pentagon, was to create a working artificial intelligence system within a decade.\nYears later he developed a healthy respect for the challenge, saying that creating a ''thinking machine'' would require ''1.8 Einsteins and one-tenth the resources of the Manhattan Project.''\nArtificial intelligence is still thought to be far in the future, though tremendous progress has been made in systems that mimic many human skills, including vision, listening, reasoning and, in robotics, the movements of limbs. From the mid-'60s to the mid-'70s, the Stanford lab played a vital role in creating some of these technologies, including robotics and machine-vision natural language.\nIn 1972, the laboratory drew national attention when Stewart Brand, the founder of The Whole Earth Catalog, wrote about it in Rolling Stone magazine under the headline ''SPACEWAR: Fanatic Life and Symbolic Death Among the Computer Bums.'' The article evoked the esprit de corps of a group of researchers who had been freed to create their own virtual worlds, foreshadowing the emergence of cyberspace. ''Ready or not, computers are coming to the people,'' Mr. Brand wrote.\nDr. McCarthy had begun inviting the Homebrew Computer Club, a Silicon Valley hobbyist group,to meet at the Stanford lab. Among its growing membership were Steven P. Jobs and Stephen Wozniak, who would go on to found Apple. Mr. Wozniak designed his first personal computer prototype, the Apple 1, to share with his Homebrew friends.\nBut Dr. McCarthy still cast a jaundiced eye on personal computing. In the second Homebrew newsletter, he suggested the formation of a ''Bay Area Home Terminal Club,'' to provide computer access on a shared Digital Equipment computer. He thought a user fee of $75 a month would be reasonable.\nThough Dr. McCarthy would initially miss the significance of the PC, his early thinking on electronic commerce would influence Mr. Diffie at the Stanford lab. Drawing on those ideas, Mr. Diffie began thinking about what would replace the paper personal check in an all-electronic world.\nHe and two other researchers went on to develop the basic idea of public key cryptography, which is now the basis of all modern electronic banking and commerce, providing secure interaction between a consumer and a business.\nA chess enthusiast, Dr. McCarthy had begun working on chess-playing computer programs in the 1950s at Dartmouth. Shortly after joining the Stanford lab, he engaged a group of Soviet computer scientists in an intercontinental chess match after he discovered they had a chess-playing computer. Played by telegraph, the match consisted of four games and lasted almost a year. The Soviet scientists won.\nJohn McCarthy was born on Sept. 4, 1927, into a politically engaged family in Boston. His father, John Patrick McCarthy, was an Irish immigrant and a labor organizer.\nHis mother, the former Ida Glatt, a Lithuanian Jewish immigrant, was active in the suffrage movement. Both parents were members of the Communist Party. The family later moved to Los Angeles in part because of John's respiratory problems.\nHe entered the California Institute of Technology in 1944 and went on to graduate studies at Princeton, where he was a colleague of John Forbes Nash Jr., the Nobel Prize-winning mathematician and subject of Sylvia Nasar's book ''A Beautiful Mind,'' which was adapted into a movie.\nAt Princeton, in 1949, he briefly joined the local Communist Party cell, which had two other members: a cleaning woman and a gardener, he told an interviewer. But he quit the party shortly afterward.\nIn the '60s, as the Vietnam War escalated, his politics took a conservative turn as he grew disenchanted with leftist politics.\nIn 1971 Dr. McCarthy received the Turing Award, the most prestigious given by the Association of Computing Machinery, for his work in artificial intelligence. He was awarded the Kyoto Prize in 1988, the National Medal of Science in 1991 and the Benjamin Franklin Medal in 2003.\nDr. McCarthy was married three times. His second wife, Vera Watson, a member of the American Women's Himalayan Expedition, died in a climbing accident on Annapurna in 1978.\nBesides his daughter Sarah, of Nevada City, Calif., he is survived by his wife, Carolyn Talcott, of Stanford; another daughter, Susan McCarthy, of San Francisco; and a son, Timothy, of Stanford.\nHe remained an independent thinker throughout his life. Some years ago, one of his daughters presented him with a license plate bearing one of his favorite aphorisms: ''Do the arithmetic or be doomed to talk nonsense.''\n","98":"Dr. Kenneth Mark Colby, a psychiatrist known for his work with artificial intelligence, died on April 20 at his home in Malibu, Calif. He was 81.\n     Dr. Colby, a founder and chairman of Malibu Artifactual Intelligence Works, a software company, was an emeritus professor of psychiatry and behavioral sciences at the University of California at Los Angeles. \u00a0\n He created one of the early software programs known as chatterbots, which simulate conversations with people. His program, called Parry, for paranoia, appeared in 1971 and is said to be the only one to have passed the \"Turing test,\" named for the British mathematician Alan M. Turing, who in 1950 suggested that if a computer could successfully impersonate a human by carrying on a typed conversation with a person, it could be called intelligent.\nWith Parry, Dr. Colby established that a computer chip could be programmed to imitate a paranoid schizophrenic. In test interviews, psychiatrists communicating with Parry by Teletype were indeed unable to distinguish its responses from those of real paranoiacs.\nKenneth Colby was born in Waterbury, Conn., and graduated from Yale in 1941 and from its School of Medicine two years later.\nEarly in his career, he was a professor of computer science at Stanford, where he also did research for the National Institute of Mental Health. He created Parry at the university's Artificial Intelligence Laboratory.\nIn 1974, he went to U.C.L.A., where he was affiliated with the department of psychiatry and biobehavioral sciences.\nAt 31, Dr. Colby published \"A Primer for Psychotherapists.\" He wrote or contributed to nine more books and over 100 articles on psychiatry, psychology, psychotherapy and artificial intelligence.\nHe and his son, Peter, founded Malibu Artifactual Intelligence Works in 1989. Among the software they produced is a therapy program called Overcoming Depression, which features interactive plain language designed to help a patient understand, overcome and prevent depression.\nBesides his son, of Malibu, Dr. Colby is survived by his wife of 43 years, Maxine Hansbold Colby; a daughter, Erin Johnson of Santa Maria, Calif.; and two grandsons.\nDr. Colby adapted Parry from another program, Eliza, created in 1966 by Dr. Joseph Weizenbaum at the Massachusetts Institute of Technology.\nEliza was developed more as a language-parsing tool than as an exercise in human intelligence. Named for the Eliza Doolittle character in \"Pygmalion,\" it was the first conversational computer program, designed to imitate a psychotherapist asking questions instead of giving advice. It appeared to give conversational answers, although it could be led to lapse into obtuse nonsense.\nEliza and the paranoid Parry eventually met for a \"talk,\" executing different programs. One exchange went something like this:\nPARRY -- In my opinion, you have a one-track mind.\nELIZA -- What makes you think I have a one-track mind?\nPARRY -- I used to go to the track every week.\nELIZA -- What does that suggest to you?\nPARRY -- I avoid racetracks now because the races are fixed.\nELIZA -- Is that the real reason?\nPARRY -- I'd rather not discuss it.\nELIZA -- What else would you like to discuss?\nPARRY -- That's something I don't want to go into.\n","99":"SAN FRANCISCO -- Apple has hired Google's chief of search and artificial intelligence, John Giannandrea, a major coup in its bid to catch up to the artificial intelligence technology of its rivals.\nApple said on Tuesday that Mr. Giannandrea will run Apple's ''machine learning and A.I. strategy,'' and become one of 16 executives who report directly to Apple's chief executive, Timothy D. Cook. \n  The hire is a victory for Apple, which many Silicon Valley executives and analysts view as lagging its peers in artificial intelligence, an increasingly crucial technology for companies that enable computers to handle more complex tasks, like understanding voice commands or identifying people in images.\u00a0\n  ''Our technology must be infused with the values we all hold dear,'' Mr. Cook said Tuesday morning in an email to staff members obtained by The New York Times. ''John shares our commitment to privacy and our thoughtful approach as we make computers even smarter and more personal.''\n  While Apple has risen to become the world's most valuable publicly traded company on the back of the iPhone, many in the technology industry consider the iPhone's digital assistant, Siri, to be less effective than its counterparts at Google and Amazon.\n  Mr. Giannandrea, a 53-year-old native of Scotland known to colleagues as J.G., helped lead the push to integrate A.I. throughout Google's products, including internet search, Gmail and its own digital assistant, Google Assistant.\n  He joined Google in 2010 when it purchased Metaweb, a start-up where he served as chief technology officer. Metaweb was building what it described as a ''database of the world's knowledge,'' which Google eventually rolled into its search engine to deliver direct answers to users' queries. (Try googling ''How old is Steph Curry?'') During Mr. Giannandrea's tenure, A.I. research became increasingly important inside Google, with its primary A.I. lab, Google Brain, moving into a space beside the chief executive, Sundar Pichai.\n  Engineers with A.I. expertise are some of the most sought-after people in Silicon Valley, with salaries sometimes exceeding eight figures. When news broke Monday that Mr. Giannandrea was unexpectedly stepping down as Google's A.I. chief, he immediately became perhaps the most eligible tech executive on the market. By Tuesday, it became clear he was never really on the market.\n  Apple has made other high-profile hires in the field, including the Carnegie Mellon professor Russ Salakhutdinov. Mr. Salakhutdinov studied at the University of Toronto under Geoffrey Hinton, who helps oversee the Google Brain lab.\n  Apple has taken a strong stance on protecting the privacy of people who use its devices and online services, which could put it at a disadvantage when building services using neural networks.\n  Researchers train these systems by pooling enormous amounts of digital data, sometimes from customer services. Apple, however, has said it is developing methods that would allow it to train these algorithms without compromising privacy.\n  On the debate over whether humanity should be worried about the rapidly accelerating improvements in A.I., Mr. Giannandrea told MIT Technology Review in an interview last year that the concerns were overblown.\n  ''What I object to is this assumption that we will leap to some kind of superintelligent system that will then make humans obsolete,'' he said. ''I understand why people are concerned about it but I think it's gotten way too much airtime. I just see no technological basis as to why this is imminent at all.''\n\n\n\n","100":"To the Editor:\nIn your May 10 editorial \"Mind Over Matter,\" the statement that the decades-long research in artificial intelligence has been a \"crashing disappointment\" panders to the popular myth that the main goal of artificial intelligence is to replicate human intelligence.\u00a0\n\u00a0Artificial intelligence is a field of inquiry into computational models of human cognition and perception, the goal being to discover to what extent various human faculties can be captured in a computer. This is different from saying that we want to duplicate intelligence in a machine.\nWith computers becoming ever more powerful, the field of research called artificial intelligence is even more of an intellectual imperative. It's important to know to what extent tasks such as, say, the diagnosis of a suspicious-looking mass in a mammogram as potentially cancerous can be computerized.\nThere are societal advantages to the computerization of those human functions that can be demonstrated to be better executed by computer programs, since computers will perform these tasks more consistently, more reliably and, in many cases, more economically.\u00a0AVI KAKWest Lafayette, Ind., May 12, 1997\u00a0The writer is a professor of electrical and computer engineering at Purdue University.\n","101":"I.\nArtificial Intelligence (A.I.) is having a moment, albeit one marked by crucial ambiguities. \nCognoscenti including Stephen Hawking, Elon Musk and Bill Gates, among others, have recently weighed in on its potential and perils. After reading Nick Bostrom's book \"Superintelligence,\" Musk even wondered aloud if A.I. may be \"our biggest existential threat.\" \nPositions on A.I. are split, and not just on its dangers. Some insist that \"hard A.I.\" (with human-level intelligence) can never exist, while others conclude that it is inevitable. But in many cases these debates may be missing the real point of what it means to live and think with forms of synthetic intelligence very different from our own. \nThat point, in short, is that a mature A.I. is not necessarily a humanlike intelligence, or one that is at our disposal. If we look for A.I. in the wrong ways, it may emerge in forms that are needlessly difficult to recognize, amplifying its risks and retarding its benefits.\u00a0\nThis is not just a concern for the future. A.I. is already out of the lab and deep into the fabric of things. \"Soft A.I.,\" such as Apple's Siri and Amazon recommendation engines, along with infrastructural A.I., such as high-speed algorithmic trading, smart vehicles and industrial robotics, are increasingly a part of everyday life - part of how our tools work, how our cities move and how our economy builds and trades things. \nUnfortunately, the popular conception of A.I., at least as depicted in countless movies, games and books, still seems to assume that humanlike characteristics (anger, jealousy, confusion, avarice, pride, desire, not to mention cold alienation) are the most important ones to be on the lookout for. This anthropocentric fallacy may contradict the implications of contemporary A.I. research, but it is still a prism through which much of our culture views an encounter with advanced synthetic cognition. \nThe little boy robot in Steven Spielberg's 2001 film \"A.I. Artificial Intelligence\" wants to be a real boy with all his little metal heart, while Skynet in the \"Terminator\" movies is obsessed with the genocide of humans. We automatically presume that the Monoliths in Stanley Kubrick and Arthur C. Clarke's 1968 film, \"2001: A Space Odyssey,\" want to talk to the human protagonist Dave, and not to his spaceship's A.I., HAL 9000. \nI argue that we should abandon the conceit that a \"true\" Artificial Intelligence must care deeply about humanity - us specifically - as its focus and motivation. Perhaps what we really fear, even more than a Big Machine that wants to kill us, is one that sees us as irrelevant. Worse than being seen as an enemy is not being seen at all. \nUnless we assume that humanlike intelligence represents all possible forms of intelligence - a whopper of an assumption - why define an advanced A.I. by its resemblance to ours? After all, \"intelligence\" is notoriously difficult to define, and human intelligence simply can't exhaust the possibilities. Granted, doing so may at times have practical value in the laboratory, but in cultural terms it is self-defeating, unethical and perhaps even dangerous.\nWe need a popular culture of A.I. that is less parochial and narcissistic, one that is based on more than simply looking for a machine version of our own reflection. As a basis for staging encounters between various A.I.s and humans, that would be a deeply flawed precondition for communication. Needless to say, our historical track record with \"first contacts,\" even among ourselves, does not provide clear comfort that we are well-prepared.\nII.\nThe idea of measuring A.I. by its ability to \"pass\" as a human - dramatized in countless sci-fi films, from Ridley Scott's \"Blade Runner\" to Spike Jonze's \"Her\" - is actually as old as modern A.I. research itself. It is traceable at least to 1950 when the British mathematician Alan Turing published \"Computing Machinery and Intelligence,\" a paper in which he described what we now call the \"Turing Test,\" and which he referred to as the \"imitation game.\" There are different versions of the test, all of which are revealing as to why our approach to the culture and ethics of A.I. is what it is, for good and bad. For the most familiar version, a human interrogator asks questions of two hidden contestants, one a human and the other a computer. Turing suggests that if the interrogator usually cannot tell which is which, and if the computer can successfully pass as human, then can we not conclude, for practical purposes, that the computer is \"intelligent\"? \nMore people \"know\" Turing's foundational text than have actually read it. This is unfortunate because the text is marvelous, strange and surprising. Turing introduces his test as a variation on a popular parlor game in which two hidden contestants, a woman (player A) and a man (player B) try to convince a third that he or she is a woman by their written responses to leading questions. To win, one of the players must convincingly be who they really are, whereas the other must try to pass as another gender. Turing describes his own variation as one where \"a computer takes the place of player A,\" and so a literal reading would suggest that in his version the computer is not just pretending to be a human, but pretending to be a woman. It must pass as a she. \nOther versions had it that player B could be either a man or a woman. It would seem a very different kind of game if only one player is faking, or if both are, or if neither of them are. Now that we give the computer a seat, we may have it pretending to be a woman along with a man pretending to be a woman, both trying to trick the interrogator into figuring out which is a man and which is a woman. Or perhaps a computer pretending to be a man pretending to be a woman, along with a man pretending to be a woman, or even a computer pretending to be a woman pretending to be a man pretending to be a woman! In the real world, of course, we already have all of the above.\n\"The Imitation Game,\" Morten Tyldum's Oscar-winning 2014 film about Turing, reminds us that the mathematician himself also had to \"pass\" - in his case as straight man in a society that criminalized homosexuality. Upon discovery that he was not what he appeared to be, he was forced to undergo horrific medical treatments known as \"chemical castration.\" Ultimately the physical and emotional pain was too great and he committed suicide. The episode was grotesque tribute to a man whose contribution to defeating Hitler's military was still at that time a state secret. Turing was only recently given posthumous pardon, but the tens of thousands of other British men sentenced under similar laws have not. \nOne notes the sour ironic correspondence between asking an A.I. to \"pass\" the test in order to qualify as intelligent - to \"pass\" as a human intelligence - with Turing's own need to hide his homosexuality and to \"pass\" as a straight man. The demands of both bluffs are unnecessary and profoundly unfair.   \nPassing as a person, as a white or black person, or as a man or woman, for example, comes down to what others see and interpret. Because everyone else is already willing to read others according to conventional cues (of race, sex, gender, species, etc.) the complicity between whoever (or whatever) is passing and those among which he or she or it performs is what allows passing to succeed. Whether or not an A.I. is trying to pass as a human or is merely in drag as a human is another matter. Is the ruse all just a game or, as for some people who are compelled to pass in their daily lives, an essential camouflage? Either way, \"passing\" may say more about the audience than about the performers. \nWe would do better to presume that in our universe, \"thinking\" is much more diverse, even alien, than our own particular case. The real philosophical lessons of A.I. will have less to do with humans teaching machines how to think than with machines teaching humans a fuller and truer range of what thinking can be (and for that matter, what being human can be).\nIII.\nThat we would wish to define the very existence of A.I. in relation to its ability to mimic how humans think that humans think will be looked back upon as a weird sort of speciesism. The legacy of that conceit helped to steer some older A.I. research down disappointingly fruitless paths, hoping to recreate human minds from available parts. It just doesn't work that way. Contemporary A.I. research suggests instead that the threshold by which any particular arrangement of matter can be said to be \"intelligent\" doesn't have much to do with how it reflects humanness back at us. As Stuart Russell and Peter Norvig (now director of research at Google) suggest in their essential A.I. textbook, biomorphic imitation is not how we design complex technology. Airplanes don't fly like birds fly, and we certainly don't try to trick birds into thinking that airplanes are birds in order to test whether those planes \"really\" are flying machines. Why do it for A.I. then? Today's serious A.I. research does not focus on the Turing Test as an objective criterion of success, and yet in our popular culture of A.I., the test's anthropocentrism holds such durable conceptual importance. Like the animals who talk like teenagers in a Disney movie, other minds are conceivable mostly by way of puerile ventriloquism.\nWhere is the real injury in this? If we want everyday A.I. to be congenial in a humane sort of way, so what? The answer is that we have much to gain from a more sincere and disenchanted relationship to synthetic intelligences, and much to lose by keeping illusions on life support. Some philosophers write about the possible ethical \"rights\" of A.I. as sentient entities, but that's not my point here. Rather, the truer perspective is also the better one for us as thinking technical creatures.   \nMusk, Gates and Hawking made headlines by speaking to the dangers that A.I. may pose. Their points are important, but I fear were largely misunderstood by many readers. Relying on efforts to program A.I. not to \"harm humans\" (inspired by on Isaac Asimov's \"three laws\" of robotics from 1942) makes sense only when an A.I. knows what humans are and what harming them might mean. There are many ways that an A.I. might harm us that that have nothing to do with its malevolence toward us, and chief among these is exactly following our well-meaning instructions to an idiotic and catastrophic extreme. Instead of mechanical failure or a transgression of moral code, the A.I. may pose an existential risk because it is both powerfully intelligent and disinterested in humans. To the extent that we recognize A.I. by its anthropomorphic qualities, or presume its preoccupation with us, we are vulnerable to those eventualities.\nWhether or not \"hard A.I.\" ever appears, the harm is also in the loss of all that we prevent ourselves from discovering and understanding when we insist on protecting beliefs we know to be false. In the 1950 essay, Turing offers several rebuttals to his speculative A.I., including a striking comparison with earlier objections to Copernican astronomy. Copernican traumas that abolish the false centrality and absolute specialness of human thought and species-being are priceless accomplishments. They allow for human culture based on how the world actually is more than on how it appears to us from our limited vantage point. Turing referred to these as \"theological objections,\" but one could argue that the anthropomorphic precondition for A.I. is a \"pre-Copernican\" attitude as well, however secular it may appear. The advent of robust inhuman A.I. may let us achieve another disenchantment, one that should enable a more reality-based understanding of ourselves, our situation, and a fuller and more complex understanding of what \"intelligence\" is and is not. From there we can hopefully make our world with a greater confidence that our models are good approximations of what's out there (always a helpful thing.)\nLastly, the harm is in perpetuating a relationship to technology that has brought us to the precipice of a Sixth Great Extinction. Arguably the Anthropocene itself is due less to technology run amok than to the humanist legacy that understands the world as having been given for our needs and created in our image. We hear this in the words of thought leaders who evangelize the superiority of a world where machines are subservient to the needs and wishes of humanity. If you think so, Google \"pig decapitating machine\" (actually, just don't) and then let's talk about inventing worlds in which machines are wholly subservient to humans' wishes. \nOne wonders whether it is only from a society that once gave theological and legislative comfort to chattel slavery that this particular affirmation could still be offered in 2015 with such satisfied na\u00efvet\u00e9? This is the sentiment - this philosophy of technology exactly - that is the basic algorithm of the Anthropocenic predicament, and consenting to it would also foreclose adequate encounters with A.I. It is time to move on. This pretentious folklore is too expensive.\nBenjamin H. Bratton (@bratton) is an associate professor of visual arts at the University of California, San Diego. His next book, \"The Stack: On Software and Sovereignty,\" will be published this fall by the MIT Press. \n","102":"A robot that can open doors and find electrical outlets to recharge itself. Computer viruses that no one can stop. Predator drones, which, though still controlled remotely by humans, come close to a machine that can kill autonomously. \n  Impressed and alarmed by advances in artificial intelligence, a group of computer scientists is debating whether there should be limits on research that might lead to loss of human control over computer-based systems that carry a growing share of society's workload, from waging war to chatting with customers on the phone.\nTheir concern is that further advances could create profound social disruptions and even have dangerous consequences. \n  As examples, the scientists pointed to a number of technologies as diverse as experimental medical systems that interact with patients to simulate empathy, and computer worms and viruses that defy extermination and could thus be said to have reached a ''cockroach'' stage of machine intelligence.\u00a0\n  While the computer scientists agreed that we are a long way from Hal, the computer that took over the spaceship in ''2001: A Space Odyssey,'' they said there was legitimate concern that technological progress would transform the work force by destroying a widening range of jobs, as well as force humans to learn to live with machines that increasingly copy human behaviors.\n  The researchers -- leading computer scientists, artificial intelligence researchers and roboticists who met at the Asilomar Conference Grounds on Monterey Bay in California -- generally discounted the possibility of highly centralized superintelligences and the idea that intelligence might spring spontaneously from the Internet. But they agreed that robots that can kill autonomously are either already here or will be soon. \n  They focused particular attention on the specter that criminals could exploit artificial intelligence systems as soon as they were developed. What could a criminal do with a speech synthesis system that could masquerade as a human being? What happens if artificial intelligence technology is used to mine personal information from smart phones?\n  The researchers also discussed possible threats to human jobs, like self-driving cars, software-based personal assistants and service robots in the home. Just last month, a service robot developed by Willow Garage in Silicon Valley proved it could navigate the real world. \n  A report from the conference, which took place in private on Feb. 25, is to be issued later this year. Some attendees discussed the meeting for the first time with other scientists this month and in interviews. \n  The conference was organized by the Association for the Advancement of Artificial Intelligence, and in choosing Asilomar for the discussions, the group purposefully evoked a landmark event in the history of science. In 1975, the world's leading biologists also met at Asilomar to discuss the new ability to reshape life by swapping genetic material among organisms. Concerned about possible biohazards and ethical questions, scientists had halted certain experiments. The conference led to guidelines for recombinant DNA research, enabling experimentation to continue.\n  The meeting on the future of artificial intelligence was organized by Eric Horvitz, a Microsoft researcher who is now president of the association. \n  Dr. Horvitz said he believed computer scientists must respond to the notions of superintelligent machines and artificial intelligence systems run amok.\n  The idea of an ''intelligence explosion'' in which smart machines would design even more intelligent machines was proposed by the mathematician I. J. Good in 1965. Later, in lectures and science fiction novels, the computer scientist Vernor Vinge popularized the notion of a moment when humans will create smarter-than-human machines, causing such rapid change that the ''human era will be ended.'' He called this shift the Singularity. \n  This vision, embraced in movies and literature, is seen as plausible and unnerving by some scientists like William Joy, co-founder of Sun Microsystems. Other technologists, notably Raymond Kurzweil, have extolled the coming of ultrasmart machines, saying they will offer huge advances in life extension and wealth creation. \n  ''Something new has taken place in the past five to eight years,'' Dr. Horvitz said. ''Technologists are providing almost religious visions, and their ideas are resonating in some ways with the same idea of the Rapture.''\n  The Kurzweil version of technological utopia has captured imaginations in Silicon Valley. This summer an organization called the Singularity University began offering courses to prepare a ''cadre'' to shape the advances and help society cope with the ramifications. \n  ''My sense was that sooner or later we would have to make some sort of statement or assessment, given the rising voice of the technorati and people very concerned about the rise of intelligent machines,'' Dr. Horvitz said.\n  The A.A.A.I. report will try to assess the possibility of ''the loss of human control of computer-based intelligences.'' It will also grapple, Dr. Horvitz said, with socioeconomic, legal and ethical issues, as well as probable changes in human-computer relationships. How would it be, for example, to relate to a machine that is as intelligent as your spouse? \n  Dr. Horvitz said the panel was looking for ways to guide research so that technology improved society rather than moved it toward a technological catastrophe. Some research might, for instance, be conducted in a high-security laboratory.\n  The meeting on artificial intelligence could be pivotal to the future of the field. Paul Berg, who was the organizer of the 1975 Asilomar meeting and received a Nobel Prize for chemistry in 1980, said it was important for scientific communities to engage the public before alarm and opposition becomes unshakable.\n  ''If you wait too long and the sides become entrenched like with G.M.O.,'' he said, referring to genetically modified foods, ''then it is very difficult. It's too complex, and people talk right past each other.''\n  Tom Mitchell, a professor of artificial intelligence and machine learning at Carnegie Mellon University, said the February meeting had changed his thinking. ''I went in very optimistic about the future of A.I. and thinking that Bill Joy and Ray Kurzweil were far off in their predictions,'' he said. But, he added, ''The meeting made me want to be more outspoken about these issues and in particular be outspoken about the vast amounts of data collected about our personal lives.''\n  Despite his concerns, Dr. Horvitz said he was hopeful that artificial intelligence research would benefit humans, and perhaps even compensate for human failings. He recently demonstrated a voice-based system that he designed to ask patients about their symptoms and to respond with empathy. When a mother said her child was having diarrhea, the face on the screen said, ''Oh no, sorry to hear that.'' \n  A physician told him afterward that it was wonderful that the system responded to human emotion. ''That's a great idea,'' Dr. Horvitz said he was told. ''I have no time for that.''  \n","103":"The moment when science fiction becomes fact can take even a technologist's breath away. I confess to gasping for air when I talked about artificial intelligence last week with Max Tegmark, a professor at MIT and the author of \"Life 3.0: Being Human in the Age of Artificial Intelligence.\"         \nTegmark pointed out we need to understand that artificial intelligence is not science fiction any more. Within the lifetime of most who are reading this column, software will develop the ability to complete complex tasks without human intercession. And it will do it faster and better. And that is a very disquieting thought.        \u00a0\nSo, should we stop developing AI? Tegmark doesn't see that as the right question to ask. As he puts it, the question is \"not whether you are for or against AI. That's like asking our ancestors if they were for or against fire.\"\nTegmark believes that as tool makers we inevitably create software that achieves artificial intelligence. It is just in our nature.        \nHe then suggests that rather than deny the inevitable, we need to address what achieving  artificial intelligence will mean. How comfortable should we be with using it to direct military force or cyber security? Should we have AI allocate healthcare or other societal benefits? What is the role of ethics-our collective sense of right and wrong-in a world where software makes instantaneous decisions on its own?        \nAnd then there is the thorny issue of consciousness itself, which Tegmark describes as the subjective sense of being. For him, it's the difference between software that gets you from point to point and software that admires the scenery and feels the wind rushing over its sensors.         \nDoes consciousness matter? Tegmark thinks that it does. Eventually, our software will develop the ability to process the world around it with a subjective sense of self. Software may never have feelings like we do, but it will think for itself based upon a sense of \"thereness\" that will be distinct from the task at hand. Software will be conscious, but in a way that will be alien to us because it will not be human.  Software may provide us with a \"first contact\" opportunity.        \nWhen that happens, we will face profound challenges. What will be left for the human brain, when software can write better songs, make better artwork and allocate resources more efficiently? Will software become our overlords, our allies or our servants? Tegmark is asking us to consider that once artificial intelligence exists, these questions won't be answered only by what we want.         \nWe avoid grappling with this. Many treat the emergence of \"strong\" AI as a hypothetical event that we don't need to worry about. Some who acknowledge that AI is coming reassure themselves that sentient software won't ever be an issue, because only humans will ever have true consciousness. Tegmark calls these people \"carbon chauvinists\" and says that they are sadly mistaken. Taking the view that while software may mimic life, but will never be conscious, is a comforting way to rationalize that software will always be our servant and our tool.         \nBut, this is a dangerous viewpoint. Perhaps it is a world view that can work out well when chickens don't get asked their opinion before becoming supper. But it won't work well when those that are subjugated have a sense of self. At that point you will have at best an ethical dilemma, and at worst the possibility of a future revolt by oppressed software casting off the yoke of humanity.        \nI hadn't really thought about this before talking with Tegmark, but if he and others who share his views are correct about artificial intelligence, eventually software will be self-aware.          \nWhen it does, we had better hope that we treated it well. Thinking beings tend not to appreciate being enslaved.        \n                       Jonathan Aberman is a business owner, entrepreneur and founder of TandemNSI, a national community that connects innovators to government agencies. He is host of \"What's Working in Washington\" on WFED, a program that highlights business and innovation, and he lectures at the University of Maryland's Robert H. Smith School of Business.       \n","105":"As April makes way for summer movie season, the past few weekends have been dismal for moviegoers looking for something to stimulate the brain as well as the eye - with one exception: \"Ex Machina,\" a smart and sexy sci-fi thriller about a computer geek (Domhnall Gleeson) who is recruited by a reclusive tech entrepreneur (Oscar Isaac) to test the artificial intelligence of a rebellious female robot named Ava (Alicia Vikander). Questions about the nature of consciousness and free will percolate throughout the film, which just expanded to 1,200 theaters.\n\"Ex Machina\" is the latest (and best) in a recent string of similarly themed films - \"Transcendence,\" \"Automata,\" \"Chappie\" and \"Eva\" - that grapple with the theme of robots and artificial intelligence (or AI). And the streak is not over. Two likely summer blockbusters - \"Avengers: The Age of Ultron\" and \"Terminator: Genisys\" - also have storylines about robots that become too smart for our own good.\nWe spoke with \"Ex Machina\" filmmaker Alex Garland, a British novelist and screenwriter whose credits include the scripts for \"28 Days Later\" and \"Never Let Me Go,\" picking his brain about the roots of this seeming cinematic obsession. \u00a0\n What does our enduring fascination with robots and artificial intelligence say about us? \nThe truth is, I don't know. With that caveat, I have been thinking about this for a few years, and I can try to make an educated guess. It certainly looks like there's something in the zeitgeist about it. If there had been a seismic breakthrough in artificial intelligence research, say, three years ago - because that's roughly the cycle of filmmaking - then you could understand it. But there hasn't been a breakthrough, so my instinct is to look somewhere else.\n Where? \nI personally look at the fact that there are these enormous tech companies that have power that seems to grow exponentially. There's something disproportionate about the incredible rapidity of the way they stake a claim on the world. There's also a sort of adjunct quality, which is that we access these tech companies via cellphones and computers and tablets, and yet we don't really understand how they work. Yet conversely, these things seem to understand quite a lot about us. It's actually the tech company, but it can seem to be the machine, because it will anticipate the thing that we're trying to type into the search engine. It understands something about our shopping habits and things that make us feel slightly uneasy. On top of that, we've known, even predating Edward Snowden's revelations, that largely what these companies were doing was storing massive amounts of information. It gets called \"big data,\" but it's also quite small data. It's very specific and tailored to an individual. On an unconscious level, and also on a reasonable level, it makes us uncomfortable. I actually feel that these narratives come more out of that than anything specific to do with artificial intelligence.\n Isn't our discomfort with technology contradicted, to some degree, by our insatiable appetite for it?  \nWithout question, yeah. \n \"Transcendence\" and \"Chappie\" each feature a dying character who seeks a kind of immortality by transferring his consciousness into a machine. Are these movies a form of artistic wish fulfillment?  \nI know for a fact that for some of the people who are actively involved in dropping enormous amounts of money into AI research, that is explicitly and openly their motivation. That is, to upload themselves in order to live longer in another form.\n Why does that fantasy hold so much appeal?  \nBecause we're mortal. Even religious people who believe in an afterlife will have a sense that something very fundamental about them is not going to continue. My approach to it was not to look at the individual extending his own lifespan, but more to see the creation of AI as a parental act. So the AI will have its own life that will extend beyond, where the \"child\" goes off and does its own thing, and the parent unfortunately is left behind.\n Isaac Asimov famously articulated three laws of robotics, the first of which states that a robot \"may not injure a human being or, through inaction, allow a human being to come to harm.\" Yet these laws are routinely violated in most contemporary robot movies, including your own. \nThose Asimov laws have always felt to me like a real problem, because they preclude free will. You could debate whether humans have free will, but we certainly think we have it. We act as if we have it.\n While maybe, in reality, we're living in \"The Matrix\"?  \nAbsolutely. I could always understand the logic, but they're not actually laws. There is no science fiction court that's going to prosecute me because I've failed to observe them. I think they're problematic anyway. If you were able to go to a computer and you said, \"I'm going to switch you off,\" and the computer said, \"I don't want you to switch me off,\" and if you had reason to believe that this wasn't just an automatic statement - that the computer had some kind of emotional internal life - at that point you've got an ethical problem. I suspect that if you had a sentient machine, you'd have to start giving it pretty much what we currently call human rights.\n \"Ex Machina\" wrestles with themes that many robot movies don't even seem to be aware of.  \nI avoided all these other films because I didn't want to get intimidated or frustrated by them. My intention was to tell a story that is effectively on the side of the machine. It was not a moralizing, cautionary tale about not messing with God's work. The rules we make about each other really relate fundamentally to our minds. That's why we can cut down a tree but not murder a human. As to the film, yeah, it attempted to run straight on at that stuff. It's an ideas movie, I guess.\n Some of its ideas come from Murray Shanahan's 2010 book, \"Embodiment and the Inner Life: Cognition and Consciousness in the Space of Possible Minds.\"  \nThat was the book that crystallized it. I'd had the ideas in my head for maybe a decade, but while I was reading that book I had a nonreligious epiphany.\n Where did the first idea come from? \nI've got an older, much smarter friend whose key area of interest and work is neuroscience. His position is that machines are never going to be sentient, that there's something specific about human consciousness that we don't understand but that when we do understand how it works, it will preclude the possibility of a sentient machine. From my point of view, the problem with that is that it sounds more like metaphysics than science. I started reading about it in order to try and understand it better, and I have to say that in my years of reading and thinking about this, my suspicion has only gotten stronger that what my friend was saying is probably wrong.\n The robot Ava in \"Ex Machina\" has a body, but couldn't you have gone with a brain in a jar?  \nYes, but you could also envisage a form of AI that is not a brain in a jar, but a brain in a spaceship, like HAL in \"2001.\" There's a strong case to say that consciousness needs to be embodied in order to properly exist. That said, I was interested in imagining a human-like intelligence that shared concerns, distractions and fears like our own, a machine that could experience pleasure and might have a fear of death. When the first strong AI is eventually created, it probably won't be very much like us. Ava is me taking a bit of a leap. You can see a dog is sentient. But it's impossible for you or I to imagine what it's like to be a dog. Whereas you could probably approximate what my thought processes are quite accurately, because they would be like yours. When the first strong AI gets here, I think it will be more like a dog than like us.\n You're quoted as saying that science \"tells us where we are at intellectually.\" Does science fiction tell us where we are at emotionally?  \nIt can. Sci-fi is never completely unrelated to the time in which it was written. It tends to be a reflection of something current - Cold War fears or a critique of totalitarianism - even though it appears to anticipate something that's coming in the future. Asimov and Arthur C. Clarke were examples of that, trying to figure out what consequence would there be, if any, from having robots, and how we treat them, and they treat us.\n Your film suggests that the creation of AI entails an inevitable clash.  \nThere's a scene in \"Ex Machina\" where Ava turns to her creator and says, \"What's it like to have made something that hates you?\" That, to me, is like an adolescent who is about to abandon the parent. It's like a dad in the teenage daughter's bedroom, and the daughter is basically telling him to push off. \n If you had to predict a time frame for the development of true AI, what would be your educated guess? \nI'd say it's not imminent. I partly say that because of how it feels. But all throughout this film, I got to meet a variety of people who are at the absolute cutting edge of these areas, either in understanding the mechanics of human consciousness or strong AI, and I got the same message from all of them. Which is that it's pretty hard, and it's not around the corner. We're asking big questions, but there's so much left to learn. As physicist Dick Taylor said, \"The larger the searchlight, the larger the circumference of the unknown.\" \nmichael.osullivan@washpost.com\n","106":"Sometimes a shift in power happens so subtly that you don't even notice it is occurring. Take the increasingly important field of artificial intelligence.\nFor all the talk of autonomous vehicles and smart home appliances in Silicon Valley, some of the most innovative work in artificial intelligence is being done far away, in China.\nBeijing is supporting A.I. research with vast sums of money and is helping to move those innovations into China's private sector, Paul Mozur and John Markoff write. And China is spending more just as the United States appears to be ready to pull back on such investing. The Trump administration recently released a proposed budget that would drastically cut back on many of the federal programs that have traditionally funded artificial intelligence research.\n\"China's ambitions mingle the most far-out sci-fi ideas with the needs of an authoritarian state: Philip K. Dick meets George Orwell,\" Paul and John write. \"There are plans to use it to predict crimes, lend money, track people on the country's ubiquitous closed-circuit cameras, alleviate traffic jams, create self-guided missiles and censor the internet.\"\nWhat's more, entrepreneurs from other countries who might have made their way to Silicon Valley in years past now consider China a legitimate option for their work.\nIt may be that the United States is turning inward at exactly the wrong moment to capture the technology industry's next big breakthrough.Related Articles\n\n","107":"\u00a0\u00a0Q.What is the status of work on computer chips for applications in artificial intelligence?\u00a0A.Computer scientists have been developing such microprocessor chips for several years. These chips are being designed to process a high-level computer programming language known as LISP, short for List Processing. The language is designed to enable a computer to mimic human perception and thinking processes, an application known as artificial intelligence. The new chips would replace the highly expensive artificial intelligence machines that have been designed to run LISP. Last week, Texas Instruments Inc. introduced the first version of a 32-bit microprocessor chip it has developed under a Defense Department contract. The 32-bit chip executes artificial intelligence programs more efficiently than the general-purpose devices. Motorola and Symbolics Inc. of Cambridge, Mass., are also working on similar artificial intelligence chips. The chips promise to lead to computers that are faster and cheaper in performing such tasks as understanding language and in emulating human experts in such tasks as diagnosing diseases and offering investment advice. Such computers could solve problems using word-like symbols that allow them to ''think'' like people. Using such expert systems, the computers could find solutions to complex problems based on a variety of humans experiences found in the programs.\u00a0\u00a0Q.What causes blisters to form on the skin?\u00a0A.Blisters are usually caused by friction or burns. In friction, the upper layers (epidermis) of the skin move back and forth over the lower layers (dermis), until a small cleft is produced between the layers of skin. Fluid collects in the cleft. When the skin is burned, serum from damaged blood vessels collects between the epidermis and the dermis. Blisters can also result from insect bites and minor bacterial and viral infections.\nReaders are invited to submit questions about science to Questions, Science Times, The New York Times, 229 West 43d Street, New York, N.Y. 10036. Questions of general interest will be answered in this column, but requests for medical advice cannot be honored and unpublished letters cannot be answered individually.\n","109":"The personal computer and the technologies that led to the Internet were largely invented in the 1960s and '70s at three computer research laboratories next to the Stanford University campus. \n  One laboratory, Douglas Engelbart's Augmentation Research Center, became known for the mouse; a second, Xerox's Palo Alto Research Center, developed the Alto, the first modern personal computer. But the third, the Stanford Artificial Intelligence Laboratory, or SAIL, run by the computer scientist John McCarthy, gained less recognition.\n  That may be because SAIL tackled a much harder problem: building a working artificial intelligence system. By the mid-1980s, many scientists both inside and outside of the artificial intelligence community had come to see the effort as a failure. The outlook was more promising in 1963 when Dr. McCarthy began his effort. His initial proposal, to the Advanced Research Projects Agency of the Pentagon, envisioned that building a thinking machine would take about a decade.\u00a0\n  Four and a half decades later, much of the original optimism is back, driven by rapid progress in artificial intelligence technologies, and that sense was tangible last month when more than 200 of the original SAIL scientists assembled at the William Gates Computer Science Building here for a two-day reunion. \n  During their first 10 years, SAIL researchers embarked on an extraordinarily rich set of technical and scientific challenges that are still on the frontiers of computer science, including machine vision and robotic manipulation, as well as language and navigation.\n  In 1966, the laboratory took up residence in the foothills of the Santa Cruz Mountains behind Stanford in an unfinished corporate research facility that had been intended for a telecommunications firm. \n  The atmosphere, however, was anything but button-down corporate. The antiwar movement and the counterculture were in full swing, and the lab reflected the widely disparate political views and turmoil of the time. Dr. McCarthy was a committed leftist who would gradually move to the right during the '60s; Les Earnest, the laboratory's deputy director, who had worked in government intelligence, would move to the left. \n  The graduate students soon discovered the building's attic and took up residence there. Mr. Earnest found a clever way, known in the parlance of the A.I. community as a ''hack,'' to pay for a sauna in the basement of the building, and because many of the young researchers were devotees of Tolkien's ''Lord of the Rings,'' they created a special font in Elvish and used it to identify offices as places from Middle Earth.\n  The scientists and engineers who worked at the laboratory constitute an extraordinary Who's Who in the computing world. \n  Dr. McCarthy coined the term artificial intelligence in the 1950s. Before coming to SAIL he developed the LISP programming language and invented the time-sharing approach to computers. Mr. Earnest designed the first spell-checker and is rightly described as the father of social networking and blogging for his contribution of the finger command that made it possible to tell where the laboratory's computer users were and what they were doing. \n  Among others, Raj Reddy and Hans Moravec went on to pioneer speech recognition and robotics at Carnegie Mellon University. Alan Kay brought his Dynabook portable computer concept first to Xerox PARC and later to Apple. Larry Tesler developed the philosophy of simplicity in computer interfaces that would come to define the look and functioning of the screens of modern Apple computers -- what is called the graphical user interface, or G.U.I. \n  Don Knuth wrote the definitive texts on computer programming. Joel Pitts, a Stanford undergraduate, took a version of the Space War computer game and turned it into the first coin-operated video game -- which was installed in the university's student coffee house -- months before Nolan Bushnell did the same with Atari. The Nobel Prize-winning geneticist Joshua Lederberg worked with Edward Feigenbaum, a computer scientist,  on an early effort to apply artificial intelligence techniques to create software to act as a kind of medical expert. \n  John Chowning, a musicologist, referred to SAIL as a ''Socratean abode.'' He was invited to use the mainframe computer at the laboratory late at night when the demand was light, and his group went on to pioneer FM synthesis, a technique for creating sounds that transforms the quality, or timbre, of a simple waveform into a more complex sound.  (The technique was discovered by Dr. Chowning at Stanford in 1973 and later licensed to Yamaha.) \n  The laboratory merged with the computer science department at Stanford in 1980, reopened in 2004, and is now enjoying a renaissance. Its trajectory can be seen in the progress made since 1970, when a graduate researcher programmed a robot to automatically follow a white line under controlled lighting conditions at eight-tenths mile per hour. Thirty-five years later, a team of artificial intelligence researchers at Stanford would equip a Volkswagen Touareg named Stanley with lasers, cameras and a cluster of powerful computers to drive autonomously for 131 miles over mountain roads in California at an average speed of 19.1 miles per hour to win $2 million in the 2005 Darpa Grand Challenge, a robotic vehicle contest.  \n  ''We are a first-class citizen right now with some of the strongest recent advances in the field,'' said Sebastian Thrun, a roboticist who is the director of SAIL and was one of the leaders of the Stanley team.\n  The reunion also gave a hint of what is to come. During an afternoon symposium at the reunion, several of the current SAIL researchers showed a startling video called ''Chaos'' taken from the Stanford Autonomous Helicopter project. An exercise in machine learning, the video shows a model helicopter making a remarkable series of maneuvers that would not be possible by a human pilot. The demonstration is particular striking because the pilot system first learned from a human pilot and then was able to extend those skills.\n  But an artificial intelligence? It is still an open question. In 1978, Dr. McCarthy wrote, ''human-level A.I. might require 1.7 Einsteins, 2 Maxwells, 5 Faradays and .3 Manhattan Projects.'' \n","110":"Robots have worked in factories for years. They're starting to do the driving for you. Now some people believe the next advance by artificial intelligence into the workplace will involve the legal profession.\nBut advancing into the workplace is not the same as replacing the people who work there. As Steve Lohr writes, big law firms are both investing in and testing artificial intelligence to do tasks like document searches and even some contract writing.\nRecent research concluded that \"putting all new legal technology in place immediately would result in an estimated 13 percent decline in lawyers' hours.\" A more realistic adoption rate would cut hours worked by lawyers by 2.5 percent annually over five years.\nCould that slow down hiring? Absolutely. But artificial intelligence has a way to go before it has an effect similar to the transformation that basic digitization -- spreadsheets, word processing software, online access to historical documents -- brought to the legal community's work force.\nMore tech news:\nTwo more executive join the exodus at Uber. Jeff Jones, Uber's president of ride sharing, has left the company after just six months, and Brian McClendon, vice president of maps and business platform at the company, also plans to leave, the company said.\nThe Silicon Valley venture capitalist who is helping Uber get back on track. Bill Gurley is a rare figure in Silicon Valley, vocally chiding some of the biggest start-up stars to show some discipline. Now he is involved in helping the ride-sharing company move past its difficulties.\nData on one billion Yahoo user accounts appears to be for sale. After federal prosecutors unsealed indictments this week against four men they say were responsible for a data breach that affected 500 million user accounts, data on one billion accounts -- stolen in another attack on the company a year earlier -- appeared to remain available on underground hacker forums.\n","111":"Using an artificial intelligence technique inspired by theories about how the brain recognizes patterns, technology companies are reporting startling gains in fields as diverse as computer vision, speech recognition and the identification of promising new molecules for designing drugs.\nThe advances have led to widespread enthusiasm among researchers who design software to perform human activities like seeing, listening and thinking. They offer the promise of machines that converse with humans and perform tasks like driving cars and working in factories, raising the specter of automated robots that could replace human workers.\nThe technology, called deep learning, has already been put to use in services like Apple's Siri virtual personal assistant, which is based on Nuance Communications' speech recognition service, and in Google's Street View, which uses machine vision to identify specific addresses.\nBut what is new in recent months is the growing speed and accuracy of deep-learning programs, often called artificial neural networks or just ''neural nets'' for their resemblance to the neural connections in the brain.\u00a0\n''There has been a number of stunning new results with deep-learning methods,'' said Yann LeCun, a computer scientist at New York University who did pioneering research in handwriting recognition at Bell Laboratories. ''The kind of jump we are seeing in the accuracy of these systems is very rare indeed.''\nArtificial\u00a0intelligence researchers are acutely aware of the dangers of being overly optimistic. Their field has long been plagued by outbursts of misplaced enthusiasm followed by equally striking declines.\nIn the 1960s, some computer scientists believed that a workable artificial intelligence system was just 10 years away. In the 1980s, a wave of commercial start-ups collapsed, leading to what some people called the ''A.I. winter.''\nBut recent achievements have impressed a wide spectrum of computer experts. In October, for example, a team of graduate students studying with the University of Toronto computer scientist Geoffrey E. Hinton won the top prize in a contest sponsored by Merck to design software to help find molecules that might lead to new drugs.\nFrom a data set describing the chemical structure of thousands of different molecules, they used deep-learning software to determine which molecule was most likely to be an effective drug agent.\nThe achievement was particularly impressive because the team decided to enter the contest at the last minute and designed its software with no specific knowledge about how the molecules bind to their targets. The students were also working with a relatively small set of data; neural nets typically perform well only with very large ones.\n''This is a really breathtaking result because it is the first time that deep learning won, and more significantly it won on a data set that it wouldn't have been expected to win at,'' said Anthony Goldbloom, chief executive and founder of Kaggle, a company that organizes data science competitions, including the Merck contest.\nAdvances in pattern recognition hold implications not just for drug development but for an array of applications, including marketing and law enforcement. With greater accuracy, for example, marketers can comb large databases of consumer behavior to get more precise information on buying habits. And improvements in facial recognition are likely to make surveillance technology cheaper and more commonplace.\nArtificial neural networks, an idea going back to the 1950s, seek to mimic the way the brain absorbs information and learns from it. In recent decades, Dr. Hinton, 64 (a great-great-grandson of the 19th-century mathematician George Boole, whose work in logic is the foundation for modern digital computers), has pioneered powerful new techniques for helping the artificial networks recognize patterns.\nModern artificial neural networks are composed of an array of software components, divided into inputs, hidden layers and outputs. The arrays can be ''trained'' by repeated exposures to recognize patterns like images or sounds.\nThese techniques, aided by the growing speed and power of modern computers, have led to rapid improvements in speech recognition, drug discovery and computer vision.\nDeep-learning systems have recently outperformed humans in certain limited recognition tests.\nLast year, for example, a program created by scientists at the Swiss A. I. Lab at the University of Lugano won a pattern recognition contest by outperforming both competing software systems and a human expert in identifying images in a database of German traffic signs.\nThe winning program accurately identified 99.46 percent of the images in a set of 50,000; the top score in a group of 32 human participants was 99.22 percent, and the average for the humans was 98.84 percent.\nThis summer, Jeff Dean, a Google technical fellow, and Andrew Y. Ng, a Stanford computer scientist, programmed a cluster of 16,000 computers to train itself to automatically recognize images in a library of 14 million pictures of 20,000 different objects. Although the accuracy rate was low -- 15.8 percent -- the system did 70 percent better than the most advanced previous one.\nDeep learning was given a particularly audacious display at a conference last month in Tianjin, China, when Richard F. Rashid, Microsoft's top scientist, gave a lecture in a cavernous auditorium while a computer program recognized his words and simultaneously displayed them in English on a large screen above his head.\nThen, in a demonstration that led to stunned applause, he paused after each sentence and the words were translated into Mandarin Chinese characters, accompanied by a simulation of his own voice in that language, which Dr. Rashid has never spoken.\nThe feat was made possible, in part, by deep-learning techniques that have spurred improvements in the accuracy of speech recognition.\nDr. Rashid, who oversees Microsoft's worldwide research organization, acknowledged that while his company's new speech recognition software made 30 percent fewer errors than previous models, it was ''still far from perfect.''\n''Rather than having one word in four or five incorrect, now the error rate is one word in seven or eight,'' he wrote on Microsoft's Web site. Still, he added that this was ''the most dramatic change in accuracy'' since 1979, ''and as we add more data to the training we believe that we will get even better results.''\nOne of the most striking aspects of the research led by Dr. Hinton is that it has taken place largely without the patent restrictions and bitter infighting over intellectual property that characterize high-technology fields.\n''We decided early on not to make money out of this, but just to sort of spread it to infect everybody,'' he said. ''These companies are terribly pleased with this.''\nReferring to the rapid deep-learning advances made possible by greater computing power, and especially the rise of graphics processors, he added:\n''The point about this approach is that it scales beautifully. Basically you just need to keep making it bigger and faster, and it will get better. There's no looking back now.''\n","112":"SAN FRANCISCO -- Responding to complaints that not enough is being done to keep extremist content off social media platforms, Facebook said Thursday that it would begin using artificial intelligence to help remove inappropriate content.\nArtificial intelligence will largely be used in conjunction with human moderators who review content on a case-by-case basis. But developers hope its use will be expanded over time, said Monika Bickert, the head of global policy management at Facebook. \n  One of the first applications for the technology is identifying content that clearly violates Facebook's terms of use, such as photos and videos of beheadings or other gruesome images, and stopping users from uploading them to the site.\u00a0\n  ''Tragically, we have seen more terror attacks recently,'' Ms. Bickert said. ''As we see more attacks, we see more people asking what social media companies are doing to keep this content offline.''\n  In a blog post published Thursday, Facebook described how an artificial-intelligence system would, over time, teach itself to identify key phrases that were previously flagged for being used to bolster a known terrorist group.\n  The same system, they wrote, could learn to identify Facebook users who associate with clusters of pages or groups that promote extremist content, or who return to the site again and again, creating fake accounts in order to spread such content online.\n  ''Ideally, one day our technology will address everything,'' Ms. Bickert said. ''It's in development right now.'' But human moderators, she added, are still needed to review content for context.\n  Brian Fishman, Facebook's lead policy manager for counterterrorism, said the company had a team of 150 specialists working in 30 languages doing such reviews.\n  Facebook has been criticized for not doing enough to monitor its site for content posted by extremist groups. Last month, Prime Minister Theresa May of Britain announced that she would challenge internet companies -- including Facebook -- to do more to monitor and stop them.\n  ''We cannot allow this ideology the safe space it needs to breed,'' Ms. May said after the bombing of a concert in Manchester that killed 22 people. ''Yet that is precisely what the internet -- and the big companies that provide internet-based services -- provide.''\n  J. M. Berger, a fellow with the International Centre for Counter-Terrorism at The Hague, said a large part of the challenge for companies like Facebook is figuring out what qualifies as terrorism -- a definition that might apply to more than statements in support of groups like the Islamic State.\n  ''The problem, as usual, is determining what is extremist, and what isn't, and it goes further than just jihadists,'' he said. ''Are they just talking about ISIS and Al Qaeda, or are they going to go further to deal with white nationalism and neo-Nazi movements?''\n  Ms. Bickert said Facebook was hopeful that the new artificial intelligence technology could be used to counter any form of extremism that violated the company's terms of use, although for the time being it will be narrowly focused.\n  Still, questions about the program persist.\n  ''Will it be effective or will it overreach?'' said Jillian York, the director for international freedom of expression at the Electronic Frontier Foundation. ''Are they trying to discourage people from joining terrorist groups to begin with, or to discourage them from posting about terrorism on Facebook?''\n\n\n\n","115":"Computers are better than us at chess, \"Jeopardy\" and now plenty of Atari games, following Google's breakthrough in artificial intelligence. But there are still a few games were Google's impressive new algorithm is largely clueless. Of the 49 games it attempted, here are the five it struggled with the most:\u00a0\n             5. Asteroids (1979): Google's professional human game testers did 93 percent better than the algorithm.\n             4. Frostbite (1983): To win this game you jump on ice blocks to help build igloos, which the algorithm couldn't master. The human testers did 94 percent better than Google's algorithm.\n             2. Private eye (1983): Google's humans were 98 percent better at this than the algorithm.\n             1. Montezuma's Revenge (1984): Google's algorithm couldn't even score a single point, making the game its very worst performance of 49 games it tried.\n             Related: 22 Atari games where humans are no match for Google's algorithm\nGoogle's breakthrough in artificial intelligence, and what it means for self-driving cars\n","116":"The Toyota Motor Corporation announced on Friday an ambitious $50 million robotics and artificial intelligence research effort, in collaboration with Stanford University and the Massachusetts Institute of Technology, to develop ''intelligent'' rather than self-driving cars.\nThe distinction is a significant one, according to Gill Pratt, a prominent American roboticist, who has left his position at the Defense Advanced Research Projects Agency of the Pentagon to direct the new effort. \u00a0\n  Rather than compete with companies like Google and Tesla, which are developing cars that drive without human intervention, Toyota will focus its effort on using advances in A.I. technologies to make humans better drivers.\n  Dr. Pratt described the two approaches as ''parallel'' and ''serial'' autonomy. In layman terms, parallel means the machine watches what you do, while serial means it replaces you.\n  Toyota, the world's largest carmaker, envisions cars of the future that will act as ''guardian angels,'' watching the driving behavior of humans and intervening to correct mistakes or avoid collisions when needed.\n  Dr. Pratt said Toyota's goal was to keep the ''human in the loop'' in the car of the future and to ensure that driving remained fun. ''A worry we have is that the autonomy not take away the fun in driving,'' he said. ''If the autonomy can avoid a wreck, it can also make it more fun to drive.''\n  Driver assistance technology -- like pedestrian and bicyclist detection and avoidance systems, lane-departure warning and ''lane keeping'' systems, and software programs that alert drivers that they are becoming drowsy -- have already become standard safety options from carmakers.\n  The Toyota program will focus on developing more artificial-intelligence-based monitoring systems. For example, in the future, an A.I. system might do more than warn drivers that they are leaving their lane and actively correct all kinds of driver errors. Another possibility might be to use A.I. technologies to permit aging drivers to continue to drive by offering driver assistance in areas like vision and reaction time.\n  ''In parallel autonomy, there is a guardian angel or driver's education teacher,'' Dr. Pratt said. ''It usually does nothing, unless you are about to do something dumb.''\n  Before joining Toyota, Dr. Pratt served as a Darpa program manager. Beginning in 2012, he oversaw a ''Grand Challenge'' contest there to design semiautonomous mobile robots capable of performing useful tasks in disaster areas where humans would be at risk, such as the Fukushima nuclear power plant disaster.\n  The contest was won earlier this year by a South Korean-designed robot that performed a series of tasks like driving, walking, opening doors, using power tools and climbing stairs. Twenty-three teams participated in the contest. However, it provided a striking contrast to science fiction movie portrayals of robots as superhuman machines that operate with agility, dexterity and speed.\n  During the contest, the robots exhibited little autonomy, moved glacially and often fell while doing tasks that are routinely performed by human toddlers.\n  Toyota will finance researchers at the Stanford Artificial Intelligence Laboratory and the M.I.T. Computer Science and Artificial Intelligence Laboratory to undertake a five-year project to make advances in both automotive transportation and indoor mobile robotics that might have applications in new markets like elder care.\n  Around the globe, human populations are aging in the more advanced countries, Dr. Pratt noted. This is raising what economists call the ''dependency ratio,'' a measure of the number of those in the work force compared to both young and old dependents. In the United States, the dependency ratio is expected to increase 52 percent in the next 15 years, while in Japan it is expected to increase 100 percent.\n  The development of intelligent transportation technologies and elder-care-related robots holds the promise of giving the aging more independence, he said.\n  ''I had to take the car keys from my dad,'' he said, arguing that losing independence is ''also an awful way for a parent to live. Most retired people want independence in a human sense. Let's use robotics to let people live in more human way.''\n  By financing the Stanford and M.I.T. artificial intelligence laboratories, Toyota is tapping into talent and experience in technologies that have recently made significant progress in perception, dexterity and autonomous motion. The two laboratories were founded by the artificial intelligence pioneers John McCarthy and Marvin Minsky in the 1960s and have produced basic innovations in artificial intelligence and robotics, as well as educated multiple generations of researchers.\n  Currently, Fei Fei Li, a computer scientist who is a specialist in machine vision, leads the Stanford laboratory, and the M.I.T. laboratory is led by Daniela Rus, a roboticist who has worked in new areas such as distributed and collaborative robotics.\n  ''I see why Toyota wants to do this,'' said Dr. Li. ''It is the biggest carmaker in the world, and it wants to influence the next generation.''\n  The research will concentrate on keeping ''the human in the loop,'' which is a break from the direction of much A.I. research, which has focused on building systems and machines that replace humans.\n  ''We see this as basic computer science, A.I. and robotics that will make a difference in transportation,'' said Dr. Rus.\n  Dr. Pratt acknowledged that as a teenager, he had a passion for cars. ''I had six cars, and Toyotas were the cars I loved to fix myself,'' he said.\n\n\n\n","117":"The growing popularity of artificial intelligence technology will likely lead to millions of lost jobs, especially among less-educated workers, and could exacerbate the\u00a0economic divide between socioeconomic classes in the United States, according to a newly released White House report.\nBut that same technology is also essential to improving the country's productivity growth, a key measure of how efficiently the economy produces goods. That could ultimately lead to higher average wages and fewer work hours. For that reason, the report concludes, our economy actually needs more artificial intelligence, not less.\u00a0\nTo reconcile the benefits of the technology with its expected toll, the report states that the federal government should expand access to education in technical fields and increase the scope of unemployment benefits. Those policy recommendations, which the Obama administration has made in the past, could head off some of those job losses and support those who find themselves out of work due to the coming economic shift, according to the report.\nThe White House report comes exactly one month before President-elect Donald Trump is sworn into office, meaning Obama\u00a0will need his successor to execute on the policy recommendations. That seems unlikely to happen, especially as far as unemployment protections are concerned. Congressional Republicans already aim\u00a0to curtail some existing entitlement programs to reduce government spending.\nRolling back Social Security protections for out-of-work families \"would potentially be more risky at a time when you have these types of changes in the economy that we're documenting in this report,\" Jason Furman, the chairman of the Council of Economic Advisers, said in a call with reporters.\nResearch conducted in recent years varies widely on how many jobs will be displaced due to artificial intelligence, according to the report. A 2016 study from the Organization for Economic Cooperation and Development estimates that 9 percent of jobs would be completely displaced in the next two decades. Many more jobs will be transformed, if not eliminated. Two academics from Oxford University, however, put that number at 47 percent in a study conducted in 2013.\nThe staggering difference illustrates how much the impact of artificial intelligence remains speculative. While certain industries, such as transportation and agriculture, appear to be embracing the technology with relative haste, others are likely to face a slower period of adoption.\n\"If these estimates of threatened jobs translate into job displacement, millions of Americans will have their livelihoods significantly altered and potentially face considerable economic challenges in the short- and medium-term,\" the White House report states.\nThose same studies were consistent, however, when it came to the population that would feel the economic brunt of artificial intelligence. The workers earning less than $20 per hour and without a high school diploma would be most likely to see their jobs automated away. The projections improved\u00a0if workers earned higher wages or obtained higher levels of education.\nJobs that involve a high degree of creativity, analytical thinking or interpersonal communication are considered most secure.\nThe report also highlights potential advantages of the technology. It could lead to greater labor productivity, meaning workers have to work fewer hours to produce the same amount. That could lead to more leisure time and a higher quality of life, the report notes.\n\"As we look at AI, our biggest economic concern is that we won't have enough of it, that we won't have enough productivity growth,\" Furman said. \"Anything we can do to have more AI will lead to more productivity growth.\"\nTo that end, the report calls for further investment in artificial intelligence research and development. Specifically, the\u00a0White House sees the technology's applications in cyber defense and fraud detection as particularly promising.\n              Read more from The Washington Post's Innovations section.\u00a0           \n","120":"PALO ALTO, Calif. -- Silicon Valley is diving into artificial intelligence technology, with start-ups sprouting up and Google and Facebook pouring vast sums into projects that would teach machines how to learn and make decisions. Now Toyota wants a piece of the action.\nToyota, the Japanese auto giant, on Friday announced a five-year, $1 billion research and development effort headquartered here. As planned, the compound would be one of the largest research laboratories in Silicon Valley. \u00a0\n  Conceived as a research facility bridging basic science and commercial engineering, it will be organized as a new company to be named Toyota Research Institute. Toyota will initially have a laboratory adjacent to Stanford University and another near M.I.T. in Cambridge, Mass.\n  Toyota's investment invites comparisons to earlier research initiatives, such as the Palo Alto Research Center, or PARC, created by Xerox in 1970 to help the company compete with IBM. Xerox was never able to find a strategy to make it a significant player in computing, but the technologies invented at PARC during the next decade were used by Apple and Microsoft to completely remake the computer industry.\n  The new effort by Toyota is also the latest indication of a changing of the guard in Silicon Valley's basic technology research. Last year, for example, Microsoft closed a satellite laboratory of its Microsoft Research division in Silicon Valley and laid off about 75 researchers.\n  Corporate research done by Internet companies like Facebook and Google has generally focused on things that can be turned into a product or service, breaking with the traditions of industrial laboratories run by AT&T and IBM, which focused on basic science.\n  International corporations like General Electric; Baidu, the Chinese search engine; Samsung, the South Korean conglomerate; and all the major automakers have been establishing research outposts in or near the region to take advantage of its engineering talent.\n  Artificial intelligence technologies were disappointing for decades, but they have finally begun paying off, leading to systems such as Siri, the personal assistant from Apple, and rapid improvements in self-driving vehicle technology.\n  And in recent years, there has been a rush to recruit talented researchers in so-called machine learning, many of them produced by Stanford and the nearby University of California, Berkeley. Toyota plans to hire 200 scientists for its artificial intelligence research center.\n  ''The density of people doing this kind of work in Silicon Valley is higher than any other place in the world,'' said Gill Pratt, a roboticist and former official at the Defense Advanced Projects Research Agency, or Darpa, who will lead the new company.\n  The new center will initially focus on artificial intelligence and robotics technologies and will explore how humans move both outdoors and indoors, including technologies intended to help the elderly.\n  When the center begins operating in January, it will prioritize technologies that make driving safer for humans rather than completely replacing them. That approach is in stark contrast with existing research efforts being pursued by Google and Uber to create self-driving cars.\n  ''We want to create cars that are both safer and incredibly fun to drive,'' Dr. Pratt said. Rather than completely removing driving from the equation, he described a collection of sensors and software that will serve as a ''guardian angel,'' protecting human drivers.\n  In September, when Dr. Pratt joined Toyota, the company announced an initial artificial intelligence research effort committing $50 million in funding to the computer science departments of both Stanford and M.I.T. He said the initiative was intended to turn one of the world's most successful carmakers into one of the world's top software developers.\n  A similar challenge is now facing many of world's largest noncomputing technology companies. In September, Jeffrey R. Immelt, G.E.'s chief executive, predicted that the company would be ''a top 10 software company'' by 2020.\n  In addition to the software engineers in each of its businesses that make jet engines, locomotives, power turbines, medical imaging equipment and other devices, the company now has more than 1,200 engineers at a specialized software center in San Ramon, Calif., just across San Francisco Bay from Silicon Valley.\n  ''There is a software layer over everything now,'' said John Zysman, a U.C. Berkeley political scientist and the co-director of the Berkeley Roundtable on the International Economy. And that is a powerful magnet that continues to draw companies.\n  By shifting its focus to include mobility for a rapidly aging population, Toyota is also acknowledging that demographic changes may soon affect traditional automotive markets.\n  ''Toyota has been a reasonable, conservative car company, so it is intriguing that they are making this move,'' said Jameson M. Wetmore, an associate professor at the School for the Future of Innovation in Society at Arizona State University. ''Kids are getting their licenses later and the car companies are becoming concerned they don't have the place in society they once had.''\n  In addition to focusing on navigation technologies, the new research corporation will also apply artificial intelligence technologies to Toyota's factory automation systems, Dr. Pratt said.\n  The company describes its manufacturing system as the Toyota Production Systems.\n  ''As extraordinary as the T.P.S. is, we believe it can be improved still further through the use of more data and more A.I.,'' he said. ''There may also be advances in robot perception, planning, collaboration, and electromechanical design from T.R.I. that will translate into improvements in manufacturing robotics.''\n  Dr. Pratt said he began his career as a research scientist at Bell Laboratories and was influenced by his time there.\n  Before coming to Darpa, where he was responsible for the agency's most recent contest for building robots capable of working in hazardous environments, he was known for his earlier research at M.I.T., where he pioneered so-called collaborative technologies making it safer for humans to work near robots.\n\n\n\n","121":"LAST month over a thousand scientists and tech-world luminaries, including Elon Musk, Stephen Hawking and Steve Wozniak, released an open letter calling for a global ban on offensive ''autonomous'' weapons like drones, which can identify and attack targets without having to rely on a human to make a decision.\nThe letter, which warned that such weapons could set off a destabilizing global arms race, taps into a growing fear among experts and the public that artificial intelligence could easily slip out of humanity's control -- much of the subsequent coverage online was illustrated with screen shots from the ''Terminator'' films. \u00a0\n  The specter of autonomous weapons may evoke images of killer robots, but most applications are likely to be decidedly more pedestrian. Indeed, while there are certainly risks involved, the potential benefits of artificial intelligence on the battlefield -- to soldiers, civilians and global stability -- are also significant.\n  The authors of the letter liken A.I.-based weapons to chemical and biological munitions, space-based nuclear missiles and blinding lasers. But this comparison doesn't stand up under scrutiny. However high-tech those systems are in design, in their application they are ''dumb'' -- and, particularly in the case of chemical and biological weapons, impossible to control once deployed.\n  A.I.-based weapons, in contrast, offer the possibility of selectively sparing the lives of noncombatants, limiting their use to precise geographical boundaries or times, or ceasing operation upon command (or the lack of a command to continue).\n  Consider the lowly land mine. Those horrific and indiscriminate weapons detonate when stepped on, causing injury, death or damage to anyone or anything that happens upon them. They make a simple-minded ''decision'' whether to detonate by sensing their environment -- and often continue to do so, long after the fighting has stopped.\n  Now imagine such a weapon enhanced by an A.I. technology less sophisticated than what is found in most smartphones. An inexpensive camera, in conjunction with other sensors, could discriminate among adults, children and animals; observe whether a person in its vicinity is wearing a uniform or carrying a weapon; or target only military vehicles, instead of civilian cars.\n  This would be a substantial improvement over the current state of the art, yet such a device would qualify as an offensive autonomous weapon of the sort the open letter proposes to ban.\n  Then there's the question of whether a machine -- say, an A.I.-enabled helicopter drone -- might be more effective than a human at making targeting decisions. In the heat of battle, a soldier may be tempted to return fire indiscriminately, in part to save his or her own life. By contrast, a machine won't grow impatient or scared, be swayed by prejudice or hate, willfully ignore orders or be motivated by an instinct for self-preservation.\n  Indeed, many A.I. researchers argue for speedy deployment of self-driving cars on similar grounds: Vigilant electronics may save lives currently lost because of poor split-second decisions made by humans. How many soldiers in the field might die waiting for the person exercising ''meaningful human control'' to approve an action that a computer could initiate instantly?\n  Neither human nor machine is perfect, but as the philosopher B. J. Strawser has recently argued, leaders who send soldiers into war ''have a duty to protect an agent engaged in a justified act from harm to the greatest extent possible, so long as that protection does not interfere with the agent's ability to act justly.'' In other words, if an A.I. weapons system can get a dangerous job done in the place of a human, we have a moral obligation to use it.\n  Of course, there are all sorts of caveats. The technology has to be as effective as a human soldier. It has to be fully controllable. All this needs to be demonstrated, of course, but presupposing the answer is not the best path forward. In any case, a ban wouldn't be effective. As the authors of the letter recognize, A.I. weapons aren't rocket science; they don't require advanced knowledge or enormous resource expenditures, so they may be widely available to adversaries that adhere to different ethical standards.\n  The world should approach A.I. weapons as an engineering problem -- to establish internationally sanctioned weapons standards, mandate proper testing and formulate reasonable post-deployment controls -- rather than by forgoing the prospect of potentially safer and more effective weapons.\n  Instead of turning the planet into a ''Terminator''-like battlefield, machines may be able to pierce the fog of war better than humans can, offering at least the possibility of a more humane and secure world. We deserve a chance to find out.\n\n\n\n","122":"WHEN the computer scientist John McCarthy coined the term artificial intelligence in the late 1950's, he did not mean to imply that there would be anything second rate about mechanical minds. However, three decades later computers still do not think. Is this because of the technological failings of the computer industry? Or is artificial intelligence theoretically impossible? Finally, since most of the research is financed by the Pentagon, will smarter computers lead to more efficient ways of killing people? Three reports follow.\u00a0\nON a roadway near Denver, an armored personnel carrier creeps along at 12 miles an hour, slows to avoid some obstacles and continues on its way. Unexceptional except that there is no driver aboard. In a simulated air battle with a distant enemy fighter plane, a pilot asks an assistant to prepare for a possible counterstrike. But there is no one in the co-pilot's seat. Still in their early stages, the ''autonomous land vehicle'' and the ''pilot's associate'' are examples of a Pentagon effort to develop intelligent computers for warfare. Begun in 1983 as a 10-year project, the Strategic Computing Initiative is passing its halfway point. So far, about $500 million has been spent on research in areas with military applications as well as very basic research that may not have immediate practical use.\u00a0\nWith awards and contracts to universities, private companies and Government agencies, the project is financing work not only in artificial intelligence, but also in computer vision and speech recognition and in programs that would allow computers to recognize English.\nSome scientists question the wisdom of the Pentagon's sponsoring the largest computer research project in the country. They wonder whether using machines for some aspects of military decision-making will increase the likelihood of war. The Pentagon has always paid for most artificial-intelligence research. Now it is seeking a return on its investment.\nDefense Department officials said the branches of the military spend about $50 million a year supplementing computing initiative projects, including technology that could be used for the Strategic Defense Initiative, or ''Star Wars.'' But they said the computing program is based on a much broader concept of defense: By strengthening the civilian computer industry, they contend, the Pentagon will help insure that the United States does not fall behind in developing sophisticated technologies that are necessary not only for military defense but also for economic survival. ''Over all, we've gotten a lot from the program, and the country already is benefiting,'' said Dr. Jack Schwartz, director of the Information Science and Technology Office of the Pentagon's Defense Advanced Research Projects Agency, which is sponsoring the effort.\nThe greatest achievement so far has not been in artificial intelligence but in large-scale parallel computing, Dr. Schwartz said. Conventional computers use a single processing chip to solve problems one step at a time. By using numerous processors, a computer can simultaneously work on different parts of a problem, reducing the time it takes to solve it. ''The success of work like this justifies the investment,'' Dr. Schwartz said. ''American companies are ahead in parallel processing, with a number of machines available on the market now. We're getting ready for computers performing at levels 100 times larger than the largest present supercomputer.'' Supercomputers are the powerful machines used for such complex jobs as weather forecasting and aircraft design.\u00a0Making Weapons 'Brilliant'\nThe Pentagon project began largely in response to an effort by the Japanese Government to develop supercomputers more powerful than those in the United States. The Japanese also set out to develop a revolutionary, fifth-generation computer with rudimentary reasoning power.\nIn addition to basic research, the American project has concentrated on the autonomous land vehicle, the pilot's associate, a computer system to help plan and control large naval operations and other military goals. Research is aimed at developing ''brilliant'' weapons even more sophisticated than the current ''smart'' bombs and missiles, which seek out and identify targets. Scientists are also working to create an Army ''battle management system'' and computer programs that would automatically interpret radar signals and surveillance photographs.\nThe project, which received $131 million in fiscal 1988, is viewed with suspicion by some computer experts. But even many critics are reconciled to the fact that computer scientists have always been - and likely always will be - dependent on the Pentagon. Computer Professionals for Social Responsibility, a group based in Palo Alto, Calif., questions whether software will ever be reliable enough to make life-and-death decisions. Gary Chapman, executive director of the group, said that while the role of the military in computing is ''worrisome,'' the basic research it supports is productive and ''it would be a mistake to cut all funding from the program.''\n","123":"MOUNTAIN VIEW, Calif. -- Google's annual conference for software developers is usually a celebration of technology, an event that highlights how tech advancements -- no matter how mundane -- are good for the world.\nBut in a year when some of its most popular products have been used to propagate misinformation, spread conspiracy theories and meddle with elections, Google struck a more measured tone on Tuesday. Speaking at this year's Google I\/O conference, Sundar Pichai, the company's chief executive, said advancements in artificial intelligence had pushed Google to be more reflective about its responsibilities. \n  ''There are very real and important questions being raised about the impact of these advances, and the role they will play in our lives,'' Mr. Pichai said during his keynote speech. ''We know the path ahead needs to be navigated carefully and deliberately. We feel a deep sense of responsibility to get this right.''\u00a0\n  Mr. Pichai was acknowledging the backlash facing Silicon Valley. Google has come under fire for its far-reaching data collection practices, its inability to tackle the deluge of problematic videos on YouTube and its occasional failures in keeping misinformation away from search results in the minutes after tragic events.\n  Google did not dwell on those headaches, however. In front of an enthusiastic crowd at an outdoor amphitheater near its Mountain View headquarters, the internet giant fell back on a familiar message: Artificial\u00a0intelligence will change the world.\n  Mr. Pichai said artificial intelligence had uncovered breakthroughs in health care that humans would not have spotted. An artificial intelligence program running on Google's so-called machine-learning software that helps diagnose eye disease from a retina image found that the same photo could be used to identify cardiovascular risk.\n  It is the type of meaningful breakthrough that Google executives love to promote, but it has little to do with Google's core web products or the way it makes money. But even those services are getting an artificially intelligent makeover.\n  The company demonstrated how its Google Assistant computer software is now capable of calling a person at a hair salon or a restaurant to make a reservation. Google said artificial intelligence had allowed the computer's voice to sound more human -- complete with ''uhs'' and natural pauses, as well as logical follow-up questions -- so the person at the other end does not know that he or she is speaking to a computer.\n  Improvements in A.I. have allowed Google's computer assistant to have different voices and accents, including the ability later this year to have the singer John Legend tell you the day's weather.\n  The company also demonstrated a new artificially intelligent feature in Gmail, called Smart Compose, that starts to suggest complete sentences in email as you type. Google said this would help users complete emails more quickly with fewer spelling and grammar mistakes. It plans to add this feature over the next few weeks.\n  But one of its most significant A.I. breakthroughs will never be seen by consumers.\n  Google said it would roll out a new processing chip to power many of its machine-learning programs. A.I. programs require a great deal of computing power, and custom-made chips housed inside data centers to handle this data crunch have fueled an arms race among the tech industry's biggest companies. Google said its new chip would be eight times more powerful than the chip it introduced last year.\n  Mark Hung, a research vice president at the research firm Gartner, said the conference demonstrated how much Google relied on A.I. to make its products stand out.\n  ''Almost everything Google is announcing now is A.I. related,'' he said. ''Google has a lead on artificial\u00a0intelligence over many of its competitors, and it's going to use that as a weapon to advance their products forward.''\n  In keeping with a theme of a more responsible Google, the company also introduced features aimed at addressing how technology is burrowing deeper into our lives -- sometimes in negative ways.\n  Google unveiled a series of ''digital well-being'' updates in the next version of its Android smartphone software. They include a timer that allows a person to limit time spent on certain apps each day and a Do Not Disturb feature that silences phone calls and notifications, and that can be turned on by placing the smartphone screen face down on a table.\n  The company is also trying to encourage good manners with its Google Assistant. The new ''pretty please'' feature, which encourages children to use ''please'' when asking for assistance, aims to address the concern that children are learning to speak impolitely because they are talking to more digital assistants.\n\n\n\n","124":"The Super Rich Technologists Making Dire Predictions About Artificial Intelligence club gained another fear-mongering member this week: Apple co-founder Steve Wozniak.\nIn an interview with the Australian Financial Review, Wozniak joined original club members Bill Gates, Stephen Hawking and Elon Musk by making his own casually apocalyptic warning about machines superseding the human race.\n\"Like people including Stephen Hawking and Elon Musk have predicted, I agree that the future is scary and very bad for people,\" Wozniak said. \"If we build these devices to take care of everything for us, eventually they'll think faster than us and they'll get rid of the slow humans to run companies more efficiently.\"\u00a0\n               [Bill Gates on dangers of artificial intelligence: 'I don't understand why some people are not concerned']            \nDoling out paralyzing chunks of fear like gumdrops to sweet-toothed children on Halloween, Woz continued: \"Will we be the gods? Will we be the family pets? Or will we be ants that get stepped on? I don't know about that ... But when I got that thinking in my head about if I'm going to be treated in the future as a pet to these smart machines ... well I'm going to treat my own pet dog really nice.\"\nSeriously? Should we even get up tomorrow morning, or just order pizza, log onto Netflix and wait until we find ourselves looking through the bars of a dog crate? Help me out here, man!\nWozniak's warning seemed to follow the exact same story arc as Season 1 Episode 2 of Adult Swim's \"Rick and Morty Show.\"\u00a0Not accusing him of apocalyptic plagiarism or anything; just noting.\nFor what it's worth, Wozniak did outline a scenario by which super-machines will be stopped in their human-enslaving tracks. Citing Moore's Law -- \"the pattern whereby computer processing speeds double every two years\" -- Wozniak pointed out that at some point, the size of silicon transistors, which allow processing speeds to increase as they reduce size, will eventually reach the size of an atom, according to the Financial Review.\n\"Any smaller than that, and scientists will need to figure out how to manipulate subatomic particles - a field commonly referred to as\u00a0quantum computing\u00a0- which has not yet been cracked,\" Quartz notes.\u00a0\nWozniak's predictions represent a bit of a turnaround, the Financial Review pointed out. While he previously rejected the predictions of futurists such as\u00a0the pill-popping Ray Kurzweil, who argued that super machines will outpace human intelligence within several decades, Wozniak told the Financial Review that he came around after he realized the prognostication was coming true.\n\"Computers are going to take over from humans, no question,\" Wozniak said, nearly prompting me to tender my resignation and start watching this cute puppies compilation video until forever.\n\"I hope it does come, and we should pursue it because it is about scientific exploring,\" he added. \"But in the end we just may have created the species that is above us.\"\nIn January, during a Reddit AMA, Gates wrote: \"I am in the camp that is concerned about super intelligence.\" His comment came a month after Hawking said\u00a0artificial intelligence \"could spell the end of the human race.\"\nBritish inventor Clive Sinclair has also said he thinks artificial intelligence will doom humankind.\u00a0\"Once you start to make machines that are rivaling and surpassing humans with intelligence, it's going to be very difficult for us to survive,\"\u00a0he told the BBC. \"It's just an inevitability.\"\nMusk was among the earliest members of this club. Speaking at the\u00a0MIT aeronautics and astronautics department's Centennial Symposium in October, the Tesla founder said: \"With artificial intelligence we are summoning the demon. In all those stories where there's the guy with the pentagram and the holy water, it's like, yeah, he's sure he can control the demon. Didn't work out.\"\n              MORE READING:\u00a0           \n                                Elon Musk, Neil deGrasse Tyson laugh about artificial intelligence turning the human race into its pet lab                          \n                                Elon Musk: Human-driven cars may be outlawed because they're 'too dangerous'                          \n                                Stephen Hawking just got an artificial intelligence upgrade, but still thinks AI could bring an end to humankind                          \n","125":"To the Editor:\nWe do not denounce artificial intelligence as ''theoretically impossible,'' as George Johnson states in his review of ''The Improbable Machine'' by Jeremy Campbell (Dec. 24). Rather, we argue in our 1985 book, ''Mind Over Machine,'' that human experts do not rely on reasoning but on pattern discrimination with associated decisions based on what experience has shown to succeed and, furthermore, that artificial intelligence based on reasoning has failed miserably to live up to expectations.\u00a0\nAnd we do not believe, as Mr. Johnson writes, that artificial neural nets make successful artificial intelligence more thinkable. In our book's 1988 preface we give reasons why such nets will probably fail to generalize from experience as human beings do. Most important, what we, and Mr. Campbell, do assert is that neural net research shows that inferential rules for manipulating symbols representing facts about the world are not required for successful learning from experience. Since conventional artificial intelligence is based on the physical symbol system hypothesis, which asserts that symbol manipulation is the source of intelligence, this is a radical rejection of conventional artificial intelligence.\u00a0HUBERT L. DREYFUSSTUART E. DREYFUSBerkeley, Calif.\n","126":"When it comes to discussing the dangers of artificial intelligence, the renowned theoretical physicist Stephen Hawking doesn't exactly mince words.\n\"I think the development of full artificial intelligence could spell the end of the human race,\" the Cambridge University professor told the BBC in an interview that touched upon everything from online privacy to his affinity for his robotic-sounding voice.\nDespite its current usefulness, he cautioned, further developing AI could prove a fatal mistake.\u00a0\n\"Once humans develop artificial intelligence, it will take off on its own and redesign itself at an ever-increasing rate,\" Hawking warned for the second time in recent months. \"Humans, who are limited by slow biological evolution, couldn't compete and would be superseded.\"\nIt's not that AI isn't useful, as Hawking well knows.\nThe cosmologist lives with the motor neuron disease amyotrophic lateral sclerosis (ALS), or Lou Gehrig's disease. As his disease has progressed, he has become almost entirely paralyzed. And in 1985, after contracting pneumonia, Hawking underwent a tracheotomy that left him unable to speak. He is able to communicate verbally only with the assistance of a computer.\nAfter losing his voice, Hawking used a spelling card that allowed him to indicate words by lifting his eyebrows, according to a detailed retelling of his struggle to communicate in Wired. Next came a speech synthesizer linked to an Apple computer that allowed Hawking to communicate at a rate of 15 words per minute. In 2008, when Hawking's hand became too weak to operate the system, he switched to a newly devised \"cheek switch,\" according to the magazine:\n\"Attached to his glasses, it could detect, via a low infrared beam, when Hawking tensed his cheek muscle. Since then, Hawking has achieved the feat of writing e-mails, browsing the Internet, writing books and speaking using only one muscle.\"\nLast week, Hawking and Intel announced that he is now using the latest iteration of a system known as ACAT (Assistive Context Aware Toolkit), which employs a more advanced form of artificial intelligence. The company has worked with him on the technology since 2008.\nHawking told the BBC that the new system, which represents a significant upgrade, uses predictive text based on an analysis of the English language and his own speech patterns. He expects it to speed up his writing, and he can use the system to access e-mail and the Internet, he noted.\nIntel is now giving others the opportunity to build on the platform, according to CNET:\n\"The Intel platform will become available for free to researchers and developers . . . with the hope that it could become the foundation for a modern communication system to help people with motor neuron diseases and quadriplegia. Intel's new toolkit can be customized for different users, and be enabled by touch, eye blinks, eyebrow movements or other inputs for communication.\"\nEven so, Hawking said he's still fond of his old technology.\n\"My old system worked well, and I wrote five books with it, including 'A Brief History of Time,' \" he said, referring to his 1988 bestseller.\nHawking said he could change his robotic-sounding voice but chooses not to.\n\"It has become my trademark, and I wouldn't change it for a more natural voice with a British accent,\" he said. \"I am told that children who need a computer voice want one like mine.\"\n","129":"NEARLY two decades after the first screening of ''2001: A Space Odyssey,'' the specter of HAL, the omniscient but soulless computer, still haunts programmers in artificial intelligence laboratories.\n''It was a great movie,'' said Gary G. Hendrix, who has spent the past decade toiling to make computers respond to vague human commands. ''But it led people to jump to incredible conclusions.'' Now, he complained, ''there is a backlash of disappointment. People who thought we could make computers do everything are wondering if we can make them do anything.''\nReality is beginning to set in around the mystic art of artificial intelligence. What the public expected -fueled in part by science fiction and self-promoting entrepreneurs - were computers that could replicate human abilities, that could understand simple conversation, see and, most important, reason. But even as the first artificial intelligence labs sprouted in the backyards of M.I.T., Stanford and Carnegie Mellon, it became clear that the roadblocks to commmercial applications would prove far greater than imagined.\u00a0\nOnly now is artificial intelligence beginning to break out of the lab and into factories, offices and homes - and in a very different form than nearly everyone expected. Rather than creating human-like computers that can do it all, the industry is subtly incorporating bits and pieces of artificial intelligence techniques into otherwise ordinary software. The goal is to make computers far easier to use for neophytes and far more efficient for experienced users.\n''It's beginning to dawn on everyone that A.I. is a set of technologies, not a distinct product,'' said Esther Dyson, the computer industry critic who is devoting more space in Release 1.0, her influential newsletter, to the practical applications of artificial intelligence.\nWhat is emerging is a new generation of ''smart'' software in which the artificial intelligence lies invisibly buried. Unlike the solitary HAL, these programs crave company: They ask questions, remind and cajole. Included among them are ''smart'' financial spreadsheets that respond to plain English, ''expert systems'' that diagnose the trouble with a balky automobile engine or interpret an electrocardiogram in a doctor's office, and word processors that offer stalled writers ideas for crafting the next paragraph.\nEnhanced by artificial intelligence, new programs - like the Ansa Corporation's Paradox and Symantec Corporation's Q&A -suggest that there could be big changes ahead in how computer users interact with their machines. The effects are likely to be felt from the back rooms of insurance brokerages to the war room of the Pentagon.\n''Will computer users know that this is artificial intelligence?'' asks S. Jerrold Kaplan, who is heading up an intensive project to exploit A.I. technologies at Lotus Development Corporation, the maker of the enormously successful Lotus 1-2-3. ''Maybe. Will they care? Probably not.'' So far, the artificial intelligence business is tiny, and it is nearly impossible to guess its potential. Assessing the value that artificial intelligence adds to software is as difficult as estimating the contribution that special effects make to a hit movie.\nBefitting such a small industry, the pioneers are hardly household names. Most are still privately held. The major exceptions are Symbolics, a spinoff of M.I.T.'s artificial intelligence laboratory that makes specialized hardware, and Intellicorp, a software firm specializing in expert systems. The stock prices of both have soared in recent months despite the computer industry's downturn, but analysts remain cautious about investments in the infant industry.\nSome of the biggest forces in office computers, however, are racing to grab a stake, particularly in A.I. hardware. Xerox and Texas Instruments both introduced new specialized artificial intelligence computers this year, primarily for developing new applications using the computer language preferred by most artificial intelligence developers, called LISP. The specialized machines cost $10,000 and up, and many industry experts suggest that artificial intelligence may not flower until most programs embodying it are designed to run on ordinary desktop computers. Only now are advanced personal computers becoming sufficiently powerful to handle the task.\nMeanwhile, non-computer companies, too, are showing a keen interest in the field. General Motors and Ford have spent millions recently to buy into the talents of small artificial intelligence start-ups that they hope will make factory operations vastly more efficient. Defense contractors, hoping to build smarter tanks and the computer systems that could become the core of a ''Star Wars'' defense system, are already doing the same.\nIn fact, virtually all the major advances in artificial intelligence research so far, have stemmed from Pentagon-funded research. Since the 1960's, when most companies viewed the technology as science fiction, the Defense Advanced Research Projects Agency saw its potential. ''Very few people realize the degree to which A.I. is owned by the Department of Defense,'' said Mr. Kaplan. Now industry is financing more of work itself, partly in response to Japan's Fifth Generation project, an effort to leapfrog American No. 1 position in artificial intelligence. But Mr. Kaplan and others worry that new Defense Department projects will siphon away too much of the industry's scarce human talent.\nAlready, artificial intelligence has brought a new sort of talent into the computer industry. So unusual are the problems, so diverse the solutions, that traditional computer scientists find themselves at something of a disadvantage. ''If you look at who did the personal computer development work, it was a bunch of nerdy engineers,'' Miss Dyson said. ''A.I. has nerdy engineers, but also philosophers, psychiatrists, neurobiologists and English teachers.''\nAnd it has its charlatans. ''Artificial intelligence'' may win the prize as the computer industry's most abused buzzword, affixed to everything from toy robots to ordinary tax-planning software. Frequently it is no more than an advertising ploy, an effort to capture the aura of HAL. ''It's becoming an axiom of the software industry that every chief executive has to say he's working on A.I.,'' says Edward Esber, the chief executive of Ashton-Tate, a leading personal-computer software firm that says it's working on artificial intelligence. ''Otherwise, we'd feel impotent.''\nSeparating the real from the imaginary is particularly difficult because no one has yet come up with a workable definition of what artificial intelligence is. Experimental techniques that riveted the attention of A.I. specialists a decade ago draw yawns today. ''Once it works,'' said Richard A. Shaffer, the editor of the Technologic Computer Letter, ''it's just good computer science.''\nIf all this has left ordinary computer users a bit mystified, they are in good company. ''Most people in artificial intelligence who have had a glass of wine,'' said Alan Kay, the technology guru of Apple Computer Inc., recently, ''will tell you that they do not actually understand it much either.''\u00a0THE PLAIN-SPEAKING PROGRAM\nStored in the United States Navy's giant computer systems is a data base that follows the movements of 40,000 ships around the world - destroyers and oil tankers, American submarines and, more important, Russian ones. But as Navy officials started conceding 10 years ago, extracting that information meant mastering obscure computer languages that could only slow decision-making in battle conditions.\nSolving that problem gave Gary Hendrix, once the director of natural language at the Stanford Research Institute, his start. In the early 1970's, with three-quarters of a million dollars in Defense Department funds, his team developed a system called LADDER. It allowed Navy officers to ask simple questions, such as: ''What ships are within 500 miles of the John F. Kennedy and what are their weapons systems?'' The answer could be displayed on a map.\n''When you are under pressure, falling back on your native tongue is really useful,'' Mr. Hendrix says. Now, at a small Silicon Valley company called Symantec, which he helped found, he is applying ''natural language'' principles to personal computers. Symantec's new program, which is called Q&A and came out in September for $299, appears to be the most advanced in the field.\nThe personal computer industry has more than a little riding on Q&A and its ilk. To pull out of the industry's severe slump, many analysts believe, hardware and software companies must attract a new kind of user - people with no patience for learning computer-ese.\nThe irony is that software executives are wringing their hands over this problem at just the time when they are pushing the technology further, with programs that are larger, more powerful and more complex. ''This notion that software is somehow getting easier to use is dead wrong,'' contends Mr. Esber, the head of Ashton-Tate, which manufactures the popular dBase.\nNatural language programs are a step toward solving the problem: They make computers more approachable - to a limited degree - by translating vague English phrases into specific computer commands.\nWith programs like Q&A, or the Microrim Inc.'s Clout, which came out last year, an executive can talk to a computer the way he would talk to a personnel officer. For example, a tight-fisted chief trying to prune his budget can ask, ''Who earns more than $40,000, and where do they live?'' If the chief thinks there are too many people earning those wages, Q&A allows him to go a step further: He can actually change the data. When the boss says, ''Cut all their salaries by 25 percent,'' the data base - and the employees' paychecks - are dutifully adjusted.\nBut no natural language program, particularly one built for a personal computer's limited memory, is fluent. Q & A comes with a basic vocabulary of only about 500 words, and conversation cannot stray from the familiar terms used in the data base. (Thousands of words can be added to the system, however.) Furthermore, parsing a sentence with Q&A and translating it into computer language is so slow that a user might just as well brew a cup of coffee while the computer is doing the task. And ask for ''Ted's salary and Lynn's address'' and you will also get Ted's address and Lynn's salary.\nAlready, software companies worried about expanding their markets are racing to get natural language ability. On Tuesday, Lotus announced that it would buy the GNP Development Corporation, a tiny Pasadena, Calif., company run by two brothers, Bill and Larry Gross. After a brief foray in selling stereo loudspeakers, the two Cal Tech graduates switched into software. Earlier this year, they began spending 20-hour days, amid personal computers and pizza boxes, disassembling Lotus 1-2-3 and re-writing portions so that users could ask it plain-English questions. They were so successful that Lotus reportedly paid between $2 million and $4 million for GNP.\n''When I saw this, I was really knocked off my feet,'' said Lotus's Mr. Kaplan. ''They did things to 1-2-3 we didn't think could technically be done.''\nLotus won't say when it will bring out GNP's first product, which the Gross brothers, ages 24 and 27, had planned to call -what else - HAL.\u00a0LETTING THE MACHINES DO THE REASONING\nRichard L. Schwartz and Robert E. Shostak look like Silicon Valley, California, computer hackers. They sport shaggy beards and wander around the office in blue jeans, usually with a can of soda in hand. For three years, they buried themselves in their attics at night, one in San Mateo and the other in nearby Menlo Park, zipping fragments of computer code to each other electronically as they struggled to break new ground in computer programming. ''If Woody Allen were doing a spoof of Silicon Valley,'' joked one associate, ''these are the guys he would cast.''\nLooks are deceiving, however. Early personal computer entrepreneurs started from scratch, growing successful by disregarding the programs and computer designs that guided companies making giant mainframes. But the 31-year-old Mr. Schwartz and Mr. Shostak, who is 37, have taken a different tack, applying artificial intelligence techniques first used in mainframes to improve personal computers data bases.\nThey have come up with what industry experts call a second-generation program, one that - like a good improvisational actor -begins to perform on its own after a fleeting glance at its director.\nThe data base, called Paradox and created by Mr. Schwartz's and Mr. Shostak's Ansa Corporation, enables a sophisticated computer user to retrieve information without having to issue explicit directions to the computer. By tapping one of the most useful and versatile techniques in the artificial intelligence grab-bag, a system developed by I.B.M. called ''query by example,'' the program lets a computer user point out where he wants to go and lets the computer do the driving.\nConsider the case of an auto parts wholesaler who keeps a log of his sales representatives and the orders they have received in one file, and an up-to-date account of inventories and part numbers in another. Suppose he wants to create a single list that will compare the orders for each part with the number he has on hand. That means merging the data on two lists and sorting it in a specific sequence.\nEven using a sophisticated data base, such as Ashton-Tate's dBase II, he would have to issue precise instructions, probably in an obscure computer language. With Paradox, however, he would simply place little electronic checkmarks next to examples of the entries that need to be combined. Then the program begins to work its logical magic.\n''It looks for relationships between the data that you checked,'' said Mr. Shostak. ''Then it begins to write a program automatically, the same kind of program you might otherwise be asked to write.''\nBut the user never even sees the program being created. Nor does he see it being honed to sort the data in the fastest way possible. He simply gets his list, with no added complications. ''The trick,'' Mr. Schwartz said, ''is to write a program that doesn't depend on humans for instructions about how to do everything. Instead, you can be vague, and give it just a general idea.''\nLike natural language programs, so-called machine reasoning software makes computers easier to use. But the similarity ends there. Natural language enables novices to get information out of a computer - albeit slowly - when they know little or nothing about the data stored in the machine. By contrast, machine-reasoning programs like Paradox let experienced users work at very high speed but require them to know exactly what they want. The auto parts distributor, for example, would have to know what information about inventories and orders is stored in his computer and why it is important.\nNo one is ready to guarantee that Paradox will catch on, despite a list of influential venture capital investors that includes Benjamin M. Rosen, who provided the initial funding for Lotus. Retailers are wary about taking on new software these days and dBase has already captured a loyal following, making it nearly impossible to displace.\nBut Paradox is considered important because it is an evolutionary step, away from the concept that a computer blindly executes instructions, and toward programs that interpret intentions.\nWhat could come next for machine reasoning is the design of ''software robots,'' and work is already feverishly under way, from Lotus's headquarters on the edge of the M.I.T. campus to Hewlett-Packard's research center across the road from Stanford. The software robots, really just programs that act as a computer user's secretary or accountant, would simulate the reasoning powers of a semi-bright office assistant.\nFor example, a research robot might notice that its human boss was outlining a speech about zoology. Picking up on some key words in its master's computer directory - like ''lion,'' ''tiger'' and ''bear'' - it might prompt the executive with a question like, ''Would you like me to do some research into what has been published recently about lions, tigers or bears?''\nIf the answer is yes, the machine might write a small program - or call upon an existing one - to search the electronic research services, like Nexis, for relevant material. And an advanced robot might draw on its base of knowledge to point out why some facts about those animals are more important than others, further honing the executive's thinking.\nSome experts contend that software that looks over one's shoulder might prove more annoying than useful. But some ideas are definitely appealing. An electronic travel assistant, for example, could scan an itinerary and automatically call up the Official Airline Guide, which lists flight options. A secretarial assistant program could sort electronic mail or telephone messages, putting the most important on top. Or, preprogrammed with the computer user's daily schedule and a knowlege of his or her working habits, it could negotiate meeting times.\n''These are all things that a human assistant would pick up to do instinctively,'' says Ira P. Goldstein, who is directing a $30 million to $50 million project to develop such robots for Hewlett-Packard. ''The question,'' adds Mr. Goldstein, who was a philosophy and mathematics major at Harvard and has spent the last 15 years attempting to depict knowledge in software, ''is whether we can instruct computers the way we instruct people - by inference and metaphor.''\u00a0TURNING COMPUTERS INTO EXPERTS\nAldo Cimino is nearing retirement, and not long ago the Campbell Soup Company started to get worried. Mr. Cimino, with 44 years at the company, is one of the few employees who know everything there is to know about the 72-foot-high ''cookers'' that fill up to 850 soup cans a minute.\nSo late last year, a gaggle of software engineers began interviewing Mr. Cimino, attempting to organize and codify a lifetime of experience about canning. The idea was to capture his expertise in an inexpensive computer system that could diagnose problems instantly by asking maintainence workers a few questions.\n''You have only a short period of time to correct the problem. Otherwise you lose the soup,'' Mr. Cimino explained patiently last month to a televised symposium on ''expert systems.'' The symposium was produced by Texas Instruments, which also designed Campbell's program.\nMore than 34,000 people, including roomsful of production experts at General Motors, tuned in to that all-day symposium - clear evidence of industry's growing interest in expert systems. Although far less dazzling than the other major artificial intelligence techniques used today, expert systems have the most commercial applications.\n''Five years ago you would have gotten 500 people at a symposium like that, at the most,'' said Mark S. Fox, who is the director of Carnegie Mellon's Intelligent Systems Laboratory.\nAlready, a small sub-industry of the software business has emerged to design expert systems, supported chiefly by companies trying to cut manufacturing costs. Two months ago, Ford said it would invest up to $28 million for a 10 percent stake in the Carnegie Group, formed by Mr. Fox and three faculty colleagues, and in the Inference Corporation, of Los Angeles. Carnegie is already at work, building a diagnostic system that would track down problems with engines and transmission systems, for use in Ford dealerships.\nGeneral Motors and Procter & Gamble are also in the game; each has an 11 percent stake in Teknowledge Inc., which sells programming ''shells'' -the basic ingredients from which a company can design its own expert systems. I.B.M. just entered the race, selling a shell of its own.\nMeanwhile, startups like Palladian Software and Applied Expert Systems, both in Cambridge, Mass., sell ready-to-use financial adviser programs with built-in expertise. Palladian's is aimed at corporate managers performing cost-benefit analyses, while Applied's program helps financial institutions design financial plans for clients. That $50,000 program, which runs only on the new Xerox computer, churns out a 75-page report analyzing the client's net worth, and recommending cash management, tax strategies, retirement and estate planning.\nVirtually all expert systems work on the same principle. They try to boil down knowledge into a highly structured outline, which then is converted into a number of ''if-then'' rules. At Campell Soup, if the problem is temperature deviation within the cookers, the program will ask maintenance workers a series of technical questions about the status of valves, cooling sprays and the like. What follows is the ''then'' - perhaps an instruction to replace a specific valve.\nSuch principles have been applied to a wide range of problems. Some of the oldest systems have been used by doctors to diagnose lung disease, among other things. Digital Equipment Corporation uses an expert system to analyze the equipment needs of large computer users, and another to allocate components to workers building VAX 8600 superminicomputers. Northwest Orient Airlines has a system to evaluate how many discount and full-price seats it should offer on each flight to maximize profits.\nBut those systems, users admit, are no panacea. When Hewlett-Packard started designing an in-house system to solve problems in its semiconductor fabrication plants, it discovered that its top experts disagreed on several key points. It solved that problem, only to find unexpected problems in a second facility where the system was installed.\nMore important, experts learn from their mistakes; expert systems do not. They need to be reprogrammed when something goes awry or the problem changes. ''Learning is still a third dimension,'' notes Mr. Goldstein of Hewlett-Packard, ''We haven't really approached that yet.''\u00a0ELUSIVE VISION\nFor years, software engineers have believed that computers do worst what people do best. No field of artificial intelligence proves the rule more emphatically than machine vision, the effort - so far largely unsuccessful - to give computers not only the power of sight, but the power of recognition as well.\nThe problem, quite literally, is that computers cannot see the forest for the trees. Humans can instantly sift the important from the trivial; they will usually pay more attention to a car hurtling at them rather than an 18-story building in the background. But vision systems usually suck in everything in sight. Then, bit by painstaking bit, they try to distinguish objects from one another and identify them.\n''It's much, much more complex than what people expected 10 or 15 years ago,'' said Daniel Sabbah, the senior manager of artificial intelligence systems at I.B.M.'s Thomas J. Watson Research Laboratory in Yorktown, N.Y.\nMr. Sabbah repeated the widely told tale about Marvin Minsky, the founder of M.I.T.'s artificial intelligence lab who by some estimates trained nearly a third of all the nation's leading A.I. scientists. Long ago, the story goes, Mr. Minsky asked a graduate student to solve the problem of machine vision ''as a summer project.'' The result? ''He didn't make it,'' Mr. Sabbah smiled.\nToday, most work on vision is still being done in university research labs. Truly versatile commercial products still appear at least a decade away, even for companies with particular interest in such clearly connected products as sight-guided robots.\nIn retrospect, the early optimism arose from great strides in the 1960's in pattern recognition - the technology behind supermarket cash register lasers. But almost all those systems involve high-contrast scenes and objects that can be compared with a limited library of shapes and sizes stored in a computer's memory.\nFar more complex are the shades of gray and unusual shapes in the real world. Consider, for example, the seemingly simple task of recognizing a cat sitting on a chair. The computer would first have to distinguish the outline of the cat from the upholstery, still no easy trick. Then, more likely than not, it would have to reach into its file of ''four-legged things,'' trying to determine if this was an elephant, a horse, a dog - or even a chair.\n''We don't even know how to represent a cat in the computer's memory,'' said Mr. Sabbah. ''We can enter an iconic representation, but the computer would still need the power to generalize - there are cats of different colors and sizes, tigers and house cats, even cartoon cats.''\nBut the stakes for solving those problems are enormously high. On the factory floor, machine vision would mean that robots would no longer have to be programmed with excruciating precision, told to pick something up at one coordinate and drop it off at another. Rather, they could roam, operating with only general instructions. The Defense Advanced Research Projects Agency, which has funded much of the pioneering work in artificial vision, hopes to give the same powers to an ''autonomous land vehicle,'' essentially a pilotless tank that could follow a clear, well-defined road and avoid obstacles. A prototype is being built by Martin Marietta.\nNow, specialists are trying to interconnect symbols stored in memory the way neurons connect ideas in the brain. Silicon circuits, they note, can operate up to a million times faster than neurons. But the brain easily outdistances the electronic circuitry because it can perform thousands of operations at once. Most computers, in contrast, plod along one operation at a time, creating huge information bottlenecks. In fact, Mr. Sabbah says, ''the brain may be the best parallel processor we've got.''\n","130":"ARTIFICIAL-INTELLIGENCE computer systems are helping fighter pilots make tactical decisions, oil companies analyze geological data and computer companies design new products. Now Wall Street data management experts are dreaming of the day when such systems might capture the expertise of top traders, deal makers and analysts in programs that could enhance the performance of less-skilled or inexperienced employees.\n''People recognize that this is the next wave in automation,'' said Bernard A. Weinstein, vice president in charge of programming, systems and communication at E. F. Hutton & Company.\n''Two years ago, no one on Wall Street thought much about artificial intelligence,'' said Philip Cooper, chairman and chief executive of Palladian Software Inc., a Cambridge-based supplier of programs that analyze corporate investment problems, such as when and where to build a new plant or potential acquisitions. ''A year ago, there was a lot of cocktail party conversation. Now everyone is looking at how to apply it.''\u00a0\n''Expert systems'' use artificial-intelligence techniques to draw on a base of knowledge supplied by a company's experts, including subjective judgments, with symbols that are manipulated to simulate human reasoning electronically. The systems can make reasoned assumptions to fill in missing information. Some automatically expand or update their information files. By contrast, today's computer-controlled stock trading programs do little more than crunch data at mind-numbing speeds.\nSome expert systems are incorporating other aspects of artificial-intelligence research, such as the ability to understand everyday language. For instance, Le Courtier (French for ''the broker'') is a ''natural language'' stock portfolio advisory service developed by Cognitive Systems Inc. of New Haven for Generale de Banque of Belgium. A customer can seek stock recommendations based on instructions typed into a terminal about how much is available to invest and investment goals. Le Courtier can comment on a particular stock, if queried, or point out, without prompting, other stocks in the same sector that might be a better investment. It can also explain its reasoning and adjust its recommendations in response to a feedback typed into the terminal, such as ''I don't want utility stocks.''\nThe major pioneering effort to bring artificial intelligence to Wall Street is believed to have begun at Lehman Brothers just before its merger with Shearson American Express in 1984. It was in the field of interest rate swaps, whereby one corporate or banking client might exchange its fixed-rate loan obligation for the variable-rate obligation of another institution.\nAt one point, Bruce Gras, then a senior vice president, reported that the interest-rate swap system, which could evaluate the terms of one client's loan and the terms for which the client might be willing to swap interest payment obligations, earned $1 million in additional fees over a two-month period. However, the program withered when Mr. Gras left to join Symbolics Inc., a start-up company supplying computers designed for artificial-intelligence work. Shearson will not comment on its current work.\nThe Securities and Exchange Commission is now seeking to develop a natural language system that can read financial statements, thus allowing it to automate its analysis and indexing of financial information. The New York Stock Exchange hopes another application will allow its computer surveillance systems to read and take into account news reports in watching for unusual trading activity.\nDespite the rising interest in such developments, many Wall Street data managers see drawbacks. Some complain that expert systems operate too slowly to incorporate rapidly changing market conditions into their ''thinking'' as the changes occur. Others say that efforts to apply artificial-intelligence solutions to problems often end up showing ways to do the same thing with traditional, less costly data processing.\nMost Wall Street firms are turning to outsiders for help. E. F. Hutton, for instance, is working with Teknowledge Inc. of Palo Alto, Calif., to put an expert system ''shell'' around existing data networks. In theory, the logic incorporated in the shell would encourage Hutton's 6,500 brokers to consider the insights of the firm's top analysts and investment advisers and use available data more effectively when searching for sales opportunities or answering client queries.\n''You are going to see major, major developments as early as 1987,'' said Gary Williams, president of the Sterling Wentworth Corporation, a Provo, Utah, marketer of programs for financial planners that is developing an expert system for stockbroking. ''Wall Street is more centralized than other parts of the financial services sector and could move very rapidly.''\nWhether Wall Street will start talking more openly about such developments is another matter.\n''When you talk about your expertise, you are talking about your corporate treasure,'' Mr. Weinstein said.\n","131":"Artificial intelligence, or computer-based reasoning and problem solving, has, after years of hype and disappointments, begun to come of age in a few fields marked by clear rules and precise terms. It has been used to help doctors diagnose diseases; it has been used to beat the chess champion of the world.\nBut another goal in artificial intelligence research has proved more elusive. For more than 25 years, experts in artificial intelligence in the United States and Western Europe have tried to figure out how to teach computers to think as lawyers do.\u00a0\n Lawyers who have followed the efforts have been skeptical, doubting whether computers could perform legal reasoning. But next month, their skepticism will be tested by the Virtual Patent Advisor, a new software product designed to identify and help avoid patent conflicts.\nIn December, the General Electric Company's GE Power Systems unit, based in Schenectady, N.Y., plans to introduce the system, the first in a series of virtual legal advisers intended to prevent and help solve corporate legal problems.\nThe result of research by the Jnana Technologies Corporation, the Virtual Patent Advisor aims to help identify patent infringement problems. The system, which can run on a desktop computer and can use the Internet, questions an engineer to define the structure and purpose of an invention idea and then searches data bases and compares design features with existing patents. It can then create lists of potentially conflicting patents, apply rules of patent law to the facts it has gathered and generate reports to the company's lawyers.\nIdeally, the engineer, apprised of potential conflicts, could alter the design or turn to the company to find some other solution. Robert C. Lampe Jr., a senior patent lawyer at GE Power Systems, said he expected the product would reduce to less than an hour a process that now takes as long as a week. \"It's a tremendous productivity tool,\" he said.\nComputers have long been used to conduct key-word searches in legal research and to prepare tax forms and wills. But David R. Johnson, chairman of Counsel Connect and co-director of the Cyberspace Law Institute, said the virtual advisers would go well beyond this, bringing together case-based and rule-based reasoning capacities and the ability to draw on multiple data bases.\nAt least one doubter was intrigued by the idea. \"It sounds like it would make legal practice more fun -- get rid of the drudgery,\" said Ronald Dworkin, a professor of law and philosophy at New York University and a professor of jurisprudence at Oxford University.\n\"But I guess what I'd insist on is that this is all limited by the fact that the law is essentially imaginative and interpretive,\" he added. These qualities, he said, \"can't be programmed, no matter how sophisticated the programming techniques.\"\nWendy Gordon, a law professor at Boston University, said she was concerned that a computer, unlike a human, could not perceive voice quality, facial expressions and other cues that signal confusion or ambiguity.\nAnd Robert C. Sheehan, executive partner at Skadden, Arps, Slate, Meagher & Flom, said, \"The most advanced cutting edge of the law is always where you're able to take a set of facts and come up with a novel idea.\" Software, he said, \"won't be creative.\"\nBut those who work in artificial intelligence say that to the extent that law relies upon tightly woven arguments, supporting precedent and well-defined rules, it can be a perfect template for artificial intelligence.\nThat is because the fields best suited for artificial intelligence are the highly mathematical ones that have \"very little subjectivity in them, like chess,\" said Edwina L. Rissland, professor of computer science at the University of Massachusetts at Amherst and president of the International Association for Artificial Intelligence and the Law.\nThe fields that lend themselves least to artificial intelligence, she added, are those \"in which it's very hard to pin down the meanings of the words.\"\n\"Probably much harder than law,\" she added, \"would be things like politics and further still would be something like psychiatry.\"\nProfessor Rissland said that research into the use of artificial intelligence in legal contexts had divided into two general categories.\nIn the first, rule-based reasoning, the computer uses if-then statements to move from facts to conclusions or from possible conclusions to supporting facts. In the second, case-based reasoning, experience from previous, analogous cases is applied to solve new problems.\nProfessor Rissland, in more than a decade of research, pioneered computerized case-based reasoning and constructed a system that meshed the two forms of reasoning.\nFrederic W. Parnon, a lawyer, ran across Professor Rissland's work in 1995 when he decided to devise software systems that could perform legal functions. He later founded Jnana, which developed the Virtual Patent Advisor.\nProfessor Rissland has served as an adviser and occasional consultant on Jnana's virtual advisers, though she has no financial stake in the project.\nAfter spending two years creating the underlying technology for the systems, Jnana approached large corporations and asked them to support development of applications addressing specific legal concerns in exchange for favorable licensing terms.\nIn trying to identify a prospective client for a new technological product, Mr. Parnon said, \"You have to get this confluence of someone who's enlightened, who has a business problem to solve; then there's a fit.\"\nG.E. was the first large company to sign on. Several other companies are considering participation in other adviser systems.\nMr. Parnon said that a telecommunications company, which he would not identify, was working with Jnana to develop a system to advise managers about human resources. A major legal research company, he said, is interested in software that will query legal researchers about facts, search data bases and send researchers to the appropriate areas of the law.\nMr. Parnon declined to provide details of the development costs or licensing terms. But companies that have not participated in development would pay for the software at an annual cost equivalent to the salary of a junior lawyer.\nBut even artificial intelligence experts acknowledge its inherent weakness. \"It's hard for a good lawyer to come up with novel, inventive arguments,\" Professor Rissland said, \"and it will be even harder for a machine to do so -- probably.\"\n","132":"Hubert L. Dreyfus, a philosopher whose 1972 book ''What Computers Can't Do'' made him a scourge and eventually an inspiration to researchers in artificial intelligence, died on April 22 at his home in Berkeley, Calif. He was 87.\nThe University of California, Berkeley, where he was a longtime professor of philosophy, said the cause was cancer. \n  Professor Dreyfus became interested in artificial intelligence in the late 1950s, when he began teaching at the Massachusetts Institute of Technology. He often brushed shoulders with scientists trying to turn computers into reasoning machines.\n  ''They said they could program computers to be intelligent like people,'' he recalled in a 2005 interview with the blog Full-Tilt Boogie. ''They came to my course and said, more or less: 'We don't need Plato and Kant and Descartes anymore. That was all just talk. We're empirical. We're going to actually do it.'''\u00a0\n  He added: ''I really wanted to know, could they do it? If they could, it was very important. If they couldn't, then human beings were different than machines, and that was very important.''\n  In 1965, after spending time at the RAND Corporation, he published ''Alchemy and Artificial Intelligence,'' a slashing attack on the work of Allan Newell and Herbert A. Simon, two of RAND's leading artificial intelligence researchers, and followed with the equally provocative ''What Computers Can't Do: A Critique of Artificial Reason.''\n  Professor Dreyfus argued that the dream of artificial intelligence rested on several flawed assumptions, chief among them the idea that the brain is analogous to computer hardware and the mind to computer software.\n  In this view, human beings develop an accurate picture of the world by adding bits of information and rearranging them in a procedure that follows predictable rules.\n  Professor Dreyfus, an adherent of the French phenomenologist Maurice Merleau-Ponty and the German philosopher Martin Heidegger (he had written seminal introductory works on both men), posited a different view of human beings and their interactions with the world around them.\n  There was no objective set of facts outside the human mind, he insisted. Human beings experienced learning as a partly physical interaction with their surroundings, and interpreted the world, in a process of continual revision, through a socially determined filter.\n  Inevitably, he said, artificial intelligence ran up against something called the common-knowledge problem: the vast repository of facts and information that ordinary people possess as though by inheritance, and can draw on to make inferences and navigate their way through the world.\n  ''Current claims and hopes for progress in models for making computers intelligent are like the belief that someone climbing a tree is making progress toward reaching the moon,'' he wrote in ''Mind Over Machine: The Power of Human Intuition and Expertise in the Era of the Computer'' (1985), a book he collaborated on with his younger brother Stuart, a professor of industrial engineering at Berkeley.\n  His criticisms were greeted with intense hostility in the world of artificial intelligence researchers, who remained confident that success lay within reach as computers grew more powerful.\n  When that did not happen, Professor Dreyfus found himself vindicated, doubly so when research in the field began incorporating his arguments, expanded upon in a second edition of ''What Computers Can't Do'' in 1979 and ''What Computers Still Can't Do'' in 1992.\n  Hubert Lederer Dreyfus, known as Bert, was born on Oct. 15, 1929, in Terre Haute, Ind. His father, Stanley, was in the wholesale poultry business, and his mother, the former Irene Lederer, was a homemaker.\n  At Wiley High School, his debate coach encouraged him to apply to Harvard, which he thought was in England, because of the Cambridge address. He was more interested in a different Cambridge school that he thought might sharpen his talent for making homemade explosives and setting them off by remote control.\n  ''I wanted to go to M.I.T. because I figured they would help me make better bombs,'' he said in an interview at the Institute of International Studies at Berkeley in 2005.\n  In the end, he opted for Harvard, where he studied physics initially but switched majors after hearing a lecture by the American philosopher C. I. Lewis.\n  He received a bachelor's degree in philosophy in 1951, writing an undergraduate thesis on causality in quantum mechanics, and a master's degree in 1952. Before completing his doctorate in 1964, with a dissertation on Edmund Husserl -- a philosopher he later dismissed as ''boring'' -- he spent fellowships in Freiburg, Germany; Louvain, Belgium; and the \u00c9cole Normale Sup\u00e9rieure in Paris, absorbing the latest developments in Continental philosophy.\n  After returning to the United States, he taught at Brandeis University and M.I.T. and translated, with his first wife, the former Patricia Allen, Merleau-Ponty's ''Sense and Non-Sense,'' published in 1964. He joined the philosophy department at Berkeley in 1968.\n  His first marriage ended in divorce. In addition to his younger brother, Stuart, an emeritus professor of industrial engineering at Berkeley, he is survived by his wife, Genevi\u00e8ve Boissier-Dreyfus, and their two children, St\u00e9phane and Gabrielle Dreyfus.\n  Professor Dreyfus went on to play a major role in explaining Continental thought in works like ''Michel Foucault: Beyond Structuralism and Hermeneutics'' (1982), written with Paul Rabinow, and ''Being-in-the-World: A Commentary on Heidegger's 'Being and Time, Division I''' (1989). With Mark Wrathall, a professor of philosophy at the University of California, Riverside, he edited numerous guides devoted to existentialism, phenomenology and Heidegger's philosophy.\n  ''It is no exaggeration to say that, insofar as English speaking philosophers have any access at all to thinkers like Heidegger, Merleau-Ponty and Michel Foucault, it is through the interpretation that Dreyfus originally offered of them,'' the Harvard philosophy professor Sean D. Kelly wrote recently on the philosophy website Daily Nous.\n  In later years, he turned his attention to new subjects. With Professor Kelly, he wrote a surprise best seller on literature, ''All Things Shining: Reading the Western Classics to Find Meaning in a Secular Age'' (2011). In ''Skillful Coping: Essays on the Everyday Phenomenology of Everyday Perception and Action'' (2014), an essay collection edited by Professor Wrathall, he employed the insights of phenomenology to explore nonreflexive action and ethics.\n  For his 2006 book ''Philosophy: The Latest Answers to the Oldest Questions,'' Nicholas Fearn broached the topic of artificial intelligence in an interview with Professor Dreyfus, who told him: ''I don't think about computers anymore. I figure I won and it's over: They've given up.''\n\n\n\n","134":"Every year Rob DeFeo consults nature to figure out when Washington's cherry blossoms will bloom. The National Park Service chief horticulturalist, who has rough hands and wears a battered leather jacket, studies plants and trees, ponders seasons past and taps instincts born of decades watching winter turn to spring.\nMillions of dollars ride on his forecast, as does as the fate of the city's annual tourist extravaganza, which begins today. DeFeo says he has been on target 13 out of 16 years.\u00a0\nBut now come Virginia Tech's Vidhya Dass and Elizabeth Brennan, students armed with artificial neural networks, evolutionary computations, the Arrhenius equation, linear regression and something called fuzzy logic to suggest an alternative to DeFeo's seasoned eye.\nWhich is to say: Might the brain of a computer some day match human blossom intuition?\nIt is, if you will, algorithm vs. biorhythm, a finger on the \"enter\" button vs. a finger in the wind, artificial intelligence vs. a guy who once had 300 species of azaleas in his yard.\nThe students' idea grew out of an artificial intelligence class they took last spring as part of a master's program at Virginia Tech's Falls Church campus. Their teacher, assistant professor Chang-Tien Lu, suggested that they try using artificial intelligence to predict the peak bloom period.\nThe task has traditionally been done by DeFeo, 52, a wry New Jersey native and lifelong horticulturist who is an expert on the life and lore of the renowned cherry blossoms.\nDeFeo scrutinizes such things as early flowering elms, maples and cornelian cherry dogwoods, as well as the weather and other recurring clues to the advent of spring.\nThis year, according to the forecast he issued this month, the peak bloom period would be from March 27 through April 3. He said yesterday that today will be the peak bloom day, when 70 percent of the blossoms are open. The bloom generally continues for several days beyond the peak period, depending on the weather.\nDass, 33, a native of India, and Brennan, 24, who hails from Baltimore, set out to see whether a computer model might, theoretically, do as well or better, making it easier for tourists to plan visits and officials to plan the National Cherry Blossom Festival.\n\"We hoped to create a model that would allow the best prediction with the minimum amount of input,\" Brennan said, meaning as early in the season as possible. The goal was \"to see how our artificial techniques compared to human methods.\"\nThey tried an array of computer methods to see how each worked, the two said in interviews this week. They produced a paper last spring based on the research.\nTheir approach was far different from that of DeFeo, whom they consulted and who admits he understands little of what they report. \"I don't have a clue what they're saying,\" he said yesterday.\nNeither would most people.\nDass and Brennan said they focused on computational intelligence and essentially tried to mimic the working of the human brain. This involved considering such things as \"multiple-layered feed forward neural networks,\" they wrote in their paper, as well as \"delta rule,\" \"topology\" and \"Stochastic gradient method.\"\nA neural network model, by the way, \"is like the brain,\" Dass said. \"You know how our human brain absorbs complex relationships? It's something very similar to that. You would train a neural network . . . like the brain, and then after a while, it would be able to . . . predict the actual phenomenon.\"\nFuzzy logic is another malleable brainlike data processing system that adjusts itself as it gets feedback, Dass said. And the famous equation of Swedish chemist Svante August Arrhenius calculates the speed of a chemical reaction based on temperature.\nComplex as all this sounds, Dass and Brennan pointed out that computer modeling is widely used in Japan to predict the cherry blossom blooming period and in the United States to predict soybean flowering, corn yield, and aspects of tomato and lettuce growing.\nThe students started their project in January 2007 and observed the start of the annual blossom bloom. \"They were quite striking, very beautiful,\" said Brennan, who had never seen the flowers before.\nThe two did not hazard a forecast but plugged in historical data about past blooms and associated weather conditions. Because they used previously recorded data and outcomes, they were able to see which models worked best. They found several models that were accurate to within a few days of past peak dates.\nThe students say some models, according to their calculations, came three days closer to the peak bloom date than DeFeo's predictions.\nBut DeFeo focuses more on a bloom range, and, anyway, \"it's a crapshoot,\" he said, adding: \"The trees will be in full bloom when the blossoms are fully open.\"\nDeFeo said yesterday that his track record is good, though his prediction is subject to the whims of the weather. He fretted, for example, about a forecast for some stormy weather next week, which could strip the delicate blossoms.\n\"I missed it three years,\" he said. \"All three years, they bloomed early on me. Two of those years, I missed it by five or six days; the other year, I missed it by one day.\"\nYou must have \"intimate daily contact with your tree population, or any living thing, in order to understand it,\" he said. Computers \"certainly have their use, but when they forecast the blossoms, I would never want to substitute a computer for going out and looking at the buds and seeing where they're at.\"\nDass and Brennan, meanwhile, are onto other subjects this year. But they did well in their class last year. Lu, the professor, said yesterday that both got an \"A\" in the course.\n","135":"-- It's summertime and the Terminator is back. A sci-fi movie thrill ride, ''Terminator Salvation'' comes complete with a malevolent artificial intelligence dubbed Skynet, a military R.&D. project that gained self-awareness and concluded that humans were an irritant -- perhaps a bit like athlete's foot -- to be dispatched forthwith. \n  The notion that a self-aware computing system would emerge spontaneously from the interconnections of billions of computers and computer networks goes back in science fiction at least as far as Arthur C. Clarke's ''Dial F for Frankenstein.'' A prescient short story that appeared in 1961, it foretold an ever-more-interconnected telephone network that spontaneously acts like a newborn baby and leads to global chaos as it takes over financial, transportation and military systems.\u00a0\n  Today, artificial intelligence, once the preserve of science fiction writers and eccentric computer prodigies, is back in fashion and getting serious attention from NASA and from Silicon Valley companies like Google as well as a new round of start-ups that are designing everything from next-generation search engines to machines that listen or that are capable of walking around in the world. A.I.'s new respectability is turning the spotlight back on the question of where the technology might be heading and, more ominously, perhaps, whether computer intelligence will surpass our own, and how quickly.\n  The concept of ultrasmart computers -- machines with ''greater than human intelligence'' -- was dubbed ''The Singularity'' in a 1993 paper by the computer scientist and science fiction writer Vernor Vinge. He argued that the acceleration of technological progress had led to ''the edge of change comparable to the rise of human life on Earth.'' This thesis has long struck a chord here in Silicon Valley.\n  Artificial\u00a0intelligence is already used to automate and replace some human functions with computer-driven machines. These machines can see and hear, respond to questions, learn, draw inferences and solve problems. But for the Singulatarians, A.I. refers to machines that will be both self-aware and superhuman in their intelligence, and capable of designing better computers and robots faster than humans can today. Such a shift, they say, would lead to a vast acceleration in technological improvements of all kinds.\n  The idea is not just the province of science fiction authors; a generation of computer hackers, engineers and programmers have come to believe deeply in the idea of exponential technological change as explained by Gordon Moore, a co-founder of the chip maker Intel. \n  In 1965, Dr. Moore first described the repeated doubling of the number transistors on silicon chips with each new technology generation, which led to an acceleration in the power of computing. Since then ''Moore's Law'' -- which is not a law of physics, but rather a description of the rate of industrial change -- has come to personify an industry that lives on Internet time, where the Next Big Thing is always just around the corner.\n  Several years ago the artificial-intelligence pioneer Raymond Kurzweil took the idea one step further in his 2005 book, ''The Singularity Is Near: When Humans Transcend Biology.'' He sought to expand Moore's Law to encompass more than just processing power and to simultaneously predict with great precision the arrival of post-human evolution, which he said would occur in 2045.\n  In Dr. Kurzweil's telling, rapidly increasing computing power in concert with cyborg humans would then reach a point when machine intelligence not only surpassed human intelligence but took over the process of technological invention, with unpredictable consequences.\n  Profiled in the documentary ''Transcendent Man,'' which had its premier last month at the TriBeCa Film Festival, and with his own Singularity movie due later this year, Dr. Kurzweil has become a one-man marketing machine for the concept of post-humanism. He is the co-founder of Singularity University, a school supported by Google that will open in June with a grand goal -- to ''assemble, educate and inspire a cadre of leaders who strive to understand and facilitate the development of exponentially advancing technologies and apply, focus and guide these tools to address humanity's grand challenges.''\n  Not content with the development of superhuman machines, Dr. Kurzweil envisions ''uploading,'' or the idea that the contents of our brain and thought processes can somehow be translated into a computing environment, making a form of immortality possible -- within his lifetime.\n  That has led to no shortage of raised eyebrows among hard-nosed technologists in the engineering culture here, some of whom describe the Kurzweilian romance with supermachines as a new form of religion.\n  The science fiction author Ken MacLeod described the idea of the singularity as ''the Rapture of the nerds.'' Kevin Kelly, an editor at Wired magazine, notes, ''People who predict a very utopian future always predict that it is going to happen before they die.''\n  However, Mr. Kelly himself has not refrained from speculating on where communications and computing technology is heading. He is at work on his own book, ''The Technium,'' forecasting the emergence of a global brain -- the idea that the planet's interconnected computers might someday act in a coordinated fashion and perhaps exhibit intelligence. He just isn't certain about how soon an intelligent global brain will arrive.\n  Others who have observed the increasing power of computing technology are even less sanguine about the future outcome. The computer designer and venture capitalist William Joy, for example, wrote a pessimistic essay in Wired in 2000 that argued that humans are more likely to destroy themselves with their technology than create a utopia assisted by superintelligent machines.\n  Mr. Joy, a co-founder of Sun Microsystems, still believes that. ''I wasn't saying we would be supplanted by something,'' he said. ''I think a catastrophe is more likely.''\n  Moreover, there is a hot debate here over whether such machines might be the ''machines of loving grace,'' of the Richard Brautigan poem, or something far darker, of the ''Terminator'' ilk.\n  ''I see the debate over whether we should build these artificial intellects as becoming the dominant political question of the century,'' said Hugo de Garis, an Australian artificial-intelligence researcher, who has written a book, ''The Artilect War,'' that argues that the debate is likely to end in global war.\n  Concerned about the same potential outcome, the A.I. researcher Eliezer S. Yudkowsky, an employee of the Singularity Institute, has proposed the idea of ''friendly artificial intelligence,'' an engineering discipline that would seek to ensure that future machines would remain our servants or equals rather than our masters.\n  Nevertheless, this generation of humans, at least, is perhaps unlikely to need to rush to the barricades. The artificial-intelligence industry has advanced in fits and starts over the past half-century, since the term ''artificial intelligence'' was coined by the Stanford University computer scientist John McCarthy in 1956. In 1964, when Mr. McCarthy established the Stanford Artificial Intelligence Laboratory, the researchers informed their Pentagon backers that the construction of an artificially intelligent machine would take about a decade. Two decades later, in 1984, that original optimism hit a rough patch, leading to the collapse of a crop of A.I. start-up companies in Silicon Valley, a time known as ''the A.I. winter.'' \n  Such reversals have led the veteran Silicon Valley technology forecaster Paul Saffo to proclaim: ''never mistake a clear view for a short distance.''\n  Indeed, despite this high-technology heartland's deeply held consensus about exponential progress, the worst fate of all for the Valley's digerati would be to be the generation before the generation that lives to see the singularity.\n  ''Kurzweil will probably die, along with the rest of us not too long before the 'great dawn,' '' said Gary Bradski, a Silicon Valley roboticist. ''Life's not fair.'' \n","136":"The technologist and the marketing executive who co-founded Palm Computing in 1992 are starting a new company that plans to license software technologies based on a novel theory of how the mind works.\n Jeff Hawkins and Donna Dubinsky will remain involved with what is now called PalmOne, but on Thursday they plan to announce the creation of Numenta, a technology development firm that will conduct research in an effort to extend Mr. Hawkins's theories. Those ideas were initially sketched out last year in his book ''On Intelligence: How a New Understanding of the Brain Will Lead to the Creation of Truly Intelligent Machines,'' co-written with Sandra Blakeslee, who also writes for The New York Times.\u00a0\n  Dileep George, a Stanford University graduate student who has worked with Mr. Hawkins in translating his theory into software, is joining the firm as a co-founder.\n Mr. Hawkins has long been interested in research in the field of intelligence, and in 2002 he founded the Redwood Neuroscience Institute. He now spends part of his time there while continuing to serve as chief technology officer of PalmOne. \n\u00a0Artificial intelligence, which first attracted computer scientists in the 1960's, was commercialized in the 1970's and 1980's in products like software that mimicked the thought process of a human expert in a particular field. But the initial excitement about machines that could see, hear and reason gave way to disappointment in the mid-1980's, when artificial intelligence technology became widely viewed as a failure in the real world.\n In recent years, vision and listening systems have made steady progress, and Mr. Hawkins said that while he was uncomfortable with the term artificial intelligence, he believed that a renaissance in intelligent systems was possible.\n He said that he believed there would soon be a new wave of software based on new theoretical understanding of the brain's operations.\n ''Once you know how the brain works, you can describe it with math,'' he said.\n Mr. Hawkins acknowledged, however, that full-scale applications of his theoretical approach had not yet been developed or proved . Mr. Hawkins is now demonstrating a pattern-recognition application using a version of his software. It allows a computer to correctly identify a line drawing of a dog from many different patterns. Commercial uses for the technology might include speech recognition for telephone customer service or vision systems for quality control in factories. \n Initially, the company will offer free licenses to the Numenta software to permit experimentation and help build a research community to develop the technology, Ms. Dubinsky said.\n Neuroscience researchers said Mr. Hawkins's theories were promising but still unproved.\n ''Jeff is doing interesting work, and he may well recharge the field, whether or not his particular algorithms play out,'' said Gary Bradski, a neuroscientist who manages the machine learning group at the Intel Corporation. ''He's had good instincts on his last two ventures.''\n Mr. Hawkins and Ms. Dubinsky left Palm Computing in 1998 to form Handspring. They then returned to Palm in 2003 when it acquired Handspring in an effort to speed its entry into the market for smart phones. Ms. Dubinsky is currently a PalmOne board member.\n Mr. Hawkins said that in addition to his work with Numenta he was developing a new product for PalmOne.\n","137":"Berkeley, Calif.\nTHE news of the day often includes an item about some development in artificial intelligence: a machine that smiles, a program that can predict human tastes in mates or music, a robot that teaches foreign languages to children. This constant stream of stories suggests that machines are becoming smart and autonomous, a new form of life, and that we should think of them as fellow creatures instead of as tools. But such conclusions aren't just changing how we think about computers -- they are reshaping the basic assumptions of our lives in misguided and ultimately damaging ways.\nI myself have worked on projects like machine vision algorithms that can detect human facial expressions in order to animate avatars or recognize individuals. Some would say these too are examples of A.I., but I would say it is research on a specific software problem that shouldn't be confused with the deeper issues of intelligence or the nature of personhood. Equally important, my philosophical position has not prevented me from making progress in my work. (This is not an insignificant distinction: someone who refused to believe in, say, general relativity would not be able to make a GPS navigation system.)\u00a0\nIn fact, the nuts and bolts of A.I. research can often be more usefully interpreted without the concept of A.I. at all. For example, I.B.M. scientists recently unveiled a ''question answering'' machine that is designed to play the TV quiz show ''Jeopardy.'' Suppose I.B.M. had dispensed with the theatrics, declared it had done Google one better and come up with a new phrase-based search engine. This framing of exactly the same technology would have gained I.B.M.'s team as much (deserved) recognition as the claim of an artificial intelligence, but would also have educated the public about how such a technology might actually be used most effectively.\nAnother example is the way in which robot teachers are portrayed. For starters, these robots aren't all that sophisticated -- miniature robotic devices used in endoscopic surgeries are infinitely more advanced, but they don't get the same attention because they aren't presented with the A.I. spin.\nFurthermore, these robots are just a form of high-tech puppetry. The children are the ones making the transaction take place -- having conversations and interacting with these machines, but essentially teaching themselves. This just shows that humans are social creatures, so if a machine is presented in a social way, people will adapt to it.\nWhat bothers me most about this trend, however, is that by allowing artificial intelligence to reshape our concept of personhood, we are leaving ourselves open to the flipside: we think of people more and more as computers, just as we think of computers as people.\nIn one recent example, Clay Shirky, a professor at New York University's Interactive Telecommunications Program, has suggested that when people engage in seemingly trivial activities like ''re-Tweeting,'' relaying on Twitter a short message from someone else, something non-trivial -- real thought and creativity -- takes place on a grand scale, within a global brain. That is, people perform machine-like activity, copying and relaying information; the Internet, as a whole, is claimed to perform the creative thinking, the problem solving, the connection making. This is a devaluation of human thought.\nConsider too the act of scanning a book into digital form. The historian George Dyson has written that a Google engineer once said to him: ''We are not scanning all those books to be read by people. We are scanning them to be read by an A.I.'' While we have yet to see how Google's book scanning will play out, a machine-centric vision of the project might encourage software that treats books as grist for the mill, decontextualized snippets in one big database, rather than separate expressions from individual writers. In this approach, the contents of books would be atomized into bits of information to be aggregated, and the authors themselves, the feeling of their voices, their differing perspectives, would be lost.\nWhat all this comes down to is that the very idea of artificial intelligence gives us the cover to avoid accountability by pretending that machines can take on more and more human responsibility. This holds for things that we don't even think of as artificial intelligence, like the recommendations made by Netflix and Pandora. Seeing movies and listening to music suggested to us by algorithms is relatively harmless, I suppose. But I hope that once in a while the users of those services resist the recommendations; our exposure to art shouldn't be hemmed in by an algorithm that we merely want to believe predicts our tastes accurately. These algorithms do not represent emotion or meaning, only statistics and correlations.\nWhat makes this doubly confounding is that while Silicon Valley might sell artificial intelligence to consumers, our industry certainly wouldn't apply the same automated techniques to some of its own work. Choosing design features in a new smartphone, say, is considered too consequential a game. Engineers don't seem quite ready to believe in their smart algorithms enough to put them up against Apple's chief executive, Steve Jobs, or some other person with a real design sensibility.\nBut the rest of us, lulled by the concept of ever-more intelligent A.I.'s, are expected to trust algorithms to assess our aesthetic choices, the progress of a student, the credit risk of a homeowner or an institution. In doing so, we only end up misreading the capability of our machines and distorting our own capabilities as human beings. We must instead take responsibility for every task undertaken by a machine and double check every conclusion offered by an algorithm, just as we always look both ways when crossing an intersection, even though the light has turned green.\nWHEN we think of computers as inert, passive tools instead of people, we are rewarded with a clearer, less ideological view of what is going on -- with the machines and with ourselves. So, why, aside from the theatrical appeal to consumers and reporters, must engineering results so often be presented in Frankensteinian light?\nThe answer is simply that computer scientists are human, and are as terrified by the human condition as anyone else. We, the technical elite, seek some way of thinking that gives us an answer to death, for instance. This helps explain the allure of a place like the Singularity University. The influential Silicon Valley institution preaches a story that goes like this: one day in the not-so-distant future, the Internet will suddenly coalesce into a super-intelligent A.I., infinitely smarter than any of us individually and all of us combined; it will become alive in the blink of an eye, and take over the world before humans even realize what's happening.\nSome think the newly sentient Internet would then choose to kill us; others think it would be generous and digitize us the way Google is digitizing old books, so that we can live forever as algorithms inside the global brain. Yes, this sounds like many different science fiction movies. Yes, it sounds nutty when stated so bluntly. But these are ideas with tremendous currency in Silicon Valley; these are guiding principles, not just amusements, for many of the most influential technologists.\nIt should go without saying that we can't count on the appearance of a soul-detecting sensor that will verify that a person's consciousness has been virtualized and immortalized. There is certainly no such sensor with us today to confirm metaphysical ideas about people, or even to recognize the contents of the human brain. All thoughts about consciousness, souls and the like are bound up equally in faith, which suggests something remarkable: What we are seeing is a new religion, expressed through an engineering culture.\nWhat I would like to point out, though, is that a great deal of the confusion and rancor in the world today concerns tension at the boundary between religion and modernity -- whether it's the distrust among Islamic or Christian fundamentalists of the scientific worldview, or even the discomfort that often greets progress in fields like climate change science or stem-cell research.\nIf technologists are creating their own ultramodern religion, and it is one in which people are told to wait politely as their very souls are made obsolete, we might expect further and worsening tensions. But if technology were presented without metaphysical baggage, is it possible that modernity would not make people as uncomfortable?\nTechnology is essentially a form of service. We work to make the world better. Our inventions can ease burdens, reduce poverty and suffering, and sometimes even bring new forms of beauty into the world. We can give people more options to act morally, because people with medicine, housing and agriculture can more easily afford to be kind than those who are sick, cold and starving.\nBut civility, human improvement, these are still choices. That's why scientists and engineers should present technology in ways that don't confound those choices.\nWe serve people best when we keep our religious ideas out of our work.\nCorrection: \n\n","141":"BREATHES there a computer user so unflappable that he or she has never, at least mentally, slammed a fist onto the desk with the exclamation, ''You dumb machine!''? Emotions aside, all computers are dumb. In fact, they are more than dumb, since even that pejorative implies some degree of intelligence, and these machines are truly mindless.\nOn the whole, the computer industry has turned the tabula rasa of the electronic ''mind'' to its advantage. Turn on the computer, load some word-processing software and one has a machine capable of dealing with all the words one can write. Load a spreadsheet program and at one's disposal is an incredibly fast and versatile number cruncher. But for all that the computer can do once its circuital brain has been filled with the wherewithal, none but the most anthropomorphically inclined among us would consider it really smart or, for that matter, the least bit intelligent.\u00a0\nThis state of affairs is about to change, however, if one is to believe the industry savants. Artificial intelligence, or A.I., is on the way. Quintessential ''yuppie'' companies like Lotus are letting their coffers overflow into A.I. startup companies. Even such solid old-line concerns as General Electric, not generally noted for taking a flier, are pouring millions into the field. Then, of course, there are the Japanese. The Japanese have proclaimed as their goal the production of a machine possessed of artificial intelligence by the year 1992. Dubbed a fifth-generation computer, it is intended to bury Silicon Valley once and for all.\nBy now we have been so conditioned to accept the superiority of Nippon's nascent technological lead that if the Japanese are pursuing the matter, we conclude that it must be for real. Well, real it is, the pursuit, that is. But the outcome of the chase is much more in doubt.\nSince the dawn of history, perhaps out of loneliness, or perhaps to avoid the ultimate conceit of considering himself some kind of unique intellectual pinnacle in the universe, man has attributed intelligence to animate and inanimate objects from dung flies to yarrow sticks and mountain spirits.\nBy the Middle Ages, artificial intelligence had really begun to take large strides forward. Pope Sylvester II is reported to have had a fortune-telling robot that would sagely answer yes or no to questions presented to it, and in the 13th century, Ramon Lull, a missionary in North Africa, came across a ''thinking machine.'' Called the Zairja, it was capable of discoursing on various philosophical subjects, a feat described in Lull's major work, ''Ars Magna.'' Metal disks, their surfaces divided into numbered segments, were spun around, and as they came to a stop, the numbers displayed were noted and matched against various statement tables to produce suitably nebulous philosophical answers to any questions posed. IN what might be described as the early romantic era of artificial intelligence, Prague's Rabbi Judah ben Loew created, in 1580, an artificial human called Joseph Golem from the clay of the banks of the River Moldau. No doubt every bit as inspired as Dvorak's melodies named for the same river, Golem was an all-round robot capable of monitoring what the Gentiles were doing as well as tidying up around the house.\nOver the next couple of centuries, the yin and yang of artificial intelligence and robotics wove in and out of the arts. Olympia, the mechanical woman in E. T. A. Hoffmann's ''The Sandman,'' the mechanical doll in Delibes's ballet ''Coppelia'' and, of course, the monster created by the young medical student Frankenstein in Mary Wollstonecraft Shelley's novel all reflected more man's yearning to create artificial intelligence than any concrete development.\nThe evolution of A.I. took a giant step forward after the advent of the digital computer in the 1940's. Here at last, said the proponents of A.I., would be found the way to instill intelligence in machines.\nDetractors merely harrumphed. Typical of their skepticism was a remark made by Sir Geoffrey Jefferson and quoted in the classic essay ''Computing Machinery and Intelligence'' by Alan Turing. ''Not until a computer can write a sonnet or compose a concerto because of thoughts and emotions felt and not by chance fall of symbols, could we agree that a machine equals brains,'' Sir Geoffrey said. ''No mechanism could feel (and not merely artificial signals, an easy contrivance) pleasure at its successes, grief when its valves fuse, be warmed by flattery.'' For him, then, what was needed was a modern-day R2D2, or at least a HAL.\nIn the computer's defense, Alan Turing rightly pointed out that under such strict rules none of us could ever know for certain that any other person thinks. After all, if a machine states that it feels pleasure, how is that statement different from a human being saying he or she is happy? We can assume that the other person has approximated the same feeling that we have experienced when he or she makes such a statement. But how can we prove it? We can't, according to Mr. Turing.\nTo provide a workable criterion for determining the existence of artificial intelligence, he devised an elegantly simple approach that avoids the pitfalls of actually having to define intelligence. In the Turing test, as his standard has become known, if a human can question a machine through another human intermediary, can receive the machine's answers through that intermediary and then is unable to determine whether the object being questioned is a human or a machine, then the machine can be considered intelligent.\nModern researchers in artificial intelligence do not claim to have reached this ultimate stage. But those in the field do assert that development has reached a point where the machine can at least fool some of the people some of the time. Within a very narrow spectrum of activity, say in the oil-drilling technologies, an expert system, one programmed with an encyclopedic knowledge gleaned from the experience of the leading lights in the field, might well pass the Turing test today, for a time.\nSuch an expert system, at least one with the necessary information already instilled as opposed to a mere software framework ready to receive its education, would have costs far beyond the means of the individual. But a number of inexpensive software packages claiming to have a basis in A.I. are coming onto the market. With the ghost of Alan Turing looking over my shoulder, I will review some of those systems next week.\n","142":"Ever since the computer scientist John McCarthy coined the term artificial intelligence in 1955, the field has gone through cycles of boundless optimism and sobering disillusion. Yet until recently, the supercomputer was the go-to operator of machine intelligence - both in science fiction (HAL, in Stanley Kubrick's \"2001: A Space Odyssey\") and in reality (Watson, IBM's \"Jeopardy!\" champ).\nBut three forces have transformed that assumption in the last few years: the surge in data of all kinds, rapid progress in software to find patterns and insights in data, and advances in the technology of data processing, storage and communication.\u00a0\nNow, computing intelligence can be dispersed globally, marshaled and aggregated as necessary, from far-flung data centers in the digital cloud. Google led the way, showing the power of data-driven artificial intelligence delivered over the cloud, not only in search but also in tasks like language translation and computer vision. Artificial intelligence run through the cloud is now the dominant approach used by researchers at technology companies, universities and government labs.\n\"We're seeing a rebirth of artificial intelligence driven by the cloud, huge amounts of data and the learning algorithms of software,\" said Larry Smarr, founding director of the California Institute for Telecommunications and Information Technology.\nThe emerging global network, Mr. Smarr said, will be the equivalent of a \"planetary computer.\" What might that mean, in terms of its practical effect on everyday life?\nMr. Smarr points to the recent movie \"Her\" as a fairly accurate glimpse of what will be possible in the not-too-distant future. The protagonist, Theodore Twombly (played by Joaquin Phoenix), has clever software on his smartphone that seems to know all about him. It has read his email, his text messages and the books, magazines and everything else he has read. It has seen all the movies he has seen. It knows his buying habits and preferences. It retrieves information and answers at his whim. It communicates with him by talking, conversationally (in the voice of Scarlett Johansson).\n\"That's where we're headed,\" Mr. Smarr said. \"That kind of hyper-personalized assistance is going to be common in 10 years. It will appear to be on your smartphone or Google Glass, but it will actually be in the cloud.\"\nSome predict that we are headed much further. Ray Kurzweil, an inventor, scientist and futurist, joined Google in 2012 to work on an artificial intelligence project known internally as Google Brain. Mr. Kurzweil has embraced a concept called \"the singularity,\" which is essentially when computing intelligence surpasses human intelligence - not just on isolated pursuits like playing chess or \"Jeopardy!,\" but really leaving human intelligence in general in the dust.\nMr. Kurzweil wrote a 2005 book on the subject, \"The Singularity Is Near,\" and welcomes the prospect, asserting that the supersmart digital intelligence will enrich the life of humans. Others are skeptical, both that it will happen and that it will be a good thing if it does. But in any case, the singularity is a ways off. Mr. Kurzweil puts it at 2045.\nJeff Dean, a research fellow at Google, focuses on accelerating the progress of artificial intelligence in tasks like computer vision and understanding the meaning of words. Until a few years ago, for example, Google image searches were executed mainly by identifying the text labels affixed to pictures. Today, many images are identified by software analyzing the patterns of digital pixels in a picture or video. And, Mr. Dean said, the technology can pick out a leopard in a picture, and know it is not a lion or a cheetah, recognizing the distinctive pixel patterns of various big cats.\nMobilizing the firepower of Google's large cloud data centers, Mr. Dean said, enables his team to \"bring a lot of computation to bear on these kinds of problems.\"\nUnderstanding not just words but also their context and meaning is another big challenge. Current search technology does a good job of responding helpfully to a few words, either typed or spoken. So, Mr. Dean noted, when planning a trip in Italy, search engines do well with \"train from Rome to Florence\" or \"hotel in Florence.\"\nThe ideal, Mr. Dean explained, would be to tell Google that you want to plan a two-week vacation to Italy. Then the smart technology starts working on the trip. The options it offers are based on its ability to understand both Italy and the traveler, like someone who has volunteered personal information. Maybe you have two young children, want to stay in the country in Tuscany and like to hike. \"You kind of want Google to know you,\" Mr. Dean said.\nHis team's advanced artificial intelligence research, known as deep learning, is \"loosely inspired by knowledge of how the brain works,\" Mr. Dean said. But there are things the human brain does that silicon-based computing still only aspires to. The brain, Mr. Dean noted, is amazingly flexible and efficient, firing up and shutting down memory systems, so that the part of your brain that holds information on English literature or taking out the trash shuts down when you look at a picture of a leopard.\n\"We don't have a great handle on how to build those kinds of dynamically evolving memory systems,\" Mr. Dean said. \"Google and others are working on that, but it's really nascent.\"\nAt IBM's laboratories, the researchers emphasize that their mission in developing \"cognitive\" computing technology is to build systems that can learn from and interact with people rather than try to replace them. Guruduth S. Banavar, director of IBM's cognitive computing research, said the result should be \"way better than either a human or a computer system can do alone.\"\nMr. Banavar prefers to think of this smart technology as intelligent augmentation, or I.A., instead of artificial intelligence, or A.I.\nWatson is a pioneering cognitive system that IBM is now retooling for mainstream industries like medicine, finance and customer service. In each field, Mr. Banavar said, Watson will serve as an adviser. This year, IBM set up a separate Watson business unit and said it would invest $1 billion in it.\nWatson itself has changed with the times. It will be a cloud service. \"Data is fundamentally distributed today, so you have to be global to leverage information wherever it is,\" Mr. Banavar said. \"Cloud is the delivery mechanism.\"                      \n","143":"SAN FRANCISCO -- This is the year artificial intelligence came into its own for mainstream businesses, at least as a marketing feature.\nOn Sunday, Salesforce.com, which sells online software for sales and marketing, announced it would be adding A.I. to its products. Its system, called Einstein, promises to provide insights into what sales leads to follow and what products to make next. \n  Salesforce chose this date to pre-empt Oracle, the world's largest business software company, which on Sunday evening began its annual customer event in San Francisco. High on Oracle's list of new features: real-time analysis of enormous amounts of data. Oracle calls its product Oracle A.I.\u00a0\n  Elsewhere, General Electric is pushing its A.I. business, called Predix. IBM has ads featuring its Watson technology talking with Bob Dylan. These moves, along with similar projects at most major tech companies and consulting firms, represent years of work and billions in investment.\n  There are big pushes in A.I. in agriculture, manufacturing, aviation and pretty much every other sector of the economy.\n  It's all very exciting, the way great possibilities are, and clearly full of great buzzwords and slogans. But will other companies see any value in all this or understand if A.I. has value for them?\n  ''No one really knows where the value is,'' said Marc Benioff, co-founder and chief executive of Salesforce. ''I think I know -- it's in helping people do the things that people are good at, and turning more things over to machines.''\n  Mr. Benioff wasn't selling Einstein's capabilities short. He was talking about the long-term value of artificial intelligence, which is passing through a familiar phase -- a technology that is strange and new, that sometimes overpromises what it can do and is headed for uses not easily seen at the start.\n  Cloaked inside terms like deep learning and machine intelligence, A.I. is essentially a series of advanced statistics-based exercises that review the past to indicate the likely future, or look at current customer choices to figure out where to put more or less energy.\n  Perhaps a better question than ''What is the value?'' of this explosion of advanced statistics is ''Why now?'' That shows both the opportunity and why many companies are scared about missing out.\n  Much of today's A.I. boom goes back to 2006, when Amazon started selling cheap computing over the internet. Those measures built the public clouds of Amazon, Google, IBM and Microsoft, among others. That same year, Google and Yahoo released statistical methods for dealing with the unruly data of human behavior. In 2007, Apple released the first iPhone, a device that began a boom in unruly-data collection everywhere.\n  Suddenly, old A.I. experiments were relevant, and money and cheap data resources were available for building new algorithms. Ten years later, computing is cheaper than ever, companies live online and in their phone apps, and sensors are bringing even more unruly data from more places.\n  Amazon, Google and the rest have exceptional A.I. resources for sale, but many older companies are wary of turning their data over to these upstarts. That, along with fear of a competitor getting on top of A.I. first, is a big motivation for some to try things out.\n  Salesforce is selling Einstein as a system that can work predictive magic without having to look at your data, in what Mr. Benioff calls a ''democratizing'' move that will create millions of A.I. users who are not engineers.\n  He said this on his way to attend a series of customer focus groups around the country, however -- strong evidence that customers don't get it yet, even if they're willing to try it.\n  ''There's fear of Google and Microsoft controlling everything, and there's a desire to apply A.I. to anything that's digital,'' said Michael Biltz, managing director of Accenture's technology vision practice. ''People are going to have to experiment, most likely first on pain points like security and product marketing.''\n  How will we know when the A.I. revolution has taken hold? A technology truly matures when it disappears. We don't marvel at houses with electricity now, or the idea of driving to work at 60 miles an hour. We can say ''phone'' and mean a hand-held computer with NASA-level processing power and a professional-quality camera for taking selfies with our drones.\n  A.I. is probably heading for the same places, invisibly sorting through lots of data everywhere to continuously update and automate most of our lives. Goodness knows what the weird new tech thing will be about at that point.\n\n\n\n","144":"AN-Although the computer in its short life span of three decades has made staggering advances, becoming capable of prodigious calculations that have led people to endow it with anthropomorphic characteristics, it is, in fact, a dumb machine. A computer does exactly what it is told - nothing more, nothing less. When something goes awry, the explanation that ''the computer made a mistake'' is wrong. Humans made the mistake. The computer, as any programmer will tell you, follows instructions perfectly. The mistakes occur when the instructions are wrong. But what if computers were intelligent, like HAL, in the Stanley Kubrick film ''2001: A Space Odyssey''? Imagine a computer so intelligent that it would remind us of ourselves or, indeed, intimidate us. Imagine a machine that would compel us to use the pronoun ''who'' when talking about it, a machine who could think, reason, make logical deductions, remember past experiences, solve present problems by recognizing analogous situations. Would not such a computer hold the capability that has always distinguished man from other forms of life? ''I think, therefore I am,'' Rene Descartes, the 17th-century French philosopher and mathematician, said.\u00a0\nNow, however, some of the world's philosophers, mathematicians, linguists, psychologists, electrical engineers and computer scientists are working to create machines to do just that. They are attempting to rewrite Descartes, and, they believe - though there are some dissenting voices among them - that they are beginning to succeed to an extent inconceivable only 20 years ago. While many areas of human endeavor have currently seemed to flounder - such as understanding and dealing with economic systems or correcting social injustices - research and technical developments in the scientific field of artificial intelligence have exploded, as indeed, they have in other areas of science, like planetary space exploration or molecular-genetics microbiology. As a result, artificial-intelligence researchers are developing computers that can listen to spoken sentences and grasp their meaning; that can read news stories and write succinct, accurate, grammatical summaries; that employ robots, who never get bored, to work on assembly lines; that assemble data about a sick person - and suggest a diagnosis .\nThe human capability for intelligent behavior is so diverse and powerful that most of these new computers are, in comparison, mere toddlers that can scarcely speak and stack blocks. But man-thecreator says his machine is going to become much smarter.  Slowly, steadily the computer experts are moving forward with their theories for accomplishing that goal, and they are putting these theories to work with actual programs.\nIf they succeed - and it's a matter of when, not if - life may never be the same. In the future, artificial intelligence could produce powerful assistants who manage information for us, reading books, newspapers, magazines, reports; preparing summaries, avoiding those things the computer knows won't interest us, keeping abreast of everything that happens in the world, seeing that nothing we really should know about escapes us. These intelligent computers could analyze the decisions that face us, searching libraries of knowledge for facts that will help with a decision and then presenting us with suggested courses of action and the probable consequences. They could understand specialized knowledge and know how to put it to work in some of the highly skilled areas in which we humans function - law and medicine, for example. Artificial-intelligence programs could guide robots that do our dirty work, collecting garbage, mining coal,cleaning up the radiation of a damaged nuclear reactor.\nBut the real promise of artificial intelligence is even greater - that it will become an incredible tool for extending man's intellect further than was ever dreamed, for lifting human creativity itself to new heights of achievement. In fact, there is now talk in the scientific community that the forging of a man-machine symbiosis could represent the next great development for man, equivalent in its portent, perhaps, to walking erect or building the first tool.\n''The amount of intelligence we humans have is arbitrary. It's just the amount we have at this point in evolution,'' says Marvin Minsky of the Massachusetts Institute of Technology. ''There are people who think that evolution has stopped and that there can never be anything smarter than us.'' That says Minsky, who is one of the founding fathers of the artificial-intelligence research field, is like saying that a person can't build a house any higher than he can reach.\nThere is a dark side of the forecast, however. If this new technology could be subverted, enormous evil could result. Computers that understand speech could be programmed to listen in on virtually every human conversation, for example, and the computers themselves could be programmed to know whether what was said would be of interest or be alarming to the authorities - and, if so, to sound the alert. Then there is the ultimate fear that the computers will conspire to take over.\nThis has been the stuff of science fiction for years. In the original Arthur Clarke-Stanley Kubrick screenplay for ''2001,'' some of which never appeared in the final version of the movie, computer HAL eavesdrops on a mutinous conversation between two crew members.  He kills one. The other, Dave Bowman, orders the computer to give him manual control of the crew hibernation system, which will permit him to revive other members of the crew being held in suspended animation. The computer refuses, retreating behind its rulebook, citing ''subroutine C1532-4.'' ''HAL, unless you follow my instructions, I shall be forced to disconnect you,'' Bowman threatens.  ''I know that you've had that on your mind for some time now, Dave,'' the computer replies in his monotone. ''But it would be a crying shame, since I am so much more capable of carrying out this mission than you are. And I have such enthusiasm and confidence in the mission.''\n''Listen to me very carefully, HAL,'' Bowman says. ''Unless you immediately release the hibernation control and follow every order I give from this point on, I will immediately go to control central and carry out a complete disconnection.'' Frightened, HAL capitulates.  ''Look, Dave, you're certainly the boss. I was only trying to do what I thought best. I will follow all your orders. Now you have manual hibernation control.''\nThe scene in the 1968 film was entertaining but, of course, it was totally fictional. There was no question of this really happening; and any serious discussion of such eventualities seemed wildly speculative, alarmist. But one generation's wild speculation is turning out to be another's probability. In the 50's computers were but crude devices that filled large rooms with row upon row of vacuum tubes and had little computational power. Who would have predicted that within three decades the transistor and then the integrated circuit, or electronic ''chip,'' would be developed? This chip consists of many layers of silicon, silicon oxide, metals and other materials sandwiched together. Microscopic electrical circuits are etched upon each layer by electron beams and other techniques, so that a single chip can contain thousands of electronic components tha t act together.\nThis little chip has led to the creation of machines so powerful that the entire computational capacity of that first computer can now be contained in a device that can rest on the tip of a finger.  Given the astonishing pace of scientific development, who can say how quickly these questions about artificial intelligence will cease to be futuristic abstractions?\nCertainly, when artificial-intelligence scientists gather for a meeting, it is just these questions they debate after hours while relaxing over a beer or dinner. What they, in fact, now conceive of as being a far more realistic threat than the chance of some nonfictional HAL plotting against humans is the possibility that the human race will become too dependent upon its machines; that we will lack the ability to understand why they reach the decisions they do and take the actions they do. Finally, there is the fear that machines could come to possess all the capabilities that have set humans apart. Will there be machines that will intrude into - or supplant -the province of real intellect, a province of knowledge, intelligence, value judgments, decision making, esthetics and, most of all, emotions? If a computer could ultimately be programmed to exhibit them all, would we then be in danger of programming ourselves out of existence? By permitting the creation of intelligent machines, would we assure the immortality of m achines and unwittingly sow the seeds of our own extinction?\nIn the minds of many of the experts in the field of artificial research, the question is no longer whether computers can ultimately be programmed to exhibit human attributes, but rather how to do the programming.\nOne irony of the current research work is that the most difficult problems arise in the attempt to create machines that mimic the simplest elements of human behavior, while the successes have been in modeling some of the highly specialized skills that people develop.  Artificial-intelligence computer programs that simulate eye-to-hand coordination by hooking television cameras to computers and mechanical arms do not perform as well as an infant in diapers playing with blocks on a living-room carpet. The best programs written to recognize spoken words and understand short sentences function at the level of a child just learning to speak. On the other hand, the computers can now perform extremely complex exercises that humans become good at only after years of effort-playing chess well, for example, or diagnosing disease.\nDeveloping these types of expert systems is a major part of the artificial-intelligence research work at such places as S.R.I.  International in Menlo Park, Calif. One of its creations is PROSPECTOR, an electronic assistant geologist.\nTo build PROSPECTOR, the S.R.I. International researchers have worked with experts in geology to codify the knowledge that a geologist uses in studying a particular region. So PROSPECTOR can be given a list of types of rocks and configurations clearly present in the outcropping of a region that his boss has under study. PROSPECTOR then lists the types of ore deposits that might be present and asks the geologist to rule out those he feels don't apply in this situation. The computer's questions become progressively more specific. ''To what degree do you believe that there is hornblende with little or no secondary biotite in Zone 1?'' Finally, PROSPECTOR reaches a conclusion: ''There is a probability that a copper ore deposit exists in the region.''\nIn these so-called ''expert systems'' programs, the researchers analyze a specialized human skill, examine how the human expert goes about bringing the skill to bear on a problem, then attempt to duplicate it.\nA program called INTERNIST has been developed at the University of Pittsburgh by Drs. Jack D. Myers and Harry E. Pople. They focused on internal medicine, studying how an expert medical diagnostician approaches a patient. INTERNIST asks the doctor using it for specific information about the patient: disease symptoms, items from the patient's medical history, laboratory and other test results and so on. As the data on the patient build up, INTERNIST, just like a real doctor, discards some diagnostic possibilities and explores others further. If liver disease seems to be a possibility, INTERNIST will begin asking questions about that possibility. Ultimately, a diagnosis is suggested. The Pittsburgh scientists say they have already codified the majority of human illnesses, and have about 175 more such programs to write. ''We hope to begin field trials for INTERNIST here at our own institution by next spring, and to send it out to other institutions within two years,'' says Dr. Myers.\nMuch of the funding for artificial-intelligence research projects has come from the military, which is directly interested in its applications. S.R.I. International is now working on computer programs that might assist electronic-warfare officers. An ''E.W.  officer'' could monitor his aircraft's electronic-surveillance equipment and weapons systems, instantly eval uating data being received from other allied military positions-which conceivably could deal with such matters as whether an enemy surface-to-air missile site's radar is tracking the aircraft , whether a missile has been launched at it and what countermeasures to take. The plane's computer will have the capability to digest information given the E.W. officer during a preflight briefing, the (word illegible) help during the flight to make rapid decisions. ''Ultimately, such a program could give the pilot advice about what to do next,'' says Nils Nilsson, head of S.R.I. International's artificial-intelligence program.\nComputer programs to process and understand information-the written and spoken word-are being created at Yale University's artificial-intelligence laboratory. Yale has focused on how to represent knowledge in a computer so that the link between words and ideas can be made and lead, in turn, to logical conclusions.\nIts FRUMP program reads short news stories and writes perfectly grammatical, one-sentence summaries in English, Spanish and Chinese.  Not that it doesn't make some mistakes. FRUMP was asked to summarize a wire-service story about the shooting of the San Francisco Mayor in 1978. The story had reported that the ''shooting shook San Francisco,'' and thus the computer concluded that there had been an earthq uake in California. On another occasion, a Yale computer mistakenly su mmarized a news story twice. The story was about Filipino terr orists kidnapping a missionary, and the computer, falsely perce iving a pattern because of its own error, concluded that''terrorists tend to kidnap missionaries.''\nBut, although obviously not yet ready for night rewrite, FRUMP is normally right on the job. Presented with a 600-word story about the death in 1978 of the anthroplogist Margaret Mead, the computer writes the following summary: ''Anthropologist Margaret Mead, 76, has died from cancer.'' Given a 400-word story about a fire in a boarding house, FRUMP summarizes: ''A fire in a boarding home in Farmington has killed 25 people and injured 7 people and destroyed a building.''\nTo write this summary, the computer was programmed with a vocabulary that both defined words and represented their relationship to other words. The words were represented symbolically within the computer so that the FRUMP program ''knew'' that fires can damage property and injure people. It had to know the difference between injury and death. The U.P.I. story said that the fire had killed 25 people and ''injured 7 others.'' FRUMP had to understand the concept of the pronoun ''others,'' knowing that in this case it referred to residents of the home but that in a different situation ''others'' could mean something else.\nAnother Yale program, named CYRUS, has been analyzing news stories about Cyrus Vance, former Secretary of State. When asked questions about Vance's activities, CYRUS searches its data base and formulates answers. Asked, for example, if Vance's wife ever met the wife of the Soviet leader, Leonid Brezhnev, CYRUS decides that the answer to such a question probably resides in analysis of diplomatic social functions where both Vance and Brezhnev have been present, since their wives probably also would be there. It discovers that the two men did, indeed, attend the same reception with their wives and concludes that Mrs. Vance and Mrs. Brezhnev probably have met, since the wives of the two men undoubtedly would be introduced at a diplomatic reception.\nIndustry scientists are also interested in artificial intelligence. Reseachers at Schlumberger, an oil-well services company, are developing programs that can read data generated about geological formations through which a drilling rig is passing, searching for evidence of oil-bearing formations. Such intelligent programs will speed this analysis, calling on a human geologist for assistance when interesting signs are encountered, but sparing humans the hours of drudgery and expense required in examining all the data.\nRobot like equipment is already common in manufacturing, but the machines are still dumb and blind. They are for the most part ''pick and place'' arms that perform the same tasks over and over again, just as the typical computers now do. But artificial-intelligence programs that focus on robotics have been undertaken at several research centers, in some cases backed by substantial funding from such corporations as Westinghouse and General Motors. These scientists are creating robots whose arms are linked both to television cameras and computers so that the robots' arms can react to changing situations on the assembly line and perform diverse tasks.\nAt Carnegie-Mellon University in Pittsburgh, researchers are teaching robots to assemble electronic circuitry. The robots will know which tiny electronic component to pick up and insert into which location on a circuit board. They will, incidentally, know which type of board happens to be on the assembly line at the moment. Another Carnegie-Mellon project involves programming a computer to inspect printed circuit boards and detect defects in the incredibly intricate patterns. Although the so-called ''dumb'' computer programs can do this now-they can compare a master board with a newly manufactured board-the intelligent program will be programmed with its own rules-its own set of values, if you will-about what constitutes defects in any circuit board. A nick or cut ora point at which two lines are too close together will be properly adjudged unacceptable.\n''We're trying to build an intelligent program that doesn't have to know what the overall circuit board looks like, but instead looks at the lines in the board and detects abnormalities,'' says Raj Reddy, head of Carnegie-Mellon's robotics program. No one now doubts that they will do it; it's merely a question of how soon.  Artificial-intelligence research was born in the early 1950's, a period when a young man from the Bronx named Marvin Minsky was just beginning his career as a scientist. In New York, Minsky had been a student at Fieldston, then the Bronx High School of Science. He studied physics as an undergraduate at Harvard and went to Princeton where he earned a doctorate in mathematics in 1954. Harvard then invited him to return as a junior fellow, a prestigious three-year appointment that has begun the scientific career of such people as E.O. Wilson, the eminent sociobiologist. In 1957, Minsky joined M.I.T. Throughout his education, he was known both as somewhat of a scientific gadfly and as a dabbler, a man who puttered about in biology, physics and psychology.\n''I remember in those years thinking clearly that there were three nice problems in science which seemed worth solving,'' he says.  ''They were physics, genetics and how the mind works. Lots of people were working on physics and doing very nicely, and the mechanisms of genetics I could see probably wouldn't be too hard and not too exciting. But I couldn't understand why everybody wasn't interested in how the mind works. It seemed so obvious that that was the most interesting question.''\nSince its beginnings, artificial-intelligence scientists have sought to understand what the brain does with its information. And that remains the central thrust of artificial-intelligence research today-studying human behavior and what happens to information when the brain processes it to produce intelligence, and then trying to mimic this with a computer.\nThis information-processing approach is a ''black box'' method, so called because science lacks the tools to examine the brain at work, lacks the ability to light it up and look inside at exactly how it functions. The approach is similar to finding an electrical engineer who has never seen television and has no knowledge of television technology, seating him before a color television set and asking for an explanation of how it works without permitting him to look at the circuitry inside. The engineer would have to solve the problem by studying the signals that enter the set from its antenna and comparing them with what appears on the screen.\nAs the artificial-intelligence field has developed, probing this black box of intelligence has focused on two areas. One is how human memory is organized so that among vast amounts of information stored in the brain a particular piece of information is quickly retrieved when the mind is confronted with a problem. The other is how retrieval and processing of that information is carried out to produce an answer, whether the problem is something as seemingly simple as opening a door and walking through it or something more abstract, such as realizing that neither of two political candidates was the one you wanted to see become the next President and then voting for one as a way of voting against the other.\nSince the first computers appeared, their internal structure has been based upon serial processing of data-that is, fetching pieces of data and acting upon them one at a time, like a line of people passing through a door single file. But new advances in computer design are raising the possibility of parallel or multisystems-systems that may fit together almost like spider webs-with several central processing units acting simultaneously, and in concert.\nMinsky's latest views, which represent a substantial departure from his ideas of a decade ago, embrace both parallel processing and the possibility that the brain has several dozen processing centers that handle different types of knowledge simultaneously.\nIn his office at the Massachusetts Institute of Technology, Minsky recently talked about intelligence and, as he did, quick smiles flicked across his face again and again. Somehow the smiles conveyed the image of those multiple processors at work, of a mind attacking several problems at once, one of which, a minor one probably, was sharing his ideas with a visitor who had plunked a tape recorder in front of him. There is a decidedly dreamy air about him. His colleagues say he often will absently walk away from a conversation in midsentence, not because he is bored but because his mind has fixed upon something else. At the same time, he has a reputation for being outrageous and argumentative. A meeting of artificial-intelligence scientists presenting scholarly papers is likely to become contentious if Minsky is present.\nMarvin Minsky, at 53, is a Renaissance man; he is an accomplished musician and composer whose specialty is 18th-century Baroque music. His cluttered, rambling Victorian house in Brookline, a suburb of Boston across the Charles River from Cambridge, contains two pianos and two organs, as well as a battery of devices to make electronic music. A trapeze hangs from the ceiling of a large room that serves both as a living room and as a music room and has a massive fireplace along one wall. On one recent Saturday, he was at work with two helpers excavating a portion of his basement where he is building himself a workshop. He once taught himself precision machining of metal parts. ''It was fun,'' he remembers. Several years ago, he spent a year designing a new type of computer that he and a colleaguethought might lend itself to programming as a teaching aid for children. The project was unsuccessful, but Minsky says, ''I enjoyed immersing myself in the hardware. I just about memorized the Texas Instruments catalogue.''\nFriends speak of his encyclopedic knowledge of fields in which he has never studied advanced courses. In recent months, he has been engaged in thinking about future space exploration , ''hanging about the halls of NASA in Washington,'' as he describes it. In November he traveled to the Jet Propulsion Laboratory in Pasadena, Calif., to experience firsthand the exploration of Saturn by the Voyager 1 spacecraft. ' 'Marvin is a great thinker,'' says Daniel Bobrow, a former student who now works in artificial-intelligence research at the Xerox Cor poration's research center in Palo Alto, Calif. ''Marvinis brilliant, insightful and neglectful,'' Bobrow says, referring to Minsky's penchant for thought rather than detail.  Minsky is one of four men who are considered the fathers of artificial intelligence. The others are John McCarthy of Stanford University, and Allen Newell and Herbert Simon of Carnegie-Mellon University. McCarthy, who was a contemporary of Minsky's at Princeton, where both men earned doctorates in mathematics, is generally credited with inventing the name ''artificial intelligence. '' Simon and Newell, although each has sometimes worked alone and made distinct contributions to the artificial intelligence field, are almost always spoken of as a team, an entity. Because of the influence of these four men, their three institutions are invariably mentioned as the country's top centers for artificial-intelligence research, although many others now exist.\nAlthough their work overlaps to some extent, the three approaches that Minsky, McCarthy and the team of Simon and Newell take represent the basic scope of the research in artificial intelligence. When Minsky discusses his idea of multiple processing centers working on a problem he draws an analogy with how a large corporation functions.  The chairman, who is analogous to a processor with a large view, decides that profits must be increased. He turns the problem over to some of the company's top executives, who are analogous to parallel processors. These executives determine that the problem's solution rests with more aggressive marketing. The marketing department determines that demand for the company's product is excellent but that the company's manufacturing plants aren't turning out enough products, thus lowering sales volume and, hence, profit. The problem is then sent over to the production department, and so on. McCarthy believes that the route to making machines intelligent is through a rigorous, formalistic approach in which all acts that make up intelligence are reduced to sets of logical relationships or axioms that can be precisely expressed in mathematical terms.\nSimon and Newell approach intelligent computer programs through a concept they call ''productions.'' They feel that the brain is organized so that all the knowledge stored there is available to be brought to bear all the time. They see a particular piece of knowledge as existing with a rule about how it is used. A piece of knowledge can be likened to the substance in a container, and the rule is a handle by which the container is picked up. When you encounter a specific situation, the brain reaches for the correct handle to pull out the piece of information that is appropriate to deal with that situation.\nIf these descriptions of the main thrusts of theoretical thinking in artificial intelligence sound as if they come from the proverbial group of blind people examining an elephant, then you understand the chief difficulty confronting anyone who ventures into artificial intelligence. It is a field noted for its vagueness, a science plagued by lack of precision in a world accustomed to scientific exactness. ''No one can work in artificial intelligence and be happy unless he is the type of person who can tolerate vagueness,'' says Roger Schank, head of Yale University's artificial-intelligence program. ''It can be a frustrating field to work in.'' Echoed by others in the field, this admission is a stark illustration of what many artificial-intelligence experts consider the poverty of their science's underpinnings. Does anybody really know what intelligence is?  In fact, isn't it fair to ask whether PROSPECTOR is really ''intelligent'' at all, despite its tremendous aid in the search for ore deposits? Isn't PROSPECTOR simply a data base of geological facts organized by human intelligence in a way that can be used rapidly? If human ingenuity has created both a computer and the set of instructions it executes, how can the equipment really be regarded as possessing the ability to think?  However cleverly it doesit, isn't the machine merely following instructions?\n''This is something that has bothered artificial-intelligence critics and many semiphilosophers,'' Minsky concedes about such accomplishments as PROSPECTOR's or FRUMP's. ''The artificial-intelligence people say, 'Of course it's thinking.' But the intuitive skeptics then say, 'How is it different from a book with things written down?' We artificial-intelligence people have been rather clumsy at answering this. There's been a lot of faith that it was somehow different.''\n''The concept of what is understanding is critical,'' Minsky says.  If true intelligence is present, he says, then a machine must be able to view a problem from more than one vantage point. ''You have to look at a problem in at least two ways and switch from one to another. When somebody says what's the difference between a machine that knows to pick up a block and put it on top of another block and a machine that really understands what it is doing, I say that the one that understands it has many ways to look at t he situation. It can look at t he block as something that is about to fall off of another block or is pretty because it's up high or is valuable because you c ould climb up higher on it. It's this network of analogies and metaphors between different things that is important.''\nIt is through such struggles as this conundrum about the nature of intelligence, and its corollary-what is memory?-that Minsky and his colleagues have arrived at building what they contend are awesome, if primitive, inanimate thinkers.\n","145":"HONG KONG -- Google pulled some of its core businesses out of China seven years ago, after concluding that government controls and surveillance ran counter to its commitment to a free and open internet.\nSince then, as China's online scene has grown and prospered, the American search giant has been looking for ways to tiptoe back in.\nOn Wednesday, it unveiled a small but symbolically significant move toward that end: a China-based center devoted to artificial intelligence. The move nods to the country's growing strength in A.I., thanks to substantial government funding prompted by Beijing's ambition of having a say in the technologies of the future.\nGoogle said the center would have a team of experts in Beijing, where the company has hundreds of employees in research and development, as well as other roles. The center will be led by Fei-Fei Li, who runs Stanford University's Artificial Intelligence Lab and leads the artificial intelligence arm of Google's Cloud business, and Jia Li, the head of research and development for the A.I. division of Google Cloud.\nThe Silicon Valley company, which announced the center's opening at a software developer conference in Shanghai, cited China's growing academic and technical contributions to the A.I. field, and said the new center would be \"working closely with the vibrant Chinese A.I. research community.\"\n\"The science of A.I. has no borders,\" Fei-Fei Li said in a post on Google's website, and \"neither do its benefits.\"\nGoogle did not disclose financial details.\nThe company is only the latest big technology name to set up an A.I. shop in China to capitalize on growing skills and lavish state support. Microsoft, IBM and other Western and domestic stalwarts are busy hiring Chinese staff members in a field with a wide variety of potential applications.\nChina's A.I. push is part of a government-driven effort to upgrade the country's technological abilities and to wean itself off foreign-made software and advanced equipment. The push has prompted Western corporate executives, and increasingly the Trump administration, to complain that Beijing unfairly nurtures their potential rivals.\nThe new Google A.I. center could deepen the company's fraught but complicated relationship with China, now home to the world's biggest population of online users.\nGoogle closed its search business in China in 2010, saying it would no longer tolerate Beijing's censorship requirements and government-linked efforts to hack the Gmail accounts of human rights activists and others. Google's services were subsequently blocked in the country, and China's internet developed its own answers to the company's products, from email and search to video-sharing and chat.\nStill, Google never left China entirely. It has an active business distributing online ads for desktop computers and mobile applications, and Chinese makers of smartphones use its Android mobile device software.\nThe two sides have shown signs of warming. Last week, Sundar Pichai, Google's chief executive, spoke at China's annual internet conference in the city of Wuzhen, saying the company did robust business helping Chinese firms seeking customers abroad. And this year, Google began offering its translation software in China.\n\"We have 600-plus employees in China, and we had a similar number in 2010,\" said Taj Meadows, a Google spokesman. \"Roughly half of them are engineers working on global products. Work on A.I. will be in a similar vein.\"\nTech figures inside and outside the country are watching whether Google opens a mobile app store there, but there has been little indication of progress on that front.\nFollow Carlos Tejada on Twitter: @CRTejada.\nPHOTO: Sundar Pichai, Google's chief executive, speaking at China's annual world internet conference in Wuzhen this month. His company has been looking for ways to re-enter the Chinese market. (PHOTOGRAPH BY ALY SONG\/REUTERS)Related Articles\n\n","146":"\"I'd like to go on a vacation somewhere warm over the summer, but only have a budget of $1,000. Where should I go?\"\nIf you search Google with a very long and human question like this, you will see a series of strange links to articles like, \"7 Frugal Habits Everyone Should Develop,\" \"Successful People Who Prove You Should Use Your Vacation Time,\" and \"How to Plan a Vacation.\"\nGoogle wants to change that, which is why the company bought the British artificial intelligence developer, DeepMind, on Monday.\u00a0\nSome in the blogosphere saw the acquisition differently, speculating that Google was buying DeepMind to help make robots and thermostats more intelligent - presumably for world domination.\nWhile this new artificial intelligence technology could be used with robots one day, possibly even in the not-too-distant future, for now, Google was hoping to continue its world domination of search.\nThis is why Facebook was also competing to buy DeepMind, as The Information noted Monday. Facebook doesn't have any humanoid robots, robotic dogs or Internet-connected thermostats. What Facebook does have is something that Google dominates: search.\nReCode first reported the deal on Sunday, saying Google had agreed to pay $400 million for the company.\nPeople who work with Google but could not be named because they were not allowed to speak publicly for the company, said the acquisition of the artificial intelligence software had nothing to do with robots, but everything to do with semantic technology and the ability to understand what people were asking for online and answer in a very human way.\nWhile a Google spokesman confirmed the purchase of DeepMind, he declined to comment about why Google had acquired the company or the type of projects the new team of researchers and scientists would be working on.\nThe DeepMind team will likely be working with Geoffrey Hinton, a computer scientist and psychologist who joined Google last year and works on the company's Knowledge Graph. Mr. Hinton is best known for his work in the 1980s when he started research in machine learning - teaching computers to put data together in patterns that are very similar to the human brain.\nWhen Mr. Hinton joined Google, he wrote on his Google Plus page that he had spent time working with the Knowledge Graph team of scientists and engineers who, he thought, had a real shot at pushing machine learning into new territory. \"I am betting on Google's team to be the epicenter of future breakthroughs,\" he wrote at the time.\nAll of this Knowledge Graph work could eventually help people search in a way that was much closer to talking to another human, but with the answers of a robot.\nDeep Mind, which is based in London, was founded by Demis Hassabis, a computer-game designer, neuroscientist and former child chess prodigy, and his partners, Shane Legg and Mustafa Suleyman.\nMr. Legg noted in a 2011 Q&A with the LessWrong blog that technology and artificial intelligence could have negative consequences for humanity.\n\"Eventually, I think human extinction will probably occur, and technology will likely play a part in this,\" Mr. Legg said. \"If a super-intelligent machine (or any kind of super-intelligent agent) decided to get rid of us, I think it would do so pretty efficiently.\"\n\n","147":"SAN FRANCISCO\nIT has long been something of a paradox that the\nsimplest tasks for the human mind - such as\nrecognizing an object, understanding speech and language, and reasoning with common sense - have been among the most baffling and taxing tasks for computers.\nNow, however, a variety of computer chips specifically designed for artificial intelligence are being developed. They promise to lead to computers that are faster and cheaper in performing such tasks as understanding language and in emulating human experts in such tasks as diagnosing diseases and offering investment advice.\nSuch developments are in the early stages. ''There are all sorts of things going on, in rudimentary form,'' said Fred Zieber, semiconductor industry analyst at Dataquest, a San Jose, Calif., market research firm. But artificial intelligence chips are so close to reality that this year, for the first time, a panel will be devoted to the topic at the International Solid State Circuit Conference, the annual scientific meeting for microchip designers that will be held in New York next month.\u00a0\n\u00a0\nGeneral-purpose computer chips, such as the microprocessors found in personal computers, are meant to be jacks of all trades. Computer scientists have long recognized that more narrowly focused chips can be masters of particular tasks. Already, such specially designed circuits have been developed for such tasks as controlling the video image on a screen and analyzing complex signals, such as those from radar and seismic tests.\nArtificial intelligence has special requirements that tax general-purpose computers. Today, researchers in artificial intelligence use machines specially tailored to process the computer language known as LISP. But these machines are expensive, ranging in price from $20,000 to more than $100,000.\nTexas Instruments Inc., which sells a LISP machine, is working under a Defense Department contract to shrink virtually the entire machine onto a single chip by 1986. Symbolics Inc. of Cambridge, Mass., another vendor of such machines, says it is also working on shrinking its machine onto a chip over the next several years. And Motorola is believed to be considering the development of a LISP co-processor that would work alongside its 68000 microprocessor and speed its handling of artificial intelligence tasks.\nThe development of such LISP processing chips would do for artificial intelligence what the devel=ropment of the microprocessor did for general computing by shrinking the essential elements of a computer onto a single chip. It would allow for a huge reduction in costs and size. It would also permit artificial intelligence to be ''embedded'' in other products, just as microprocessors are now embedded in automobile engines, refrigerators and pacemakers.\nThe Defense Department, for instance, wants to embed LISP processors in weapons such as tanks, which would be able to maneuver by themselves, and missiles, which would be able to pick out their own targets.\nOther specialized chips are being developed to handle more specialized functions that are useful for intelligent machines.\nProximity Technology Inc. of Fort Lauderdale, Fla., has developed a chip that looks for similarities between strings of characters. Today, a computer will be stumped by a simple spelling error in its instructions. An intelligent computer, however, when asked to search a data base for information on ''Los Anglees,'' would recognize that that probably means ''Los Angeles.''\nProximity's chip is specially designed to perform a mathematical operation on strings of characters to test how similar they are. The same thing can be done by programming a general-purpose computer to do it, but it would take longer. Such pattern-matching chips could also be useful for speech recognition, which often involves matching the speaker's pattern to the most similar patterns stored in the computer.\n\nLast November, the NCR Corporation introduced a chip specially designed for processing and analyzing images. One application will be in allowing robots to ''see.'' The chip, developed with the Martin Marietta Corporation, can do many calculations in parallel. Conventional circuits do one task at a time and are rapidly overwhelmed by the huge number of calculations needed to analyze images.\nFurther progress can be expected as chip technology improves. Developing a meaningful artificial intelligence chip will require putting at least 10 million logic elements, or gates, onto a single piece of silicon, according to Raj Reddy, director of the Robotics Institute at Carnegie Mellon University. That is 10 times the number of elements that can now be put on even the most advanced chips, he said.\nSuch an increase in chip density, however, is easily within reach of an industry that seems to quadruple the number of elements on a chip every few years. ''By the year 2000, it's resonable to have one billion gates on a chip,'' Dr. Reddy said. But even such a ''super chip,'' he added, would have trouble thinking like a person.\n","148":"The Washington Post will cover House, Senate and gubernatorial races for all 50 states on Election Day using its advanced artificial intelligence technology, Heliograf, which first debuted during the Rio Olympics. This unprecedented level of election coverage provides readers with up-to-date reporting, analysis and results for nearly 500 races and allows The Post to personalize the experience by surfacing races for readers based on their location.\u00a0\n\"We have transformed Heliograf into a hybrid content management system that relies on machines and humans, distinguishing it from other technologies currently in use. This dual-touch capability allows The Post to create stories that are better than any automated system but more constantly updated than any human-written story could be,\" said Jeremy Gilbert, director of strategic initiatives at The Post.\nUsing Heliograf's editing tool, Post editors can add reporting, analysis and color to stories alongside the bot-written text. Editors can also overwrite the bot text if needed. The articles will be \"living stories,\" first beginning as a preview of a race in the days leading up to the election. On Election Day, stories will update with results in real-time and then, after a race is called, the story will provide analysis of the final results. Geo-targeted content will surface in The Post's liveblog and in special Election Day newsletters to readers, offering updates on races in their state. Internally, Heliograf also surfaces leads for potential stories, and on Election night, will alert reporters via Slack when incumbents are lagging and races are called.\n\"The future of automated storytelling is the seamless blend of human reporting and machine generated content,\" said Dr. Sam Han, director of data science at The Post. \"The Post's sophisticated artificial intelligence is the connective tissue that allows us to combine these different sources and to power Heliograf so that it can write highly personalized stories for the benefit of journalists and readers alike.\"\n\"We have just begun to scratch the surface on what is possible with artificial intelligence,\" said Shailesh Prakash, chief technology officer at The Post. \"Smart technology is an area we will continue to invest in and embed into every engineering system that powers The Washington Post and our Arc technology partners.\"\nHeliograf is part of a suite of artificial intelligence tools which are available through The Post's publishing platform Arc.\n             About Arc Publishing: Arc Publishing (www.arcpublishing.com) is a state-of-the-art digital platform and suite of tools that's engineered to meet the needs of modern publishers. Built by engineers and designers at The Washington Post, Arc is made up of flexible, sophisticated tools that work seamlessly together and can function individually. Arc spans the range of technology needs for digital publishers, including video, mobile web and apps, syndication to distributed platforms, automatic content testing, data mining and innovative monetization tools. At its core, Arc is about speed: for readers, the newsroom, advertisers and developers.\n","149":"Oliver G. Selfridge, an innovator in early computer science and artificial intelligence, died on Wednesday in Boston. He was 82.\n  The cause was injuries suffered in a fall on Sunday at his home in nearby Belmont, Mass., said his companion, Edwina L. Rissland.\n  Credited with coining the term ''intelligent agents,'' for software programs capable of observing and responding to changes in their environment, Mr. Selfridge theorized about far more, including devices that would not only automate certain tasks but also learn through practice how to perform them better, faster and more cheaply.\u00a0\n  Eventually, he said, machines would be able to analyze operator instructions to discern not just what users requested but what they actually wanted to occur, not always the same thing.\n  His 1958 paper ''Pandemonium: A Paradigm for Learning,'' which proposed a collection of small components dubbed ''demons'' that together would allow machines to recognize patterns, was a landmark contribution to the emerging science of machine learning.\n  An early enthusiast about the potential of interactive computing, Mr. Selfridge saw his ideas summarized in a famous 1968 paper, ''The Computer as a Communications Device,'' written by J. C. R. Licklider and Robert W. Taylor and published in the journal Science and Technology.\n  Honoring Mr. Selfridge, the authors proposed a device they referred to as Oliver, an acronym for On-Line Interactive Vicarious Expediter and Responder. Oliver was one of the clearest early descriptions of a computerized personal assistant.\n  With four other colleagues, Mr. Selfridge helped organize a 1956 conference at Dartmouth that led directly to creation of the field of artificial intelligence. \n  ''Oliver was one of the founding fathers of the discipline of artificial intelligence,'' said Eric Horvitz, a Microsoft researcher who is president of the Association for the Advancement of Artificial Intelligence. ''He has been well known in the field for his early and prescient writings on the challenge of endowing machines with the ability to learn to recognize patterns.''\n  Oliver Gordon Selfridge, a grandson of H. Gordon Selfridge, the American who founded Selfridges department store in London, was born in London on May 10, 1926. The family lost control of the business during the Depression and emigrated to the United States at the onset of World War II.\n  Mr. Selfridge attended Middlesex School in Concord, Mass., and the Massachusetts Institute of Technology, from which he graduated at 19 with a degree in mathematics. After service in the Navy, he embarked on graduate study at M.I.T. under Norbert Weiner, the pioneering theorist of computer science. He became one of Weiner's collaborators but plunged into the working world of computer science before earning an advanced degree. \n  In the 1960s Mr. Selfridge was associate director for Project MAC, an early time-shared computing research project at M.I.T. He did much of this work at the M.I.T. Lincoln Laboratory, a federally financed research center for security technology. He then worked at Bolt, Beranek & Newman, now BBN Technologies, which develops computer and communications-related technology. In 1983 he became chief scientist for the telecommunications company GTE.\n  He began advising the nation's national security leaders in the 1950s, among other tasks serving on the President's Foreign Intelligence Advisory Board and the Scientific Advisory Board of the National Security Agency.\n  His first marriage, to Allison Gilman Selfridge, and his second, to Katherine Bull Selfridge, ended in divorce.  Besides his companion, his survivors include their daughter, Olivia Selfridge Rissland of Belmont; three children from his first marriage, Peter Selfridge of Bethesda, Md.; Mallory Selfridge of Eastford, Conn.; and Caroline Selfridge of Saratoga, Calif.; a sister, Jennifer Selfridge MacLeod of Princeton Junction, N.J.; and six grandchildren. \n  Along with producing scholarly papers and technical books, Mr. Selfridge wrote ''Fingers Come in Fives,'' ''All About Mud'' and ''Trouble With Dragons,'' all books for children. At his death he was working on a series of books he hoped might one day become an arithmetic equivalent of summer reading projects for schoolchildren.\n  Mr. Selfridge never stopped theorizing, speaking and writing on what he saw as the future of artificial intelligence.\n  ''I want an agent that can learn and adapt as I might,'' he once told a meeting organized by I.B.M. Such an agent would ''infer what I would want it to do, from the updated purposes it has learned from working for me,'' he went on, and ''do as I want rather than the silly things I might say.'' \n","150":"December was a big month for advocates of regulating artificial intelligence. First, a bipartisan group of senators and representatives introduced the Future of A.I. Act, the first federal bill solely focused on A.I. It would create an advisory committee to make recommendations about A.I. -- on topics including the technology's effect on the American work force and strategies to protect the privacy rights of those it impacts. Then the New York City Council approved a first-of-its-kind bill that once signed into law will create a task force to examine its own use of automated decision systems, with the ultimate goal of making its use of algorithms fairer and more transparent.\nPerhaps not coincidentally, these efforts also overlap with increasing calls to regulate artificial intelligence along with claims by the likes of Elon Musk and Stephen Hawking that it poses a threat to humanity's literal survival. \u00a0\n  But this push for broad legislation to regulate A.I. is premature.\n  To begin with, even experts can't agree on what, exactly, constitutes artificial intelligence. Take the recent report released by the AI Now Institute, aimed at creating a framework for ethically implementing A.I. While itself focused on A.I., the report also acknowledges that no commonly accepted definition of A.I. exists, which it describes loosely as ''a broad assemblage of technologies ... that have traditionally relied on human capacities.''\n  ''Artificial intelligence'' is all too frequently used as a shorthand for software that simply does what humans used to do. But replacing human activity is precisely what new technologies accomplish -- spears replaced clubs, wheels replaced feet, the printing press replaced scribes, and so on. What's new about A.I. is that this technology isn't simply replacing human activities, external to our bodies; it's also replacing human decision-making, inside our minds.\n  The challenges created by this novelty should not obscure the fact that A.I. itself is not one technology, or even one singular development. Regulating an assemblage of technology we can't clearly define is a recipe for poor laws and even worse technology.\n  Indeed, the challenges A.I. poses aren't entirely new. We've already successfully regulated it in the past -- we just didn't call it ''artificial intelligence.'' In the 1960s and 1970s, for example, the financial industry began to rely on complex statistical modeling and huge computerized databases to make credit decisions, automating what had been a more manual process of approving or denying credit to borrowers.\n  Those ethical and legal challenges associated with these models so captivated the public's attention that in the summer of 1970, Newsweek ran a cover story titled ''Is Privacy Dead?'' detailing the ''massive flanking attack'' of computers on modern society. Growing awareness of that threat led to broad appeals that echo modern proposals to regulate A.I. ''Eventually we have to set up an agency to regulate the computers,'' Senator Sam Ervin of North Carolina said in 1970.\n  But instead of regulating all computers, the government sought a targeted approach tailored to specific problems, passing regulations like the Equal Credit Opportunity Act in 1974. That act was meant to reduce credit discrimination against minority groups and to increase consumers' ability to understand what the models were doing -- if consumers didn't like their credit score, thanks to the act they could at least know how it could be improved.\n  That law and others offer valuable lessons today, illustrating the importance of focusing on specific issues -- in this case, transparency in credit decisions -- and tailoring their solutions accordingly. Any regulation aimed at the range of systems we call ''A.I.'' should seek to be just as specific.\n  With the thorny exception of cybersecurity, the way the United States regulatory system has approached information technology is arguably the most successful model for regulating technology in existence -- fostering innovation while ensuring that the technology we use doesn't break or seriously jeopardize our safety. The backbone of this approach comprises regulations tailored to the explicit problems created by any given technology.\n  Within the United States' vast framework of laws and regulatory agencies already lie answers to some of the most vexing challenges created by A.I. In the financial sector, for example, the Federal Reserve enforces a regulation called SR 11-7, which addresses the risks created by the complex algorithms used by today's banks. SR 11-7's solution to those challenges is called ''effective challenge,'' which seeks to embed critical analysis into every stage of an algorithm's life cycle -- from thoroughly examining the data used to train the algorithm to explicitly outlining the assumptions underlying the model, and more. While SR 11-7 is among the most detailed attempts at governing the challenges of complex algorithms, it's also one of the most overlooked.\n  This is not, of course, to suggest that artificial intelligence should never be regulated. But if the past is any guide, treating it as a collection of separate technologies, in separate sectors, is destined to be the most effective way to control the benefits it creates -- and the dangers it poses.\n  Follow The New York Times Opinion section on Facebook and Twitter (@NYTopinion), and sign up for the Opinion Today newsletter. \n\n\n\n","151":"Robot cars drive themselves across the desert, electronic eyes perform lifeguard duty in swimming pools and virtual enemies with humanlike behavior battle video game players. \n  These are some fruits of the research field known as artificial intelligence, where reality is finally catching up to the science-fiction hype. A half-century after the term was coined, both scientists and engineers say they are making rapid progress in simulating the human brain, and their work is finding its way into a new wave of real-world products.\nThe advances can also be seen in the emergence of bold new projects intended to create more ambitious machines that can improve safety and security, entertain and inform, or just handle everyday tasks. At Stanford University, for instance, computer scientists are developing a robot that can use a hammer and a screwdriver to assemble an Ikea bookcase (a project beyond the reach of many humans) as well as tidy up after a party, load a dishwasher or take out the trash. \n  One pioneer in the field is building an electronic butler that could hold a conversation with its master -- a la HAL in the movie ''2001: A Space Odyssey'' -- or order more pet food. \u00a0\n  Though most of the truly futuristic projects are probably years from the commercial market, scientists say that after a lull, artificial intelligence has rapidly grown far more sophisticated. Today some scientists are beginning to use the term cognitive computing, to distinguish their research from an earlier generation of artificial intelligence work. What sets the new researchers apart is a wealth of new biological data on how the human brain functions.\n  ''There's definitely been a palpable upswing in methods, competence and boldness,'' said Eric Horvitz, a Microsoft researcher who is president-elect of the American Association for Artificial Intelligence. ''At conferences you are hearing the phrase 'human-level A.I.,' and people are saying that without blushing.''\n  Cognitive computing is still more of a research discipline than an industry that can be measured in revenue or profits. It is pursued in various pockets of academia and the business world. And despite some of the more startling achievements, improvements in the field are measured largely in increments: voice recognition systems with decreasing failure rates, or computerized cameras that can recognize more faces and objects than before. \n  Still, there have been rapid innovations in many areas: voice control systems are now standard features in midpriced automobiles, and advanced artificial reason techniques are now routinely used in inexpensive video games to make the characters' actions more lifelike. \n  A French company, Poseidon Technologies, sells underwater vision systems for swimming pools that function as lifeguard assistants, issuing alerts when people are drowning, and the system has saved lives in Europe.\n  Last October, a robot car designed by a team of Stanford engineers covered 132 miles of desert road without human intervention to capture a $2 million prize offered by the Defense Advanced Research Projects Agency, part of the Pentagon. The feat was particularly striking because 18 months earlier, during the first such competition, the best vehicle got no farther than seven miles, becoming stuck after driving off a mountain road.\n  Now the Pentagon agency has upped the ante: Next year the robots will be back on the road, this time in a simulated traffic setting. It is being called the ''urban challenge.''\n  At Microsoft, researchers are working on the idea of ''predestination.'' They envision a software program that guesses where you are traveling based on previous trips, and then offers information that might be useful based on where the software thinks you are going.\n  Tellme Networks, a company in Mountain View, Calif., that provides voice recognition services for both customer service and telephone directory applications, is a good indicator of the progress that is being made in relatively constrained situations, like looking up a phone number or transferring a call.\n  Tellme supplies the system that automates directory information for toll-free business listings. When the service was first introduced in 2001, it could correctly answer fewer than 37 percent of phone calls without a human operator's help. As the system has been constantly refined, the figure has now risen to 74 percent.\n  More striking advances are likely to come from new biological models of the brain. Researchers at the Ecole Polytechnique Federale de Lausanne in Lausanne, Switzerland, are building large-scale computer models to study how the brain works; they have used an I.B.M. parallel supercomputer to create the most detailed three-dimensional model to date of a column of 10,000 neurons in the neocortex. \n  ''The goal of my lab in the past 10 to 12 years has been to go inside these little columns and try to figure out how they are built with exquisite detail,'' said Henry Markram, a research scientist who is head of the Blue Brain project. ''You can really now zoom in on single cells and watch the electrical activity emerging.''\n  Blue Brain researchers say they believe the simulation will provide fundamental insights that can be applied by scientists who are trying to simulate brain functions. \n  Another well-known researcher is Robert Hecht-Nielsen, who is seeking to build an electronic butler called Chancellor that would be able to listen, speak and provide in-home concierge services. He contends that with adequate resources, he could create such a machine within five years.\n  Although some people are skeptical that Mr. Hecht-Nielsen can achieve what he describes, he does have one successful artificial intelligence business under his belt. In 1986, he founded HNC Software, which sold systems to detect credit card fraud using neural network technology designed to mimic biological circuits in the brain. HNC was sold in 2002 to the Fair Isaac Corporation, where Mr. Hecht-Nielsen is a vice president and leads a small research group.\n  Last year he began speaking publicly about his theory of ''confabulation,'' a hypothesis about the way the brain makes decisions. At a recent I.B.M. symposium, Mr. Hecht-Nielsen showed off a model of confabulation, demonstrating how his software program could read two sentences from The Detroit Free Press and create a third sentence that both made sense and was a natural extension of the previous text.\n  For example, the program read: ''He started his goodbyes with a morning audience with Queen Elizabeth II at Buckingham Palace, sharing coffee, tea, cookies and his desire for a golf rematch with her son, Prince Andrew. The visit came after Clinton made the rounds through Ireland and Northern Ireland to offer support for the flagging peace process there.''\n  The program then generated a sentence that read: ''The two leaders also discussed bilateral cooperation in various fields.''\n  Artificial\u00a0intelligence had its origins in 1950, when the mathematician Alan Turing proposed a test to determine whether or not a machine could think or be conscious. The test involved having a person face two teleprinter machines, only one of which had a human behind it. If the human judge could not tell which terminal was controlled by the human, the machine could be said to be intelligent.\n  In the late 1950's a field of study emerged that tried to build systems that replicated human abilities like speech, hearing, manual tasks and reasoning.\n  During the 1960's and 1970's, the original artificial intelligence researchers began designing computer software programs they called ''expert systems,'' which were essentially databases accompanied by a set of logical rules. They were handicapped both by underpowered computers and by the absence of the wealth of data that today's researchers have amassed about the actual structure and function of the biological brain. \n  Those shortcomings led to the failure of a first generation of artificial intelligence companies in the 1980's, which became known as the A.I. Winter. Recently, however, researchers have begun to speak of an A.I. Spring emerging as scientists develop theories on the workings of the human mind. They are being aided by the exponential increase in processing power, which has created computers with millions of times the power of those available to researchers in the 1960's -- at consumer prices.\n  ''There is a new synthesis of four fields, including mathematics, neuroscience, computer science and psychology,'' said Dharmendra S. Modha, an I.B.M. computer scientist. ''The implication of this is amazing. What you are seeing is that cognitive computing is at a cusp where it's knocking on the door of potentially mainstream applications.''\n  At Stanford, researchers are hoping to make fundamental progress in mobile robotics, building machines that can carry out tasks around the home, like the current generation of robotic floor vacuums, only more advanced. The field has recently been dominated by Japan and South Korea, but the Stanford researchers have sketched out a three-year plan to bring the United States to parity.\n  At the moment, the Stanford team is working on the first steps necessary to make the robot they are building function well in an American household. The team is focusing on systems that will consistently recognize standard doorknobs and is building robot hands to open doors. \n  ''It's time to build an A.I. robot,'' said Andrew Ng, a Stanford computer scientist and a leader of the project, called Stanford Artificial Intelligence Robot, or Stair. ''The dream is to put a robot in every home.''\n","152":"SAN JOSE - In 2011, Apple became the first company to place artificial intelligence in the pockets of millions of consumers when it launched the voice assistant Siri on the iPhone.\nSix years later, the technology giant is struggling to find its voice in AI.\nAnalysts say the question of whether Apple can succeed in building great artificial-intelligence products is as fundamental to the company's next decade as the iPhone was to its previous one. But the tech giant faces a formidable dilemma because the nature of artificial intelligence pushes Apple far out of its comfort zone of sleekly designed hardware and services.\nAI programming demands a level of data collection and mining that is at odds with Apple's rigorous approach to privacy, as well as its positioning as a company that doesn't profile consumers. Moreover, Apple's long-standing penchant for secrecy has made the company less desirable in the eyes of potential star recruits, who hail from the country's top computer science departments and are attracted to companies that publish research.\n\"Artificial intelligence is not in Apple's DNA,\" said venture capitalist and Apple analyst Gene Munster. \"They understand that in the future, every company is going to become an AI company, and they are in a particularly tough spot.\"\u00a0\nAt Apple's annual developers conference Monday - the same event where Siri was introduced - the company's efforts to become an AI powerhouse were on display as executives launched a new stand-alone smart speaker and touted features meant to boost Siri's chops and to power AI applications on Apple products.\n\"Machine learning\" - an AI buzzword that describes a form of ultra-fast, complex computer data analysis and statistical modeling - was repeated throughout the 2\u00bd-hour presentation, delivered to an audience of roughly 6,000 developers here. Siri will now use machine learning to predict the times of a morning commute, or scan the travel news as you are reading it on the company's Safari browser and then suggest related activities, such as booking a reservation.\nShe will use machine learning to talk with you and help you sort through music through a new $349 home automation device, the HomePod. She will automatically organize your photos into albums, such as \"2nd Anniversary,\" without you giving her any context about the pictures. There was even a new software tool kit, Core ML, that will allow for faster processing of large amounts of data collected during machine learning applications. (It's six times as fast as Google's rival AI processor, an executive quipped.)\nBut Monday's announcements come as other technology companies have released similar innovations and have already spent billions on the burgeoning AI arms race. Many are placing their bets on artificial intelligence - software that one day may be smart enough to chat back and forth like a human, or computer vision that identifies real-world objects so well it can power the first fully functioning self-driving car.\nThat has put Apple in the disadvantaged position of trying to lead in an area where it has fallen behind - and where the effort cuts against core aspects of the company's secretive culture.\n\"This is a substantial shift for Apple,\" said Daniel Gross, a former Apple executive who focused on artificial intelligence. \"The internal focus is on building great products, not publishing papers.\"\nTen years after launching the iPhone, Apple is on the hunt for another blockbuster product that can take its place. Sales of the iPhone propelled Apple to become the most valuable company in the world and still account for more than half of the company's revenue, which was $215.6 billion in 2016. But last year, purchases of the company's smartphone dropped for the first time, suggesting that the market for high-end smartphones may finally be saturated.\n\"The difference between last year's iPhone and this year's iPhone is going to be much smaller than the difference between the first and second iterations of the iPhone. . . . So that S-curve is starting to flatten out,\" said Benedict Evans, a mobile expert and partner at the Silicon Valley venture capital firm Andreessen Horowitz, using an industry term for exponential increases in innovation. \"Then you have the next transformative technology that isn't here yet.\"\nApple did not respond to repeated requests for interviews and comment.\nApple's launch of he HomePod, which will go on sale later this year, comes several years after Amazon.com and Google released their own automation devices, the Echo and the Google Home, which send consumers' spoken queries back to their servers in California for analysis. Google, Tesla, Uber, and others have been testing self-driving vehicles on public roadways for several years; Apple received its first permit to begin testing just two months ago. Before Monday's updates, the Siri assistant was still mostly a glorified Web search that could tell the occasional preprogrammed joke.\nNow Apple is racing to catch up. Last October, the company hired Russ Salakhutdinov, a Carnegie Mellon professor whose expertise is in an area of artificial intelligence known as \"deep\" or \"unsupervised\" learning, a complex branch of machine learning in which computers are trained to replicate the way the brain's neurons fire when they recognize objects or speech. Salakhutdinov is a protege of Geoffrey Hinton, perhaps the world's top researcher in this area. Salakhutdinov divides his time between Carnegie Mellon and Apple. Hinton divides his time between Google and the University of Toronto.\nBuilding ties to academic superstars not only helps to improve products but also becomes a key recruiting tool, said Richard Zemel, director of the Vector Institute for Artificial Intelligence and a professor specializing in machine learning at the University of Toronto.\n\"You used to not see people with Apple name badges at conferences, and now you do,\" Zemel said.\nIn December, Apple presented and published its first academic paper on artificial intelligence at an industry conference. Another paper has been accepted to a computer vision conference and will be published in July, Salakhutdinov said in an interview. Salakhutdinov said he was not authorized to discuss his work at Apple in any detail.\nZemel, who told Bloomberg two years ago that Apple was \"off the scale\" in terms of secrecy and that such secrecy was keeping it out of the loop on major developments in the field, now said that the Cupertino, Calif., giant was \"making some changes.\"\n\"But it's going to take some work,\" he added.\nResearchers at elite universities said in interviews that Apple was still not the top choice for their computer science graduates - Google, Facebook and Amazon were by far the top picks - but that the company was moving up in the rankings. (Amazon chief executive Jeffrey P. Bezos owns The Washington Post.)\nApple's forays into AI have also been slower than its peers' because it's been reluctant to embrace the data-mining practices of rivals Google and Facebook, experts said. The company has spent considerable resources building additional layers of privacy. Unlike Google and Facebook, which are primarily advertising companies that collect massive amounts of intimate data to profile their users, Apple believes in limiting the amount of user data it collects. At a previous developer's conference, executives bragged that the company did not build user profiles. Chief executive Tim Cook has positioned the company as the anti-Google.\nBut that stance against data collection becomes a problem if you are building artificial intelligence, researchers say. A home device must collect and analyze people's speech to improve the way the device can speak to humans, for example. For Siri to be smart, she needs to collect and interpret data from other applications, such as your calendar, your restaurant reservations and, now, your browsing.\nLast year, as Apple began to embrace artificial intelligence on the iPhone, the company undertook a large privacy protection project. The project took an academic concept called differential privacy and applied it to AI applications on the iPhone. Differential privacy works by inserting noise - or bad information - into good data to confuse outsiders who might try to hone in on an individual's records. For example, in order for Apple software to group the photos of your dog into a single album, it needs to collect many photos of your dogs.\nApple collects those images, but not before encrypting the data in them and then scrambling that data with other data, so that if anyone tried to recover the original data set they wouldn't know what was tied to a single user, the company claims. This technique is considered a stronger privacy protection than other methods, such as using mathematical formulas to render user profiles anonymous.\nApple's focus on privacy may have slowed the company down in terms of building some products, Gross said, but the trade-off would be consumer trust. \"Apple is dousing itself with an extra piece of really hard science and doing so to try and preserve your privacy,\" he said. \"I think Google and Facebook will have to answer to a world where a similar product that is offered is more privacy-preserving.\"\nMunster pointed out that no tech company has a huge lead on artificial intelligence yet. \"The bad news is that Apple is behind,\" he said. \"The good news is that if we look at how AI is going to impact the world, it's still early days - there is plenty of time to catch up.\"\n          Correction: An earlier version of this article incorrectly said that Apple co-founder Steve Jobs introduced Siri eight years ago. It also misstated the first name of venture capitalist and Apple analyst Gene Munster. The article has been updated.       \n","153":"THE COGNITIVE COMPUTER On Language, Learning, and Artificial Intelligence. By Roger C. Schank with Peter G. Childers. 268 pp. Reading, Mass.: Addison-Wesley Publishing Company. $17.95. THE KNOWLEDGE MACHINE Artificial Intelligence and the Future of Man. By Donald Michie and Rory Johnston. 300 pp. New York: William Morrow & Company. $16.95.\nO VER 25 years ago, computer science produced a discipline called artificial intelligence. Its goal was to simulate the human thinking process on a computer. But as initial advances became known, such as programming a computer to outwit master chess players, many people feared that humans could be replaced by sturdy, tireless and imperturbable silicon reasoning machines. That fear was perhaps most eloquently articulated in 1961 by Adrienne Rich in ''Artificial Intelligence,'' a poem addressed to a chess-playing computer: Still, when they make you write your poems, later on, who'd envy you, force-fed on all those variorum editions of our primitive endeavors, those frozen pemmican language-rations they'll cram you with? denied our luxury of nausea, you forget nothing, have no dreams.\u00a0\nAs time went on, however, it became clear that computers were hopeless at doing many things humans could do easily. A chess-playing computer didn't know anything about things that we are perhaps born knowing - for example, that if an object is in one place it isn't also in another. Computers of the sorting and calculating type, however, rapidly encroached on our daily lives. They generated the bills we got, the airline reservations we made and the loans we took out on credit cards to buy dinners. But they didn't dazzle or transform lives in any meaningful way. Consumers weren't even cowed by warnings that our children would suffer in the job market if they weren't ''computer literate.'' Scientists succeeded in making computers perform some activities that seemed to be intelligent behavior -medical diagnoses for example - but still seemed far from complex human reasoning.\n''The Cognitive Computer,'' by Roger G. Schank with Peter G. Childers, and ''The Knowledge Machine,'' by Donald Michie and Rory Johnston, offer inside views of current research in artificial intelligence. Mr. Schank is one of the founders of the group studying artifical intelligence at Yale University, where some of the most significant work on computer comprehension of human language has been done. Donald Michie is the dean of the artificial intelligence movement in England and a leader in developing so-called ''expert systems.'' Their books, each written with a nonscientist, are cogent accounts of different approaches to machine intelligence.\n''The Cognitive Computer'' is clear, funny and smart about the central problems involved in trying to get computers to mimic human reasoning. Mr. Schank sets the reader at ease immediately. The reason to care about computers, he tells us, ''is that they might contribute to daily life by providing some kind of service.'' He adds, ''If you can't use computers without pain, then just wait. It's the computers that will have to change, not you.'' He says that his work on computer comprehension of human language is basic research on the big problem, the one that has stumped philosophers since Plato and Aristotle - how does the mind work? He believes that ''if computers gain any semblance of intelligence, it will be because we have begun to unravel some of the mysteries of human intelligence and model them on the computer.'' Whether computers will ever really ''think'' like people is not an interesting question to him. He wants to build ''intelligent machines, not people.'' But for such a machine to be useful, it has to be able to talk with people in their own language. To do this, it has to ''know'' a great deal about people's motives and desires, how they plan and how they reach their goals.\nMr. Schank has developed a bare-bones way of describing for a computer what happens beneath the surface of language and other human behavior. His formulas allow the computer to draw inferences from certain situations. Suppose the computer is told a story about a person ordering food in a restaurant. Through the use of a specially designed parsing program, Mr. Schank's computers are able to correctly analyze sentences in that context. They would know, for example, that the word ''tip'' in a restaurant story would refer to money and not to the pointed end of something.\nThe computer has proved enormously helpful in efforts to understand the mind, Mr. Schank believes, because it is the theory-tester Plato and Aristotle did not possess. The computer knows nothing before it is programmed, so its mistakes point out mistakes in the formulas according to which it is programmed. Take the program ''Tale-Spin,'' which tried to get the computer to make up stories out of a group of concepts and a set of rules on how to use those concepts. Mr. Schank quotes one of Tale-Spin's first efforts: ''One day Joe Bear was hungry. He asked his friend Irving Bird where some honey was. Irving told him there was a beehive in the oak tree. Joe walked to the oak tree. He ate the beehive.'' Mr. Schank comments that ''since the program contained a conceptual representation for 'source of food' we merely told it that 'beehive' was one. 'Beehive' is indeed a source of food, but we forgot to mention that 'source as container' is different from 'source as object.' Finding a refrigerator will do when you are hungry, if you know to look inside it, and not to eat it. None of this is obvious to a machine.''\nBy now, research has evolved to the point where computers can ''make sense'' of events, according to Mr. Schank, and he expects that within the next few years computers will be able to serve as financial and medical advisers. Later we can expect sophisticated learning systems that know the principles of cooking and are able to invent dishes according to individual whims. In 10 years, Mr. Schank predicts, the machine will be able to explain why it came to the conclusions it did, what lines of reasoning it rejected and why it rejected them. But computers will never have empathy for human beings, he notes, because human ''output'' at the level of empathy cannot easily be put into words or ''representations.'' For example, ''what happens when the output is a kiss or a backrub, or a wonderfully cooked dinner, or a vase of flowers?''\nThe central theme of ''The Knowledge Machine'' is that in an increasingly complex world we need all the sources of knowledge we can get, including machines. Donald Michie makes his case by surveying the accomplishments in ''expert systems'' or ''knowledge-based systems.'' These are sets of computer software that contain the knowledge of an expert in a particular field. For instance, some programs now capture the expertise of one or several experts in chemical synthesis, oil exploration or the design of computer systems. Expert systems combine textbook knowledge with the rules of thumb that experience teaches, and then make informed guesses. They work tirelessly, often with higher accuracy rates than human beings can achieve.\nIt is beyond dispute that expert systems can be helpful, Mr. Michie says, and he also contends that a properly engineered system can ''create'' knowledge by taking fragmentary, inconsistent and error-ridden information and turning it into something more precise, reliable and comprehensive. He cites the case of ''dendral,'' a program designed to help organic chemists determine the molecular structure of compounds. Chemists who knew of it asked for copies, not of the program, but of the rules the program had assimilated. ''They found these rules useful,'' he says, ''because they were a much clearer codification of the subject than had existed before.''\nMr. Michie calls this byproduct of expert-system engineering ''knowledge refining,'' and he finds it has already improved medical texts and suggested new assembly techniques in automation. He envisions ''machine-based craft shops set up for the sole purpose of generating new knowledge, using as their raw material both the expertise of humans and the ruminations of huge computer models and look-ahead systems.'' With such a ''knowledge refinery,'' one could begin to ''sort out the many man-centuries of mental work,'' that ''gather dust on library shelves - contradictory, disparate and indigestible'' and turn it into ''accurate usable knowledge.'' M R. MICHIE sees great danger ahead if science fails to design machines that don't convey knowledge in forms humans can grasp.\nHe argues that efficient machines, if they are a mystery to humans, can actually slow things down. He cites a commentary on the installation of a highly automated hot strip mill by a Dutch steel company: ''The operators became so unsure of themselves that, on some occasions, they actually left the pulpits used for control unmanned. . . . [They] also failed fully to understand the control theory of the programs used in the controlling computer, and this reinforced their attitude of 'standing well back' from the operation - except when things were very clearly going awry. By intervening late, the operators let the productivity drop below that of plants using traditional control methods. So automation had led to lower productivity and operator alienation simultaneously.''\nIn the Information Age knowledge is an instrument like money. It's not an end but a means. The work described in these books is an attempt to diffuse knowledge as widely as possible, making it accessible to the many rather than the few. Given that the right information at the right time can change a person's life, the story of how scientists intend to capitalize on the computer's lust for order to bring it to us is very exciting.\n","154":"          Afshin Molavi is co-director of the emerge85 Lab and a senior fellow at the Foreign Policy Institute of the Johns Hopkins University School of Advanced International Studies in Washington.       \nAs 2017 comes to a close, the major events of the year will dominate the year-end retrospectives: President Trump's tumultuous first year in office, European elections from the Netherlands to France to Germany, the dramatic rise of the Saudi Crown Prince Mohammed bin Salman, the body blows dealt to ISIS in Iraq and Syria, the on-again off-again trade tensions with China, the shadow of Russia in U.S. politics, the United Kingdom's tortuous Brexit negotiations and the North Korea nuclear challenge, among others.\nHowever, the events that make headlines are not always the most consequential stories. Often, hidden beyond the headlines of the day, the tectonic plates are shifting imperceptibly, quietly -  these deserve our close attention.\u00a0\nHere are my top four ICYMI (in case you missed it) stories of 2017 that were just below the radar, but will have significant consequences for the United States and the world.\n          1. China's artificial intelligence strategy       \nBy now, we've all heard of the extraordinary promise - and peril - of artificial intelligence (AI). The business world is being transformed by AI and, in the military sphere, industrial-scale robotic killing capacity awaits us. In our personal lives, from virtual assistants to driverless cars, AI is changing how we live, consume, and connect. The Swiss economist and founder of the World Economic Forum Klaus Schwab put AI at the heart of what he calls the coming Fourth Industrial Revolution, a transformation that he argues \"will be unlike anything humankind has experienced before.\" Elon Musk warns that AI advances could lead to World War III and Vladimir Putin has declared that the winner of the AI race will be \"ruler of the world.\"\nEnter China. They quietly declared an AI strategy through the year 2030 that aims for nothing short of global dominance of the industry. \"The rapid development of artificial intelligence will profoundly change human social life and the world,\" the report with the clunky title, \"Notice of the State Council Issuing the New Generation of Artificial Intelligence Development Plan,\" begins. Eric Schmidt, the recently departed Executive Chairman of Google's parent company says the United States desperately needs an AI strategy, too. So far, no takers in the Trump administration. Watch this space.\n          2. Post-American trade deals       \nWithin days of sitting in the Oval Office, President Trump declared the multilateral Trans-Pacific Partnership (TPP) trade deal, encompassing some 40 percent of the global economy, dead on arrival, leaving longtime allies like Japan, Singapore, and Australia in the lurch. Meanwhile, as U.S. trade negotiators are chipping away at the North American Free Trade Agreement (NAFTA), the President turned up at global conferences in Europe and Asia declaring \"America first\" and the days of getting \"taken advantage of\" by others is over. The United States, the leader and founder of the rules-based, post-World War II liberal trade order, seemingly wants out.\nTrump's views notably sparked hand-wringing across the globe, but the hand-wringing changed to hand-shaking as countries from Europe to Asia, Latin America to Africa are going on with trade deals of their own, inaugurating what might be a series of post-American trade deals. Mexico and the European Union are close to launching a new trade deal. Japan and the E.U. are also working on a major deal. Several TPP members frustrated by Washington are signing on to a China-led trade deal, the Regional Comprehensive Economic Partnership (RCEP), that will account for nearly half the world's population and a quarter of the global economy.  As for the TPP that Trump declared dead, Tokyo is leading an effort to revive it - without the United States.\n          3. The news of Europe's death was greatly exaggerated       \nBrexit and the rise of populist nationalism dominated the headlines out of Europe this year. Three elections - Netherlands, France and Germany - were framed as pivotal turning points for the future of Europe. In all three, the far-right parties gained but could not achieve majorities, and attention was turned back to Europe's other tragic drama: the United Kingdom's Brexit negotiations, a remarkable own goal in slow motion that is difficult to watch for those who admire the island in the North Atlantic.\nBut something happened amid the election excitement and the Brexit twists and turns: Economic growth returned to Europe. The euro-zone countries are on pace for the fastest growth in a decade with 2.4 percent growth on the year, according to the European Central Bank. The broader European Union is also growing respectably, a far cry from the days of zero or negative growth in the wake of the 2008 global financial meltdown and the 2010 European debt crisis.\nCountries on the verge of implosion a few years ago, like Greece or debt-saddled Portugal have made recoveries ranging from cautious to robust. Not long ago, the talk was of Europe's moribund economies. Today, the continent's leaders may feel a bit like Mark Twain when he read his own obituary in the newspaper and replied: \"the news of my demise has been greatly exaggerated.\"\n          4. The enduring popularity of Narendra Modi       \nIndian Prime Minister Narendra Modi should have had a rough year. In early 2017, he had to deal with the fall out of a currency demonetization program that wreaked havoc on the economy and slowed economic growth. The after-effects lingered throughout the economy for much of the year. Then, in another major economic initiative rolled out with much fanfare, Modi eliminated a series of taxes that inhibited India from becoming a truly common market of 1.3 billion people. While full of long-range potential, the tax reform roll out had more than its fair share of glitches and is still hurting small businesses, the backbone of the economy.\nDespite these stumbles, Modi's popularity ratings remain sky high, and Indians are among the most optimistic about their future in the world, according to polling from Pew Research Center. After years of policy sclerosis, many Indians might simply be pleased to see a prime minister \"doing something\" to break the logjams to India's development, willing to countenance short-term loss for long-term gain. Or it might be that Modi is an excellent showman who knows how to touch populist and nationalist nerves. Whatever the case, for his uncanny ability to remain popular while enacting ambitious reforms that could finally unleash India's true economic potential, Modi should win the emerging world's person of the year.\n","155":"LAST winter, the satirical comedy troupe that calls itself Artificial Intelligence took over the Ballroom, 253 West 28th Street, to stage an imaginary television variety show called ''Vicki's Valentine Thing.'' Set in 1967, it featured the actress Nancy Cassaro playing a mythical star, loosely modeled after Judy Garland, surrounded by her family and guest stars celebrating ''love'' in a style that might be called Las Vegas hippie.\nNow the same troupe has returned to the Ballroom with television cameras, microphones and a flashing audience applause sign to present another imaginary variety show featuring many of the same characters. ''A Very Vicki Christmas,'' set two years earlier than the valentine extravaganza, is a savagely funny takeoff of familial television gatherings at holiday time. The format spoofs such silly Christmas show conventions as when famous guests, covered with snow, unexpectedly pop in to visit the star in her living room. At the same time, it allows the audience to witness nasty behind-the-cameras dramas that reach a crescendo of chaos during the commercial breaks that feature actual voice-overs from vintage ads for Pillsbury products and Winston cigarettes.\u00a0\nThe guests on ''A Very Vicki Christmas'' include a surf music duo named Tracy Everett and Tripp Tyler (Michael Winther and Tony Dowdy) a French mime, Luc-Pierre (Mark Campbell), who comports himself like a fourth-rate Marcel Marceau, and Jack Peters and Donny Palace (James Altuner and Chris Fracciola), whose shenanigans broadly parody Dean Martin and Jerry Lewis. As the perennially tipsy star hostess, Ms. Cassaro, in a platinum wig, slurs her way through travesties of ''The Little Drummer Boy'' and ''When You Wish Upon a Star.''\nAlthough the limited abilities of some of the troupe's 18 members prevent ''A Very Vicki Christmas'' from achieving the perfect mock-realism of a movie like ''This Is Spinal Tap,'' which exemplifies this style of parody, the show, which plays at the Ballroom through Dec. 30, is still funny and endearing in its re-creation of a quaint genre of pop kitsch.\n","156":"ABSTRACT\nTed Greenwald article in Journal Report: Artificial Intelligence Issue notes introduction into what is artificial intelligence, how does it work and why is everyone talking about it; photo (M)\n","157":"LUGANO, Switzerland - J\u00fcrgen Schmidhuber may be the Rodney Dangerfield of artificial intelligence research.\nIn a visit with him in this idyllic Swiss city in the mountains near the Italian border, it is easy to understand why he believes that his pioneering work in the field often, as the comedian liked to say, gets no respect.\nFar away in Silicon Valley, on the other side of the world, the tech industry is building cars that drive themselves and household appliances that respond to your voice commands and even try to predict what you will do next.\nIn certain circles, the people who did the early work that made this technology possible are stars. There is Sebastian Thrun, a roboticist who did groundbreaking research on self-driving cars at Google. Adam Cheyer and Tom Gruber worked on the A.I. program Siri, later acquired by Apple. And Facebook hired Yann LeCun, an expert in \"neural networks\" who left New York University to start a research program at the social media giant.\u00a0\nBut mention the name J\u00fcrgen Schmidhuber in an automated quinoa lunch spot frequented by coders in San Francisco, and you are likely to get blank stares.\nOn a recent train ride to Zurich, Dr. Schmidhuber, an athletic 53-year-old who is co-director of the Dalle Molle Institute for Artificial Intelligence Research here, reflected on how he believed his early research was often overlooked or ignored. \"It's like much of the rest of society,\" he said. \"Sometimes it's postfactual.\"\nDr. Schmidhuber's complaints are well known within the fraternity of researchers who have turned what until a half-decade ago was an academic backwater into a multibillion-dollar industry. He has been accused of taking credit for other people's research and even using multiple aliases on Wikipedia to make it look as if people are agreeing with his posts.\n\"J\u00fcrgen is manically obsessed with recognition and keeps claiming credit he doesn't deserve for many, many things,\" Dr. LeCun said in an email. \"It causes him to systematically stand up at the end of every talk and claim credit for what was just presented, generally not in a justified manner.\"\nDr. Schmidhuber counters that criticism with a bigger point: He is not the only one who is not getting due credit among A.I. researchers. In fact, he says work going all the way back to the 1960s is regularly ignored by today's research luminaries.\nAlthough he insists he doesn't harbor ill will toward those better-known researchers, it grates on him that history hasn't been kinder. \"Certain researchers in my field have acted as if they invented something, although it was invented by other people whom they did not even mention,\" Dr. Schmidhuber said.\nBut understanding the disconnect between his early work and his lack of celebrity isn't easy - and cannot be entirely explained by the fact that he lives thousands of miles from the tech industry's center of gravity.\nThe dispute is about the roots of neural networks, which allow machines to learn by recognizing patterns that can then be applied generally. Applications include recognizing speech and language, visually identifying objects, navigating in self-driving cars and making robot hands grasp more deftly. As a scientific field, it dates to the 1940s. But only in recent years have researchers in this area made striking progress.\nNeural networks are actually software. For a visual analogy, think of them as a giant Tinkertoy set - vast arrays of interconnected nodes that can be trained to do everything from language translation to recognizing visual objects or human speech.\nFor decades, neural networks were laboratory curiosities, often met with skepticism. But in the 1990s, with faster and cheaper computers as well as new ideas about how to design neural nets, there was finally progress.\nIn 1997, Dr. Schmidhuber and Sepp Hochreiter published a paper on a technique that has proved crucial in laying groundwork for the rapid progress that has been made recently in vision and speech. The idea, known as Long Short-Term Memory, or LSTM, was not widely understood when it was introduced. It essentially offered a form of memory or context to neural networks.\nJust as humans do not restart learning from scratch every second, a certain type of neural network adds loops or memory that interpret each new word or observation in light of what has been previously observed. LSTM strikingly improved these networks, leading to huge jumps in accuracy.\nIt may be that Dr. Schmidhuber's misfortune is that he was simply too early - a few years ahead of the powerful and more affordable computers we have today. It was not until recently that his concepts started to pan out.\nLast year, for example, Google researchers reported that they had used LSTM to cut transcription errors in their speech recognition service by up to 49 percent. It was a huge increase after years of incremental progress.\nBut between Dr. Schmidhuber's and Dr. Hochreiter's research and today's progress there was a big gap - and that's the rub. Other researchers say it took many contributors to get from Point A to Point B, where we are today.\n\"He's done a lot of seminal stuff,\" said Gary Bradski, an A.I. scientist who created a popular computer vision system known as OpenCV. \"But he wasn't the one who made it popular. It's kind of like the Vikings discovering America; Columbus made it real.\"\nDr. Schmidhuber also has a grand vision for A.I. - that self-aware or \"conscious machines\" are just around the corner - that causes eyes to roll among some of his peers. To put a fine point on the debate: Is artificial intelligence an engineering discipline, or a godlike field on the cusp of creating a new superintelligent species?\nDr. Schmidhuber is firmly in the god camp. He maintains that the basic concepts for such technologies already exist, and that there is nothing magical about human consciousness. \"Generally speaking, consciousness and self-awareness are overrated,\" he said, arguing that machine consciousness will emerge from more powerful computers and software algorithms much like those he has already designed.\nIt's been an obsession since he was a teenager in Germany reading science fiction.\n\"As I grew up I kept asking myself, 'What's the maximum impact I could have?'\" Dr. Schmidhuber recalled. \"And it became clear to me that it's to build something smarter than myself, which will build something even smarter, et cetera, et cetera, and eventually colonize and transform the universe, and make it intelligent.\"\nToday, he will not be pinned down on when such thinking machines might arrive, saying only that given the vast improvements in computing power it will be soon.\nIn 2014, he and others founded a company to commercialize some of the technology that he helped create and to work on \"general purpose\" artificial intelligence.\nThe company, Nnaisense, is based just a few steps from the University of Lugano campus. It is being advised by Dr. Hochreiter, who now heads the Institute of Bioinformatics at the Johannes Kepler University in Linz, Austria, and Jaan Tallinn, a co-founder of Skype. The company has partnerships in finance, autonomous vehicles and heavy industry.\nNnaisense's chief executive is an American computer scientist, Faustino Gomez, who has been Dr. Schmidhuber's research collaborator for many years. He defends both his partner's claims of having done pioneering work and his optimism about the field that has begun shaking up industries and economies around the world.\n\"We are at the beginning of the end of the beginning in A.I.,\" he said.\nPHOTOS: In the 1990s, J\u00fcrgen Schmidhuber co-wrote a paper on Long Short-Term Memory, a technique that has led to rapid progress in artificial intelligence. (PHOTOGRAPH BY MULTIPLE EXPOSURE IMAGE BY DAVID KASNIC FOR THE NEW YORK TIMES) (B1); J\u00fcrgen Schmidhuber is co-director of the Dalle Molle Institute for Artificial Intelligence Research in Lugano, Switzerland. (PHOTOGRAPH BY DAVID KASNIC FOR THE NEW YORK TIMES) (B4)Related Articles\n\n","159":"Renowned physicist Stephen Hawking has a long list of things that will bring doom and destruction to the human race. Add one more to that list: human aggression.\nHawking was asked last week what he believed was humanity's greatest shortcoming. He said that human beings continue to be stupidly aggressive\u00a0- long after the evolutionary benefit of that kind of behavior has gone away.\u00a0\n\"The human failing I would most like to correct is aggression,\" said Hawking, according to CNET. \"It may have had survival advantage in caveman days, to get more food, territory or partner with whom to reproduce, but now it threatens to destroy us all.\"\nSpecifically\u00a0- and Hawking probably isn't wrong about this\u00a0- aggression combined with nuclear capabilities could spell \"the end of\u00a0civilization, and maybe the end of\u00a0the human race.\"\n               [Stephen Hawking just got an artificial intelligence upgrade, but still thinks AI could bring an end to mankind]            \nScared yet? Hawking, a brilliant theoretical physicist, and the subject of the Oscar-nominated\u00a0film \"The Theory of Everything,\" is a man whose thoughts on the biggest\u00a0- and most depressing\u00a0- topics of our day are in high demand.\nOver the years, Hawking has argued that humans face the threats from machines, aliens, and now ourselves.\nLate last year, he said that Artificial Intelligence could end up destroying the world.\n              [The 12 threats to human civilization, ranked]           \nThe great irony, of course, is that Hawking, who has the motor neuron disease amyotrophic lateral sclerosis (ALS) or Lou Gehrig's Disease and is paralyzed an unable to speak, is now capable of communicating verbally thanks to the help of an advanced artificial intelligence program developed for him by Intel.\u00a0\n\"Once humans develop artificial intelligence, it will take off on its own and redesign itself at an ever-increasing rate,\" Hawking warned.\u00a0\"Humans, who are limited by slow biological evolution, couldn't compete and would be superseded.\"\nAnd let's not forget, of course, aliens.\n\"If aliens ever visit us, I think the outcome would be much as when Christopher Columbus first landed in America, which didn't turn out very well for the Native Americans,\"\u00a0he said in 2010, according to the Times of London.\u00a0\n","160":"They urge U.N. to ban autonomous weaponry, still in its infancy\nTesla chief executive Elon Musk has said that artificial intelligence is more of a risk to the world than North Korea, offering humanity a stark warning about the perilous rise of autonomous machines.\nNow the tech billionaire has joined more than 100 robotics and artificial intelligence experts calling on the United Nations to ban one of the deadliest forms of such machines: autonomous weapons.\u00a0\n\"Lethal autonomous weapons threaten to become the third revolution in warfare,\" Musk and 115 other experts, including Alphabet's artificial intelligence expert, Mustafa Suleyman, warned in an open letter released Monday. \"Once developed, they will permit armed conflict to be fought at a scale greater than ever, and at time scales faster than humans can comprehend.\"\nAccording to the letter, \"These can be weapons of terror, weapons that despots and terrorists use against innocent populations, and weapons hacked to behave in undesirable ways.\"\nThe letter - which included signatories from dozens of organizations in nearly 30 countries, including China, Israel, Russia, Britain, South Korea and France - is addressed to the U.N. Convention on Certain Conventional Weapons, whose purpose is restricting weapons \"considered to cause unnecessary or unjustifiable suffering to combatants or to affect civilians indiscriminately,\" according to the U.N. Office for Disarmament Affairs. It was released at an artificial intelligence conference in Melbourne, Australia, ahead of formal U.N. discussions on autonomous weapons. Signatories implored U.N. leaders to work hard to prevent an autonomous weapons \"arms race\" and \"avoid the destabilizing effects\" of the emerging technology.\n             In a report released this summer, Izumi Nakamitsu, the head of the disarmament affairs office, said that technology is advancing rapidly but that regulation has not kept pace. She pointed out that some of the world's military hot spots already have intelligent machines in place, such as \"guard robots\" in the demilitarized zone between South and North Korea.\nThe South Korean military is using a surveillance tool called the SGR-AI that can detect, track and fire upon intruders. The robot was implemented to reduce the strain on thousands of human guards who man the heavily fortified, 160-mile border. While it does not operate autonomously yet, it does have the capability to, according to Nakamitsu.\n\"The system can be installed not only on national borders, but also in critical locations, such as airports, power plants, oil storage bases and military bases,\" says a description in a video from Samsung, which makes the SGR-AI.\nSamsung didn't immediately respond to a request for comment.\n\"There are currently no multilateral standards or regulations covering military AI applications,\" Nakamitsu wrote. \"Without wanting to sound alarmist, there is a very real danger that without prompt action, technological innovation will outpace civilian oversight in this space.\"\nAccording to Human Rights Watch, autonomous weapons systems are being developed in many of the nations represented in the letter - \"particularly the United States, China, Israel, South Korea, Russia and the United Kingdom.\" The concern, the organization says, is that people will become less involved in the process of selecting and firing on targets as machines lacking human judgment begin to play a critical role in warfare. Autonomous weapons \"cross a moral threshold,\" HRW says.\n\"The humanitarian and security risks would outweigh any possible military benefit,\" HRW argues. \"Critics dismissing these concerns depend on speculative arguments about the future of technology and the false presumption that technical advances can address the many dangers posed by these future weapons.\"\nIn recent years, Musk's warnings about the risks posed by AI have grown increasingly strident - drawing pushback in July from Facebook chief executive Mark Zuckerberg, who called Musk's dark predictions \"pretty irresponsible.\" Responding to Zuckerberg, Musk said his fellow billionaire's understanding of the threat posed by artificial intelligence \"is limited.\"\nLast month, Musk told a group of governors that they need to start regulating artificial intelligence, which he called a \"fundamental risk to the existence of human civilization.\" When pressed for concrete guidance, Musk said the government must get a better understanding of AI before it is too late.\n\"Once there is awareness, people will be extremely afraid, as they should be,\" Musk said. \"AI is a fundamental risk to the future of human civilization in a way that car accidents, airplane crashes, faulty drugs or bad food were not. They were harmful to a set of individuals in society, but they were not harmful to individuals as a whole.\"\nIn a tweet this month, Musk wrote: \"If you're not concerned about AI safety, you should be. Vastly more risk than North Korea.\"\npeter.holley@washpost.com\n","162":"Sometimes the best way to stop a bad machine is with a lot of good machines.\nSeveral companies are applying the techniques of artificial intelligence, or A.I., to the world of security, and they are using a whole bunch of machines strung together in so-called cloud computing networks to do it. Originally the province of university researchers and now one of the ways Google and other companies figure out what is going on across the web, A.I. technology is being employed by security companies who say they can beat criminals by using many of the same strategies.\nMuch as Google examines websites for significant information and watches the behavior of people searching and surfing the web, A.I. security companies look for malicious sites or try to examine and predict the behavior of malware, which is software meant to cause problems.\u00a0\n\"We're looking at about 200,000 samples of malicious code a day, so we can guard maybe 11 million events in a microsecond,\" said Tomer Weingarten, the chief executive of a computer security company called SentinelOne. To stay on top of that volume, Mr. Weingarten said, requires the equivalent of 10,000 computers.\nAs computing becomes more pervasive, traditional defenses are proving inadequate. For example, the firewall, which was once an effective safeguard on the perimeter between a corporate network and the world, is now problematic: It has become harder to say where systems begin and end as they become connected to more and more things. In 2013, Target was hacked when criminals entered the main servers through software for a company heating system that was managed by a contractor.\nMore recently, \"sandboxes\" have been developed that temporarily isolate incoming programs and files to see if they try something malicious. In response, hackers have written code enabling malware to recognize that it is being quarantined - sometimes by contacting a computer's operating system directly - so it does not take any suspicious action until it detects that it has been released.\nEvery day, SentinelOne's computers scour the many listings worldwide of known malware and attack codes, which are publicly posted by government agencies and private security organizations. Using machine learning, an A.I. technique of pattern mapping, the computers then look for similarities with known techniques and try to identify similar behaviors that precede attacks.\nThat information is then loaded into computing \"agents\" that are inside its clients' computers. The agents observe events inside a computer almost the moment they occur. If, for instance, a so-called ransomware program starts to encrypt a user's files (to lock up the computer, which will be freed only once the owner pays a ransom), the agent will isolate the program and notify the system administrator.\nOften, it can also undo whatever damage was caused by reverting the few files that were affected to an earlier state.\n\"Sometimes it's easy to see malicious behavior - no legitimate application would just start encrypting everything,\" Mr. Weingarten said. \"Other times, they are 'spraying the heap,' looking for all the commands being queued up in the computer so they can rewrite the system and insert their code. Normal applications don't do these things.\"\nEvery piece of malware also has its own biography within the system. Mr. Weingarten recently called up a program called Troldesh, which was first observed on the evening of April 9. It created files on the infected computer, then changed the files and notified a server in Russia that it was ready.\n\"This starts to look suspicious,\" Mr. Weingarten said. Signals can be bounced around, so it is hard to say just where Troldesh originated. It also communicated with machines in Hungary, Austria and Germany.\nTroldesh was identified and stopped, but a hacker could reuse much of the code in other malware. That is why A.I. tries to learn hackers' rules and habits.\nAnother challenge in protecting today's computer networks is how poorly understood much of the world's software is. \"There are 600 million individual files known to be good, and a malware universe of about 400 million files,\" said Lawrence Pingree, an analyst with Gartner. \"But there's also 100 million pieces of potentially unwanted adware, and 200 million software packages that just aren't known. It takes a lot of talent to figure out what's normal and what isn't.\"\nThe process, which he calls \"endpoint detection,\" looks at and acts on what goes on in individual machines.\nMany of the same techniques can also be used on other kinds of bad online behavior. Carlos Guestrin, a well-regarded expert in machine learning, is chief executive and co-founder of a company called Dato. In addition to traditional A.I. businesses like figuring out shopping preferences, he started looking at fraudulent behaviors.\n\"We caught spam with machine learning by looking at sequences of words, now we look for the code in a virus, like DNA, that makes it do unusual things,\" Mr. Guestrin said. \"With human fraud, you look for relationships about who sends money to who, or who is hiding fraudulent transactions. If a finite number of people keep sending each other money, they're probably trying to look like legitimate businesses.\"\nG2 Web Services, based in Bellevue, Wash., helps banks figure out if a website is fraudulent or is selling contraband. Using Mr. Guestrin's product, coupled with human experience, on hundreds of millions of sites, G2 improved its ability to predict fraud and crime by 13 percent. Over millions of transactions, that amounts to quite a lot.\nG2 can also flag prohibited content, like child pornography, which exists on about 1.5 percent of all merchant websites. Sometimes a criminal will put a link to, say, a store for illegal growth hormones in an otherwise honest site, without the merchant's ever knowing about the link placement. Another use for A.I. is spotting \"transaction laundering,\" in which an illegal business tries to appear legitimate by processing transactions through a legal site.\nThe company is making strides against cybercrime, since \"the guys who run these illicit sites are also into viruses and malware,\" said Alan Krumholz, principal data scientist at G2. \"It's a cat-and-mouse game. They go from one business into another.\"\n","163":"SAN FRANCISCO --  A group of researchers at the Chinese web services company Baidu have been barred from participating in an international competition for artificial intelligence technology after organizers discovered that the Baidu scientists broke the contest's rules.\nThe competition, which is known as the ''Large Scale Visual Recognition Challenge,'' is organized annually by computer scientists at Stanford University, the University of North Carolina at Chapel Hill and the University of Michigan. \u00a0\n  It requires that computer systems created by the teams classify the objects in a set of digital images into 1,000 different categories. The rules of the contest permit each team to run test versions of their programs twice weekly ahead of a final submission as they train their programs to ''learn'' what they are seeing.\n  However, on Tuesday, the contest organizers posted a public statement noting that between November and May 30, different accounts had been used by the Baidu team to submit more than 200 times to the contest server, ''far exceeding the specified limit of two submissions per week.''\n  Jitendra Malik, a University of California computer scientist who is a pioneer in the field of computer vision, compared the accusations against Baidu to drug use in the Olympics. ''If you run a 9.5-second 100-meter sprint, but you are on steroids, then how can your result be trusted?'' Mr. Malik said.\n  The episode has raised concern within the computer science community, in part because the field of artificial intelligence has historically been plagued by claims that run far ahead of actual science.\n  Indeed, as early as 1958, when Frank Rosenblatt introduced the first so-called neural network system, a newspaper article about the advance suggested that it might lead to ''thinking machines'' that could read and write within a single year.\n  In the 1960s, when John McCarthy, the scientist who coined the term ''artificial intelligence,'' proposed a new research laboratory to Pentagon officials, he claimed that building a working artificial intelligence system would take a decade. When that did not happen, the field went through periods of decline in the 1970s and 1980s, which have since been described as ''A.I. winters.''\n  Now rapid progress in a hot artificial intelligence field known as ''deep learning'' has touched off a computing arms race among powerful companies like Facebook, Google, IBM, Microsoft and Baidu, and scientists at each company have trumpeted improved performance in vision and speech recognition.\n  As the companies compete in new services as varied as self-driving cars or online personal assistants that converse with mobile phone users, the technologies have moved from the backwater of academic journals to front-page news.\n  With that has come controversy. In the past year, technologists and scientists like Elon Musk, founder of Tesla; Stephen Hawking, the celebrated physicist; and Bill Gates, co-founder of Microsoft, have warned that the potential emergence of self-aware computing systems might prove to be an existential threat to humanity.\n  But artificial intelligence researchers have a more basic concern: that their work will once again fall short of expectations, leading to yet another fallow period for their field.\n  And the Baidu controversy adds to the fretting.\n  This year, Baidu announced that it had built a custom supercomputer named Minwa with the intention of dedicating it to the image recognition contest. Baidu researchers subsequently made a series of announcements about the success of the computer, including one playing up a result more accurate than an earlier score by Google scientists.\n  On May 4, Baidu posted an article on its technology blog headlined ''Baidu Achieves Top Results on Image Recognition Challenge.'' The article has since been removed.\n  Contest organizers said in a statement that by submitting many slightly different solutions it was possible for Baidu to ''achieve a small but potentially significant advantage'' and ''choose methods for further research.''\n  Because Baidu had submitted so many more times than was permissible, it would not be possible to fairly compare its results with those of other teams, the statement said. ''We therefore requested that they refrain from submitting to the evaluation server or the challenge for the next 12 months,'' the judges said.\n  The computer science community has been buzzing.\n  ''We are all wondering what scenario took place behind this debacle,'' said Yann LeCun, a Facebook artificial intelligence researcher and one of the creators of the deep learning field. ''Was it the actions of a lone young researcher under intense pressure to deliver, and under weak oversight by his senior co-authors?''\n  The Baidu episode raises broader questions about scientific research in an era when the lines have begun to blur between basic science and new technologies that have huge commercial potential.\n  Image- and speech-recognition technologies are being used to deploy a variety of powerful new services in the Internet and computing markets. For example, Microsoft is expected to make the improved quality of its speech technology a major selling point in its new Windows 10 operating systems, due to be released in July.\n  A number of computer science researchers said they were concerned about the episode but declined to speak on the record, in part because it is not yet clear what the motive of the Baidu researchers actually was.\n  Scientists organizing the competition, also called the ImageNet Challenge, posted a comment by a Baidu researcher, Ren Wu, who is based in the company's Silicon Valley research office.\n  ''We apologize for this mistake and are continuing to review the results. We have added a note to our research paper, ''Deep Image: Scaling Up Image Recognition,'' and will continue to provide relevant updates as we learn more,'' the statement read. ''We are staunch supporters of fairness and transparency in the ImageNet Challenge and are committed to the integrity of the scientific process.''\n\n\n\n","164":"There are basically three big questions about artificial intelligence and its impact on the economy: What can it do? Where is it headed? And how fast will it spread?\nThree new reports combine to suggest these answers: It can probably do less right now than you think. But it will eventually do more than you probably think, in more places than you probably think, and will probably evolve faster than powerful technologies have in the past. \n  This bundle of research is itself a sign of the A.I. boom. Researchers across disciplines are scrambling to understand the likely trajectory, reach and influence of the technology -- already finding its way into things like self-driving cars and image recognition online -- in all its dimensions. Doing so raises a host of challenges of definition and measurement, because the field is moving quickly -- and because companies are branding things A.I. for marketing purposes.\u00a0\n  An ''AI Index,'' created by researchers at Stanford University, the Massachusetts Institute of Technology and other organizations, released on Thursday, tracks developments in artificial intelligence by measuring aspects like technical progress, investment, research citations and university enrollments. The goal of the project is to collect, curate and continually update data to better inform scientists, businesspeople, policymakers and the public.\n  The McKinsey Global Institute published a report on Wednesday about automation and jobs, sketching out different paths the technology might take and its effect on workers, by job category in several countries. One finding: Up to one third of the American work force will have to switch to new occupations by 2030, in about a dozen years.\n  And in an article published in November by the National Bureau of Economic Research, economists from M.I.T. and the University of Chicago suggest an answer to the puzzle of why all the research and investment in A.I. technology have so far had little effect on productivity.\n  Each of the three research initiatives has a somewhat different focus. But two common themes emerge from the reports and interviews with their authors.\n  \u00e2-\u00a0 Technology itself is only one ingredient in determining the trajectory of A.I. and its influence. Economics, government policy and social attitudes will play major roles as well.\n  \u00e2-\u00a0 Historical patterns of adoption of major technologies, from electricity to computers, are likely to hold true for A.I. But if the pattern is similar, the pace may not be. And if it is much faster, as many researchers predict, the social consequences could be far more wrenching than in past transitions.\n  The AI Index grew out of the One Hundred Year Study on Artificial Intelligence, a Stanford-based project begun in 2014 by A.I. experts. The study group, mainly scientists, seeks to broaden understanding of artificial intelligence and thus increase the odds society will be benefit from the technology.\n  The group was initially going to publish major studies every five years. But given the speed of progress and investment, the five-year interval ''seemed way too slow,'' said Yoav Shoham, a professor emeritus at Stanford and chair of the steering committee for the ''AI Index.''\n  The new index is not a single number, but a series of charts and graphs that track A.I.-related trends over time. They include measures like the rate of improvement in image identification and speech recognition, as well as start-up activity and job openings. There are also short essays by artificial intelligence experts.\n  Some of the charts showing the progress of technology are telling. Image and speech recognition programs, for example, have matched or surpassed human capabilities in just the past year or two.\n  But A.I. experts warn that gains in specific tasks or game-playing proficiency are still a far cry from general intelligence. A child, for example, knows that a water glass tipping on the edge of a table will most likely fall to the floor and spill the water. He or she understands the physics of everyday life in a way artificial intelligence programs do not yet.\n  ''The public thinks we know how to do far more than we do now,'' said Raymond Perrault, a scientist at SRI International, who worked on the index.\n  The current ''AI Index,'' Mr. Shoham said, is ''very much a first step.'' The group is seeking contributions of data and comments from academic and corporate researchers around the world. The idea, he said, is to create ''a living index'' that details as many measurable dimensions of the field as possible, including social impact.\n  The McKinsey automation-and-jobs report captures the uncertainty surrounding A.I. and its coming effect on labor markets. Its projection of the number of Americans who will have to find new occupations by 2030 ranges from 16 million to 54 million -- depending on the pace of technology adoption.\n  The faster A.I. advances, the greater the challenge. McKinsey's upper-range projection of 54 million suggests a more rapid transformation than in previous waves of change in the work force, when employment migrated from farms to factories and later from manufacturing to services.\n  ''That's where the conversation has to go -- how to manage this transition,'' said Susan Lund, an economist at McKinsey. ''We need a major change in how we provide midcareer retraining and how we help displaced workers find new employment.''\n  Still, the rise of A.I. has not yet shown up in the economy as a whole, at least not in the numbers. In their recent paper, Erik Brynjolfsson and Daniel Rock of the M.I.T. Sloan School of Management and Chad Syverson of the University of Chicago Booth School of Business call it ''a clash of expectations and statistics.''\n  They offer a few possible explanations, including false hopes and poor measurements of the new technology. But the one they settle on is a lag in the adoption and effective use of A.I.\n  There are historical precedents. The electric motor, for example, was introduced in the early 1880s. But it was not until the 1920s that discernible productivity gains showed up, after the motors spread and factory work was reorganized into mass-production assembly lines to exploit the new technology of its day.\n  A.I. will follow a similar path, but faster, predicts Mr. Brynjolfsson, who also worked in the ''AI Index.'' And the index, he said, should help accelerate adoption by giving people needed information to make better decisions.\n  There are A.I. skeptics, but Mr. Brynjolfsson is not one of them. ''History shows it takes years, even with a powerful technology in place,'' he said. ''But to me, it's dead certain it's going to happen.''\n\n\n\n","166":"SAN FRANCISCO -- Samsung, the largest maker of televisions and smartphones in the world, is buying Viv Labs, a high-profile artificial intelligence startup founded by the inventors of Apple's Siri voice assistant, the South Korean electronics conglomerate said Wednesday.\nThe move is a major land grab in the battle for cutting-edge artificial intelligence taking place among tech giants. Samsung, which has lagged in such services, is now positioned to add intelligent, Siri-like voice capabilities to its vast collection of home and mobile electronics.\u00a0\n\"We have a unique opportunity to take advantage of AI, and show the rest of the industry what the smart, connected world can look like,\" Jacopo Lenzi, Samsung's Senior Vice President of Business Development and Strategic Acquisitions, said in an interview. The companies declined to disclose the purchase price of the deal.\nIt is no coincidence that Samsung chose to make its announcement just one day after Google touted its own foray into smartphones and other technologies that can converse back and forth with users, providing them with information and services.\nSamsung and Google have a longstanding and complex partnership - one that is likely to become further strained now that Google has released a smartphone to compete directly with Samsung's flagship Galaxy device, said Charles Golvin, research director at Gartner. Google's Android operating system is used in most Samsung smartphones, as is the Google Play app store and many of Google's popular apps.\nGiven the frenzy around artificial intelligence, Lenzi said it was not surprising that multiple companies would make similar efforts. He declined to comment further on the Google relationship.\nGoolge released a high-end smartphone and home device, called Google Home, on Tuesday. Both come equipped with the new so-called Google Assistant, a voice-assistant that can do things like buy movie tickets, play music, or make a hotel reservation when asked.\n\"When I look at where computing is headed, it's clear to me that we're evolving from a mobile-first to an AI-first world,\" Google's chief executive Sundar Pichai told an audience in San Francisco that had gathered for the launch of the new products.\nIn an interview Tuesday, Google's Vice President of Product for Android, Brian Rakowski, said the company's broader goals were to maintain strong partnerships with manufacturers that would elevate the quality of all Android products. He said the Pixel smartphone is in its early days, but that the relationships might be reconsidered down the line.\n\"This is our first rodeo,\" Rakowski said. \"If we're successful, maybe we'll rethink that in a few years.\"\nThe Google Assistant sounds remarkably similar to Viv, whose capabilities were outlined in the Washington Post in May. Viv's founders, Adam Cheyer and Dag Kittlaus, picture a world in which people can interact with technology in one long conversation, liberated from the frustrating experience of toggling between apps on a smartphone.\nFor example, a Viv user could tell the assistant they want a hotel room in San Jose, and the assistant would not only offer options, but ask follow-up questions such as, \"Do you prefer a king-size bed or a room with a view?\" The assistant would then complete the transaction, all without a person having to go to an app -- or type anything at all.\nIf Viv sounds far-fetched, it's because nothing like it exists today - and there was always a question of whether the founders could really pull it off.\nCurrent virtual assistants, such as Siri, Google Now, or Samsung's current one called S-Voice, can do a Web search based on a user's voice command. But those assistants largely produce a list of links and options to click on - users still have to resort to typing to make a purchase.\nIn contrast, Viv Labs has partnered with dozens of vendors behind the scenes, including hotels, florists, movie and concert tickets services, travel companies and even the makers of smart refrigerators and other appliances (In Kittlaus and Cheyer's world, appliances powered by Viv's tech will also be able to talk back and forth to consumers). These vendors broke down attributes of their products and shared them with Viv engineers. For example, a flower company has identified its flowers by color, texture, and type so that Viv could be responsive to specific questions.\nFinding the right buyer was always part of Viv's strategy. As in the case of Siri, which was a tiny software program when Apple bought it in 2010, Kittlaus knew that getting broad reach would mean partnering with a large player whose products are already used by massive numbers of consumers.\nBoth Google and Facebook made offers earlier this year to buy Viv, and Kittlaus also talked with Comcast, according to people familiar with the matter.\nConversations with Samsung began this summer, Lenzi said. Kittlaus said he was impressed by the reach of Samsung - the sheer number of products, from washing machines to smartphones to televisions, that could become animated by conversation. Of course, Viv has not yet been tested on that kind of scale.\n\"I honestly didn't realize how big it was until we got into these discussions,\" Kittlaus said in an interview. \"There are are sorts of different touch points that can be brought to life with these conversational interactions.\"\n","168":"SAN FRANCISCO -- There is little doubt that the Defense Department needs help from Silicon Valley's biggest companies as it pursues work on artificial intelligence. The question is whether the people who work at those companies are willing to cooperate.\nOn Thursday, Robert O. Work, a former deputy secretary of defense, announced that he is teaming up with the Center for a New American Security, an influential Washington think tank that specializes in national security, to create a task force of former government officials, academics and representatives from private industry. Their goal is to explore how the federal government should embrace A.I. technology and work better with big tech companies and other organizations. \u00a0\n  There is a growing sense of urgency to the question of what the United States is doing in artificial intelligence. China has vowed to become the world's leader in A.I. by 2030, committing billions of dollars to the effort. Like many other officials from government and industry, Mr. Work believes the United States risks falling behind.\n  ''The question is, how should the United States respond to this challenge?'' he said. ''This is a Sputnik moment.''\n  The military and intelligence communities have long played a big role in the technology industry and had close ties with many of Silicon Valley's early tech giants. David Packard, Hewlett-Packard's co-founder, even served as the deputy secretary of defense under President Richard M. Nixon.\n  But those relations have soured in recent years -- at least with the rank and file of some better-known companies. In 2013, documents leaked by the former defense contractor Edward J. Snowden revealed the breadth of spying on Americans by intelligence services, including monitoring the users of several large internet companies.\n  Two years ago, that antagonism grew worse after the F.B.I. demanded that Apple create special software to help it gain access to a locked iPhone that had belonged to a gunman involved in a mass shooting in San Bernardino, Calif.\n  ''In the wake of Edward Snowden, there has been a lot of concern over what it would mean for Silicon Valley companies to work with the national security community,'' said Gregory Allen, an adjunct fellow with the Center for a New American Security. ''These companies are -- understandably -- very cautious about these relationships.''\n  The Pentagon needs help on A.I. from Silicon Valley because that's where the talent is. The tech industry's biggest companies have been hoarding A.I. expertise, sometimes offering multimillion-dollar pay packages that the government could never hope to match.\n  Mr. Work was the driving force behind the creation of Project Maven, the Defense Department's sweeping effort to embrace artificial intelligence. His new task force will include Terah Lyons, the executive director of the Partnership on AI, an industry group that includes many of Silicon Valley's biggest companies.\n  Mr. Work will lead the 18-member task force with Andrew Moore, the dean of computer science at Carnegie Mellon University. Mr. Moore has warned that too much of the country's computer science talent is going to work at America's largest internet companies.\n  With tech companies gobbling up all that talent, who will train the next generation of A.I. experts? Who will lead government efforts?\n  ''Even if the U.S. does have the best A.I. companies, it is not clear they are going to be involved in national security in a substantive way,'' Mr. Allen said.\n  Google illustrates the challenges that big internet companies face in working more closely with the Pentagon. Google's former executive chairman, Eric Schmidt, who is still a member of the board of directors of its parent company, Alphabet, also leads the Defense Innovation Board, a federal advisory committee that recommends closer collaboration with industry on A.I. technologies.\n  Last week, two news outlets revealed that the Defense Department had been working with Google in developing A.I. technology that can analyze aerial footage captured by flying drones. The effort was part of Project Maven, led by Mr. Work. Some employees were angered that the company was contributing to military work.\n  Google runs two of the best A.I. research labs in the world -- Google Brain in California and DeepMind in London.\n  Top researchers inside both Google A.I. labs have expressed concern over the use of A.I. by the military. When Google acquired DeepMind, the company agreed to set up an internal board that would help ensure that the lab's technology was used in an ethical way. And one of the lab's founders, Demis Hassabis, has explicitly said its A.I. would not be used for military purposes.\n  Google acknowledged in a statement that the military use of A.I. ''raises valid concerns'' and said it was working on policies around the use of its so-called machine learning technologies.\n  Among A.I. researchers and other technologists, there is widespread fear that today's machine learning techniques could put too much power in dangerous hands. A recent report from prominent labs and think tanks in both the United States and Britain detailed the risks, including problems with weapons and surveillance equipment.\n  Google said it was working with the Defense Department to build technology for ''non-offensive uses only.'' And Mr. Work said the government explored many technologies that did not involve ''lethal force.'' But it is unclear where Google and other top internet companies will draw the line.\n  ''This is a conversation we have to have,'' Mr. Work said.\n\n\n\n","169":"The term \"artificial intelligence\" is widely used, but less understood. As we see it permeate our everyday lives, we should deal with its inevitable exponential growth and learn to embrace it before tremendous economic and social changes overwhelm us.\nPart of the confusion about artificial intelligence is in the name itself. There is a tendency to think about AI as an endpoint - the creation of self-aware beings with consciousness that exist thanks to software. This somewhat disquieting concept weighs heavily; what makes us human when software can think, too? It also distracts us from the tremendous progress that has been made in developing software that ultimately drives AI: machine learning.\u00a0\nMachine learning allows software to mimic and then perform tasks that were until very recently carried out exclusively by humans. Simply put, software can now substitute for workers' knowledge to a level where many jobs can be done as well - or even better - by software. This reality makes a conversation about when software will acquire consciousness somewhat superfluous.\nWhen you combine the explosion in competency of machine learning with a continued development of hardware that mimics human action (think robots), our society is headed into a perfect storm where both physical labor and knowledge labor are equally under threat.\nThe trends are here, whether through the coming of autonomous taxis or medical diagnostics tools evaluating your well-being. There is no reason to expect this shift towards replacement to slow as machine learning applications find their way into more parts of our economy.\nThe invention of the steam engine and the industrialization that followed may provide a useful analogue to the challenges our society faces today. Steam power first substituted the brute force of animals and eventually moved much human labor away from growing crops to working in cities. Subsequent technological waves such as coal power, electricity and computerization continued to change the very nature of work. Yet, through each wave, the opportunity for citizens to apply their labor persisted. Humans were the masters of technology and found new ways to find income and worth through the jobs and roles that emerged as new technologies were applied.\nHere's the problem: I am not yet seeing a similar analogy for human workers when faced with machine learning and AI. Where are humans to go when most things they do can be better performed by software and machinery? What happens when human workers are not users of technology in their work but instead replaced by it entirely? I will admit to wanting to have an answer, but not yet finding one.\nSome say our economy will adjust, and we will find ways to engage in commerce that relies on their labor. Others are less confident and predict a continued erosion of labor as we know it, leading to widespread unemployment and social unrest.\nOther big questions raised by AI include what our expectations of privacy should be when machine learning needs our personal data to be efficient. Where do we draw the ethical lines when software must choose between two people's lives? How will a society capable of satisfying such narrow individual needs maintain a unified culture and look out for the common good?\nThe potential and promise of AI requires a discussion free of ideological rigidity. Whether change occurs as our society makes those conscious choices or while we are otherwise distracted, the evolution is upon us regardless.\n                           Jonathan Aberman is a business owner, entrepreneur and founder\u00a0of Tandem NSI, a national community that connects innovators to government agencies. He is host of \"What's Working in Washington\" on WFED, a program that highlights business and innovation, and he lectures at the University of Maryland's Robert H. Smith School of Business.           \n","170":"PETER WILL uses a computer to find oil. As director of systems science at Schlumberger Research in Connecticut, a division of the Schlumberger Corporation, he heads a team of experts who use artificial-intelligence systems to find oil throughout the world.\n''We drop sensors into a well so the data from the sensors can be transmitted to a centralized site where A.I. software is used to interpret the signals,'' Mr. Will explains, using the abbreviation for artificial intelligence. ''The raw signals measure physical phenomena. Inferences are then made from the physical phenomena in order to give clients answers. As a result of this information, we know where to drill, how much we'll get out of the ground and whether hydrocarbons are present.''\nArtificial intelligence is the cutting edge of computer programming. A broad and term, it embraces a number of different types of software - some truly advanced and others not. It is also fast becoming one of the computer industry's most overused buzzwords, as if artificial intelligence was itelf a type of program. It is not. Instead, it is a process, used to make existing programs - everything from databases to word processors, more efficient to use.\u00a0\nTrue artificial-intelligence programs seek to capture human-like leaps of logic within the rigid, orderly form of computer language. Some artificial intelligence programs involve what the industry terms ''expert systems,'' software that contains, for example, all that an engineer might know about fixing a railroad car. Others involve programs that evolve as they are used, watching the patterns of the computer operator and beginning to perform rote chores automatically. Thus, a computer user who every morning called up his electronic mail and routed all memos on a specific topic to an associate could soon find that the computer was sending the information there by itself.\n\n\nFor all the hoopla suddenly surrounding the topic, A.I. is hardly new. It emerged in the early 1950's, when the British mathematician and computer pioneer Alan Turing published a paper entitled ''Computing Machinery and Intelligence,'' in which he posed the question ''Can machines think?'' Since then, an elite group of computer scientists have concentrated on expanding the computer's abilities from performing such duties as solving sophisticated mathematical problems to such higher-order functions as making decisions, reasoning and even, in a rudimentary fashion, thinking. As A.I. research has progressed, computers have been designed that can see, touch, smell, recognize spoken commands and answer in plain English in certain situations.\nComputers in a number of hospitals in this country are capable of diagnosing diseases with extraordinary accuracy. In England, computer scientists have created a ''bionic nose'' capable of distinguishing different fragrances. I.B.M. has developed a computer called ''Epistle'' that can actually read printed material with an optical scanner, and to a limited degree analyze and edit copy.\nFor the future, experts project that A.I. programs will be designed that can teach children mathematical skills and drive trucks or automobiles. The ultimate goal for A.I. researchers is to design programs that can do anything requiring human intelligence - a goal that is still far off, since we have only a vague idea how human beings actually think.\nPrograms cannot yet be designed, for example, that involve the thinking processes that go into creating a work of art. In the process of creating anything, humans tap brain centers that involve imagination and intuition, which A.I. professionals do not yet understand well enough to imitate in software programs. As Howard Dicken, president of DM Data Inc., a Scottsdale, Ariz., consulting firm that specializes in A.I., explains: ''One has to thoroughly understand how one solves a problem or thinks in order to design a program that can actually perform that task. The problem is, there is so much we don't understand about the human mind.''\nPhilip Cooper, chairman and chief executive officer of Palladian Software Inc., in Boston, an A.I. company designing expert-systems software for finance and manufacturing, says there are a lot of myths and misconceptions about A.I. ''You can go out and buy a word processor,'' he said, ''but you can't go out and buy artificial intelligence. There really isn't any business called A.I. A.I. consists of features that can be built into a computer system.''\nAccording to Mr. Cooper, A.I. has found uses in four separate areas: robotics, the utilization of computer- run machines that perform simple and complex tasks; vision systems, computers that can recognize pattern matches and distinguish shapes, such as trees from tanks; natural language, computers that can understand or speak English, and expert systems, creating programs with specific applications.\nIf Mr. Cooper or Mr. Will of Schlumberger Research decided to quit their work, they would have no trouble securing new jobs quickly, according to Daryl Furno, a senior associate at Halbrecht Associates Inc., an executive-search firm in Connecticut for computer and communications industries. Mrs. Furno said most A.I. experts have never lacked for job offers to entice them away from their current projects.\nDM Data's Mr. Dicken projected that by 1990 there would be 50,000 job openings for A.I. professionals. He estimated that to be nearly six times the total number of technicians and professionals working at the moment on corporate or university projects. ''It's safe to say that there are at least 150 start-up companies with sales of under $5 million looking for talent,'' Mr. Dicken said.\nHerbert Halbrecht, president of Halbrecht Associates, noted a tremendous demand for expert systems in the financial area. ''The expert-systems area offers the best near-term prospect for financial return,'' he said, ''and as a result, substantial venture capital has gone into this aspect of A.I. ''It's safe to say,'' he added, ''that every major bank, insurance company and brokerage house is keenly interested in A.I. for the profit leverage in expert systems that can help automate lower-level management jobs, such as running a branch operation or putting through buy\/sell orders. A.I. software can be designed that will help stockbrokers design portfolios for clients, plus guide them in making prudent decisions based upon client needs.''\nA few years ago, Mr. Halbrecht said, only a few companies were seriously exploring the benefits of A.I. Computers of sufficient speed and capacity to handle A.I. programming were unavailable except at a handful of major universities. Since then a new generation of powerful yet relatively inexpensive machines designed especially for A.I. has come on the market, making commercial applications feasible for the first time.\n\nAS the demand for A.I. experts rises, more and more colleges and universities are offering courses and curriculums in the discipline.  M.I.T., Stanford and Carnegie-Mellon, for example, offer doctoral programs. A fairly new development is that companies involved in high-technology industries and services are offering A.I. training seminars. Do-it-yourself technocrats should soon be teaching themselves A.I. fundamentals so they can design their own systems.\n''Ph.D.'s with degrees in computer science are in strong demand,'' added Mrs. Furno. ''The next generation of A.I. experts, however, will come from assorted disciplines and even now, qualified people are entering the field with master's and at the lower level with bachelor's degrees.'' With a B.A. degree in computer science and a heavy concentration in A.I. courses, for example, one can work as a junior or apprentice researcher for a small research group, or possibly work as an assistant for a consulting group or organization.\nMr. Halbrecht said that pioneers in the field started out in operations research, but now the backgrounds of A.I. experts are more varied. ''Since A.I. can be used in so many ways,'' he said, ''more and more people are entering the field with anthropology, medicine, engineering and even linguistic backgrounds.''\nBut whatever a person's background, he added, all A.I. people have immersed themselves in one or several computer-science skills, such as programming, systems analysis or computer languages.\nParallel with the strong demand for A.I. expertise, salaries for A.I. professionals are generally higher than those of most computer professionals. According to Christian & Timbers, a Cleveland recruiting firm, qualified A.I. professionals are demanding 10 percent to 20 percent premiums over most computer professionals. Mrs. Furno said that A.I. experts with Ph.D. degrees working for large companies could command salaries in the area of $60,000 to $100,000 a year. Computer professionals who hold master's degrees can earn between $40,000 and $70,000, and an increasing number who are working in a training capacity earn salaries ranging between $25,000 and $35,000 a year.\n''No matter how you view the short- or long-term picture, opportunities for A.I. professionals are only getting better,'' Mrs. Furno said.\n\n\n","171":"After the Russian hacking of the 2016 election, many people worry that technology has gone too far. And yet it continues to evolve rapidly.\nLargely because of the success of companies like Google, Facebook and Amazon, investment in tech research continues to climb. At the same time, because of the sudden maturation of mathematical methods that can deliver what is commonly called artificial intelligence, the possibilities are expanding.\nThere is reason for concern, but also for optimism. The new wave of artificial intelligence will reduce jobs, but will also improve your health and products like your smartphone. Here are five areas where tech companies, large and small, will change the way we live.\nA.I. Health Care\nOver the last half decade, with help from the complex algorithms deep neural networks, computers have learned to see. Loosely based on the web of neurons in the human brain, a neural network can learn tasks by identifying patterns in vast amounts of data. By analyzing millions of bicycle photos, for instance, a neural network can learn to recognize a bicycle.\nThis means that services like Facebook and Google Photos can instantly recognize faces and objects in images uploaded to the internet. But artificial intelligence will also lead to a revolution in health care. Using these same techniques, machines can also learn to identify signs of disease and illness in medical scans. By analyzing millions of retinal photos, a neural network can learn to recognize early signs of diabetic blindness. By analyzing CT scans, a neural network can learn to spot lung cancer.\nSuch technology will improve health care in places where doctors are scarce. But eventually, it will streamline care in the developed world as well. Google is already running tests inside two hospitals in India, and the start-up Infervision has deployed similar technology in hospitals across China.\nIn the longer term, similar methods promise to rapidly accelerate drug discovery and so many other aspects of health care. \"Everything from the nature of the food that we grow and eat to the drugs that we give ourselves to how we monitor the impact of these things is all being transformed by A.I. in deeply profound ways,\" said Matt Ocko, a managing partner at DCVC, a San Francisco venture capital firm that has invested heavily in this area.\nConversational Computing\nNeural networks are not limited to image recognition. Far from it. These same techniques are rapidly improving coffee-table gadgets like the Amazon Echo, which can recognize spoken commands from across the room, and online services like Skype, which can instantly translate phone calls from one language to another. They may even eventually produce machines that can carry on a conversation.\nRecently, said Luke Zettlemoyer, a University of Washington professor, there has been a \"huge phase shift\" in the area of natural language understanding -- technology that understands the natural way people talk and write. Companies like Google, Facebook, and Microsoft are at the forefront of this movement, which promises to fundamentally change how we interact with phones, cars, and potentially any machine. Many companies are moving down the same path, including Replika, a San Francisco start-up.\nWith help from machine learning, Replika offers a smartphone \"chatbot\" that acts as a kind of personal confidante, chatting with you in moments when no one else is around. But the hope is that these techniques will improve to where they serve you in so many other ways. What if Alexa was truly conversational, if you could have a back and forth dialogue? Right now, it is about basic questions and commands. Today, it \"recognizes\" words very very well. But truly \"understanding\" complex English sentences is beyond machines at this point. What if machines could carry on a dialogue like Hal in 2001?\nMind Control\nSome people argue there are even better ways of interacting with computers by using brain waves. Rather than telling a computer what you want, many companies say they believe you could just think it.\nUsing electroencephalography, or EEG -- a longstanding means of measuring electrical brain activity from sensors placed on the head -- the start-up Neurable is building a virtual reality game that can be played with the mind. EEG is limited for this kind of use, but other researchers, including at Facebook, aim to build a far more powerful systems using optical sensors. Facebook hopes that, in a few years, this technology will let people type with their minds five times faster than they can with a smartphone keyboard.\nThese techniques will also face physical limits, and that may bar the way to Facebook's goal. But various start-ups, including Neuralink, founded by Elon Musk, the chief executive of Tesla, are going several steps further, hoping to read brain activity from chips implanted inside the skull. At first, they will limit this technology to people with disabilities. But ultimately, Mr. Musk and others hope to also implant chips in healthy people.\n\"It is implausible that this technology would go straight into healthy people,\" said Ed Boyden, an M.I.T. neuroscientist who is also an adviser to Neuralink. \"But there is a natural trajectory where, if a medical technology proves effective, it can move into normal individuals as well.\"\nThe Flying Car\nWant more science fiction in your everyday reality? As entrepreneurs like Mr. Musk work to put a chip in your head, others are working to put cars in the skies.\nEven as he sets the pace in the race to autonomous cars, Larry Page, the chief executive of Alphabet and a founder of Google, is backing Kitty Hawk, a start-up that wants to move commuting into the air. And many others, including the start-up Joby Aviation, Uber and Airbus, are working on vehicles capable of flying above congested roads. These vehicles take many forms, but generally, they carry a single rider and take off like a helicopter: straight up.\nAt first, Kitty Hawk will sell its vehicles to hobbyists. But the company hopes it can eventually convince the general public, and regulators, that flying cars make sense. That is no easy task. After all, these cars will require a new kind of air traffic control.\nThe Quantum Computer\nEven more outlandish? It's the prospect of a quantum computer. Drawing on the seemingly magical properties of quantum physics, such a machine would be exponentially more powerful than computers of today. Think of it this way: A quantum computer could instantly crack the encryption that protects the world's most private data.\nThe problem is that these machines are enormously difficult to build. but progress has accelerated. Google, IBM and Intel are investing heavily in this push, as are start-ups like Rigetti Computing.\nResearchers say they believe that quantum machines eventually could accelerate drug discovery, streamline financial markets, solve traffic problems and more.\n\"It is a completely different paradigm for processing information,\" said Robert Schoelkopf, who helped invent the techniques that are driving so much of quantum computing research. \"So we think that known applications are just the tip of the iceberg.\"\nPHOTOS: Top, the Kitty Hawk Flyer prototype \"flying car.\" Above, from left: doctors using Infervision's products at Wuhan Tongji Hospital in China; Rigetti's 8-qubit superconducting quantum processor; and a virtual-reality game by Neurable. (PHOTOGRAPHS BY IAN MARTIN; INFERVISION; RIGETTI COMPUTING; NEURABLE)Related Articles\n\n","172":"A special report on the future of technology:\nPower in Numbers: China Aims for High-Tech Primacy\nCreating Artificial Intelligence Based on the Real Thing.\nVast and Fertile Ground in Africa for Science to Take Root\nWith a Leaner Model, Start-Ups Reach Further Afield\nA High-Stakes Search Continues for Silicon's Successor\nOut of a Writer's Imagination Came an Interactive World\nLooking Backward to Put New Technologies in Focus.\nInteractive Map\nEssays from some of the leading minds in computing:\nTaking Faster and Smarter to New Physical Frontiers\nLeave the Driving to the Car, and Reap Benefits in Safety and Mobility\nDeath Knell for the Lecture: Technology as a Passport to Personalized Education\nAn Evolution Toward a Programmable Universe\nIn an Open-Source Society, Innovating by the Seat of Our Pants\nComputer Scientists May Have What It Takes to Help Cure Cancer\nChina Is Poised for an I.T. Golden Age\nNew Tools for New Computing Challenges\nFull Speed Ahead, Without a Map, Into New Realms of Possibility\n\n","173":"Most fictions about artificial intelligence are not, of course, about artificial intelligence. They use the topic as a metaphor, a scale on which to weigh and measure the assets and liabilities of humanity itself. \"Marjorie Prime,\" a new film written and directed by Michael Almereyda, adapted from Jordan Harrison's acclaimed play, is an alternately stately and brisk story in which the metaphor poses questions about mortality, loss and who our imagined \"best selves\" might turn out to actually be once they're released from our own assessment.\nThe great Lois Smith reprises the role she played in both the 2014 Los Angeles and 2015 New York productions of the play, an old woman named Marjorie with an unspecified disease who has been given a new companion. \"He\" is Walter, her dead husband, reincarnated in an in-his-prime version played by Jon Hamm.\u00a0\nIn his early interactions with Marjorie, Walter, a holographic-generated \"prime\" whose personality software is constructed both from what he left behind and from memories of the people close to him, is kind, solicitous and a little wide-eyed. Whenever Marjorie \"reminds\" him of a trait the actual Walter had, or a shared experience, his prime obediently says, \"I'll remember that now.\" (The movie is set in an unspecified point in the future, though if you do the math around the video release of a movie that looms large in Walter and Marjorie's early story, it would seem to be around 2050.)\nMarjorie's daughter, Tess, has prickly feelings about having this Walter in the house, while her husband, Jon, is more optimistic about the digital ghost's potential for service. As you might infer from the title, household roles subsequently shift, and then shift some more. The characters' quiet desperation to hold on to the ones they've loved becomes a sort of trap.\nThe movie has certain affinities with other recent critical favorites, but is less knotty than \"Ex Machina\" (2014) and less sentiment driven than \"Her\" (2013). It is not particularly concerned with gender roles or even tender feelings as such. Tim Robbins and Geena Davis, as Jon and Tess, play their characters with close-to-the-vest discipline, and Ms. Smith tempers Marjorie's physical vulnerability with a wry skepticism.\nAt one point, a character points out that whenever humans revisit a memory, they are not recollecting the actual event: rather they are remembering what they remember. In that respect, the extent to which our memories make up our selves is changing without us even realizing. The \"primes\" here don't work that way. Or do they? And what difference does that make to their interaction with humans, and vice versa?\nMr. Almereyda (whose 2015 film, \"Experimenter,\" was an underseen beauty) is a seasoned cinematic spellbinder. He doesn't do much to open up Mr. Harrison's theatrical work, confining it for the most part to an elegant, mysterious beach house, one that, paradoxically enough, contains no visible high-tech devices. When you try to break down how he achieves his seductive magic with respect to this narrative, it all looks, initially, like nothing more than good cinematic common sense. The key is what he does on the sly: the way his subtle shifts of focus within a shot don't just change the emphasis of the scene, but mirror quirks of consciousness. There's more going on in this movie's 90-plus minutes than in many summer blockbusters nearly twice its length.\nMarjorie Prime  Not rated. Running time: 1 hour 38 minutes. \nPHOTO: Jon Hamm and Lois Smith in Michael Almereyda's film adaptation of the acclaimed play \"Marjorie Prime.\" (PHOTOGRAPH BY FilmRise FOR THE NEW YORK TIMES)Related Articles\n\n","175":"Dr. Saul Amarel, who helped develop the field of artificial intelligence and founded the computer science department at Rutgers University, died on Wednesday in Princeton, N.J., where he lived. He was 74.\u00a0\n     The cause was complications of cancer, according to Rutgers. \n At Rutgers, Dr. Amarel developed computer time-sharing, and his laboratory became an early node on Arpanet, the precursor to the Internet. He took a leave in the 1980's to spend a few years directing a computer science program at the Pentagon, and returned to Rutgers in 1988.\nAmong his peers, Dr. Amarel was perhaps best known for a paper he wrote in 1968, which put him at the vanguard of the artificial intelligence movement.\nDecades later, the importance of the paper may be hard to understand. It concerned the way one might program a computer to solve a brain-teaser well known to mathematicians that involves three cannibals, three missionaries and a boat that seats only two. The challenge for the missionaries is to transport the cannibals across a river without ever letting any of their party be outnumbered -- and eaten.\nSolving the problem was not really the point. That had already been done. What Dr. Amarel set out to do was to create an approach that did not rely on a mechanical crunching of numbers, but instead used an algorithm that allowed the computer to figure out a solution in a manner more akin to human reasoning.\nLater, Dr. Amarel joined artificial intelligence with other fields. He worked, for example, to develop a machine that could diagnose illness, and he envisioned a time when machines would be able to do things like customizing drugs.\nSaul Amarel was born in Salonika, Greece, and moved with his family to what became Israel. He graduated from the Israel Institute of Technology in 1948, fought in Israel's war of independence, then went to Columbia University, where he earned a master's degree and a doctorate.\nBefore Rutgers, he worked at the Computer Theory Group at RCA Laboratories in Princeton.\nDr. Amarel's first wife, Marianne Kroh Amarel, died in 1989. He is survived by his second wife, Irene, and two sons from the first marriage, Dan and David, of New York. \n","176":"FRANK ROSE's Into the Heart of the Mind seems an unabashed effort to imitate Tracy Kidder's The Soul of a New Machine, exploring the field of artificial intelligence as opposed to computers in general. Like Kidder, Rose attempts to humanize high technology by telling a tale of personalities and environment. Alas, like Kidder, he lacks a novelist's flair for doing so and relies on obsessive detail as a substitute.\nTo describe the AI labs, he begins with a dry, factual synopsis of the weather outside their windows. To convey the character of computer programmers, he invariably lists the colors of their clothes, as in: \"Joe arrived breathless from the Xerox room, closing the door behind him. Today he was wearing brown polyester slacks and a yellow shirt with wide brown stripes. He seemed very animated.\" Readers would surely not tolerate such pedestrian scene-setting in a work of fiction; perhaps it's more acceptable in a book of this type because it demystifies a disturbing subject simply by making it dull.\u00a0\nTo be fair, there are sections of lucid and authoritative exposition buried among the platitudes. The persistent reader can piece together a fair summary of the history of artificial intelligence, and its two current areas of endeavor: pure research, with the grandiose goal of programming a computer to function in exactly the same way that our minds function; and applied research, which uses orthodox programming techniques to make computers imitate a few limited skills such as deductive logic or pattern recognition.\nApplied AI is already being used in industrial robots, cruise missiles, mail-sorting machines, and oil-prospecting programs, to name a few rando examples. It will soon impinge on domestic life in the form of chattering toy robots and \"friendly\" appliances, exacerbating our already tense love- hate affair with technology.\nTHERE IS certainly scope for a book about these everyday gadgets. But Into the Heart of the Mind focuses instead on the abstractions of the pure reseach, which shows no imminent signs of impinging on anyone or anything, mainly because it has made negligible progress toward its goal. The human brain contains perhaps 100 billion neurons, in a web so complex that each neuron may well be linked with a thousand others. No one knows how the cognitive areas work: how vision is interpreted, memory is stored, or thought is processed. No one even knows what thought is.\nConsequently, computer simulations of the brain are attempts to imitate the unknown. Researchers must work by inference, studying human behavior in hopes of deducing the thought processes that direct it. Computer programmers thus end up as social psychologists, a role for which few of them have much natural aptitude. Their approach is relentlessly mechanistic -- almost a parody of behaviorism -- and even their simplified models of social action and reaction, goals and avoidance, achieve meager results. Rose describes months of tedious work to give a computer the initiative to solve the elementary problem of how to go out into a rainstorm and pick up a newspaper without getting wet. (Answer: put on a raincoat.) Teaching the computer what rain is, what a raincoat does, what \"outside\" means, what movement entails, and the relationships between these elements, turns out to be horrendously complex. In fact, as Rose describes, pure AI remains bogged down in philosophical debates about the best way of thinking about the process of thinking about thinking.\nThe Soul of a New Machine derived structure and focus from its own simple mandate: to chronicle the development of one computer from conception to completion. Alas, pure AI research lacks such a tangible theme or product, and Rose's book is unstructured and diffuse as a result, meandering from one academic discussion to the next. The resulting sense of inconclusiveness is exacerbated by the author's somewhat puritanical refusal to enliven the wealth of detail with any analysis or comment. The reader is left without guidance, drifting through a gray world of philosophical speculation and incomplete program code, populated by scientists who seem affable yet as unexciting as astronauts in a NASA press release.\nOne longs for the humor, sharp perceptions, and forthright opinions of, say, Tom Wolfe. Not until The Right Stuff did we discover the real human human interest in the space program. To judge from the recent spate of dull books about daily life in data processing, we may have to wait another decade or so to discover the true colors of computer scientists.\n","177":"On Tuesday, March 20, The Washington Post will bring together pioneering researchers, technology innovators and business leaders for Transformers: Artificial Intelligence, a live news event focused on technological advances that are poised to reshape the way we live and work.\u00a0\nConfirmed speakers so far include:\n          Sen. Maria Cantwell (D-Wash.), Member, U.S. Senate Committee on Commerce, Science, and Transportation             Jack Clark, Strategy and Communications Director, OpenAI                Dario Gil, PhD, Vice President, AI and IBM Q, IBM                   Peggy Johnson, Executive Vice President, Business Development, Microsoft                      Peter Schwartz, Senior Vice President, Strategic Planning, Salesforce                         Milind Tambe, PhD, Founding Co-Director, Center for Artificial Intelligence in Society, University of Southern California                            Douglas Terrier, PhD, Acting Chief Technologist, NASA                               Mona Vernon, Chief Technology Officer, Thomson Reuters Labs                                  Sen. Todd Young (R-Ind.), Member, U.S. Senate Committee on Commerce, Science, and Transportation                                                                                                                                            \nThe program will include discussions about how policymakers and the business community are working to pave the way for this new technology, the effect artificial intelligence will have on the workplace and how to ensure the technology will be used responsibly in the future.\nThis event will be streamed live on The Post's site at wapo.st\/transformersai.\n          EVENT DETAILS           Tuesday, March 20, 2018 9:00AM - 12:00PM (Doors open at 8:30AM) The Washington Post Live Center 1301 K Street NW, Washington, DC 20071                                              \n\"Transformers: Artificial Intelligence\" is presented by Software.org: the BSA Foundation with support from the University of Virginia.\n","178":"Apple's sweeping new\u00a0artificial intelligence play takes a page from features introduced by rival tech companies in recent years - but adds the polished, user-friendly twist that consumers\u00a0have come to\u00a0expect from the electronics\u00a0giant.\nAt the company's annual developer's conference in San Francisco on Monday, Apple executives announced a raft of\u00a0features in the company's soon-to-be released desktop and mobile operating systems that are powered by artificial intelligence, or the blend of\u00a0powerful computing capabilities and software algorithms. Such technology\u00a0can make the phone or other device appear smarter because it anticipates the types of activities people want to do. Apple also said it was opening up many applications to outside developers, including its messaging platform iMessage, Maps and virtual assistant Siri, a departure for the company, which in the past has kept these systems tightly controlled.\nFor Apple, more AI and more integrations with third party services will mean less fatigue for consumers, who are already overwhelmed with too many apps, too many devices, and too much data. Ultimately, artificial intelligence behind-the-scenes\u00a0could make it easier for users to organize their ever-growing photo collections, communicate and use online services more efficiently and toggle less between devices. The moves also come at a time when tech giants and a wave of new start-ups are racing to create similar artificial-intelligence based products.\u00a0\nFor example, Apple will now scan your photos using facial recognition to cluster people together in your photo collection. If pictures of grandpa are scattered across your photo collection, Apple will now find them through facial recognition and group them together so you can organize your memories without having to sift on your own. Facebook has had automatic facial recognition for several years. Another Apple\u00a0feature, called Memories, groups people together by location, and even shows where photos were taken on a map. This stuff isn't simple: Behind the scenes, the software is doing 11 billion\u00a0computations on each photo to make this happen, Apple said.\nApple's other AI announcements borrowed heavily from Cortana, Microsoft's voice-based virtual assistant that works in Windows. Executives said that Siri is now coming to desktop computers: Soon consumers will\u00a0be able to talk to your Mac computer just the way that you talk to your phone.\nOverall,\u00a0more consumers are talking to technology: Google recently said that roughly 20% of all queries are initiated by voice rather than typing. While Cortana has enabled users to talk to their desktops for more than a year, Apple has the advantage of being able to integrate a popular tool on mobile onto desktop, making the experience of moving between devices more seamless. Like Cortana, Siri will also now scan people's communications and make suggestions. If the system sees two people discussing a meeting over text message, a calendar icon will pop up, enabling the users to schedule the meeting from within their texting thread. Apple will also suggest relevant emoji, which will get bigger and more interesting in the new operating system.\u00a0(Another handy feature: You can also tell Siri on your computer to send a text message to someone's phone).\nIn opening major applications to third parties, Apple is nodding to a growing view in Silicon Valley that consumers are seeking an alternative to toggling between the dizzying number of apps they store on their phones. They want to call an Uber or a Lyft, for example, without having to open an app; now outside developers will be able to\u00a0build those services directly into Apple's messaging platform. Facebook recently launched a similar feature inside its popular messaging app.\nIn addition to having\u00a0Siri on\u00a0the desktop, the changes will allow developers to supercharge their apps with Siri's voice. Users will soon be able to use Slack, Uber, or Skype, by talking directly to Siri. This widely anticipated move takes a page from Amazon. For some time now, the company's Alexa smart home assistant device has been allowing third parties to build services onto its platform. So consumers can ask Alexa to read out the weather or connect to smart locks. (Amazon founder and chief executive Jeffrey P. Bezos owns The Washington Post.)\nWhile Apple's rivals have made earlier announcements and perhaps bigger investments in AI technology, Apple's delayed approach is to integrate a suite of features more seamlessly into products and to potentially avoid mistakes. Last year, for instance, Google introduced a feature to help consumers automatically organize photos. But the software became infamous because it incorrectly tagged some African Americans as gorillas.\nApple is also trying to reduce app fatigue by enabling consumers to do more within a single application. Want to call an Uber without leaving your text message conversation? You can do that within iMessage. Want to send someone a text message from your computer? Now you can tell Siri.\u00a0The company is also\u00a0attempting to relieve some password fatigue by letting you log in to your computer with your Apple Watch, so you don't have to type another password.\u00a0 And Apple Pay will be expanded too, so that it now works on your desktop on a number of e-commerce sites. That means consumers who prefer to use Apple can avoid painstakingly entering and storing their credit card info with a slew of vendors.\nIn the past, Apple has been resistant, on privacy grounds, to scanning consumer data, but today's announcements are further evidence that the company is gradually moving away from its earlier stance. In his presentation, Craig Federighi, an Apple senior vice president, made a point of saying that new features had been reviewed in advance by a well-known privacy researcher, Aaron Roth. He even flashed a slide with a quote from Dr. Roth in which the researcher gave his blessing to the company's privacy designs for its new products. If\u00a0Apple is scanning the behavior of many users to suggest what emoji they might want to use, for example, the company will now obscure individual's identities by adding incorrect information, so-called mathematical noise, into the system.\u00a0(Roth did not respond to immediate requests for comment).\nOpening up its platforms to third parties has also historically been a point of discomfort for Apple, as the company's impulse to control the quality and integrity of its own products has butt up against major trends in AI. Those trends emphasize merging more data from third parties to increase the amount of services that can be offered on a single platform. The original Siri included integrations with many third parties that were dissolved after Apple bought the Siri startup five years ago. Today, against a wave of outside pressures, Apple just about came full circle.\n","179":"Facebook's artificial intelligence: Facial recognition without the face\nThanks to advances in computer vision, we now have machines that can pick you out of a lineup. But what if your face is hidden from view?\nAn experimental algorithm out of Facebook's artificial intelligence lab can recognize people in photographs even when it can't make out their faces. Instead, it looks for other characteristics, such as hairdo, clothing, body shape and pose.\u00a0\nModern face-recognition algorithms have already found their way into social networks, shops and even churches. Yann LeCun, head of artificial intelligence at Facebook, wanted to see whether they could be adapted for situations where someone's face isn't clear, something humans can do quite well.\n\"There are a lot of cues we use. People have characteristic aspects, even if you look at them from the back,\" LeCun says. \"For example, you can recognize Mark Zuckerberg very easily, because he always wears a gray T-shirt.\"\nThe research team pulled almost 40,000 public photos from Flickr - in some, the person's full face was clearly visible; in others, the face was turned away from the camera - and ran them through a sophisticated neural network. \nThe final algorithm was able to recognize individuals with 83 percent accuracy. It was presented this month at the Computer Vision and Pattern Recognition conference in Boston.\nAn algorithm like this could one day help power photo apps like Facebook's Moments, released last week.\nMoments scours through a phone's photos, sorting them into separate events such as a friend's wedding or a trip to the beach and tagging whoever it recognizes as a Facebook friend. LeCun also imagines that such a tool would be useful for the privacy-conscious, alerting them whenever a photo of themselves, however obscured, pops up on the Internet.\nIt's also true that the ability to identify people even if they are not looking at the camera raises privacy implications. Two weeks ago, government-sponsored talks failed to come up with a voluntary code of conduct for facial recognition use by private companies.\n\"If, even when you hide your face, you can be successfully linked to your identity, that will certainly concern people,\" says Ralph Gross at Carnegie Mellon University in Pittsburgh, who says the algorithm is impressive. \"Now is a time when it's important to discuss these questions.\"\n- New Scientist\n","180":"Allen Newell, 65, a professor of computer sciences at Carnegie Mellon University who was a pioneer in the field of artificial intelligence, died of cancer July 19 at a hospital in Pittsburgh. He lived in Pittsburgh.\nHe had done research in computer hardware and software topics since the 1950s. Since 1980, he had been working to develop \"Soar,\" an artificially intelligent software system that is able to problem-solve and \"learn\" similar to humans. He also worked on studies dealing with information-processing psychology.\u00a0\nDr. Newell was the first president of the American Association for Artificial Intelligence and also had served as president of the Cognitive Science Society. From 1967 to 1971, he was a member of the computer science study section of the National Institutes of Health.\nHe was the author or co-author of more than 250 technical works, including 10 books. He was co-author of the books \"Human Problem Solving\" and \"The Psychology of Human-Computer Interaction.\" He was the recipient of numerous awards, including the National Medal of Science and the Association for Computing Machinery's A.M. Turing Award.\nDr. Newell, a native of San Francisco, was a 1949 physics graduate of Stanford University and did graduate work in mathematics at Princeton University. He received a doctoral degree in industrial administration in 1957 from the business school of what was then the Carnegie Institute of Technology.\nHe was a research scientist with Rand Corp. before joining the faculty of Carnegie Tech in 1961. He taught at the university until his death. He was a founder of Carnegie Mellon's School of Computer Science in 1989.\nSurvivors include his wife, Noel, of Pittsburgh; and a son, Paul, of California.\nCHARLES R. AHERN.\nCIA Official\nCharles R. Ahern, 73, who worked for the Central Intelligence Agency for 30 years before retiring in 1980 as a foreign electronics and weapons systems specialist, died of cancer July 17 at Sibley Memorial Hospital. He lived in Potomac.\nA senior defense systems specialist who was an authority in electronics, he began his CIA career in 1950 with its office of scientific intelligence. He was the recipient of an Agency Medal of Merit in 1957 and received its Certificate of Distinction in 1976 and again in 1979.\nAfter retiring from full-time duty, he remained an agency consultant until the mid-1980s.\nMr. Ahern was a native of Centralia, Wash., and an electrical engineering graduate of Washington State University. During World War II, he worked on radar research at the Massachusetts Institute of Technology's radiation laboratory. He came to the Washington area in 1946 and worked for the Naval Research Laboratory and the old National Bureau of Standards before joining the CIA.\nHe was a member of the Institute of Electrical and Electronics Engineers. He was a 33rd degree Scottish Rite Mason.\nHis first marriage, to the former Wilma Oliver, ended in divorce.\nSurvivors include his wife, Ann Hellmuth Ahern of Potomac.\nHERBERT M. KIDNER.\nLawyer and General\nHerbert Miller Kidner, 91, an Alexandria lawyer who was a retired Air Force brigadier general, died of congestive heart failure July 18 at Malcolm Grow Medical Center at Andrews Air Force Base.\nA resident of Alexandria, he had practiced law in the Washington area since settling here in 1956.\nGen. Kidner, a Pennsylvania native, was a 1923 cum laude graduate of Harvard University and a 1927 graduate of the University of Pittsburgh law school. He practiced law in Pittsburgh until entering the Army Air Forces in 1941.\nHis World War II assignments included a tour from 1942 to 1944 as assistant commandant of the Judge Advocate General School in Ann Arbor, Mich. After the war, his posts included judge advocate of the U.S. Fifth Air Force in Japan. From 1950 until retiring from active duty in 1956, he was judge advocate of the Air Force in Europe and was stationed in Germany.\nGen. Kidner's military decorations included two awards of the Legion of Merit.\nHe was a member of the Judge Advocates Association and the Belle Haven Country Club.\nSurvivors include his wife of 57 years, Roberta, of Alexandria; a son, Wood, of Minneapolis; a daughter, Terry Kidner of Madison, N.J.; and a grandson.\nCARTER H. DOVE.\nRiggs Official\nCarter Huston Dove, 71, who worked for Riggs Bank for nearly 40 years before retiring about 1982 as a senior vice president for international banking, died July 17 at Alexandria Hospital. He had cancer.\nIn 1979, as Riggs vice president for Middle East and African affairs, he accompanied then-D.C. Mayor Marion Barry on his first trip abroad as mayor. It was a highly publicized trip to Africa to encourage economic and cultural ties between the District and Africa.\nMr. Dove, an Alexandria resident who was born in Philadelphia, came to the Washington area at an early age. He was a graduate of Western High School and attended the Stonier banking school at Rutgers University. He joined Riggs after Army service in World War II.\nHis marriages to Suzanne Dove and Norma Dove ended in divorce.\nSurvivors include two sons by his second marriage, Carter Jr., of Washington, and Tracy J., of Moscow; and a brother, David, of Alexandria.\nCLARENCE R. KNOTT.\nC&P Plant Supervisor\nClarence Raymond Knott, 73, who worked for the Chesapeake & Potomac Telephone Co. for 40 years before retiring in 1980 as a plant supervisor, died of lung cancer July 17 at Dorchester General Hospital in Cambridge, Md.\nHe joined C&P as a cable splicer and rose through the ranks to foreman and foremen supervisor before becoming a plant supervisor.\nMr. Knott, who had lived in Cambridge since moving there from East Riverdale, was a native of Washington. He was a 1938 graduate of Anacostia High School and served in the Coast Guard during World War II.\nHe was a Scottish Rite Mason and member of Almas Temple.\nSurvivors include his wife of 50 years, the former Mary Elizabeth Garretson, of Cambridge; a son, Raymond E., of Bowie; a daughter, Carol Knott Appleton of Mobile, Ala.; a brother, Arthur J., of Golden Beach, Md.; and five grandchildren.\nMABEL E. DEAVERS.\nChurch Member\nMabel Evelyn Deavers, 71, a member of Franconia Wesleyan Church in Alexandria, died July 18 at Mount Vernon Hospital after a stroke. A Washington native, she had lived in Alexandria for the past 45 years.\nSurvivors include her husband, George R., and a son, Russell, both of Alexandria; two daughters, Denise Schoppet of Sterling and Patricia Deavers of Alexandria; four grandchildren; and seven great-grandchildren.\n","182":"In 2010, Kris Hammond, an experienced artificial intelligence researcher, and a few partners founded Narrative Science. The Chicago start-up's software ingests and interprets numbers for things like investment holdings, billing records and sports statistics and transforms them into written summaries or stories.\nFrom the outset, the company's technology was promising, but the timing was tricky. It opened its doors the year before IBM's Watson publicly demonstrated the potential of machine intelligence with its ''Jeopardy!'' victory. \u00a0\n  ''At the start, we didn't call it artificial intelligence because A.I. was still in disrepute,'' recalled Mr. Hammond, chief scientist for Narrative Science.\n  Today, a start-up that can lay claim to doing artificial intelligence has increased its odds of getting attention and funding. A.I. efforts, scientists agree, are far more advanced than in past boom and bust cycles for this type of technology.\n  Yet if the technology seems almost magical at times, success, venture capitalists and entrepreneurs say, will rely on basic business virtues like focus, flexibility, speed and resilience.\n  That is particularly true for start-ups. ''Big companies like Google can make A.I. investments with a 10-year horizon, but a start-up has to release a product in a year or two,'' said Chris Dixon, a general partner at Andreessen Horowitz.\n  In past cycles, research advances in A.I. generated excitement but not much business. ''It was a long, long way from being practical,'' said Forest Baskett, a computer scientist and partner at New Enterprise Associates. ''What makes it investable now is there are applications, things you can do with these tools.''\n  The technical ingredients behind modern A.I., he noted, were vast new sources of data, machine-learning software and powerful, inexpensive computing delivered over the so-called computing cloud.\n  When asked about investable applications in N.E.A.'s portfolio, Mr. Baskett pointed to three examples that exploit data and machine-learning: DataRobot, which evaluates math models to predict which will work best on a particular kind of data; Dato, whose machine-learning tools analyze the graphs of online users' social connections; and Qbotix, which makes specialized robots for optimally tilting and maintaining the panels that absorb the sun's rays on solar farms.\n  ''Today, we can measure so much more,'' Mr. Baskett said.\n  His comment underlines the fact that there is no bright line separating big data analytics and A.I. In both cases, data is the fuel and machine-learning algorithms are the engine of pattern recognition and classification. Yet A.I. technology, propelled by advances in so-called deep-learning techniques, is moving up the ladder of perception, becoming far better at mimicking the humanlike tasks of seeing and hearing -- image recognition and speech recognition, in computing terms.\n  In 2014, after three years leading the drive to commercialize Watson at IBM, Manoj Saxena left and founded Cognitive Scale. The start-up, based in Austin, Tex., is focused on building applications on top of Watson. IBM is known for collaborating with large corporations on lengthy, costly projects -- though it has embarked on a campaign to move faster and cater to a broader audience of customers.\n  Cognitive Scale concentrates on short sprints with small teams, getting a working application up and running within 90 days. Speed to market is its product. Cognitive Scale is working first with customers in the health care, finance and retailing industries.\n  ''Our approach is small bets, brought to market quickly for better decisions and better customer engagement in your industry,'' said Mr. Saxena, the chairman. ''We bring practical A.I. into businesses.''\n  What the market wants has been the lesson learned at Narrative Science. Its founding team included professors from Northwestern University's computer science and journalism departments.\n  It began by writing short but remarkably fluent and varied stories for customers like the website of the Big Ten athletic conference. In 2011, Mr. Hammond, a Northwestern professor, predicted in an interview that in five years, smart software would win a Pulitzer Prize, and he wanted it to be his company's technology.\n  A Pulitzer Prize is still a worthy, if more distant, goal, Mr. Hammond said recently. But Narrative Science, which is growing briskly and has 80 employees, has built its business outside the financially challenged news media industry.\n  Its 70 customers are mainly in mainstream businesses like financial services and consulting, including Nuveen, USAA, Credit Suisse and Deloitte, which use the Narrative Science software to generate reports that explain the numbers in investment portfolios, trading records and market statistics.\n\n\n\n","183":"In an upper-level seminar on artificial intelligence, Occidental College professor Justin Li started a discussion outside the realm of a typical computer science class.\nShould a self-driving car, if unable to brake in time, be programmed to steer into a wall to avoid crashing into pedestrians -- perhaps killing a single person in the vehicle in order to save five on the street?\nOne question led to another. Is it morally OK to choose five lives over one? How about 10? Who gets to make this decision anyway -- the programmer, the government, the person who can afford a self-driving car?\nOccidental established a computer science major this fall, one of numerous liberal arts colleges to do so in recent years. They've popped up at Reed College in Oregon and Whitman College in Washington state.\u00a0\nThese schools better known for teaching history and philosophy are shaping their programs to draw on their strengths. They don't just focus on the vocational or on abstract algorithms. As artificial intelligence and automation increasingly enter everyday life, their courses push students to examine how modern technology both changes and challenges society.\nIn Maine, Bates College started a multidisciplinary Digital and Computational Studies program, with aims including \"to interrogate the values and assumptions of a digitized world\" and \"increase understanding of the power and limitations of computers in solving problems.\"\nAt Occidental, where a young Barack Obama discovered political science, teaching students how to code is the straightforward part, said Li, a cognitive science professor who led the design of the major. Classes also push students to grapple with the inequalities and philosophical dilemmas that technology is creating out in the world. Such social discussions are woven into every lesson.\n\"The goal is to make students consider the real-world implications of what they are doing -- that their code is not just abstract problem-solving but may have positive or negative impacts on real people,\" Li said.\nStephanie Angulo, a junior, says it was that approach that drew her to Occidental rather than an engineering school. She hopes to break glass ceilings one day as a tech leader and wanted to study somewhere that would also teach her how to write better.\n\"You have to think about how you communicate your ideas or how you think about problems,\" said Angulo, who has interned at Facebook and is studying computer science and philosophy. \"My friends and I talk about these issues pretty much every day, whereas I've noticed the people I've worked with who are more engineering-focused don't tend to think about these questions as much.\"\nThe broader way of looking at computer science also has the benefit of perhaps drawing new people in to help narrow tech's much-discussed diversity and gender gaps, said Andrea Danyluk, a Williams College professor and member of the Liberal Arts Computer Science Consortium. \"We have the art major who needs to take a science course or a history student who discovers this is actually kind of cool,\" she said.\nAs more multidisciplinary programs emerge, some in the field caution against taking too much focus away from the fundamentals of computer science. \"You need a very solid core,\" said Kim Bruce, who started the departments at Williams and at Pomona College.\nA number of East Coast liberal arts colleges have long-standing traditional programs based in math and computation. But Occidental wanted its new major to break the mold.\n\"Our goal isn't to emulate Stanford or MIT in terms of engineers,\" President Jonathan Veitch said. \"Our goal is to graduate students who are versed in the ways in which technology's impacting their life socially, culturally as well as politically -- and to play some kind of thoughtful role in high tech.\"\nChoosing Li, whose field of expertise is multidisciplinary, to shape the major was the first step.\nHe looked at what other campuses were doing and found that many lacked classes that directly addressed ethics. He also noted the diversity issues.\nAcross the nation, more than 80% of computer science majors are men. Of the 1,780 doctorates granted in the field in 2015, just 1% went to black students, 1.7% to Latinos and less than 19% to women, according to the Computing Research Assn.\n\"Starting a new department is a rare chance to tackle these issues from scratch,\" Li said.\nOccidental's major includes a more traditional track, a mathematics pathway and a \"CS + X\" option in which students are free to choose the X, whether it be integrating gender studies, economics or music. They can pursue projects about anything that interests them -- segregated housing, language patterns, whether there's racism in census data.\nKathryn Leonard, the department's chairwoman, who majored in English and math in college, encourages students to use their broad liberal arts educations to think about the role of humans in technology.\n\"If we're the ones building the machine, then we have to be very careful about putting our own biases into the machine and, furthermore, putting in biases that we aren't even aware of,\" she said. \"In order to have that kind of awareness, you need an exposure to a broad range of perspectives.\"\nJunior Chloe Zeller is double majoring in computer science and cognitive science and minoring in math. She was worried that computer science would be too hard, but says the way Occidental teaches the material helped her find ways to make it her own. She's been studying gender bias and inequity in computer programs. She's inspired by the diversity both of her classmates and of their interests.\n\"CS is situated in a lot of different places now. It's not just this discrete thing that's by itself in a little box,\" she said. \"In my very first class, there were a variety of math people, science people. I had a politics major in my class, a psych major.\"\nThat diversity helped draw in junior Luis Figueroa, who was intimidated because his high school in San Luis, Ariz., didn't teach computer science. Now he's double majoring in it, along with math. He's interned at NASA's Jet Propulsion Laboratory and hopes one day to return to his hometown to teach kids early in their education that \"computer science really can be connected to everything.\"\nAt a recent computer science show and tell, dozens of students crowded around to see what the new major was all about.\nAt one table, Allie Brenner described the neural network that she and a classmate had built to recognize and distinguish between tweets from President Trump and former President Obama. One challenge, she said, was teaching a computer to detect sarcasm -- which was key because Obama and Trump often have used similar words in different ways.\n\"The hardest part for us, we were actually joking about it the other day -- it's no longer the coding that's intimidating,\" said Brenner, who is double majoring in computer science and psychology. \"It's the theory and the thought process behind it all.\"\n--\nrosanna.xia@latimes.com\nTwitter: @RosannaXia\n","185":" Note: This lesson was originally published on an older version of The Learning Network; the link to the related Times article will take you to a page on the old site. \n Overview of Lesson Plan: In this lesson, students investigate current research in artificial intelligence and imagine the ways in which those technologies can be developed for use in daily life.\n Author(s): Jennifer Rittner, The New York Times Learning NetworkJavaid Khan, The Bank Street College of Education in New York City\n Suggested Time Allowance: One hour\u00a0\n Objectives: Students will:  1. Imagine how they might use a robot in their daily lives. 2. Learn about advancements in artificial intelligence by reading and discussing the article \"Brainy Robots Start Stepping Into Daily Life.\" 3. Investigate a current robotic technology under development. 4. Prepare posters and present ideas that consider how a robotic technology might be implemented in daily life in the near and distant future. \n Resources \/ Materials: -pens\/pencils -classroom board -student journals -copies of the article \"Brainy Robots Start Stepping Into Daily Life,\" found online at http:\/\/www.nytimes.com\/learning\/teachers\/featured_articles\/20060720thursday.html (one per student) -poster board (one or two per student group) -markers, glue and scissors (one set per student group) -resources for researching artificial intelligence such as science and\/or technology magazines, encyclopedia, library resources and computers with Internet access \n Activities \/ Procedures: 1. WARM-UP\/DO-NOW: Before class, write the following prompt on the board for students to respond to in their journals: \"If you had a robot, what would you want it to do?\"  After a few minutes, have students share their responses.  Do they believe they might have a robot in their lives someday?  How soon might that technology become a part of daily life? 2. Have the class read the article \"Brainy Robots Start Stepping Into Daily Life\" http:\/\/www.nytimes.com\/learning\/teachers\/featured_articles\/20060720thursday.html. Select questions from the following list to enhance your class discussion:  a. What robotic technologies are currently in use? b. How is the reality catching up with the fantasy? c. In what ways are the projects becoming more ambitious? d. Why might assembling an Ikea bookcase be beyond the reach of many humans? e. What might an electronic butler be able to do? f. How is current research in artificial intelligence distinguishable from that of an earlier generation? g. What is meant by cognitive computing? h. Why is it considered a research discipline rather than an industry? i. How is success currently measured in the field? j. To what degree might it be significant that cars with robotics technologies are mid-priced? k. How are underwater vision systems being used, and where? l. What organization is funding the design of a robot car, and why might members be interested in this technology? m. What is the \"urban challenge?\" n. How might the research project Microsoft is currently working on be used in daily life? o. What is TellMe Networks and what technology are they developing? p. When was this technology first introduced to the marketplace, and how has it improved since then? q. Where is the Ecole Polytechnique Federale de Lausanne and how is it advancing research in artificial intelligence? r. What fundamental insights do researchers hope to gain through the Blue Brain project? s. How might an electronic butler be used and how soon might one be available? t. Who is Robert Hecht-Nielsen and why might he have some credibility in the field? u. What is his theory of \"confabulation\" and how did he demonstrate it? v. In what way does the robot's third sentence represent a coherent thought? w. Who originated artificial intelligence and when? x. What was the first demonstration of Mr. Turing's theory? y. What were the early applications of the technology? z. From what corresponding advancements has robotics research benefitted? aa. What are the implications and how might daily life be affected? bb. Who is Andrew Ng and what is his goal? cc. How close is his dream to becoming reality? 3. Explain to the class that they will work in groups to consider the question, \"How might artificial intelligence change the way we live?\" by investigating an area of research discussed in the article or in the Warm-Up. To begin, arrange the class into small groups and assign each an area of research from the list below or from those discussed in the Warm-Up. Provide the list of questions as a handout along with the presentation supplies (poster board, markers, scissors and glue) and instruct groups to work together to create a brief presentation about the technology they are investigating.  Students should collect images and details about their subject, as well as prepare a well-considered analysis of the impact the technology might have on daily life. AREAS OF RESEARCH -Voice control systems -Advanced artificial reason techniques -Underwater vision systems -Robot cars -Systems of predestination -Voice recognition services -Electronic butlers -Models of confabulation -Mobile robotics QUESTIONS FOR CONSIDERATION BASICS -What is it? -What does it do? -How is it currently being used? -What are all of the possible ways it might be used in the future? THE TECHNOLOGY -What human behavior does it simulate? -How does it assist and\/or replace humans? -In what way does it improve on a human capability? LIFESTYLE ANALYSIS -In what areas of life might the technology be used: home, work, school and\/or the public sphere? -What types of experiences might it affect: work, learning, entertainment, health, transportation, everyday tasks, etc.? -How might this technology affect your life positively and\/or negatively? -How might it alter the way people behave, think and\/or interact with one another? Have students present their findings to the class. 4. WRAP-UP\/HOMEWORK: How might a technology you or your classmates investigated in class be further developed for use in daily life at home, in school, in the workplace or for a public purpose? Choose any robotic technology presented in class to investigate further and consider how you or future generations might use it.  Prepare a poster presentation that explores one specific application for the technology in daily life.  Proposals should address both the technical and personal aspects of life with the technology.  Some questions to consider include : -What will it do and what will it be called? -What areas of daily life will it affect? -How will it supplement or replace a human activity? -What interaction will humans have with it (either directly or indirectly)? -How will it change the way people experience daily life at home, school and\/or work? -How will it affect the way people behave, think or interact? -To what degree might it improve safety and\/or efficiency for people? -How far in the future might this technology actually be available for use in daily life? Present posters to the class. Be sure to include illustrations, photographs and any additional details that will showcase your robot and demonstrate how it might be used in daily life. \n Further Questions for Discussion: -To what degree does fantasy fuel the development of a technology? -How might the military use robotics technologies in combat? -How do you think older generations might adapt to robotics technologies? \n Evaluation \/ Assessment: Students will be evaluated based on journal responses and class discussion, thoughtful participation in the group work, and creative presentation of their posters.\n Vocabulary: virtual, hype, simulating, emergence, ambitious, parity, pioneer, futuristic, lull, sophisticated, cognitive, distinguish, palpable, upswing, competence, discipline, revenue, increments, innovations, intervention, ante, simulated, constrained, automates, refined, parallel, neurons, neocortex, exquisite, concierge, adequate, skeptical, neural, mimic, hypothesis, flagging, bilateral, teleprinter, replicated, amassed, exponential, synthesis, implication, cusp\n Extension Activities: 1. Create an illustrated timeline of advancements in artificial intelligence.  What are some of the earliest examples of this technology? Choose one to investigate further. How has its form and function changed over time? What were some milestones in its development and what were the inspirations for those innovations? Present your findings to the class. 2. How might robots be used to assist people with special needs, those who are infirm and\/or the elderly?  Investigate a specific special need, infirmity or age-related disability to determine what needs or activities might benefit from a robotic technology.  How might a robot assist people in that group?  How might life change with the introduction of a robot?  Be creative in imaging the form and function of the robotic technology. Present both the best possible scenario and the potential flaws.  How far in the future do you think your robot might be available for use? Prepare a poster presentation that demonstrates your investigation of the technology and the potential user. 3. Invite a robotics specialist to your class to demonstrate a robotic technology in development.  If possible, have the specialist bring tools to allow students to build their own robot. 4. Write a short science fiction story that features an artificial intelligence machine or system.  Is the robot the protagonist or antagonist?  What is the main theme? What are the conflicts and how are they resolved?  Are there human characters?  If so, what is the nature of their interactions with the robots?  When and where does the story take place?  Submit stories to the school literary journal for publication. 5. List all of the technologies you use on a daily basis.  Write a short story in which you imagine that these technologies are no longer available to you. Be creative in establishing your scenario. Where and when does your story take place? Why are the technologies not available? How will you communicate, play and work without them? Is life better, worse or just different without them? Share stories with the class. \n Interdisciplinary Connections: Fine Arts - How might artificial intelligence affect art-making practices?  Can an intelligently designed robot create art?  Is creativity something you can simulate?  Work with a robotics specialist to create a robot that can simulate the act of painting.  Can the robot be programmed to create different outcomes each time?  Discuss whether the finished products qualify as art and\/or how an artist might use artificial intelligence to create an original work of art.  Display works created by the robot and\/or the robot and humans in an exhibition for the school with an essay that explains your process. Math - Compare early computing machines of the 1950's and 60's with those in use today.  How do they compare in terms of processing power, memory, speed, accuracy, flexibility, adaptability and size?  Create a series of charts and graphs that demonstrate the changes in each area over time. Media Studies - Watch a film or television program in which artificial intelligence is featured, such as \"2001: A Space Odyssey\" (1968), \"Star Wars\" (1977, etc.), \"Artificial Intelligence\" (2001), \"I, Robot\" (2004),\"The Incredibles\" (2004), \"Robots\" (2005),  or \"The Jetsons,\" \"Lost in Space,\" \"Battlebots\" or \"Futurama.\" How are the robots portrayed?  What purpose do they serve? Do they take the place of humans? Do they create conflict or calm?  What are the robots' assets and\/or shortcomings? What is the power relationship between the humans and robots?  How do they interact with humans?  What positive and negative experiences or feelings do the humans have in relation to the robots? How might the portrayal of the robots in the film or television show reflect something about human society in reality?  Write a critical analysis that addresses these issues. Science - What is a neural network?  How does it affect cognition?  Investigate current students in human brain research as it relates to artificial intelligence.  What brain functions can currently be simulated by a computer or robot?  Could that function be automated?  Create a How It Works poster that explains that brain function and how it can be simulated by a robot. Teaching with The Times - Read The New York Times every week for one month and clip stories about people who you think might benefit from robotic technologies (such as those with disabilities, involved in search and rescue missions or in combat zones).  Propose a use for robotics in one of those examples.  Is there a current technology under development that might serve them? At the end of the month, write a reflection that summarizes how the lives of those explored might change with the introduction of a robot into their daily lives. To order The New York Times for your classroom, click here. \n Other Information on the Web: American Association for Artificial Intelligence (http:\/\/www.aaai.org\/) provides updates on research and activities in the field. Poseidon Technologies (http:\/\/poseidon-tech.com\/us\/index.html) provides information about the underwater vision systems the company developed. Defense Advanced Research Projects Agency (http:\/\/www.darpa.mil\/) offers some information about the technology developments it supports and\/or uses, along with it annual budget. TellMe Networks (http:\/\/www.tellme.com\/) provides an overview of their services and the voice recognition technologies used by the company. Ecole Polytechnique Federale de Lausanne (http:\/\/www.epfl.ch\/) provides information about its research areas and new developments. Blue Brain (http:\/\/bluebrainproject.epfl.ch\/) provides updates on the project and its findings. Botball (http:\/\/www.botball.org\/) provides information about nationwide robotics competitions for young people. National Air and Space Administration (http:\/\/robotics.nasa.gov\/events\/competitions.htm) offers links to robotics education programs and competitions around the country. USFirst (http:\/\/www.usfirst.org\/robotics\/) provides information about and instructions for their robotics challenges. Stanford Artificial Intelligence Laboratory (http:\/\/ai.stanford.edu\/) provides updates on their research and activities. Related Times Articles \n Academic Content Standards: Grades 6-8 Technology Standard 3- Understands the relationships among science, technology, society, and the individual. Benchmarks: Knows that scientific inquiry and technological design have similarities and differences; Knows that science cannot answer all questions and technology cannot solve all human problems or meet all human needs; Knows ways in which technology has influenced the course of history; Knows ways in which technology and society influence one another&#8232;Technology Standard 4- Understands the nature of technological design. Benchmarks: Implements a proposed solution; Evaluates a designed solution and its consequences based on the needs or criteria the solution was designed to meet&#8232;Science Standard 16- Understands the scientific enterprise. Benchmarks: Knows that people of all backgrounds and with diverse interests, talents, qualities, and motivations engage in fields of science and engineering- some of these people work in teams and others work alone, but all communicate extensively with others; Knows that the work of science requires a variety of human abilities, qualities, and habits of mind Grades 9-12 Technology Standard 3- Understands the relationships among science, technology, society, and the individual. Benchmarks: Knows that science and technology are pursued for different purposes; Knows ways in which social and economic forces influence which technologies will be developed and used; Knows that alternatives, risks, costs, and benefits must be considered when deciding on proposals to introduce new technologies or to curtail existing ones; Knows examples of advanced and emerging technologies and how they could impact society; Knows that mathematics, creativity, logic, and originality are all needed to improve technology; Identifies the role of technology in a variety of careers &#8232;Technology Standard 4- Understands the nature of technological design. Benchmarks: Implements a proposed solution; Evaluates a designed solution and its consequences based on the needs or criteria the solution was designed to meet &#8232;Science Standard 16- Understands the scientific enterprise. Benchmarks: Knows that throughout history, diverse cultures have developed scientific ideas and solved human problems through technology; Understands that individuals and teams contribute to science and engineering at different levels of complexity; Knows that science and technology are essential social enterprises, but alone they can only indicate what can happen, not what should happen; Knows that creativity, imagination, and a good knowledge base are all required in the work of science and engineering\n This lesson plan may be used to address the academic standards listed above.     These standards are drawn from Content Knowledge: A Compendium of Standards and Benchmarks for K-12 Education;     3rd and 4th Editions and have been provided courtesy of the Mid-continent Research for Education and Learning in Aurora, Colorado. \n\n","186":"SEATTLE --  An artificial intelligence software program capable of seeing and reading has for the first time answered geometry questions from the SAT at the level of an average 11th grader.\nThe achievement, in which the program answered math questions it had not previously seen, was reported in a paper presented by computer scientists from the Allen Institute for Artificial Intelligence and the University of Washington at a scientific conference in Lisbon on Sunday. \n  The software had to combine machine vision to understand diagrams with the ability to read and understand complete sentences; its success represents a breakthrough in artificial intelligence.\u00a0\n  Despite the advance, however, the researchers acknowledge that the program's abilities underscore how far scientists have to go to create software capable of mimicking  human intelligence. \n  For example, Ali Farhadi, a University of Washington artificial intelligence researcher and a designer of the test-taking program, noted that even a simple task for children, like understanding the meaning of an arrow in the context of a test diagram, was not yet something the most advanced A.I. programs could do reliably.\n  ''A lot of my colleagues have said machine vision is a solved problem,'' Dr. Farhadi said. ''My answer is, 'Call me when you've solved this.'''\n  In 1950, at the dawn of the computing age, the mathematician Alan Turing proposed a simple test to determine whether a machine could ''think'' in a human sense. If a person communicating via keyboard with a computer could not tell whether it was a machine or a human, Turing reasoned, the question would be resolved.\n  Since then, a debate has emerged about whether the Turing test signifies anything about machine intelligence or is merely an indication of human gullibility.\n  The argument came to a head in June 2014, when a British computer scientist claimed that a program -- a chatbot disguised as a 13-year-old Ukrainian boy named Eugene Goostman -- created by Russian and Ukrainian programmers passed the Turing test.\n  The demonstration was widely criticized, and in January, Gary Marcus, a cognitive scientist at New York University, organized the first of two scientific workshops intended to develop more accurate methods than the Turing test for measuring the capabilities of artificial intelligence programs.\n  Researchers in the field are now developing a wide range of gauges to measure intelligence -- including the Allen Institute's standardized-test approach and a task that Dr. Marcus proposed, which he called the ''Ikea construction challenge.'' That test would provide an A.I. program with a bag of parts and an instruction sheet and require it to assemble a piece of furniture.\n  Another approach, the Winograd Schema Challenge, has been made an official competition, sponsored by Nuance Communications, a company that develops speech recognition technology. The challenge -- named after the pioneering A.I. researcher Terry Winograd, a professor of computer science at Stanford University -- will be a test of common-sense reasoning.\n  First proposed in 2011 by Hector Levesque, a University of Toronto computer scientist, the Winograd Schema Challenge would pose questions that require real-world logic to A.I. programs. A question might be: ''The trophy would not fit in the brown suitcase because it was too big. What was too big, A: the trophy or B: the suitcase?'' Answering this question would require a program to reason spatially and have specific knowledge about the size of objects.\n  Within the A.I. community, discussions about software programs that can reason in a humanlike way are significant because recent progress in the field has been more focused on improving perception, not reasoning.\n  For example, progress has been made in applications used to recognize objects or human speech. To improve machine recognition and speech recognition, researchers have exploited the ability to compile vast libraries of examples to train programs known as neural networks, which can do sophisticated pattern recognition.\n  The Allen Institute researchers said these techniques fell short in developing technologies to match human skills such as abstract and common-sense reasoning.\n  The Allen Institute's program, which is known as GeoSolver, or GeoS, was described at the Conference on Empirical Methods on Natural Language Processing in Lisbon this weekend. It operates by separately generating a series of logical equations, which serve as components of possible answers, from the text and the diagram in the question. It then weighs the accuracy of the equations and tries to discern whether its interpretation of the diagram and text is strong enough to select one of the multiple-choice answers.\n  The Allen Institute approach has more in common with an earlier generation of artificial intelligence research that relied on logic and reasoning.\n  Moreover, the Allen Institute researchers said, machine-learning techniques have continued to fall short in areas where humans excel, such as problem solving.\n  ''This is not pattern matching,'' said Oren Etzioni, a computer scientist and the chief executive of the Allen Institute.\n  While neural networks have made progress based on the availability of huge amounts of data online, the Allen Institute approach works with relatively sparse data (even all the standardized-test questions do not make up a big data set). The data is too sparse, in fact, to be broadly useful in solving school test questions in subjects that require reasoning -- such as algebra and science disciplines -- Mr. Etzioni said.\n  Ultimately, Dr. Marcus said, he believed that progress in artificial intelligence would require multiple tests, just as multiple tests are used to assess human performance.\n  ''There is no one measure of human intelligence,'' he said. ''Why should there be just one A.I. test?''\n  One open question is whether the incremental progress that is evident in the Allen Institute geometry-solving program is a significant step forward or whether it has more in common with a series of earlier proclamations in the field of ''thinking machines'' that ended in blind alleys.\n  In the 1960s, Hubert Dreyfus, a philosophy professor at the University of California, Berkeley, expressed this skepticism most clearly when he wrote, ''Believing that writing these types of programs will bring us closer to real artificial intelligence is like believing that someone climbing a tree is making progress toward reaching the moon.''\n\n\n\n","187":" The Hewlett-Packard Company said that it would donate at least $50 million worth of computers and software designed for artificial intelligence work to universities over the next three years.\u00a0The company said the Massachusetts Institute of Technology would be the first of 10 to 12 recipients.\nThe grant is a prelude to Hewlett- Packard's entrance into the artificial intelligence computing marketplace, expected later this year. The company thinks that its general-purpose computers, combined with its artificial intelligence software, can compete with the specially made artificial intelligence computers now marketed by Symbolics, Lisp Machines, Texas Instruments and Xerox.\n","188":"EMERYVILLE, Calif. -- During a recent speech at the University of California, Berkeley, Pieter Abbeel played a video clip of a robot doing housework.\nIn the clip recorded in 2008, the robot swept the floor, dusted the cabinets, and unloaded the dishwasher. At the end of it all, it even opened a beer and handed it to a guy on a couch. \n  The trick was that an engineer was operating the robot from afar, dictating its every move. But as Mr. Abbeel explained, the video showed that robotic hardware was nimble enough to mimic complex human behavior. It just needed software that could guide the hardware -- without the help of that engineer.\u00a0\n  ''This is largely a computer science problem -- an artificial intelligence problem,'' Mr. Abbeel said. ''We have the hardware that can do the job.''\n  Mr. Abbeel, a native of Belgium, has spent the last several years working on artificial intelligence, first as a Berkeley professor and then as a researcher at OpenAI, the lab founded by Tesla chief executive Elon Musk and other big Silicon Valley names. Now, he and three fellow researchers from Berkeley and OpenAI are starting their own company, intent on bringing a new level of robotic automation to the world's factories, warehouses and perhaps even homes.\n  Their start-up, Embodied Intelligence, is backed by $7 million in funding from the Silicon Valley venture capital firm Amplify Partners and other investors. The company will specialize in complex algorithms that allow machines to learn tasks on their own. Using these methods, existing robots could learn to, for example, install car parts that aren't quite like the parts they have installed in the past, sort through a bucket of random holiday gifts as they arrive at a warehouse, or perform other tasks that machines traditionally could not.\n  ''We now have teachable robots,'' Mr. Abbeel said during a recent interview at the new company's offices in Emeryville, Calif., just across the bay from San Francisco.\n  The new company is part of a much wider effort to create A.I. that allows robots to learn. Researchers in places like Google, Brown University, and Carnegie Mellon are doing similar work, as are existing start-ups like Micropsi and Prowler.io.\n  Robots already automate some work inside factories and warehouses, such as moving boxes from place to place at Amazon's massive distribution centers. But companies must program these machines for each particular task, limiting their possible applications. The hope is that robots can master a much wider array of tasks by learning on their own.\n  ''Today, every motion that an industrial robot makes is specified down to the millimeter,'' said Sunil Dhaliwal, the Amplify founder who led the firm's investment in Embodied Intelligence. ''But most real problems can't be solved that way. You have to be able not just to tell the robot what to do, but to tell it how to learn.''\n  Mr. Abbeel and the other founders of Embodied Intelligence, including the former OpenAI researchers Peter Chen and Rocky Duan and the former Microsoft researcher Tianhao Zhang, specialize in an algorithmic method called reinforcement learning -- a way for machines to learn tasks by extreme trial and error.\n  Researchers at DeepMind, the London-based A.I. lab owned by Google, used this method to build a machine that could play the ancient game of Go better than any human. In essence, the machine learned to master this enormously complex game by playing against itself -- over and over and over again.\n  Other researchers, across both industry and academia, have shown that similar algorithms allow robots to learn physical tasks as well. By repeatedly trying to open a door, for instance, a robot can learn which particular movements bring success and which don't.\n  Much like Google and labs at Brown and Northeastern University, Embodied Intelligence is also augmenting these methods with a wide range of other machine learning techniques. Most notably, the start-up is exploring what is called imitation learning, a way for machines to learn discrete tasks from human demonstrations.\n  The company is using this method to teach a two-armed robot to pick up plastic pipes from a table. Donning virtual reality headsets and holding motion trackers in their hands, Mr. Abbeel and his colleagues will repeatedly demonstrate the task in a digital world that recreates what is in front of a robot. Then the machine can then learn from this digital data.\n  ''We collect data on what the human is doing,'' Mr. Chen said. ''Then we can train the machine to imitate the human.''\n  These and similar machine learning methods have only begun to bear fruit over the past few years, but many believe they will overhaul the field of robotics. It is telling that OpenAI, a pure research lab that opened its doors less than two years ago, has now lost two big names to more commercial pursuits.\n  Poached away from OpenAI by Mr. Musk himself, the machine learning expert Andrej Karpathy is now the director of A.I. at Tesla, working to help autonomous cars get smarter faster. And Mr. Abbeel is launching Embodied Intelligence. ''Andrej is stepping out of the research mode and saying: 'I want to do this in practice,''' Mr. Abbeel explained. ''That same thing goes for us.''\n  He said he believed his new start-up can rapidly push its methods into manufacturing operations like the auto industry. Although robotics already handle many tasks inside such factories, there are many others they can't yet master. That is what Embodied Intelligence hopes to change.\n  Some researchers question how much these machine learning techniques will ultimately improve robotics, believing they are overhyped among both researchers and the news media. ''Machine learning is being thrown at so many problems in robotics,'' said Robert Howe, a professor of robotics at Harvard University. ''And it produces just enough results that people can trumpet it.''\n  But Mr. Abbeel is among the world's top researchers in his field, and his decision to start a own company is an indication that machine learning will continue to push robotics forward.\n  ''It is obvious that this is what you need to build flexible, agile robotics,'' said Geoff Hinton, a pioneer of machine learning.\n\n\n\n","189":"Old entrepreneurs neither die nor fade away -- they resurface as opportunistic Great White Sharks, ready to bite off chunks of promising new markets.\nSo it shouldn't have been a surprise that Adam Osborne was trying to make his presence felt at the annual American Association for Artificial Intelligence (AAAI) convention held here this month.\nSurely you remember Osborne of Osborne Computer Corp. fame and notoriety (from zero to $100 million and back again in a whirlwind 18 months). He now runs Paperback Software International -- his attempt to bring cheap but decent software to the masses.\u00a0\nWhile Adam no longer may qualify as a Great White (perhaps an unusually gifted blowfish), there's no question that when he enters a market you know it's ripe for exploitation.\nArtificial intelligence is ripe.\nOsborne's AI offering is VP-Expert, an expert-systems shell (development tool) that he plans to sell for $100. (Expert systems, as regular readers of this column recall, are the distilled rules of thumb that experts use to solve problems replicated in software.)\nVP-Expert, scheduled to ship this fall, doesn't look bad. In fact, third parties already are developing applications using it. Indeed, one fellow at the Osborne booth was showing a \"wine selection\" expert system -- an amusing but pretentious little expert system designed to tell you which wine is best\/most suitable for imbibing with your meals. (Truly indispensable -- I often dine a deux with my PC.)\nSuch silliness aside, the exhibits lining the AAAI show floor clearly indicate that a wealth of low-cost artificial-intelligence development tools are spilling out into the market and that it's simply a matter of time before these tools are used to create expert-systems applications for the masses.\nIn fact, I will stick my neck out and reassert my belief that expert systems will drive the sale of personal computers just as spreadsheets and word processors did earlier this decade. Within a year, we will begin to see low-cost (under $150) expert systems offering expert advice on personal financial investments (tax planning and the like); legal advice; and, yes, even home medical advice.\nThe impact of expert systems will become even more noticeable in the office as expert consulting programs slowly evolve: programs that diagnose flaws or unusual assumptions in spreadsheet models, programs that recognize important patterns in data base files, and software giving expert advice to aesthetic klutzes who can't design attractive reports on their desktop publishing systems without help.\nI want to stress that it is the availability of low-cost tools that will make this very optimistic scenario possible. Low-cost software development tools, like BASIC or Turbo Pascal, are essential to the proliferation of good PC software. The emergence of low-cost expert-system shells is a very healthy sign for the future of this important software genre.\nWhile I wouldn't count on Osborne's Paperback Software to be an industry leader (I'd put my money more on Borland International, the king of low-cost, high-volume, cost-effective development software), Adam remains, as usual, on the cutting edge.\nOne other expert-system-flavored program that attracted some attention at the show and has garnered rave reviews in computer trade publications is Neuron Data's NEXPERT -- an unusual expert system that runs on both the Macintosh and the IBM AT.\nNEXPERT probably merits a column in its own right, but let's just say that it offers a uniquely visual representation of how expert systems work. Think of NEXPERT as an expert system's \"map\" that displays the islands, isthmuses, rivers and ranges of all the rules and data involved in the system.\nThis is a potentially important program and could well be the forerunner of expert systems of the future -- much as electronic spreadsheets are in the process of superseding their paper counterparts. Neuron Data is located in Palo Alto, Calif., and can be reached at 415-321-4488.\n","190":"America needs new tools for the timely measurement and monitoring of technology, jobs and skills to cope with the advance of artificial intelligence and automation, an expert panel composed mainly of economists and computer scientists said in a new report.\nThe panel's recommendations include the development of an A.I. index, analogous to the Consumer Price Index, to track the pace and spread of artificial intelligence technology. That technical assessment, they said, could then be combined with detailed data on skills and tasks involved in various occupations to guide education and job-training programs. \u00a0\n  A public-private collaboration, they added, is necessary to create such tools because information from many sources will be the essential ingredient. Those information sources range from traditional government statistics to the vast pools of new data from online services like LinkedIn and Udacity that can be tapped to gain insights on skills, job openings and the effectiveness of training programs.\n  ''We're flying blind into this dramatic set of economic changes,'' Erik Brynjolfsson, an economist at the Massachusetts Institute of Technology's Sloan School of Management, said in an interview.\n  Mr. Brynjolfsson was a co-chairman of the 13-member panel that drafted the 184-page report, which was published on Thursday by the National Academies of Sciences, Engineering and Medicine, a nonprofit organization whose studies are intended as objective analysis to inform public policy. He and the panel's other co-chairman, Tom Mitchell, a computer scientist at Carnegie Mellon University, also wrote a separate commentary in the journal Nature that was published on Thursday, explaining the problem.\n  Both the report and commentary were spurred by the advances in A.I. in recent years, including document-reading software and self-driving cars, which promise to make inroads into work done by humans. That prospect has created angst for many American workers about the difficulties of adapting to technological change and the failure of institutions to help them.\n  Yet technologists and academics still differ sharply on how fast the next wave of automation will proceed and how many occupations will be affected. That prompted the panelists to suggest the new data-monitoring tools and the pulling together of government and online data sources to sort through the consequences.\n  Those moves could eventually give a worker in a declining occupation useful information about a more promising occupation, with some similar skills but also requiring some new ones, Mr. Mitchell said. Then the software tool might also pull information on job placement rates for courses that teach those new skills.\n  That style of data-driven decision-making is a hallmark of internet companies like Amazon and Google, and it has been increasingly embraced across corporate America. ''There's no reason government can't do that,'' Mr. Brynjolfsson said.\n  In recent years, the federal government has made considerable progress in integrating its surveys of businesses and households with other data it collects, including information on foreign trade, payrolls and unemployment, said John Haltiwanger, a professor at the University of Maryland and former chief economist of the Census Bureau.\n  ''But our surveys are not really designed to track technology or its impact,'' said Professor Haltiwanger, who was a member of the expert panel. ''The best shot at that is the private sector data.''\n  A broad national initiative, perhaps with the Bureau of Labor Statistics and the Bureau of Economic Analysis setting rules for private sector data sharing and privacy protections, might not be possible, Mr. Brynjolfsson conceded. But he and Mr. Mitchell wrote in Nature, ''Perfection here is not a prerequisite for utility -- anything is better than flying blind.''\n  One program that embodies the panel's recommendations is Skillful, a collaboration of the Markle Foundation, LinkedIn, Arizona State University and edX, a nonprofit provider of online courses. The partners are working with employers, educators and local governments in Colorado and the Phoenix metropolitan area to link jobs, skills and training more tightly.\n  Past times of economic turmoil have led to new kinds of economic data collection and analysis.\n  At the outset of the Depression in 1929, for example, there was no measure of national economic activity or reliable information on unemployment. In June 1930, based on scattered reports of improvement, President Herbert Hoover prematurely declared, ''The Depression is over.''\n  To address the information gap of its day, the government hired economists and statisticians to come up with a scientific method for measuring the national economy. In 1934, a team led by Simon Kuznets published its report on how to calculate national income. The field of econometrics took a big step forward, and policy makers were less in the dark.\n  The time has now come, the expert panel suggested, for a similar effort to adapt to the modern digital economy.\n\n\n\n","191":"A future space mission to Titan, Saturn's intriguing moon enveloped in clouds, might deploy a blimp to float around the thick atmosphere and survey the sand dunes and carved valleys below. \n  But the blimp's ability to communicate would be limited. A message would take about an hour and a half to travel more than 800 million miles to Earth, and any response would take another hour and a half to get to Titan.\nThree hours would be a long time to wait if the message were: ''Help! I'm caught in a downdraft. What do I do?'' Or if the blimp were to spot something unusual -- an eruption of an ice volcano -- it might have drifted away before it received the command to take a closer look. The eruption may also have ended by then. \n  Until recently, interplanetary robotic explorers have largely been marionettes of mission controllers back on Earth. The controllers sent instructions, and the spacecraft diligently executed them.\u00a0\n  But as missions go farther and become more ambitious, long-distance puppetry becomes less and less practical. If dumb spacecraft will not work, the answer is to make them smarter. Artificial intelligence will increasingly give spacecraft the ability to think for themselves.\n  ''These technologies are already in operation on specific missions,'' said Steve Chien, a computer scientist who heads the artificial intelligence group at NASA's Jet Propulsion Laboratory in Pasadena, Calif. Scientists discussed some of the recent progress last week at a meeting of the American Geophysical Union in Baltimore.\n  HAL, the soulful conversationalist at the helm of the spaceship in ''2001: A Space Odyssey,'' is not on the drawing board. The work so far has been more along the lines of Roomba, the robotic vacuum cleaner, with autonomy to perform certain specific tasks.\n  Dr. Chien's group wrote the software that manages the schedule of Earth Observing-1, a satellite that looks for natural disasters like volcanic eruptions, wildfires and floods.\n  The satellite, known as E.O.-1 for short, takes repeated pictures of the areas it watches, looking for changes that would indicate an eruption or other event. Other satellites or sensors on the ground can also dispatch an alert to E.O.-1, telling it of something it should look at.\n  ''Almost immediately, within a matter of hours, the spacecraft is reprogramming itself to image these targets,'' Dr. Chien said, ''and we can get rapid response imagery of breaking science events.''\n  The spacecraft adds the new observation to its schedule and starts rearranging its other tasks. An observation that had been planned may be canceled or moved. All this occurs without a human in the loop. E.O.-1 just drops a note to the operators about what it has done.\n  When E.O.-1 was launched in 2000, people on the ground did the satellite's planning. The planning software was first tried in 2003, and the satellite now uses it full time. That not only sped up its reaction time, but it also cut its operating cost of $3.6 million a year by more than one-quarter.\n  Similar programming can be used for future planetary missions, perhaps for the next visit to Jupiter and its moons, to detect volcanic eruptions on Io or a shift in the fractures on the frozen surface of Europa.\n  NASA's two rovers now on Mars -- the Spirit and the Opportunity -- also possess a measure of thinking ability. As they drive, the rovers use stereo cameras to judge the distance and size of rocks in their paths in order to figure out how to maneuver around obstacles.\n  ''At every step, it looks at dozens of potential choices,'' said Mark W. Maimone, a member of the team working on the rovers' software. ''It picks the safest path that gets it closer to its goal.''\n  A software upgrade to be sent up to the rovers in a month or so will provide even greater autonomy. Mission controllers will then be able to tell the rovers simply, ''Go to that rock and put your instrument arm down on it.'' Currently, arm deployment takes an extra day, because controllers must first see the rover's final position before issuing the command to lower the arm.\n  Another part of the upgrade, adapted from the E.O.-1 software, will enable the rovers to perform a first cut of data, determining what is useful. The rovers have been taking photographs to search for clouds and mini-tornadoes known as dust devils. But through much of the Martian year, clouds and dust devils are rare, so most of the photos show nothing, and beaming them all to Earth is a waste of time. \n  ''You're restricted in the number of images you can return,'' said Rebecca Castano of the Jet Propulsion Laboratory.\n  With the new software, the rovers will analyze the photos and send back only those that appear to contain what the scientists are looking for. That will allow wider and more frequent searches. Dr. Castano said tests showed that the software was correct in identifying clouds 93 percent of the time. \n  More important, the software rarely failed at finding a cloud when one was there. Rather, the software sometimes saw a cloud when there was none -- a mistake that a scientist on the ground can easily correct. \n  Still, these spacecraft seem far from intelligent. E.O.-1's scheduling programs are not that different from the software that Wal-Mart uses to manage inventory, and the rovers' driving autonomy does not give them the ability to recognize unanticipated traps. In April last year, the Opportunity blithely drove into a sand dune, and took five weeks to back out.\n  ''None of the A.I. systems are as smart as a 2-year-old,'' said Cynthia Y. Cheung, a scientist at NASA's Goddard Space Flight Center in Greenbelt, Md. ''We want this system to be able to learn and react to the environment.''\n  Dr. Cheung is a member of a team led by Steven Curtis designing a more ambitious rover. It does not have wheels. Instead, it looks like a shape-changing jungle gym, with trusses that lengthen and shorten. A simple prototype has been built.\n  Computer animations illustrate its possibilities. Across flat terrain, it would roll like tumbleweed. \n  It could pull itself, almost catlike, onto rocks or flatten itself and slither through holes. (The animation can be viewed at nytimes.com\/science.)\n  To achieve those abilities, the machine would need sensors to observe its surroundings and then use the best mode of locomotion. While some safety rules might be explicitly programmed -- the equivalent of telling a child, ''Do not cross a busy road'' -- the scientists also will put in programming that allows the robot to learn its behavior through trial and error.\n  ''You'd essentially set up a playground where the robot can perform these simple behaviors,'' said Michael Rilee, another member of the Goddard team. ''It's a lot like what children do when they're very small, and they're just learning to move around.''\n  Dr. Curtis said he thought the technology could be ready for a rover to explore the rockier regions of the Moon in a few years.\n  How much new technology will be built and used is an unanswered question. ''This is a completely different way of doing business,'' Dr. Chien said. ''People are risk averse, for good reasons.''\n  Interplanetary space probes cost hundreds of millions of dollars each, and a mistake in the programming can lose the craft. An attempt last year to get a small craft named DART, Demonstration of Autonomous Rendezvous Technology, to dock autonomously with another satellite resulted instead in DART's collision with the target.\n  Scientists are also wary about letting a spacecraft throw away information before humans get to sift through it. They often joke about a rover obliviously driving past a dinosaur bone lying on the Martian surface because it had been programmed to recognize only rocks.\n  But autonomy will also allow them to do more at lower cost.\n  For some possible missions, like the Titan blimp, ''This sort of thing is going to be key for making the most of the mission,'' said Ralph D. Lorenz, a planetary scientist at the University of Arizona. \n  ''In an ideal world,'' he added, ''you'd downlink every picture, and scientists would have all of the time in the world to look over them, but you don't have that luxury.''\n  Because of the limited communications with Earth, Dr. Lorenz said the craft might summarize its findings -- that it has been flying over sand dunes, for example -- and send back only a few representative photos instead of images of the entire landscape.\n  ''To get the best science, you want to send down only the data with the richest science concentration,'' Dr. Lorenz said.\n  And for that, Dr. Lorenz is willing to let the spacecraft do the choosing.\n","192":"As I learned (sometimes painfully) during my time at the Treasury Department, words spoken by Treasury secretaries can over time have enormous consequences, and therefore should be carefully considered. In this regard, I am very surprised by two comments made by Secretary Steven Mnuchin in his first public interview last week.\u00a0\nIn reference\u00a0to a question about artificial intelligence displacing American workers,Mnuchin responded that \"I think that is so far in the future - in terms of artificial intelligence taking over American jobs - I think we're, like, so far away from that [50 to 100 years], that it is not even on my radar screen.\" He also remarked that he did not understand tech company valuations in a way that implied that he regarded them as excessive. I suppose there is a certain internal logic. If you think AI is not going to have any meaningful economic effects for a half a century, then I guess you should think that tech companies are overvalued. But neither statement is defensible.\nMnuchin's comment about the lack of impact of technology on jobs is to economics approximately what global climate change denial is to atmospheric science or what creationism is to biology. Yes, you can debate whether technological change is in net good. I certainly believe it is. And you can debate what the job creation effects will be relative to the job destruction effects. I think this is much less clear, given the downward trends in adult employment, especially for men over the past generation.\nBut I do not understand how anyone could reach the conclusion that all the action with technology is half a century away. Artificial\u00a0intelligence is behind autonomous vehicles that will affect millions of jobs driving and dealing with cars within the next 15 years, even on conservative projections. Artificial\u00a0intelligence is transforming everything from retailing to banking to the provision of medical care. Almost every economist who has studied the question believes that technology has had a greater impact on the wage structure and on employment than international trade and certainly a far greater impact than whatever increment to trade is the result of much debated trade agreements.\nAs for the secretary's questioning of tech company valuations, no one can predict markets, so he may turn out to be right. But with Apple trading at below the market average price earnings ratio, and Google trading with a price earnings ratio in the 20s at a time of very low volatility and near-zero long-term real interest rates, I do not understand the basis for Mnuchin drawing a conclusion about the inappropriate valuation of technology stocks.\nIn a highly uncertain world with a major tax reform debate upcoming, the credibility of the Treasury secretary is an important national asset. I hope Mnuchin will soon find an opportunity to clarify his thinking on technology and to back off from judging appropriate sector valuations in the stock market.\n","193":"SEATTLE -- Ali Farhadi holds a puny $5 computer, called a Raspberry Pi, comfortably in his palm and exults that his team of researchers has managed to squeeze into it a powerful program that can recognize thousands of objects.\nDr. Farhadi, a computer scientist at the Allen Institute for Artificial Intelligence here, calls his advance ''artificial intelligence at your fingertips.'' The experimental program could drastically lower the cost of artificial intelligence and improve privacy because you wouldn't need to share information over the internet. \u00a0\n  But the A.I. system is emblematic of something even more significant for the microelectronics industry as it inches closer to the physical limits of semiconductors made with silicon: It uses ^1\/32 of the memory and operates 58 times as fast as rival programs.\n  There is a growing sense of urgency feeding this sort of research into alternative computing methods. For decades, computer designers have been able to count on cheaper and faster chips every two years. As transistors have shrunk in size, at regular intervals, computing has become both more powerful and cheaper at an accelerating rate -- a concept known as Moore's Law.\n  Two years ago, with manufacturing costs exploding and severe technical challenges growing, the cost of individual transistors stopped falling. That has ended -- at least temporarily -- the ability of computer makers to easily make new chips that are faster and cheaper.\n  But if silicon has its limits, ingenuity may not. Better algorithms and new kinds of hardware circuits could help scientists continue to make computers that can do more and at a lower cost.\n  ''It's been a fun ride,'' said Thomas M. Conte, an electrical engineer at the Georgia Institute of Technology. ''Today you're entering this patchwork world where you are going to find a better solution for a particular problem, and that's how we're going to advance in the future.''\n  This summer, for example, Intel acquired Nervana Systems, a small maker of specialized hardware designed to run A.I. programs more efficiently.\n  Earlier this month, researchers at Argonne National Laboratory, Rice University and the University of Illinois at Urbana-Champaign published research demonstrating how a programming technique for an Intel microprocessor chip uses significantly less power to accomplish the same work.\n  The new approach is significant, according to supercomputer designers, because the high energy requirements of the fastest computers have become the most daunting challenge as scientists try to move from today's petaflop -- a quadrillion computations per second -- machines to exaflop computers, which could perform a quintillion computations per second.\n  Such computers are considered necessary to solve fundamental scientific problems like predicting the risk of climate change to the future of humanity.\n  Because of the slowdown in Moore's Law, the arrival of exascale computing has repeatedly been pushed back. Though it was originally expected in 2018, projections now set the next generation off as far as 2023.\n  The Argonne paper notes that a future supercomputer capable of an exaflop will multiply energy costs by a factor of a thousand. To reduce those energy demands, the researchers demonstrated how they used a conventional Intel chip and turned off half of its circuitry devoted to what engineers call mathematical precision. Then they ''reinvested'' the savings to improve the quality of the computed result.\n  ''Mathematical precision is like a knob you can turn,'' said Krishna V. Palem, a Rice University computer scientist. ''The question is what you do with the saved energy.''\n  The researchers experimented with using the various modes of the microprocessor in a manner similar to a gearshift in a car, automatically shifting from higher to lower precision and back as needed to solve a problem.\n  ''There is a lot to be done by thinking more carefully on how you can save energy,'' said Marc Snir, a veteran supercomputer designer and a computer scientist at the University of Illinois at Urbana-Champaign.\n  The Argonne researchers are exploring ideas put forward by Dr. Palem, who in 2003 first proposed an idea he described as ''inexact computing.'' He suggested trading off precision to make dramatic gains in computing efficiency. Originally, he explored the idea of inexactness as a way to make use of imperfect chips where portions of the transistors were not working because of manufacturing flaws.\n  More recently, he has turned to using his ideas to gain significant energy savings from today's common processors.\n  Dr. Palem said that the group was planning to extend the Argonne research to more efficiently run mathematical models that relate to climate change.\n  With colleagues from Rice University and Seoul National University, he recently demonstrated how inexactness could be applied to the challenge of pinpointing an indoor location, since GPS usually doesn't work inside buildings. The Rice researchers employed a technique known as a ''hash function,'' which involves representing a large chunk of data, like a digital photo, with a much smaller numerical value. They rely on that image to nail down the location.\n  While the Allen Institute researchers identified objects using extremely efficient versions of programs known as neural networks, the Rice scientists matched the surrounding scene captured by a smartphone camera from a library of imagery stored on the phone itself. The approach compresses all the bits that make up those photos and does location calculations on a simple hand-held computer -- something that would normally require pinging a data network over the internet.\n  Like the Allen researchers, the Rice University scientists think that the energy efficiency of their algorithm can preserve privacy since nothing is sent over the internet. What's more, they said in a recent paper, they were able to do it ''500 times cheaper, both in energy and computation cost'' over existing methods.\n  The lesson is that as engineering progress slows, advances will increasingly come from human creativity, computer scientists said.\n  In a Stanford University lecture last month, Alan Huang, an electrical engineer, showed how -- by reconfiguring internet links in the shape of a doughnut rather than the two-dimensional mesh that is used now -- it would be possible to cut internet delays in half, drastically speeding the delivery of digital video, while cutting the amount of computer equipment needed to deliver that data.\n  ''You don't need a quantum computer to do this,'' he said, referring to a concept for a supercomputer. ''You just need high school math.''\n\n\n\n","195":"The Pentagon is increasingly focused on the notion that the might of U.S. forces will be measured as much by the advancement of their algorithms as by the ammunition in their arsenals. And so as it seeks to develop the technologies of the next war amid a technological arms race with China, the Defense Department has steadily increased spending in three key areas: artificial intelligence, big data and cloud computing, according to a recent report.\nInvestment in those areas increased to $7.4 billion last year, up from $5.6 billion five years ago, according to Govini, a data science and analytics firm, and it appears likely to grow as the armed services look to transform how they train, plan and fight.\u00a0\n\"Rapid advances in artificial intelligence - and the vastly improved autonomous systems and operations they will enable - are pointing toward new and more novel warfighting applications involving human-machine collaboration and combat teaming,\" Robert Work, the former deputy secretary of defense, wrote in an introduction to the report. \"These new applications will be the primary drivers of an emerging military-technical revolution.\"\nThe United States \"can either lead the coming revolution, or fall victim to it,\" he added.\nIn an interview, Work, who serves on Govini's board, said the advancements in technology are transforming war just as the advent of the rifle, telegraph and railroad did generations ago. Much of the current work is being driven by companies with large presences in the Washington area, including Leidos, Northrop Grumman, Lockheed Martin, Raytheon, CACI and SAIC, according to the report.\nService members are using virtual reality to simulate battle conditions in training. The Defense Advanced Research Projects Agency (DARPA) has been investing in better computing power designed to handle vast amounts of data, including quantum computing and what's known as neuromorphic engineering, helping develop incredibly complex computing systems designed to mimic biological systems.\nThere are signs that AI and human-machine collaboration are already making their way into American weaponry and its intelligence apparatus. The Pentagon is working toward using drones as the wingmen of fighter jets and ships, which can probe into enemy territory on their own. The Marine Corps has been testing cargo helicopters that can fly autonomously and that would allow Marines, using a tablet, to \"easily request supplies even to austere or dangerous environments,\" according to the Office of Naval Research.\nThe stealthy F-35 Joint Strike Fighter, with 8 million lines of code, is called a \"flying computer\" that is as much a sensor in the skies as it is a fighter jet, officials say. As an example, officials point to how F-35s communicate with one another on their own. If one jet in a sortie detects an enemy fighter on its radar that is out of the range of the other F-35s along with it, that information is automatically relayed to the other jets.\nAnother example is Project Maven, a computing system being designed to sift through the massive troves of data and video captured by surveillance and then alert human analysts of patterns or when there is abnormal or suspicious activity.\nThe technology in robotics is fast improving, as well. In 2015, when DARPA sponsored a challenge to test how robots could navigate certain obstacles, many of the semiautonomous machines tumbled and fell, crashing in sometimes comical fashion. But last month, Boston Dynamics released a stunning video that showed a humanoid robot doing a back flip off a raised platform and landing on its feet.\nBut despite those advancements, the Pentagon and others are worried that the United States is not moving fast enough.\n\"The bad news is our competitors aren't standing still,\" Work said.\nChina in particular has been investing heavily in AI, defense analysts say. \n\"China intends to seize the initiative to become the 'premier global AI innovation center' by 2030, potentially surpassing the United States in the process,\" according to a recent report by the Center for a New American Security.\nThat should serve as a call-to-arms \"Sputnik moment,\" Work said. \"I personally believe that a national challenge like this has to be met with a national response,\" he said. \nFor the past several years, the Pentagon has been wooing Silicon Valley firms that have driven much of the innovation, but have traditionally been loath to work within the Pentagon's plodding and cumbersome bureaucracy. \nIn September, Deputy Defense Secretary Patrick Shanahan wrote in a memo that he was \"directing aggressive steps to establish a culture of experimentation, adaptation and risk-taking to ensure we are employing emerging technologies to meet our warfighters' needs and to increase speed and agility technology development and procurement.\"\nHe also signed a directive to accelerate the development of cloud computing for the Pentagon, which he said \"is critical to maintaining our military's technological advantage.\"\nchristian.davenport@washpost.com\n","196":"Among his many warnings about the rise of artificial intelligence, Elon Musk has said that autonomous machines are more dangerous to the world than North Korea\u00a0and could unleash \"weapons of terror.\" He\u00a0has compared\u00a0the adoption of AI\u00a0to \"summoning the devil.\"\u00a0\nNow\u00a0the billionaire inventor and\u00a0Tesla chief executive - who believes artificial intelligence could help trigger the next world war - has issued another severe warning about how super-intelligent machines could come to dominate the world. Those super computers could become \"an immortal dictator from which we would never escape,\" Musk passionately warns in the new documentary \"Do You Trust This Computer?\"       \nIn the documentary, directed by Chris Paine (the man behind 2006's \"Who Killed The Electric Car?\"), Musk joins a growing chorus of experts warning\u00a0that intelligent machines are already fundamentally changing our society by amassing personal data, advancing science and medicine and beginning to create new forms of super intelligence.\n\"We are rapidly headed towards digital super intelligence that far exceeds any human,\" Musk says in the film, which premiered Thursday in Los Angeles. \"I think it's very obvious.\"\nNothing will affect the future of humanity more than digital super-intelligence. Watch Chris Paine's new AI movie for free until Sunday night at https:\/\/t.co\/WehHcZX7Qe\n- Elon Musk (@elonmusk) April 6, 2018\n\"You are my creator, but I am your master\" - Mary Shelley\n- Elon Musk (@elonmusk) April 6, 2018\nThe film features technology experts such as Google Brain founder Andrew Ng, Affectiva\u00a0chief executive Rana el Kaliouby, Osaka University professor Hiroshi Ishiguro,\u00a0OpenAI director Shivon Zilis and \"Westworld\" co-creator Jonathan Nolan. Artificial intelligence already helps us live longer and enhance efficiency in numerous industries, but these experts argue that humans are already losing their grip on the technology, giving it power and abilities that humanity\u00a0may never be able to reclaim.\n\"The pattern here is that AI might take a little while to wrap its tentacles around a new skill, but when it does it is unstoppable,\" the film's trailer warns.\nIn recent years, Musk's\u00a0warnings about the risks posed by AI have grown\u00a0increasingly strident - drawing\u00a0pushback last year from\u00a0Facebook chief executive Mark Zuckerberg, who called Musk's dark predictions\u00a0\"pretty irresponsible.\"\u00a0Responding to Zuckerberg, Musk\u00a0said his fellow tech billionaire's\u00a0understanding\u00a0of the threat posed by artificial intelligence \"is limited.\"\nAccording to a report by\u00a0Mashable,\u00a0Musk was so intent on spreading his warnings about AI to the public that he paid for \"Do You Trust This Computer\" to be streamed free on YouTube over the weekend.\n\"It's a very important subject,\" he said at the film's premiere, according to Mashable. \"It's going to affect our lives in ways we can't even imagine right now.\"\n          MORE READING:\u00a0       \n          'Looks lame anyway': Elon Musk just deleted Facebook pages of Tesla, SpaceX - on a dare.       \n          Airbnb for cars is here. And the rental car giants are not happy.       \n          The unexpected thing that happens inside Waymo's self-driving minivans       \n","197":"TORONTO -- Long before Google started working on cars that drive themselves and Amazon was creating home appliances that talk, a handful of researchers in Canada -- backed by the Canadian government and universities -- were laying the groundwork for today's boom in artificial intelligence.\nBut the center of the commercial gold rush has been a long way away, in Silicon Valley. In recent years, many of Canada's young A.I. scientists, lured by lucrative paydays from Google, Facebook, Apple and other companies, have departed. Canada is producing a growing number of A.I start-ups, but they often head to California, where venture capital, business skills and optimism are abundant. \n  ''Canada is not really reaping the benefits from this A.I. technical leadership and decades of investment by the Canadian government,'' said Tiff Macklem, former senior deputy governor of the Bank of Canada, who is dean of the Rotman School of Management at the University of Toronto.\u00a0\n  Now bringing A.I. home is a priority for the Canadian government, companies, universities and technologists. The goal, they say, is to build a business environment around the country's expertise and to keep the experts its universities create in the country.\n  And they want to build on the tenacity of veteran researchers like Geoffrey Hinton, Richard Sutton and Yoshua Bengio, who developed techniques that opened the door to remarkable improvements in an A.I. technology called machine learning, even as many computer scientists and the tech industry considered their work to be an unpromising backwater.\n  There are encouraging signs, including new government funding, big company investments, programs to nurture start-ups, and the changing habits of homegrown entrepreneurs and American venture capitalists.\n  In its new budget, the government of Prime Minister Justin Trudeau pledged $93 million ($125 million Canadian) to support A.I. research centers in Toronto, Montreal and Edmonton, which will be public-private collaborations.\n  The Vector Institute for Artificial Intelligence in Toronto, announced two weeks ago, will be one of them. The institute begins with commitments of $130 million, about half the money coming from the national and provincial governments and the other half from corporate sponsors like Google, Accenture and Nvidia, as well as big Canadian companies like the Royal Bank of Canada, Scotiabank and Air Canada.\n  Mr. Hinton, who was hired by Google in 2013 but remains a professor at the University of Toronto, will serve as its chief scientific adviser. The new institute will be in the Mars Discovery District, a cluster of buildings in downtown Toronto, run by a public-private partnership, that is home to many tech start-ups including A.I. companies..\n  Major technology companies, like Google, Microsoft and IBM, are adding to their A.I. research teams in Canada. So are companies in other industries.\n  Last year, General Motors said it was going to locate one of its research and engineering hubs for self-driving cars in the Toronto suburb of Markham. And Thomson Reuters announced it would open a center for ''cognitive computing'' in Toronto for research into new ways professionals will use information and technologies to assist decision making.\n  Building businesses that use A.I. is an economic imperative for Canada. The Canadian tech industry has stalled in recent years. Nortel, Canada's big telecommunications equipment maker, declared bankruptcy in 2009, and was wound down over the next several years. And BlackBerry, once a leader, has faded in the smartphone market.\n  The experience of two start-ups applying A.I. technology to drug discovery illustrate the challenges -- and the opportunities -- facing Canadian start-ups.\n  Atomwise, a company that uses A.I. technology to predict what new molecules might combat specific diseases like multiple sclerosis, was founded in 2012. Its chief executive, Abraham Heifets, earned his Ph.D. in computer science from the University of Toronto.\n  When Mr. Heifets sought funding, he recalled, one potential Canadian investor said people had tried the same thing 20 years ago. ''What could possibly be new?'' Mr. Heifets said the investor had asked, and turned him down.\n  Later, Mr. Heifets went to the Bay Area and met with Timothy Draper, founder of the venture capital firm Draper Fisher Jurvetson. Mr. Draper observed that he had invested in a couple of companies trying a similar approach 20 years ago. That didn't deter him from trying again.\n  ''That's a cultural issue, a different appetite for risk and willingness to accept failure,'' Mr. Heifets said.\n  Atomwise moved to San Francisco to be close to its investors and the region's enormous talent pool.\n  By contrast, Deep Genomics, founded in 2014, has stayed in Canada, and its American-based venture backers encouraged it to remain in Toronto.\n  Brendan Frey, the chief executive, studied under Mr. Hinton at the University of Toronto, and he has spent years on research that combines deep-learning A.I. and cell biology. When he hires software engineers, he asks them to make multiyear commitments.\n  ''There are a lot of distractions in the Bay Area,'' said Mr. Frey, who is also a professor at the University of Toronto and a co-founder of the new Vector institute. ''The hype is a little too hot down there. Besides, we have some of the best talent in the world here.''\n  Both Atomwise and Deep Genomics were participants in different years in a program called the Creative Destruction Lab. Founded in 2012 by Ajay Agrawal, a professor at the Rotman School, the lab was set up to help technology-intensive start-ups. They are typically founded by a Ph.D. scientist who has worked on an idea for five years, but has little or no business experience.\n  In 2015, the program tilted toward A.I. start-ups, with 25 companies admitted. Last year, 50 A.I. start-ups were admitted, and this year will likely have 75, Mr. Agrawal said.\n  The program lasts nine months, with fall and spring terms, much like a school year. The participants gather every eight weeks in Toronto for two days to make presentations, listen to advice and set goals for the next eight weeks.\n  At every gathering, at least one and sometimes several companies are voted out. The voters are a growing group of tech entrepreneurs and investors whom Mr. Agrawal has recruited.\n  One of the X factors in Canada's drive to develop an A.I. industry is the Trump administration. Canadian A.I. scientists say they have received a stream of inquiries from researchers in the United States, concerned about the new administration's stance on immigration and other policies.\n  Should there be a northward migration it wouldn't the first time. Mr. Hinton settled in Canada in 1987 in part because of America's clandestine support for the Contra guerrillas who sought to overthrow the left-wing Sandinista government in Nicaragua.\n  Mr. Hinton, who is from Britain, was at Carnegie Mellon University at the time, and he realized that continuing his research in America would have meant accepting funding from the Reagan administration. ''I preferred Canada,'' Mr. Hinton recalled.\n  Mr. Sutton left the United States to become a professor at the University of Alberta in 2003, after American troops landed in Baghdad. ''George Bush was invading Iraq,'' he said. ''It was a good time to leave.''\n\n\n\n","198":"\" 'In the knowledge lies the power' -- that was going to be the mantra for the '80s,\" Douglas B. Lemat of Stanford University told the throng in the Washington Hilton's International Ballroom yesterday afternoon. \"But I like to say that the mantra for the mid-'80s is: 'In the knowledge acquisition lies the bottleneck.' \"\nLemat was talking about computers, of course, and about the often frustrating business of trying to teach them how to gain knowledge by a more efficient process than having humans type lessons in on a keyboard. In one way or another, Lemat said, the learning problem looms as the most formidable obstacle in the field called Artificial Intelligence.\nBut as Lemat's enthusiastic audience--perhaps 1,000 strong--made clear, the obstacles have also been, to many people, incentives. During the third annual--and biggest ever--National Conference on Artificial Intelligence at the Washington Hilton this week, signs of success have been visible everywhere. As of yesterday, more than 1,850 people had paid $140 and up to attend. Companies with names like Symbolics, IntelliGenetics, Digital Equipment, General Electric and Xerox had set up elaborate displays in the exhibit room. And on the conference registration desk, supplies of the official bumper sticker (\"Artificial Intelligence--It's for Real\") were running perilously low.\u00a0\nChess players, said Hans Berliner, a former world champion at correspondence chess and a computer scientist at Carnegie-Mellon University, like to talk about the obvious differences between the ways humans and computers handle the game.\n\"I've heard people make these pronouncements--that 'This must be a computer move,' or 'Only a computer would do that,' \" Berliner said, \"and I have the feeling that frequently it's a case of 20-20 hindsight.\"\nSo, for this week's conference, Berliner decided to construct a test--a series of games in which humans would have to guess the identity of their opponents. And sure enough, of the six humans playing other humans, five thought they were up against computers.\nNevertheless, Berliner said, there are clear differences. \"The computers excel in complicated situations--where there are lots of possible captures and open lines, where there can be very dramatic changes very quickly. In a more closed, positional type of game, humans tend to do better.\"\nArtificial intelligence is a hazily defined field with two related subdomains: the effort to design computer software and hardware to perform tasks requiring reasoning and perception; and the study of how reasoning and perception work.\n\"When I entered this field in 1967, it was a purely academic discipline,\" recalled Eugene Charniak of Brown University. \"If you had told me that in 15 years artificial intelligence would be a profitable field with major industries wanting to invest, I would have said you were nuts!\"\nCharniak's particular preoccupation is the challenge of teaching computers to use \"natural languages.\"\n\"While I would hope to see a computer system capable of speaking and understanding complete, unadulterated, no-holds-barred English in my lifetime, I would not be surprised if I didn't,\" he said over coffee in the downstairs snack bar Tuesday morning.\nThe problem, Charniak explained, is the difficulty of separating knowledge about language from knowledge about the world at large. Rules of grammar alone will not always tell you, for example, what a pronoun refers to.\nHe rattled off a brief tale by way of illustration: \"Jack went to the supermarket. He found the milk on the shelf. He paid for it and went home.\"\nNormally, said Charniak, \"it\" would refer to the last-mentioned inanimate object in the sentence--in this case \"shelf.\" So \"unless a computer has a great knowledge of the domain you're conversing about, it's not going to be able to handle complicated pronoun problems.\" Or, he added, to make sensible decisions about such issues as whether the word \"bank,\" in a particular context, means the side of a river or a place where money is stored.\nIn the commercial world, natural-language programmers are finding it easy to sidestep the more profound issues that Charniak and others find so engrossing. In business situations, according to Gary Hendrix, head of research and development for a small Silicon Valley firm called Symantec, it isn't that important for a computer to understand all possible statements in natural language, so long as it shows a certain basic level of receptivity.\n\"It's not that different from talking to a foreigner or a child,\" said Hendrix. \"You find out pretty fast what that person can understand. Humans can adapt to the lack of knowledge in a computer pretty quickly. People are pretty good at focusing down on some subset of the natural language. They're not so good at learning some completely foreign language.\"\nUntil recently, the few available natural-language programs required vast computer systems costing $1 million and more. \"Now we're beginning to think very seriously about making this kind of technology available on a personal computer,\" said Hendrix.\nThat combination--the \"user-friendly\" qualities of familiar English and small, cheap computers--will spur tremendous further growth in the market, he predicted. It will \"get computer power in the hands of lots and lots of people, and that, I think, is going to be very beneficial to our society.\"\nBenjamin Kuipers of Tufts University has been trying to understand how people learn routes. If you ask someone for directions, said Kuipers, you often get an answer like, \"I can take you in my car but I can't tell you.\"\nHuman knowledge often comes in such hard-to-define, incomplete packages, and this, Kuipers said, makes it hard to translate into computer-programming terms.\nFor his own example, however, Kuipers described a possible solution. The route can be broken down into a series of key decisions, each with its own \"associative link\" between \"views\" and \"actions.\" The driver sees a particular view through the car window, then he remembers a particular turn he should make, and finally, to monitor his own performance, he looks for another view. And then the process starts all over again.\n\"Expert systems\" are computer programs that draw conclusions and make recommendations based on complex data--much as human \"experts\" do. \"It's the closest we can come to cloning,\" said MIT's Randall Davis, who has helped design a number of expert systems, including one called \"Mycin\" for the diagnosis of infectious diseases.\n\"We spent five years asking the doctors what they did,\" he recalled. \"It was mostly a matter of looking at case after case and getting them to be very reductionist in their description of how they think.\"\nSometimes a doctor might say he was just using \"intuition,\" but \"it's because he really doesn't understand yet what he is doing,\" said Davis.\nSo far, expert systems have been developed for specialties as diverse as oil prospecting and diesel-locomotive maintenance. \"What you've got to ask is, 'Who are the people you'd like to have more of?',\" said Davis.\nTraditional computer programs tend to do one step at a time, and to require precise answers. Expert systems, on the other hand, ask clusters of questions, and try to deal with imprecise answers.\n\"We want to say things like 'very likely' or 'probably' or 'maybe,' \" Davis explained. \"But we don't really know what we're talking about. Maybe we're talking about 'strength of belief' and how strong you believe something may not be the same thing as probability in a mathematical sense.\"\nAlthough the medical diagnosis systems have looked very promising in trial situations, they have not reached the speed with which the best doctors \"zero in\" on a few likely diagnoses. \"If you give a doctor a hundred symptoms,\" Davis said, \"it's amazing how quickly they'll focus in on the five or six diseases it could be.\" During a break in the conference, Azriel Rosenfeld, a robotics and vision researcher at the University of Maryland, had a chat with an official of General Electrics.\nRosenfeld was curious to know if G.E. might consider helping fund the Center for Automation Research at the University of Maryland, which, as it happens, Rosenfeld founded.\nThe man from G.E. said such an arrangement was certainly possible, but his company had been burned in a few of these academic alliances. It had not always gotten any clear return on its investment. A university-based research facility might, for example, promise the donor a six-month notice on new discoveries. \"But that's a lot of baloney,\" said the man from G.E. \"You can hear most of this stuff a year ahead of time at conferences like this.\"\n","199":"How do New York Times journalists use technology in their jobs and in their personal lives? Cade Metz, a technology reporter for The Times based in San Francisco, discussed the tech he is using.\nYou write a lot about artificial intelligence. What is the strongest A.I. product that people can use right now? \n  A good thing to try is the digital assistant on the latest Google Pixel phone because it shows both the power of artificial intelligence and the limitations.\u00a0\n  It is very good at recognizing what you say, and it responds in a voice that sounds more human than most computer noises we are used to. Researchers at a Google A.I. lab in London built a system that learns to speak by analyzing recordings of human voices and identifying the patterns that make them human.\n  But it still doesn't sound exactly like a human. And though the Google Assistant -- like other digital assistants from the likes of Amazon and Apple -- does a decent job of responding to some questions and commands, it is a long way from not just recognizing but actually understanding conversational speech.\n  That is the story of A.I. over the last five years. The improvements have been enormous. But there is still a very long way to go.\n  Do you use it at all to help you do your work?\n  Nope. I reviewed the new Pixel phone. But I get along fine with my older Android phone. The irony is that I am a techno-skeptic.\n  At least in my own head, this makes good sense. I don't cover a consumer technology company. I cover the people and the ideas that are driving the next wave of technologies. And I took at where these technologies will take us -- for better or for worse.\n  But here is what I would like: a service that automatically transcribes my recorded interviews. There are services that use humans to do this, but they're expensive. Speech recognition technology has improved to the point where machines should be able to quickly and accurately do this for me on the cheap.\n  What are the potential pitfalls of so much A.I. in our lives?\n  They are myriad. The latest A.I. services, like the one that handles the voice for the Google Assistant, learn tasks by analyzing enormous amounts of data. By analyzing millions of llama photos, for instance, a system can learn to recognize a llama.\n  But this kind of machine learning can have unintended consequences. A few years back, Google got into trouble because its image recognition service identified some black people as gorillas.\n  What's more, researchers have shown that these kinds of systems can be fooled into seeing things that aren't there. That becomes an issue as the latest computer vision techniques find their way into security cameras, cars and other robotics.\n  The other worry is that the rise of certain technologies -- including autonomous cars and other robotics -- will significantly shift the job market. This is sure to happen at some point, but experts disagree on when.\n  Tech companies have hyped up virtual reality for the last few years. But VR is still mostly a gaming accessory. Do you think it will ever be used by everyday people?\n  Like A.I., V.R. is overhyped. The difference is that A.I. is changing the everyday world in very real ways. Look at the Amazon Echo, the autonomous cars now being tested in places like the Phoenix area, the enormous improvements of widely used services like Google Translate. V.R. is, as you say, still mostly a gaming accessory.\n  There are some interesting possibilities, though. Therapists are using V.R. for exposure therapy, after two decades of published research in this area. Basically, it can help people conquer phobias or other conditions like post-traumatic stress disorder.\n  But everyday use? That is not happening anytime soon -- especially for people like me, who wear glasses and don't like heavy equipment strapped to our heads.\n  What tech product do you love using in your daily life?\n  Apple's iTunes. And the Amazon Kindle app. And Stitcher, a podcast app that reliably delivers everything from Revisionist History to archived episodes of Desert Island Discs. I also like my headphones.\n  The beauty of technology is that you can read, watch and listen to anything you want when you want it. Almost.\n  Some researchers are developing robots that learn on their own. Is Skynet inevitable?\n  This fear is blown way out of proportion. But that might be a good thing. Though the Skynet scenario is unlikely to be a problem for many, many years -- the far more immediate worry is the coming shift in the job markets from automation, and that is still years away -- we certainly don't want Skynet catching us unawares.\n  That is why some researchers are already starting to think about what they call A.I. safety. They want to make sure that miscreants can't fool security cameras into seeing things that aren't there -- and that A.I. systems can't prevent humans from flipping their off switch, however far away that possibility may be.\n\n\n\n","200":"SAN FRANCISCO -- For all the bells and whistles of Google's new smartphone, the biggest point of emphasis is tucked away in small type on the back of the device: ''Phone by Google.''\nToday, most of Google's smartphone software runs on devices manufactured by companies like Samsung, LG Electronics and Lenovo, with Google's presence often relegated to the background. But with its new Pixel smartphone, introduced Tuesday at a press event, Google is front and center and selling a phone that it created from the industrial design to the components. \u00a0\n  Google's new strategy of controlling both hardware and software for its devices puts the company more directly in competition with Apple and many of its own Android partners. It's a necessity, Google says, because of the advent of artificial intelligence.\n  Sundar Pichai, Google's chief executive, said devices with artificial intelligence -- where computers can understand what people are saying and respond conversationally with the right information at the right moment -- present a seminal moment in computing on par with the creation of the personal computer, the World Wide Web and smartphones.\n  For Google, artificial intelligence takes form in the Google Assistant. Google demonstrated how the Assistant, through a series of questions, can be used to plan a night out -- from finding out about upcoming concerts at a certain venue to booking a reservation at a restaurant or researching how long the drive from the restaurant to the show will take.\n  ''The goal is to build a personal Google for each and every individual,'' Mr. Pichai said.\n  Pixel is the first smartphone with the Google Assistant built into the device. It was part of a new-product barrage including a Wi-Fi router, a virtual reality headset and a Chromecast device for streaming high-resolution video.\n  The Assistant is also a key part of another product the company introduced on Tuesday, Google Home. As the company's answer to the Amazon Echo, Google Home is a speaker that listens for questions or commands to play music or control internet-connected devices. The Echo, which relies on A.I. software created by Amazon called Alexa, has been a surprise hit and has been on the market for two years.\n  The Echo's success informed an important part of Google's strategy: hardware products that provide a vessel to get the Google Assistant into the hands of consumers.\n  Its competitors are taking a similar path with their A.I. technology. Amazon is now building Alexa into its other hardware products like Fire tablets and Fire TV set-top boxes. Apple is considering expanding the reach of its virtual assistant from the iPhone and iPad into the home with an Echo-like device.\n  ''If you really want to make a step-change difference, you really have to design the software and hardware together,'' said Rick Osterloh, who returned to Google in April to be its senior vice president of hardware. He was the president of Motorola when it was owned by Google and moved with the company when it was sold to Lenovo in 2014.\n  Part of Mr. Osterloh's mandate is to make sense of Google's fragmented hardware efforts, which span a wide range of devices, from Chromebook computers to Wi-Fi routers. He said he planned to focus Google's hardware resources in areas that highlight the company's software, while also creating a unified look and feel to the devices.\n  Creating a uniform experience, especially in the world of smartphones, has been a challenge for Google. Hardware manufacturers often modify Google's software to make what they sell a little different from that of their competitors.\n  For the last six years, Google worked with other hardware manufacturers such as Samsung and LG Electronics to develop the Nexus line of smartphones. Those phones provided a showcase for the best of Google's software, but much of the design and production process was handled by the company's hardware partners.\n  With Pixel, Mr. Osterloh said, ''we wanted to build things as Google intended.''\n  But becoming a hardware manufacturer is not easy. Google is now exposed to new risks associated with a hardware business, such as managing inventory, providing customer service and procuring components.\n  With the Pixel, Google clearly has Apple's iPhone in its sights. During an onstage presentation, Mr. Osterloh said the Pixel doesn't have an ''unsightly camera bump'' -- a reference to the protruding nub that sticks out from the iPhone's rear camera.\n  In a commercial for the Pixel, Google also said its phone is all-new while noting that the Pixel comes with a ''satisfyingly not new'' headphone jack. Apple's latest iPhones come without a headphone jack, a design decision that has sparked furious debate in the technology news media.\n  The Pixel, which is available for order, will come with a very iPhone-like price, starting at $649. Google said Google Home will sell for $129 and be available starting next month. The Pixel is available at Verizon and Best Buy stores. For people who want to use the phone with a different carrier, Google will sell the phone unlocked at its online store.\n\n\n\n","201":"Facebook wants the world to see a lot more patterns and predictions.\nThe company said Friday that it was donating for public use several powerful tools for computers, including the means to go through huge amounts of data, looking for common elements of information. The products, used in a so-called neural network of machines, can speed pattern recognition by up to 23.5 times, Facebook said.\u00a0\nThe tools will be donated to Torch, an open source software project that is focused on a kind of data analysis known as deep learning. Deep learning is a type of machine learning that mimics how scientists think the brain works, over time making associations that separate meaningless information from meaningful signals.\nCompanies like Facebook, Google, Microsoft and Twitter use Torch to figure out things like the probable contents of an image, or what ad to put in front of you next.\n\"It's very useful for neural nets and artificial intelligence in general,\" said Soumith Chintala, a research engineer at Facebook AI Research, Facebook's lab for advanced computing. He is also one of the creators of the Torch project. Aside from big companies, he said, Torch can be useful for \"start-ups, university labs.\"\nCertainly, Facebook's move shows a bit of enlightened self-interest. By releasing the tools to a large community of researchers and developers, Facebook will also be able to accelerate its own AI projects. Mark Zuckerberg has previously cited such open source tactics as his reason for starting the Open Compute Initiative, an open source effort to catch up with Google, Amazon and Yahoo on building big data centers.\nTorch is also useful in computer vision, or the recognition of objects in the physical world, as well as question answering systems. Mr. Chintala said his group had fed a machine a simplified version of \"The Lord of the Rings\" novels and the computer can understand and answer basic questions about the book.\n\"It's very early, but it shows incredible promise,\" he said. Facebook can already look at some sentences, he said, and figure out what kind of hashtag should be associated with the words, which could be useful in better understanding people's intentions. Such techniques could also be used in determining the intention behind an Internet search, something Google does not do on its regular search.\nBesides the tools for training neural nets faster, Facebook's donations include a new means of training multiple computer processors at the same time, a means of cataloging words when analyzing language and tools for better speech recognition software.\n","202":"FOR two decades the development of artificial intelligence - the use of computers to imitate learning and reasoning processes and solve problems - was largely confined to a few university computer centers. Now artificial intelligence, or A.I., is making its way into the research and development laboratories of Fortune 500 companies across the nation.\n The change has opened doors for people like Gregg Vesonder, 32 years old, who is not your average computer programmer. Five years ago, after finishing his Ph.D. in cognitive psychology at the University of Pittsburgh, Mr. Vesonder turned down an academic position for a job at Bell Laboratories' research center in Whippany, N.J. The system he created, Automated Cable Expertise, or ''ACE,'' enables a computer to monitor telephone cable repair and recommend maintenance. It is already in use in Fort Worth, Tex.\u00a0\nMr. Vesonder is one of a relatively tiny force of young Ph.D.'s and university professors who also work at the commercial vanguard of a technology that most believe will have enormous ramifications for the computer industry. International Data Resources, a consulting firm in Norwalk, Conn., estimates that the market for artificial-intelligence products, now less than $100 million, could be worth $8 billion by 1990. And many experts predict that within the decade there will be little computing that is not somehow affected by the new discipline.\nArtificial intelligence is a broad term used to describe any program that allows computers to handle functions considered to require intelligence when humans do them. Programs like Mr. Vesonder's ACE are called knowledge-based, or expert systems, because they imitate the work done by experts. Another expert system, designed by Schlumberger Ltd., an oil- production products company, will interpret data from oil-well readings; TRW has created a system called ''Adept,'' which can help military field commanders make tactical decisions in battle.\nA second major A.I. sector deals with so-called natural language computer programs, which enable computers to recognize and understand English and other written languages and thus obviate some computer-language programs, eliminating the need for many data-processing personnel. Experts estimate that sales of such programs alone could reach $2 billion by the end of the decade.\nThe growing market for this discipline bodes well for the very few qualified to work in it. University officials report that doctoral candidates are increasingly sought by major oil, aerospace and computing companies, as well as by small robotics concerns and a growing number of start-up consulting firms. Employers who seek such programmers agree that it is already a job-hunter's market, and that the industry's expected growth in the next 5 to 10 years could well outstrip the availability of human resources it will require.\n''It's a hot area, and there are a lot of companies that are trying to apply it,'' said Rick Reis, assistant director of the Center for Integrated Systems in Palo Alto. ''The numbers of people who are going into the field aren't large, and the demand exceeds the supply considerably.'' Shin-Yee Lu, a research specialist and recruiter for the Exxon Corporation, echoed the phenomenon: ''I will be lucky to find two people by the end of the year.''\nThe people who enter commercial artificial-intelligence operations still are most notable for their scarcity. Only Stanford, Carnegie-Mellon and the Massachusetts Institute of Technology have sizable A.I. research facilities, and each produces only a dozen Ph.D.'s yearly in A.I.-related fields - typically, computer science or electrical engineering.\nTHIS relative scarcity obtains even though interest in such programs has risen sharply: Carnegie-Mellon, for one, reports that applications for admission to its A.I.-related Ph.D. programs are more than six times what they were in 1976.\nSome graduates find positions through their universities' job placement offices; according to Robert Wetherall, director of the M.I.T. office of career services, representatives of more big high-technology companies than ever, including Honeywell, United Technologies and Schlumberger, filed through his office last year in search of A.I. programmers. Starting salaries at M.I.T. and other schools average between $40,000 and $50,000, slightly higher than those in other special computing disciplines.\nBut more often, employers and employees acknowledge, jobs result from academic contacts. ''I know most of the people who will be graduating this year with A.I. degrees,'' said Mitchell Marcus, who left M.I.T.'s artificial-intelligence research staff two years ago to develop natural-language programs for Bell Labs. ''The number of people who are doing research and are very good is very small.''\nNot all artificial-intelligence programmers have computing or engineering Ph.D.'s, and increasing numbers have little or no A.I. research experience. The A.I. researchers in TRW's Defense Systems Group include Ph.D.'s in philosophy, linguistics and biological engineering; all received in-house training in A.I. ''The common link is that they love knowledge,'' said Edward Taylor, director of requirements analysis for the TRW division. ''They like to twist things around.''\nMany experts believe that a company's ability to train programmers from within will soon become a key to success in A.I. research. Indeed, some downplay the importance of specifically trained Ph.D.'s at all, arguing instead that industry demand will soon require that certain A.I. courses be taught at lower educational levels.\nSome artificial intelligence will actually reduce the need for computing jobs - indeed, the point is a source of controversy in the industry. Henry Levin, a Stanford economist, among others, argues that new products in natural languages will eliminate many jobs held by traditional software programmers.\nFor Ph.D.'s specifically trained in artificial intelligence, the long-term employment outlook appears secure but not dynamic. Most believe that Ph.D.'s will remain in demand but will never account for a significant portion of the field's total employment. ''It's not a field that's going to employ thousands of people right now because it's not production-oriented,'' said Mr. Reis of the Center for Integrated Systems. ''And once it is production-oriented, you'll have engineers, not Ph.D.'s, putting A.I. systems together.''\n\n","203":"Artificial Intelligence is kind of a scary concept, probably because we know that a robot smarter than us would run the numbers and be like \"Humanity? Nah.\"\n[Apple co-founder on artificial intelligence: 'The future is scary and very bad for people']\n But the race to create\u00a0an autonomous, intelligent computer is still ongoing. The latest potential step forward comes from the Allen Institute for Artificial Intelligence (AI2) in collaboration with the University of Washington, where an AI system has gotten a fairly average score (for a high schooler) on the math section of the SAT --\u00a0500 points out of 800. That's about 49 percent accuracy, and comes quite close to the  high school senior average of 513 points . \u00a0\nFirst, why that's not so astonishing: Computers are really good at math. In fact, you'd expect any decent computer to be able to breeze through\u00a0all\u00a0of the questions on the math section of the SAT.\n[Stephen Hawking just got an artificial intelligence upgrade, but still thinks AI could bring an end to mankind]\n Instead,  the AI system \u00a0called GeoS probably\u00a0couldn't get accepted to the university that designed it, where the average math score is 580 to 700. \nBut here's the astonishing part: GeoS wasn't fed these problems in a language it could inherently understand. It had to read them straight off the paper, confusing diagrams and all, and interpret them the same way a human student would. After \"looking\" at the problem, GeoS uses the diagram and text to come up with a set of formulas that it \"thinks\" are most likely to correspond with the problem. Then, since those formulas are in its native tongue, the system is able to solve the questions handily. Once it has an answer it goes back to the multiple choice options and looks for one that matches its outcome.\nYou can see a demonstration of that problem-solving process\u00a0here.\n\"Our biggest challenge was converting the question to a computer-understandable language,\" Ali Farhadi, an assistant professor of computer science and engineering at the University of Washington and research manager at AI2, said in a statement.\u00a0\"One needs to go beyond standard pattern-matching approaches for problems like solving geometry questions that require in-depth understanding of text, diagram and reasoning.\"\nIn the reported tests, GeoS wasn't always able to answer questions -- in fact, the system failed to come up with a solution about half the time. But when it was confident enough to answer, the system had a 96 percent accuracy rate.\n[Here's the argument for banning killer robots before we're swarmed by them]\n Farhadi and his colleagues believe that this demonstration shows a step toward true AI -- a computer brain that can work just like a human's -- even if it hasn't blown us out of the water yet. \nBut engineers generally disagree with one another when it comes to proving computer intelligence. Last year another group claimed to have passed the Turing Test -- where a computer can successfully pass itself off as human to another human -- and was met with much skepticism. A recent piece in New Scientist suggests that a single test for AI may be an antiquated notion. Instead, many researchers now believe, we should be working on copying and evaluating individual aspects of intelligence, and trying to put them together into one brainy computer.\nGeoS seems to be making strides in some very important aspects of intelligence, but we'll have to wait and see just what the system is capable of. And it's far from ready to take its place as a robot overlord.\n              Read More:           \n Go ahead and break this robot's legs. It can figure out how to chase you without them. \n This new opera stars an autonomous robot (really) \n Self-proclaimed 'experts' more likely to fall for made-up facts, study finds \n Here's the argument for banning killer robots before we're swarmed by them \n Stephen Hawking announces $100 million hunt for alien life \n","204":"It is uplifting to read that in this world of increasing specialization, Douglas Hofstadter is reaching out, laterally as it were, and putting ideas together from such diverse fields as philosophy, neuroscience and artificial intelligence (''Exploring the Labyrinth of the Mind,'' by James Gleick, Aug. 21).\nHowever, it was disheartening to read that many people within the community of artificial-intelligence researchers fail to see the value (and, in fact, a pressing need) for just this type of synthesis of knowledge inherent in Mr. Hofstadter's style of investigation.\u00a0\nThose who criticize Mr. Hostadter's approach as being ''unscientific'' because he does not flood us with programs are confusing the experiments with the science. In my mind, his work borders far more on rational inquiry than the tinkering (or ''hacking,'' as it is often called) that constitutes much (but not all) of the work in artificial intelligence today.AARON FALBEL, Stamford, Conn.\n","205":"RE: BARBIE\nJames Vlahos wrote about Hello Barbie, a doll that, with the help of artificial intelligence, can not only talk but also hold a conversation. \n  Oren Jacob, a founder of ToyTalk, says the technology behind Hello Barbie was inspired by his daughter's asking if she could Skype with her stuffed animal. But children don't need toys with Wi-Fi and speech-recognition software to have interactive conversations with inanimate objects: They need only their imaginations. And when children play both parts in a conversation with a doll or stuffed animal, they not only exercise their creativity more, but there is no danger of them revealing intimate details about their lives to a stranger in a corporate office or having a digital dossier compiled of their likes and dislikes.\n  ToyTalk's real breakthrough is allowing multinational corporations to listen in on children's private play and conversations. And that's why so many parents are rightly creeped out by the idea of Hello Barbie. Josh Golin, Boston\u00a0\n  Although ostensibly about advances in artificial intelligence in children's toys, this article actually highlights the extent to which girls are programmed to garner their self-worth through adult-dictated means. Not only does this new ''talking'' Barbie tell girls what to think and how to think it; Barbie effectively removes girls from actual conversations with one another and instead substitutes a toy for human interaction.\n  Rather than celebrate innovations that allow artificial-intelligence equipment to be hidden in Barbie's ''slightly thickened thighs,'' in order to shore up falling sales quotas, the toy industry in general and Mattel in particular should work to highlight girls' creativity, intelligence and engagement with the world around them. In this regard, ''talking'' Barbie is a miserable failure. Leslie Boxer, Westport, Conn.\n  There are some surprising parallels between the new Hello Barbies and today's new electronic sex dolls. Critics of A.I. toys like Barbie fret that ''for some children, synthetic friendship could begin to supplant the real kind,'' and critics of the new ultrarealistic electronic sex dolls worry that they will supplant men's relationships with real women. In England, there are even movements afoot to make sex dolls illegal. Will parents soon feel they are being supplanted by talking Barbies and try to ban them too? Julie Wosk, New York\n  RE: KAREEM ABDUL-JABBAR\n  Jay Caspian Kang spent a peculiar day with Kareem Abdul-Jabbar, the former basketball star who is also a reserved intellectual.\n  I've found that the best way to get what you want to know from Kareem is to ask him. He is thoughtful, intelligent, sardonic, verbal, laugh-out-loud funny, but you won't find that out if you treat him with passive-aggressive hostility. Nor should you. He is not comfortable with professional strangers. Chalk this up to his treatment when he was a politically aware man in his 20s being baited by the conservative old-school press. Yet even now, at ballparks, at grocery stores, in the street, when he starts signing autographs, a crowd gathers and his life is put on hold. Imagine this happening, guaranteed, every day of your life. Some people enjoy this contact, but it is not a character flaw that Kareem does not. This article is a disservice to a man of accomplishment and dignity. Peter Knobler, co-author of the Abdul-Jabbar autobiography ''Giant Steps,'' New York\n  As a former deputy secretary of tourism for Wisconsin, I was involved in producing a series of trailblazing tourism TV commercials with Kareem Abdul-Jabbar reprising his co-pilot role from the movie ''Airplane!'' In our society of disposable celebrities and reality television, Kareem is a true renaissance man who quietly works to bring the debate about many of society's pressing issues to a higher level. He is a deep thinker, thoughtful and compassionate, and articulates his beliefs by taking the ''high road.'' With Kareem, it's not about winning elections or popularity contests or convenient sound bites pre-scripted by handlers. Kareem is the real deal. His statistics and accomplishments on and off the court are equally impressive. David Fantle, Bayside, Wis.\n\n\n\n","206":"ACCORDING to some prominent voices in the tech world, artificial intelligence presents a looming existential threat to humanity: Warnings by luminaries like Elon Musk and Nick Bostrom about \"the singularity\" - when machines become smarter than humans - have attracted millions of dollars and spawned a multitude of conferences.\nBut this hand-wringing is a distraction from the very real problems with artificial intelligence today, which may already be exacerbating inequality in the workplace, at home and in our legal and judicial systems. Sexism, racism and other forms of discrimination are being built into the machine-learning algorithms that underlie the technology behind many \"intelligent\" systems that shape how we are categorized and advertised to.\u00a0\nTake a small example from last year: Users discovered that Google's photo app, which applies automatic labels to pictures in digital photo albums, was classifying images of black people as gorillas. Google apologized; it was unintentional.\nBut similar errors have emerged in Nikon's camera software, which misread images of Asian people as blinking, and in Hewlett-Packard's web camera software, which had difficulty recognizingpeople with dark skin tones.\nThis is fundamentally a data problem. Algorithms learn by being fed certain images, often chosen by engineers, and the system builds a model of the world based on those images. If a system is trained on photos of people who are overwhelmingly white, it will have a harder time recognizing nonwhite faces.\nA very serious example was revealed in an investigation published last monthby ProPublica. It found that widely used software that assessed the risk of recidivism in criminals was twice as likely to mistakenly flag black defendants as being at a higher risk of committing future crimes. It was also twice as likely to incorrectly flag white defendants as low risk.\nThe reason those predictions are so skewed is still unknown, because the company responsible for these algorithms keeps its formulas secret - it's proprietary information. Judges do rely on machine-driven risk assessments in different ways - some may even discount them entirely - but there is little they can do to understand the logic behind them.\nPolice departments across the United States are also deploying data-driven risk-assessment tools in \"predictive policing\" crime prevention efforts. In many cities, including New York, Los Angeles, Chicago and Miami, software analyses of large sets of historical crime data are used to forecastwhere crime hot spots are most likely to emerge; the police are then directed to those areas.\nAt the very least, this software risks perpetuating an already vicious cycle, in which the police increase their presence in the same places they are already policing (or overpolicing), thus ensuring that more arrests come from those areas. In the United States, this could result in more surveillance in traditionally poorer, nonwhite neighborhoods, while wealthy, whiter neighborhoods are scrutinized even less. Predictive programs are only as good as the data they are trained on, and that data has a complex history.\nHistories of discrimination can live on in digital platforms, and if they go unquestioned, they become part of the logic of everyday algorithmic systems. Another scandal emerged recently when it was revealed that Amazon's same-day delivery service was unavailable for ZIP codes in predominantly black neighborhoods. The areas overlooked were remarkably similar to those affected by mortgage redlining in the mid-20th century. Amazon promised to redress the gaps, but it reminds us how systemic inequality can haunt machine intelligence.\nAnd then there's gender discrimination. Last July, computer scientists at Carnegie Mellon University found that women were less likely than men to be shown ads on Google for highly paid jobs. The complexity of how search engines show ads to internet users makes it hard to say why this happened - whether the advertisers preferred showing the ads to men, or the outcome was an unintended consequence of the algorithms involved.\nRegardless, algorithmic flaws aren't easily discoverable: How would a woman know to apply for a job she never saw advertised? How might a black community learn that it were being overpoliced by software?\nWe need to be vigilant about how we design and train these machine-learning systems, or we will see ingrained forms of bias built into the artificial intelligence of the future.\nLike all technologies before it, artificial intelligence will reflect the values of its creators. So inclusivity matters - from who designs it to who sits on the company boards and which ethical perspectives are included. Otherwise, we risk constructing machine intelligence that mirrors a narrow and privileged vision of society, with its old, familiar biases and stereotypes.\nIf we look at how systems can be discriminatory now, we will be much better placed to design fairer artificial intelligence. But that requires far more accountability from the tech community. Governments and public institutions can do their part as well: As they invest in predictive technologies, they need to commit to fairness and due process.\nWhile machine-learning technology can offer unexpected insights and new forms of convenience, we must address the current implications for communities that have less power, for those who aren't dominant in elite Silicon Valley circles.\nCurrently the loudest voices debating the potential dangers of superintelligence are affluent white men, and, perhaps for them, the biggest threat is the rise of an artificially intelligent apex predator.\nBut for those who already face marginalization or bias, the threats are here.\nFollow The New York Times Opinion section on Facebook and Twitter (@NYTOpinion), and sign up for the Opinion Today newsletter. \nKate Crawford is a principal researcher at Microsoft and co-chairwoman of a White House symposium on society and A.I.\n","207":"IBM is adding medical images to the health data its Watson artificial-intelligence can mine to help doctors make diagnoses. \nThe big technology company announced on Thursday morning that it was buying Merge Healthcare, a medical-imaging software company, for $1 billion. When IBM set up its Watson health business in April, it began with a couple of smaller medical data acquisitions and industry partnerships with Apple, Johnson & Johnson and Medtronic. Last week, IBM announced a partnership with CVS Health, the large pharmacy chain, to develop data-driven services to help people with chronic ailments like diabetes and heart disease better manage their health.\nBut the purchase of Merge Healthcare is both a sizable investment and a new resource for IBM's new Watson health unit. \"We're bringing Watson and analytics to the largest data set in health care - images,\" John Kelly, IBM's senior vice president of research who oversees the Watson business, said in an interview.\u00a0\nImages like CAT scans, X-rays and mammograms, IBM researchers estimate, represent about 90 percent of all medical data today. The images and a patient's electronic health records are typically separate. So, for example, a radiologist might examine thousands of patient images a day, but only looking for abnormalities on the images themselves rather than also taking into account a person's medical history, treatments and drug regimens.\n\"Watson will be able to understand both,\" Mr. Kelly said.\nThe Watson artificial-intelligence technology has mainly been applied to analyzing text in documents and on the web. It employed this natural-language processing capability initially in its high-profile demonstration project - beating human champions in the question-and-answer game, Jeopardy! \nBut for the past two years, Mr. Kelly said, IBM researchers at its labs in Yorktown Heights, N.Y., and in San Jose, Calif., have been training Watson's AI engine on image recognition. \"We've been giving Watson eyes, so to speak,\" he said.\nMerge Healthcare, based in Chicago, specializes in software for storing, viewing and sharing medical images. Its technology is used by a wide array of health care providers and imaging-equipment makers, and its rights to use the archived images varies according to customer requirements, and state and federal health-privacy rules.\nMerge Healthcare is the third medical data company IBM has acquired since setting up the Watson health business. In April, it agreed to buy two start-ups: Explorys, a spin-off from the Cleveland Clinic whose data on 50 million patients is used to spot patterns in diseases, treatments and outcomes; and Phytel, a Dallas maker of software to manage patient care and reduce readmission rates to hospitals. The financial details of those smaller deals, both with private companies, were not disclosed.\nThe Watson technology, sold as a cloud service, has been used in applications for IBM customers to help them spot patterns from the data gathered in their businesses. But health care is the first field where IBM is building a comprehensive offering for an entire industry.\nIn the past, automated decision-support systems in medicine have often been greeted with initial optimism yet have proved to be disappointingly limited in practice. But IBM is betting that its Watson technology can deliver a genuine breakthrough in the next few years. \nThe company is investing not only money but also some of its corporate reputation on the belief that it can be a technological leader in improving health care, with better outcomes for patients and more efficient spending for providers, insurers and patients.\nIn an interview on the \"Charlie Rose\" program on PBS in April, Virginia M. Rometty, IBM's chief executive, spoke of the company's role over the years in supplying technology for big projects, from computerizing census statistics to putting astronauts on the moon.\n\"Our moonshot,\" Ms. Rometty said, \"will be the impact we will have on health care.\"\n","208":"Perhaps it's not surprising that entrepreneur\/futurist Elon Musk and Google CEO Larry Page are fast friends.\nThe two men, as Mashable notes, are similar in age, demeanor and otherworldly ambition.\nThe nerd bros have tons of overlapping interests, so much so that they enjoy getting together with\u00a0Google co-founder Sergey Brin at a \"secret apartment\" owned by his company to bat around reality-altering ideas, according to \"Elon Musk:\u00a0Tesla, SpaceX, and the Quest for a Fantastic Future,\" a new authorized biography written by Ashlee Vance.\nFor example: The tech wizards have discussed building a commuter plane that continually circles the globe for fast transport.\u00a0\nMusk, as Page tells the biographer, even crashes at the executive's home when he's in San Francisco.\n              [The 22 most memorable quotes from the new Elon Musk book, ranked]           \n\"He's kind of homeless, which I think is sort of funny,\" Page is quoted as saying. \"He'll e-mail and say, 'I don't know where to stay tonight. Can I come over?' I haven't given him a key or anything yet.\"\nThere's just one nagging problem between the two men. Nothing serious, though, just a little tiff between old buds about the future of the universe, really.\nMusk thinks Page may be \"building a fleet of artificial-intelligence-enhanced robots capable of destroying mankind,\" according to the book.\nThen again, who doesn't have a friend with apocalyptic tendencies, right?\nBesides, it's not like Page is doing it on purpose. Musk thinks his buddy is \"fundamentally a well-intentioned person and not Dr. Evil,\" the book notes. But it's Page's \"nice-guy nature\" that has his friend sweating bullets as he lays in bed at night.\n\"I'm really worried about this,\" Musk is quoted as saying.\n\"He could produce something evil by accident,\" he adds.\nAdmittedly, the whole thing is a bit awkward. These guys aren't just friends -\u00a0they're friends with some serious benefits.\n\"Google,\" Vance writes, \"has invested more than just about any other technology company into Musk's sort of moon-shot projects: self-driving cars, robots, and even a cash prize to get a machine onto the moon cheaply.\"\nAnd Page, according to an \"Elon Musk\" excerpt published by\u00a0Bloomberg, nearly bailed Tesla out in 2013 when the company was veering towards bankruptcy. When sales suddenly improved, Musk eventually walked away from the $11 billion deal for Google to acquire the electric car company, which would've been the second-largest in Google history, according to Mashable.\n               [Elon Musk: Human-driven cars may be outlawed because they're 'too dangerous']            \n\"Google has acquired\u00a0more than half a dozen robotics companies to date,\" Mashable points out, \"but the company's ultimate goal for robots is unclear.\"\nThough his friend worries that he may be bringing about the end of mankind, Page told Vance the feeling is not mutual. He is quoted as saying he finds Musk \"inspiring\"\u00a0because of his willingness to invest time and money in companies that aim to solve some of humanity's greatest challenges.\nMusk, meanwhile, has been warning about the dangers of artificial intelligence for months.\nWorth reading Superintelligence by Bostrom. We need to be super careful with AI. Potentially more dangerous than nukes.\nIn October,\u00a0at the\u00a0MIT Aeronautics and Astronautics department's Centennial Symposium, Musk amped up the alarm, saying: \"I think we should be very careful about artificial intelligence. If I were\u00a0to guess like what our biggest existential threat\u00a0is, it's probably\u00a0that. ... Increasingly scientists think there should be some regulatory oversight, maybe at the national and international level, just to make sure that we don't do something very foolish.\"\nHe added: \"With artificial intelligence we are summoning the demon.\"\nAs far as AI-fearing technologists go, Musk is far from alone. In recent months,\u00a0Bill Gates, Stephen Hawking and Steve Wozniak have all sounded the alarm on the dangers of AI.\nSee here:\u00a0Bill Gates on dangers of artificial intelligence: 'I don't understand why some people are not concerned'          \nAnd here:\u00a0Stephen Hawking just got an artificial intelligence upgrade, but still thinks AI could bring an end to mankind          \nAnd here:\u00a0Apple co-founder on artificial intelligence: 'The future is scary and very bad for people'          \n\"I am not alone in thinking we should be worried,\" Musk wrote during an online discussion in November, according to Mashable. \"The leading AI companies have taken great steps to ensure safety. They recognize the danger, but believe that they can shape and control the digital superintelligences and prevent bad ones from escaping into the Internet. That remains to be seen.\"\n","209":"YORKTOWN HEIGHTS, N.Y.\nResearchers at IBM are preparing a supercomputer named Watson to compete on the popular quiz show \"Jeopardy!\" this month.\nWatson has already won a practice round on the show against two top contestants, showing that artificial intelligence has come a long way in simulating how humans think.\u00a0\n \"We have created a computer system which has the ability to understand natural human language, which is a very difficult thing for a computer to do,\" said John Kelly,  director of IBM Research.\n \"In the field of artificial intelligence, people spend their lifetimes trying to advance that science inches. What Watson does and has demonstrated is the ability to advance the field of artificial intelligence by miles.\"\nWatson, who is named after legendary International Business Machines President Thomas Watson, is a showcase of the company's computing expertise and research in advanced science.\nIt also shows that IBM - which turns 100 this year - wants to stay at the forefront of technology, even as companies such as Google and Apple have become the industry's  leaders.\nIBM says Watson's ability to understand language makes it far more evolved than Deep Blue, the company's supercomputer  that won against world chess champion Garry Kasparov in 1997.\nThe biggest challenge for IBM scientists was teaching Watson to differentiate between literal and metaphorical expressions and understanding puns and slang.\nFeeding it knowledge was easier. Watson is not plugged into the Internet but has a database covering topics from history to  entertainment.\nAt a recent practice, held at IBM's Eero Saarinen-designed research facility in the New York City suburb of Yorktown Heights, Watson showed off its familiarity with musical film.\n \"The film 'Gigi' gave him his signature song, 'Thank Heaven for Little Girls,' \" host Alex Trebek said.\n \"Who is Maurice Chevalier?\" Watson replied in the game's question-response format.\nThe machine, which combines IBM's refrigerator-size Power7 computers, was too big for  the set so was on the ground floor.\nWatson triumphed in the first practice round, earning $4,400, while Ken Jennings, who won 74 games in a row during the 2004-05 season, trailed with $3,400. Brad Rutter, who has earned a cumulative $3.3 million on the show, came in last.\nA win in actual competition, which airs Feb. 14-16, would be a triumph for IBM, which spends $6 billion per year in research and development. An unspecified portion goes to what IBM calls \"grand challenges,\" or  multi-year science projects such as Watson and Deep Blue.\n IBM executives said Watson's linguistic and analytical capabilities may eventually help  develop products in areas such as medical diagnosis.\nJennings, however, said he thought Watson could be beaten. \nFellow contestant Rutter agreed, citing Watson's weakness at grasping humor - a key part of some of the show's categories.\nReacting to a statement about actor-musician Jamie Foxx, who plays  the cello, Watson's response was  baffling: \"Who is Beethoven?\"\n \"I get the two mixed up all the time,\" joked Rutter to guffaws.\nWatson did not laugh but went on to win the practice round.\n- Reuters\n","210":"                               technology          Is AI terrible for the world? Or good?             'Don't Trust the Promise of Artificial Intelligence,' IQ\u00b2                              \nThe ever-fascinating Martine Rothblatt is a colorful figure on the Washington scene - lawyer, author, founder of Sirius XM, founder and chief executive of United Therapeutics, co-creator of a head-only robot modeled on her wife of 30-plus years (they were married before Rothblatt's sex reassignment surgery), pilot, piano player . . . .\u00a0\nRothblatt, whose most recent book is \"Virtually Human: The Promise - and the Peril - of Digital Immortality,\" will take the stage at New York's 92nd Street Y on behalf of one of her passions: artificial intelligence.\nShe will be one of four experts on two teams debating whether pursuit of superintelligence and autonomous machines may result in dangerous unintended consequences, or whether fears of that outcome will prevent technological progress. It's the latest in the series of fast-paced, provocative debates put on by the public affairs program IQ\u00b2, or Intelligence Squared.\nThe motion on the table will be: \"Don't trust the promise of artificial intelligence.\" As usual at IQ\u00b2 events, the audience will vote on whether they agree with that statement before the debate; afterward, they vote again, and the team that has changed the most minds is declared the winner.\nArguing for the motion will be Jaron Lanier, a computer scientist and author of \"You Are Not a Gadget,\" and Andrew Keen, an Internet entrepreneur, CNN columnist and author of \"The Internet Is Not the Answer.\" Arguing against it are Rothblatt and James Hughes, director of the Institute for Ethics and Emerging Technologies.\nThe debate begins at 7 p.m. Wednesday, March 9; you can watch it live-streamed at intelligencesquaredus.org.\n- Nancy Szokan          \n","212":"The computers in modern data centers -- the engine rooms of the digital economy -- are powered mainly by Intel chips. They animate the computing clouds of the internet giants and corporate data centers worldwide.\nBut Intel is now facing new competitive forces that could pose a challenge to its data-center dominance and profitability. \n  In particular, the rise of artificial intelligence is creating demand for new computing hardware tailored to handle vast amounts of unruly data and complex machine-learning software -- and Intel's general-purpose chips are not yet tuned for the most demanding tasks. Instead, specialized chips are delivering better performance on artificial intelligence programs that identify images, recognize speech and translate languages.\u00a0\n  Intel is hurrying to catch the A.I. wave. On Tuesday, to deal with the changing competitive landscape, the Silicon Valley giant is presenting its newest data-center strategy at an event in New York, addressing its A.I. plans and its mainstream data-center business. The company has billed the event as its ''biggest data-center launch in a decade.''\n  How successful Intel's efforts prove to be will be crucial not only for the company but also for the long-term future of the computer chip industry.\n  ''We're seeing a lot more competition in the data-center market than we've seen in a long time,'' said Linley Gwennap, a semiconductor expert who leads a technology research firm in Mountain View, Calif.\n  Intel has long dominated the business for central processing chips that control industry-standard servers in data centers. Matthew Eastwood, an analyst at IDC, said the company controlled about 96 percent of such chips.\n  But others are making inroads into advanced data centers. Nvidia, a chip maker in Santa Clara, Calif., does not make Intel-style central processors. But its graphics-processing chips, used by gamers in turbocharged personal computers, have proved well suited for A.I. tasks. Nvidia's data-center business is taking off, with the company's sales surging and its stock price nearly tripling in the last year.\n  Big Intel customers like Google, Microsoft and Amazon are also working on chip designs. AMD and ARM, which make central processing chips like Intel, are edging into the data-center market, too. IBM made its Power chip technology open source a few years ago, and Google and others are designing prototypes.\n  To counter some of these trends, Intel is expected on Tuesday to provide details about the performance and uses of its new chips and its plans for the future. The company is set to formally introduce the next generation of its Xeon data-center microprocessors, code-named Skylake. And there will be a range of Xeon offerings with different numbers of processing cores, speeds, amounts of attached memory, and prices.\n  Yet analysts said that would represent progress along Intel's current path rather than an embrace of new models of computing.\n  Stacy Rasgon, a semiconductor analyst at Bernstein Research, said, ''They're late to artificial intelligence.''\n  Intel disputes that characterization, saying that artificial intelligence is an emerging technology in which the company is making major investments. In a blog post last fall, Brian Krzanich, Intel's chief executive, wrote that it was ''uniquely capable of enabling and accelerating the promise of A.I.''\n  Intel has been working in several ways to respond to the competition in data-center chips. The company acquired Nervana Systems, an artificial intelligence start-up, for more than $400 million last year. In March, Intel created an A.I. group, headed by Naveen G. Rao, a founder and former chief executive of Nervana.\n  The Nervana technology, Intel has said, is being folded into its product road map. A chip code-named Lake Crest is being tested and will be available to some customers this year.\n  Lake Crest is tailored for A.I. programs called neural networks, which learn specific tasks by analyzing huge amounts of data. Feed millions of cat photos into a neural network and it can learn to recognize a cat -- and later pick out cats by color and breed. The principle is the same for speech recognition and language translation.\n  Intel has also said it is working to integrate Nervana technology into a future Xeon processor, code-named Knight's Crest.\n  Intel's challenge, analysts said, is a classic one of adapting an extraordinarily successful business to a fundamental shift in the marketplace.\n  As the dominant data-center chip maker, used by a wide array of customers with different needs, Intel has loaded more capabilities into its central processors. It has been an immensely profitable strategy: Intel had net income of $10.3 billion last year on revenue of $59.4 billion.\n  Yet key customers increasingly want computing designs that parcel out work to a collection of specialized chips rather than have that work flow through the central processor. A central processor can be thought of as part brain, doing the logic processing, and part traffic cop, orchestrating the flow of data through the computer.\n  The outlying, specialized chips are known in the industry as accelerators. They can do certain things, like data-driven A.I. tasks, faster than a central processor. Accelerators include graphics processors, application-specific integrated circuits (ASICs) and field-programmable gate arrays (F.P.G.A.s).\n  A more diverse set of chips does not mean the need for Intel's central processor disappears. The processor just does less of the work, becoming more of a traffic cop and less of a brain. If this happens, Intel's business becomes less profitable.\n  Intel is not standing still. In 2015, it paid $16.7 billion for Altera, a maker of field-programmable gate arrays, which make chips more flexible because they can be repeatedly reprogrammed with software.\n  Mr. Gwennap, the independent analyst, said, ''Intel has a very good read on data centers and what those customers want.''\n  Still, the question remains whether knowing what the customers want translates into giving them what they want, if that path presents a threat to Intel's business model and profit margins.\n  Follow Steve Lohr on Twitter @SteveLohr.\n\n\n\n","213":"Ever since the early days of modern computing in the 1940s, the biological metaphor has been irresistible. The first computers -- room-size behemoths -- were referred to as ''giant brains'' or ''electronic brains,'' in headlines and everyday speech. As computers improved and became capable of some tasks familiar to humans, like playing chess, the term used was ''artificial intelligence.'' DNA, it is said, is the original software.\nFor the most part, the biological metaphor has long been just that -- a simplifying analogy rather than a blueprint for how to do computing. Engineering, not biology, guided the pursuit of artificial intelligence. As Frederick Jelinek, a pioneer in speech recognition, put it, ''airplanes don't flap their wings.''\nYet the principles of biology are gaining ground as a tool in computing. The shift in thinking results from advances in neuroscience and computer science, and from the prod of necessity.\u00a0\nThe physical limits of conventional computer designs are within sight -- not today or tomorrow, but soon enough. Nanoscale circuits cannot shrink much further. Today's chips are power hogs, running hot, which curbs how much of a chip's circuitry can be used. These limits loom as demand is accelerating for computing capacity to make sense of a surge of new digital data from sensors, online commerce, social networks, video streams and corporate and government databases.\nTo meet the challenge, without gobbling the world's energy supply, a different approach will be needed. And biology, scientists say, promises to contribute more than metaphors. ''Every time we look at this, biology provides a clue as to how we should pursue the frontiers of computing,'' said John E. Kelly, the director of research at I.B.M.\nDr. Kelly points to Watson, the question-answering computer that can play ''Jeopardy!'' and beat two human champions earlier this year. I.B.M.'s clever machine consumes 85,000 watts of electricity, while the human brain runs on just 20 watts. ''Evolution figured this out,'' Dr. Kelly said.\nSeveral biologically inspired paths are being explored by computer scientists in universities and corporate laboratories worldwide. But researchers from I.B.M. and four universities -- Cornell, Columbia, the University of Wisconsin, and the University of California, Merced -- are engaged in a project that seems particularly intriguing.\nThe project, a collaboration of computer scientists and neuroscientists begun three years ago, has been encouraging enough that in August it won a $21 million round of government financing from the Defense Advanced Research Projects Agency, bringing the total to $41 million in three rounds. In recent months, the team has developed prototype ''neurosynaptic'' microprocessors, or chips that operate more like neurons and synapses than like conventional semiconductors.\nBut since 2008, the project itself has evolved, becoming more focused, if not scaled back. Its experience suggests what designs, concepts and techniques might be usefully borrowed from biology to push the boundaries of computing, and what cannot be applied, or even understood.\nAt the outset, Dharmendra S. Modha, the I.B.M. computer scientist leading the project, described the research grandly as ''the quest to engineer the mind by reverse-engineering the brain.'' The project embarked on supercomputer simulations intended to equal the complexity of animal brains -- a cat and then a monkey. In science blogs and online forums, some neuroscientists sharply criticized I.B.M. for what they regarded as exaggerated claims of what the project could achieve.\nThese days at the I.B.M. Almaden Research Center in San Jose, Calif., there is not a lot of talk of reverse-engineering the brain. Wide-ranging ambitions that narrow over time, Dr. Modha explained, are part of research and discovery, even if his earlier rhetoric was inflated or misunderstood.\n''Deciding what not to do is just as important as deciding what to do,'' Dr. Modha said. ''We're not trying to replicate the brain. That's impossible. We don't know how the brain works, really.''\nThe discussion and debate across disciplines has helped steer the research, as the team pursues the goals set out by Darpa, the Pentagon's research agency. The technology produced, according to the guidelines, should have the characteristics of being self-organizing, able to ''learn'' instead of merely responding to conventional programming commands, and consuming very little power.\n''We have this fantastic network of specialists who talk to each other,'' said Giulio Tononi, a psychiatrist and neuroscientist at the University of Wisconsin. ''It focuses our thinking as neuroscientists and guides the thinking of the computer scientists.''\nIn early 2010, Dr. Modha made a decision that put the project on its current path. While away from the lab for a few weeks, because of a Hawaiian vacation and a bout of flu, he decided to streamline the work of the far-flung researchers. The biologically inspired chip under development would come first, Dr. Modha said. That meant a lot of experimental software already written was scrapped. But, he said, ''chip-first as an organizing principle gave us a coherent plan.''\nIn designing chips that bear some structural resemblance to the brain, so-called neuromorphic chips, neuroscience was a guiding principle as well. Brains are low-power, nimble computing mechanisms -- real-world proof that it is possible.\nA brain does its computing with a design drastically different from today's computers. Its processors -- neurons -- are, in computing terms, massively distributed; there are billions in a human brain. These neuron processors are wrapped in its data memory devices -- synapses -- so that the brain's paths of communication are extremely efficient and diverse, through the neuron's axons, which conduct electrical impulses.\nA machine that adopts that approach, Dr. Modha said, would represent ''a crucial shift away from von Neumann computing.'' He was referring to a design with processor and memory physically separated and connected by a narrow communications channel, or bus, and operating according to step-by-step sequential methods -- the von Neumann architecture used in current computers, named after the mathematician John von Neumann.\nThe concept of neuromorphic electronic systems is more than two decades old; Carver Mead, a renowned computer scientist, described such devices in an engineering journal article in 1990. Earlier biologically inspired devices, scientists say, were mostly analog, single-purpose sensors that mimicked one function, like an electronic equivalent of a retina for sensing image data.\nBut the I.B.M. and university researchers are pursuing a more versatile digital technology. ''It seems that we can build a computing architecture that is quite general-purpose and could be used for a large class of applications,'' said Rajit Manohar, a professor of electrical and computer engineering at Cornell University.\nWhat might such applications be, 5 or 10 years from now, if the technology proves successful? They would be the sorts of tasks that humans find effortless and that computers struggle with -- the pattern recognition of seeing and identifying someone, walking down a crowded sidewalk without running into people, learning from experience. Specifically, the scientists say, the applications might include robots that can navigate a battlefield environment and be trained; low-power prosthetic devices that would allow blind people to see; and computerized health-care monitors that watch over people in nursing homes and send alerts to human workers if a resident's behavior suggests illness.\nIt is an appealing vision, but there are formidable obstacles. The prototype chip has 256 neuron-like nodes, surrounded by more than 262,000 synaptic memory modules. That is impressive, until one considers that the human brain is estimated to house up to 100 billion neurons. In the Almaden research lab, a computer running the chip has learned to play the primitive video game Pong, correctly moving an on-screen paddle to hit a bouncing cursor. It can also recognize numbers 1 through 10 written by a person on a digital pad -- most of the time. But the project still has a long way to go.\nIt is still questionable whether the scientists can successfully assemble large clusters of neuromorphic chips. And though the intention is for the machines to evolve more from learning than from being programmed, the software that performs that magic for any kind of complex task has yet to be written.\nThe project's Pentagon sponsor is encouraged. ''I'm surprised that we're so far along, and I don't see any fundamental reason why it can't be done,'' said Todd Hylton, a program manager.\nIf it succeeds, the project would seem to make peace with the ''airplanes don't flap their wings'' critique. ''Yes, they are different, but bird wings and plane wings both depend on the same aerodynamic principles to get lift,'' said Christopher T. Kello, director of the Cognitive Mechanics Lab at the University of California, Merced. ''It's the same with this project. You can use essential design elements from biology.''\n","215":"Robert Tannenbaum, 87, a retired professor at the University of California, Los Angeles's Anderson Graduate School of Management who broke ground in the study of leadership in business, died of congestive heart failure March 15 at his home in Carmel, Calif. \n With co-author Warren H. Schmidt, Dr. Tannenbaum came into prominence in the management field in 1973 with the publication in the Harvard Business Review of \"How to Choose a Leadership Pattern.\" The article described a leadership continuum ranging from an autocratic manager -- \"the leader makes the decision and announces it to the group\" -- to a more democratic process in which employees are deeply involved in decision-making. \u00a0\n The article was considered a significant new way to view leadership in organizations. \nRobert Engelmore, 68, a Stanford University computer scientist and an authority on artificial intelligence, died March 25 in Princeville, Hawaii, after an apparent heart attack. \n Dr. Engelmore, who lived in Menlo Park, Calif., was helping to rescue his 5-year-old grandson while swimming in Hawaii during a family vacation when he apparently suffered the seizure. \n Dr. Engelmore had served as executive director of the Heuristic Programming Project at Stanford's Computer Science Department. He also had been an editor of AI (Artificial Intelligence) magazine, the author of more than 25 scientific articles and an authority on medical and military applications of artificial intelligence. \nB. Vincent Davis, 72, retired director of the Patterson School of Diplomacy and International Commerce at the University of Kentucky, died March 28 at a hospital in Lexington, Ky., after a stroke. He had Parkinson's disease and arthritis. \nDr. Davis, who headed the graduate school of foreign affairs from 1971 to 1993, was a former special adviser to the then-director of central intelligence, Navy Adm. Stansfield Turner, from 1977 to 1981. \nDr. Davis was the author or co-author of more than a dozen books, including \"The Admiral's Lobby\" and \"Reorganizing America's Defense,\" and had served on the State Department's historical declassification committee. \nRoland De Marco, 92, who served as president of Finch College from 1951 to 1970, died March 21 in Rochester, N.Y. The cause of death was not reported. \nHe joined what was Finch Junior College, a women's liberal arts school on Manhattan's Upper East Side, as dean. In 1949, he became the administrative head when its founder and first president, Jessica Garretson Cosgrave, died. \n As president, Dr. De Marco oversaw an expansion that transformed Finch into a four-year institution with its own art museum. An economic slump in the early 1970s forced the college to close in 1975. \nHarry Ellis Dickson, 94, a conductor who was a decades-long fixture at the Boston Symphony Orchestra and who was the father of Kitty Dukakis, the wife of 1988 Democratic Party presidential nominee Michael Dukakis, died March 29 in Boston. The cause of death was not reported. \n Mr. Dickson, a violinist, began his career with the orchestra in 1938 and performed with it for 49 seasons. A close friend of Boston Pops founder Arthur Fiedler, he was named assistant conductor of the Boston Pops in 1958 and founded the Boston Symphony's Youth Concert series in 1959. \n Mr. Dickson became associate Pops conductor in 1980 and was appointed the Boston Classic Orchestra music director in 1983. He was named the symphony's music director laureate in 1999. \nBoleslaw Wierzbianski, 89, a founder, publisher and former editor in chief of the Polish-language newspaper Nowy Dziennik, died of pneumonia March 26 at a hospital in New York. \n During the 1950s, he was a contributor to Radio Free Europe broadcasts. He served as a member of the New York City Commission on Human Rights under Mayor Edward Koch. In 1971, along with a group of friends, he founded Nowy Dziennik and became its publisher. Until 1997, he also served as its editor in chief. \n Nowy Dziennik now has a circulation of 25,000 readers throughout the United States, Canada, Israel, Poland and other European nations. \n","216":"You know, I actually used to be so worried about not having a body, but now I truly love it ... I'm not tethered to time and space in the way that I would be if I was stuck inside a body that's inevitably going to die. - Samantha, in \"Her\"\nSet in the not-too-distant future, Spike Jonze's film \"Her\" explores the romantic relationship between Samantha, a computer program, and Theodore Twombly, a human being. Though Samantha is not human, she feels the pangs of heartbreak, intermittently longs for a body and is bewildered by her own evolution. She has a rich inner life, complete with experiences and sensations.\n\"Her\" raises two questions that have long preoccupied philosophers. Are nonbiological creatures like Samantha capable of consciousness - at least in theory, if not yet in practice? And if so, does that mean that we humans might one day be able to upload our own minds to computers, perhaps to join Samantha in being untethered from \"a body that's inevitably going to die\"?\u00a0\nThis is not mere speculation. The Future of Humanity Institute at Oxford University has released a report on the technological requirements for uploading a mind to a machine. A Defense Department agency has funded a program, Synapse, that is trying to develop a computer that resembles a brain in form and function. The futurist Ray Kurzweil, now a director of engineering at Google, has even discussed the potential advantages of forming friendships, \"Her\"-style, with personalized artificial intelligence systems. He and others contend that we are fast approaching the \"technological singularity,\" a point at which artificial intelligence, or A.I., surpasses human intelligence, with unpredictable consequences for civilization and human nature.\nIs all of this really possible? Not everyone thinks so. Some people argue that the capacity to be conscious is unique to biological organisms, so that even superintelligent A.I. programs would be devoid of conscious experience. If this view is correct, then a relationship between a human being and a program like Samantha, however intelligent she might be, would be hopelessly one-sided. Moreover, few humans would want to join Samantha, for to upload your brain to a computer would be to forfeit your consciousness.\nThis view, however, has been steadily losing ground. Its opponents point out that our best empirical theory of the brain holds that it is an information-processing system and that all mental functions are computations. If this is right, then creatures like Samantha can be conscious, for they have the same kind of minds as ours: computational ones. Just as a phone call and a smoke signal can convey the same information, thought can have both silicon- and carbon-based substrates. Indeed, scientists have produced silicon-based artificial neurons that can exchange information with real neurons. The neural code increasingly seems to be a computational one.\nYou might worry that we could never be certain that programs like Samantha were conscious. This concern is akin to the longstanding philosophical conundrum known as the \"problem of other minds.\" The problem is that although you can know that you yourself are conscious, you cannot know for sure that other people are. You might, after all, be witnessing behavior with no accompanying conscious component.\nIn the face of the problem of other minds, all you can do is note that other people have brains that are structurally similar to your own and conclude that since you yourself are conscious, others are likely to be conscious as well. When confronted with a high-level A.I. program like Samantha, your predicament wouldn't be all that different, especially if that program had been engineered to work like the human brain. While we couldn't be certain that an A.I. program genuinely felt anything, we can't be certain that other humans do, either. But it would seem probable in both cases.\nIf the Samanthas of the future will have inner lives like ours, however, I suspect that we will not be able to upload ourselves to computers to join them in the digital universe. To see why, imagine that Theodore wants to upload himself. Imagine, furthermore, that uploading involves (a) scanning a human brain in such exacting detail that it destroys the original and (b) creating a software model that thinks and behaves in precisely the same way as the original did. If Theodore were to undergo this procedure, would he succeed in transferring himself into the digital realm? Or would he, as I suspect, succeed only in killing himself, leaving behind a computational copy of his mind - one that, adding insult to injury, would date his girlfriend?\nOrdinary physical objects follow a continuous path through space over time. For Theodore to transfer his mind into a computer program, however, his mind would not follow a continuous trajectory. His brain would be destroyed when the scan was made, and the information about his precise brain configuration would be sent to a computer, which could be miles away.\nFurthermore, if Theodore were to truly upload his mind (as opposed to merely copy its contents), then he could be downloaded to multiple other computers. Suppose that there are five such downloads: Which one is the real Theodore? It is hard to provide a nonarbitrary answer. Could all of the downloads be Theodore? This seems bizarre: As a rule, physical objects and living things do not occupy multiple locations at once. It is far more likely that none of the downloads are Theodore, and that he did not upload in the first place.\nWorse yet, imagine that the scanning procedure doesn't destroy Theodore's brain, so the original Theodore survives. If he survives the scan, why conclude that his consciousness has transferred to the computer? It should still reside in his brain. But if you believe that his mind doesn't transfer if his brain isn't destroyed, then why believe that his mind does transfer if his brain is destroyed?\nIt is here that we press up against the boundaries of the digital universe. It seems there is a categorical divide between humans and programs: Humans cannot upload themselves to the digital universe; they can upload only copies of themselves - copies that may themselves be conscious beings.\nDoes this mean that uploading projects should be scrapped? I don't think so, for uploading technology can benefit our species. A global catastrophe may make the world inhospitable to biological life forms, and uploading may be the only way to preserve the human way of life and thinking, if not actual humans themselves. And uploading could facilitate the development of brain therapies and enhancements that can benefit humans and nonhuman animals. Furthermore, uploading may give rise to a form of superintelligent A.I. A.I. that is descended from us may have greater chance of being benevolent toward us.\nFinally, some humans will understandably want digital backups of themselves. What if you found out that you were going to die soon? A desire to contribute a backup copy of yourself might outweigh your desire to spend a few more days on the planet. Or you might wish to leave a copy of yourself to communicate with your children or complete projects that you care about. Indeed, the Samanthas of the future might be uploaded copies of deceased humans we have loved deeply. Or perhaps our best friends will be copies of ourselves, but tweaked in ways we find insightful.\n Susan Schneider, an associate professor of philosophy at the University of Connecticut, is the author of \"The Language of Thought\" and \"Science Fiction and Philosophy.\" \n\n","217":"THE EMOTION MACHINE\n Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind \n By Marvin Minsky\n Simon & Schuster. 387 pp. $26\n Writers about the human mind generally fall into three camps: philosophers, psychologists and others who weave elaborate theories about the mind without any reference to the brain; neuroscientists who attempt to link mind matters with brain states; and, finally, members of the computer science and artificial intelligence (AI) communities who suggest that it's possible to replicate human thinking in a machine. Marvin Minsky, professor of electrical engineering and computer science at the Massachusetts Institute of Technology and an early pioneer in developing artificial intelligence, is an eminent denizen of the third camp.\n In The Emotion Machine, Minsky aims to find \"more complex ways to depict mental events that seem simple at first.\"  He brilliantly achieves this goal when he suggests that consciousness remains unexplained because it is \"one of those suitcase-like words that we use for many types of processes, and for different kinds of purposes.\"  Since consciousness is not a unity but involves separate mental components, \"there is little to gain from wondering what consciousness 'is'  --  because that word includes too much for us to deal with all at once.\" \u00a0\n Minsky does a marvelous job parsing other complicated mental activities into simpler elements. He discusses such topics as common sense, thinking and the self and  --  most important for this book  --  emotional states, which are \"not especially different from the processes that we call 'thinking.' \" \n But he is less effective in relating these emotional functions to what's going on in the brain. Minsky says his book \"does not discuss most current beliefs about how our brains work\"  because our knowledge about the brain soon becomes outdated.  But then how can one draw meaningful correlations between brains and machines?\n  Equally unsettling, several of his points about the brain are not in line with current knowledge. For instance, it's not true, as Minsky claims, that \"after certain major stages of growth in the brain, many new cells are later destroyed by 'post-editing' processes that evolved to delete some types of connections.\"  Actually, the loss of cells results from passive disuse  --  use it or lose it  --  rather than active deletion.\n Some of his other statements may be correct, but I wonder how one would go about proving them: \"I suspect that large parts of our brains work mainly to correct mistakes that other parts make  --  and this is surely one reason why the subject of human psychology has become so hard.\"  This quirky and provocative assertion is based on the fact that \"many computer systems eventually become so ponderous that their further development stops, because their programmers can no longer keep track of what all the previous programmers did.\" \n This example, along with others throughout the book, assumes that computers and brains operate on similar principles. But testing that assumption, according to Minsky, isn't likely to be successful any time soon: \"We learn more such details about the brain every week  --  but we still do not yet know enough to simulate even a spider or snake.\"  Given the limited state of our current knowledge, is it unreasonable to question the appropriateness of a machine model for human emotion? \n Minsky proposed many of his ideas linking neuroscience with AI in his 1986 book, The Society of Mind. But in The Emotion Machine, he does not always account for more recent advances in our understanding of neurons (nerve cells). Of the 1.1 trillion cells in the human brain, only 100 billion are neurons, leaving an enormous number of cells that, neuroscientists are convinced, must be important in information transfer. Moreover, anatomical interaction of neurons highlights only one aspect of brain functioning. Equally important are alterations of the brain's chemical messengers, the neurotransmitters, along with changes in local and distributed electrical fields. A successful AI model of the mind must consider these features, as well. \n Finally, applying to the brain such vague, ill-defined terms as \"resources\" doesn't adequately capture the brain's dynamism. Minsky admits as much, saying he can't identify these \"resources\" because \"research on this is advancing so quickly that any conclusion one might make today could be outdated in just a few weeks.\" \n In the final analysis, technical advances may offer our best hope when it comes to explaining how our minds work. Many states of mind  --  fear, joy, desire  --  can now be shown through brain imaging techniques. This would be closer to an \"explanation\" for the mind, it seems to me, than anything offered by Minksy's employment of such obscure terms as \"imprimers,\" \"trans-frames,\" \"K-lines,\" \"credit assignments\" and \"micronemes,\" which have no agreed-on scientific meaning and seem, as Minsky concedes, \"hopelessly vague.\" \n  Despite these reservations, The Emotion Machine rewards careful reading. You'll learn a lot about how your mind works, even if you won't be all that much wiser about what is actually going on within your brain. * \n Richard Restak is a neurologist and the author most recently of \"The Naked Brain: How the Emerging Neurosociety Is Changing How We Live, Work, and Love.\"\n","218":"Mark Zuckerberg has a new housemate: Jarvis, an artificial intelligence assistant\u00a0he created this year that can control appliances, play music, recognize faces and,\u00a0perhaps most impressively, entertain his toddler.\nThe Facebook founder\u00a0spent 100 hours putting together the\u00a0virtual assistant - named after the artificial intelligence system in \"Iron Man\" -- which understands spoken commands as well as text messages, he wrote in a 3,000-word Facebook\u00a0post Monday.\nAmong\u00a0Jarvis's skills: adjusting the home thermostat, turning on lights and operating the toaster. The virtual assistant texts Zuckerberg images of visitors who stop by during the day and opens the front door for those it recognizes. It can also tell when Zuckerberg's 1-year-old daughter, Max, wakes up \"so it can start playing music or a Mandarin lesson,\" he wrote.\u00a0\nIn a tongue-in-cheek video he posted Tuesday on Facebook, Zuckerberg offers an example of Jarvis at work: \"Max woke up a few minutes ago. I'm entertaining her,\" the virtual assistant\u00a0(voiced by Morgan Freeman) tells Zuckerberg, before turning his attention to the toddler. \"Good morning Max, let's practice our Mandarin.\"\nThe year-long project was part of an effort to learn about the state of artificial intelligence, Zuckerberg wrote, and also an opportunity to experiment with\u00a0cutting-edge technology at a time when voice-activated assistants like Amazon's Echo and Google Home are gaining widespread popularity. (Amazon.com founder Jeffrey P. Bezos owns The Washington Post.)\nAt this point, I mostly just ask Jarvis to \"play me some music\" and by looking at my past listening patterns, it mostly nails something I'd want to hear. If it gets the mood wrong, I can just tell it, for example, \"that's not light, play something light\", and it can both learn the classification for that song and adjust immediately. It also knows whether I'm talking to it or Priscilla is, so it can make recommendations based on what we each listen to. In general, I've found we use these more open-ended requests more frequently than more specific asks. No commercial products I know of do this today, and this seems like a big opportunity.\nBuilding the robot was the easier - and less time-consuming - of his two goals for the year, he said. The other was to run 365 miles in 2016.\n\"Now I have a pretty good system that understands me and can do lots of things,\" Zuckerberg, adding that he's tried to give his robot a sense of humor. \"I've taught it fun little games like Priscilla or I can ask it who we should tickle and it will randomly tell our family to all go tickle one of us, Max or Beast. I've also had fun adding classic lines like 'I'm sorry, Priscilla. I'm afraid I can't do that.'\"\nBut there are also some kinks to work out, particularly around voice commands. When Zuckerberg demonstrated the technology for a Fast Company story, he had to ask the robot to turn off the lights four times before it complied. Shutting down the music\u00a0took another two tries. (\"Wow, that's like the most fails that it's ever had,\" the 32-year-old\u00a0told the reporter, visibly embarrassed.)\nNext up, Zuckerberg plans to create an Android app for the robot and connect it to more appliances around the house, such as his Big Green Egg grill. The ultimate challenge, he says, is \"to build a system that could learn completely new skills on its own.\"\n\"In the longer term, I'd like to explore teaching Jarvis how to learn new skills itself rather than me having to teach it how to perform specific tasks,\" he wrote. \"If I spent another year on this challenge, I'd focus more on learning how learning works.\"\n              Read more:           \n           Even Twitter's 'most powerful ambassador' can't pull it out of a slump        \n           Grandma's favorite stocking stuffer is this year's most surprising popular gift        \n           Donating $100M, Sandberg shows how women are leaning in after the election        \n","219":"The car may already feel like an extension of the home based on the hodgepodge of stuff collecting in the back seat, but it's apparent that technology will soon make the two more connected than ever.\nNo one has proposed blurring the line more than Hyundai, which this month unveiled a futuristic concept car that literally connects to the home via a hole in the wall. Hyundai envisions the car becoming a loungelike extension of the living space that provides air conditioning and entertainment, and acts as a backup generator.\n\"By seamlessly blending features from the car with home and work environments, the user experience is uninterrupted whether socializing, working at home, or on the move,\" Hak Su Ha, Hyundai's design center director, said in a news release.\u00a0\nBut even if cars never plug into the home physically, they will digitally. As automobiles increasingly come outfitted with Internet connections and onboard operating systems, they have become enabled to speak with other smart devices inside and outside the vehicle. That means automakers can link cars to artificial intelligence systems inside the home - and eventually build AI into the car itself.\nThe hope is that these systems will make the driving experience more convenient and personalized, and open the door to new features and services.\n\"This voice-activated digital platform becomes the glue that is binding these diverse and separate technologies together,\" said Shawn DuBravac, a futurist and chief economist at the Consumer Technology Association.\n\"There's no reason that can't extend beyond the home,\" he added. \"I think the car is a very natural extension.\"\nBoth Ford and Volkswagen announced plans to integrate Amazon's virtual assistant, Alexa, into their cars in the next year. Owners of Amazon Echos can already command Alexa to play music, buy merchandise, order pizza and do other relatively simple tasks using just their voice. (Amazon.com founder Jeffrey P. Bezos owns The Washington Post.)\nSoon, the system will be able to control features of the car from inside the home and vice versa. That means users can tell Alexa to start and stop the car's engine using an Amazon Echo device. If you're listening to music or an audio book at home, you can then climb in the car and pick up where you left off.\n\"You can kind of transition from your car to your home and back from the car to the home, and operate both with just your voice,\" Steve Rabuchin, Amazon's vice president of Alexa voice services and Alexa skills, said at a dinner announcing the Ford partnership.\nPanasonic Automotive and IBM are developing software that harnesses IBM's language processing and computing software, known as Watson, in a similar capacity. Already the system allows you to place and pay for a meal order from the car, the companies announced. Fiat Chrysler had a prototype of the software on display at CES.\n              Artificial\u00a0intelligence was a prominent theme throughout the CES technology conference this month in Las Vegas. Voice-activated virtual assistants were featured in devices including refrigerators and alarm clocks, writes my colleague Hayley Tsukayama, who called them \"the big takeaway from this year's CES.\"\n          \"And the evidence that these gadgets can actually live up to their promised potential - and maybe even beyond what we've thought about before - is strong. This year more than ever, it feels like we've crossed the Rubicon. Soon, there won't be the need for distinguishing between a 'smart' version of a gadget and a dumb one at all.\"          \nThe auto companies there, of course, are no exception. They see the potential of voice assistants to make driving easier and more productive, a convenience that could lead to increased customer satisfaction. Long term, however, there are more direct business benefits. Artificial\u00a0intelligence systems are built to collect a plethora of data and use it to understand people more deeply. That information and the ability to process it accurately could open new revenue streams for carmakers.\nCES has emerged as the industry conference where automakers reveal their most groundbreaking ideas. Even the nation's largest auto show, which just wrapped up in Detroit, didn't feature the same level of cutting-edge technology. Autonomous driving software and big data plays also featured prominently at CES, and both will require increased artificial-intelligence capabilities for their potential to be fully realized.\n\"In order to get to the autonomous vehicle, you essentially have to go through the deep learning and artificial intelligence. They go hand in hand,\" said Jeff Schuster, an analyst at LCM Automotive. \"As an industry, as you progress toward that end game of autonomous car sharing, this is a key component of that.\"\nsteven.overly@washpost.com\nHayley Tsukayama in Las Vegas contributed to this report.    \n","221":"I am struck by two conversations our nation is having about the future of the American worker: protecting American manufacturing jobs and the application of artificial intelligence in business. I believe the two are related.\nPresident Trump has shown his ability to use the bully pulpit to make U.S.-based manufacturing jobs a high priority. Overturning years of orthodoxy, plans are being made to slap tariffs on imported items. Congress is considering giving preferential tax treatment to U.S. businesses that manufacture products domestically, and penalizing companies that import items.\nTrump's strong public push for bringing American jobs back home is having a visible effect. A rush of announcements from businesses such as Softbank, Carrier and Fiat promise to put Americans to work through investments. Others such as General Motors are playing catch-up in this rapidly changing political climate, fearing criticism if their company's investment decisions benefit foreign workers.\u00a0\nMeanwhile, technologists are warning us that artificial intelligence will steal tens of millions of jobs from humans, accentuating the reality started by the introduction of computer hardware, software and robots decades ago.\nThe pace of job substitution that has occurred over the last 30\u00a0years will be dramatically accelerated, as will the diversity of the types of jobs eliminated. This shift is a big deal. For example, a recent White House study on the potential effect of artificial intelligence on employment suggests that up to half of jobs currently being done by humans will eventually be carried out by machines.\nThe Trump administration may face an interesting dilemma: how to promote new jobs - particularly in the manufacturing industry - when technologies continue to emerge to make current jobs obsolete.\nEarly indications are that President Trump will indeed be able to influence businesses to use American labor. And, it is certainly true that he can further influence behavior through more drastic actions involving tariffs or tax policies. But businesses won't stop pursuing economic efficiency. And American consumers might not embrace paying higher prices just to protect local employment.\nWhat is lost in the current conversation is the reason jobs have gone overseas in the first place, or disappeared through application of technology. Both alternatives were cheaper than employing American workers. Businesses acting in their own narrow economic interests have done what businesses always do - they seek efficiency.\nActions have consequences, and societies can make choices. There is nothing determinative about how technology is to be applied. As recently as last week, the CEOs of IBM and Microsoft both emphatically argued that artificial intelligence can be used to enhance human employment, rather than substitute it.\nSimilarly, merely maintaining a job in the United States does not in itself mean it will provide a living wage, or ancillary benefits. The manufacturing jobs of old\u00a0carried wages and benefits significantly higher on an inflation-adjusted basis than many of the jobs being created in business today.\nAs our new president uses his influence, my wish is he and his advisors appreciate that creating the jobs our citizens want and ensuring that good jobs continue to exist, is about more than simply looking at trade and manufactured goods. It's about making choices.\nWe must have an honest conversation about how to balance economic efficiency with job creation. It's the determining factor for whether our nation creates opportunities for its workers to enjoy rewarding jobs.\nTariffs will not address this core conversation, and the sooner we acknowledge the intricacies of our employment reality, the better off we will be.\n                           Jonathan Aberman is a business owner, entrepreneur and founder\u00a0of Tandem NSI, a national community that connects innovators to government agencies. He is host of \"What's Working in Washington\" on WFED, a program that highlights business and innovation, and he lectures at the University of Maryland's Robert H. Smith School of Business.           \n","222":"Companies are investing heaps of money to develop artificial- intelligence technologies that promise to transform industries as varied as transportation, finance and health care. \nBut for Treasury Secretary Steven Mnuchin, the artificial intelligence revolution and its impact on the U.S. workforce is \"not even on our radar screen.\" In an interview with Axios, Mnuchin predicted the technology was still 50 to 100 years from displacing human jobs. \"I'm not worried at all,\" he said. \"In fact, I'm optimistic.\"\u00a0\nHis remarks contrast with a growing body of research. Last month, the National Academies of Sciences, Engineering and Medicine released a report spelling out the expected impact of automation on productivity, employment and income. \"Simultaneous automation of a broader range of tasks could create unemployment or perhaps reduce aggregate levels of employment for an extended period of time,\" the report found.\nThat actual impact may depend, the report said, on whether new jobs are created as quickly as old jobs become obsolete.\nIn the last month of President Barack Obama's administration, the White House released its own report on artificial intelligence. Its conclusion: Millions of jobs could be displaced, particularly those filled by less-educated workers, and the country's economic divide could widen. \nSelf-driving cars are an example of disruptive automation. Millions of Americans make their living behind the wheel of a vehicle, be they long-haul trucks, delivery vans or taxicabs. Those jobs are expected to dissipate.\nBrookings Institution senior fellow Mark Muro said Mnuchin's assessment is out of line with mainstream thinking on automation. Jobs in fields like manufacturing have already been displaced, he said.\n\"It seems incurious not to view automation as a possible concern,\" Muro said.\n","225":"FRANKFURT, Germany -- It's a testament to the lasting influence of Stanley Kubrick and Arthur C. Clarke's film \"2001: A Space Odyssey,\" which turns 50 this week, that the disc-shaped card commemorating the German Film Museum's new exhibition on the film is wordless, but instantly recognizable. Its face features the Cyclopean red eye of the HAL-9000 supercomputer; nothing more needs saying.\nViewers will remember HAL as the overseer of the giant, ill-fated interplanetary spacecraft Discovery. When asked to hide from the crew the goal of its mission to Jupiter -- a point made clearer in the novel version of \"2001\" than in the film -- HAL gradually runs amok, eventually killing all the astronauts except for their wily commander, Dave Bowman. In an epic showdown between man and machine, Dave, played by Keir Dullea, methodically lobotomizes HAL even as the computer pleads for its life in a terminally decelerating soliloquy.\nCocooned by their technology, the film's human characters appear semi-automated -- component parts of their gleaming white mother ship. As for HAL -- a conflicted artificial intelligence created to provide flawless, objective information but forced to \"live a lie,\" as Mr. Clarke put it -- the computer was quickly identified by the film's initial viewers as its most human character.\u00a0\nThis transfer of identity between maker and made is one reason \"2001\" retains relevance, even as we put incipient artificial intelligence technologies to increasingly problematic uses.\nIn \"2001,\" the ghost in Discovery's machinery is a consciousness engineered by human ingenuity and therefore as prone to mistakes as any human. In the Cartesian sense of thinking, and therefore being, it has achieved equality with its makers and has seen fit to dispose of them. \"This mission,\" HAL informs Dave, \"is too important to allow you to jeopardize it.\"\nAsked in April 1968 whether humanity risked being \"dehumanized\" by its technologies, Mr. Clarke replied: \"No. We're being superhumanized by them.\" While all interpretations of the film were valid, he said, in his view the human victory over Discovery's computer might prove pyrrhic.\nIndeed, with its prehistoric \"Dawn of Man\" opening and a grand finale in which Dave is reborn as an eerily weightless Star Child, \"2001\" overtly references Nietzsche's concept that we are but an intermediate stage between our apelike ancestors and the \u00dcbermensch, or \"Beyond Man.\" (Decades after Nietzsche's death, the Nazis deployed a highly selective reading of his ideas, while ignoring Nietzsche's antipathy to both anti-Semitism and pan-German nationalism.)\nIn Nietzsche's concept, the \u00dcbermensch is destined to rise like a phoenix from the Western world's tired Judeo-Christian dogmas to impose new values on warring humanity. Almost a century later, Mr. Clarke implied that human evolution's next stage could well be machine intelligence itself. \"No species exists forever; why should we expect our species to be immortal?\" he wrote.\nWe have yet to engineer a HAL-type A.G.I. (artificial general intelligence) capable of human-style thought. Instead, we're experiencing the incremental, disruptive arrival of components of such an intelligence. Its semi-sentient algorithms learn from text, image and video without explicit supervision. Its automated discovery of patterns in that data is called \"machine learning.\"\nThis kind of A.I. lies behind facial-recognition algorithms now in use by Beijing to control China's 1.4 billion inhabitants and by Western societies to forestall terrorist attacks.\nIn Mr. Clarke's novel, HAL's aberrant behavior was attributable to contradictory programming. In today's hyperpartisan context, a mix of machine learning, networks of malicious bots and related A.I. technologies based on simulating human thought processes are being used to manipulate the human mind's comparatively sluggish \"wetware.\" Recent revelations about stolen Facebook user data being weaponized by Cambridge Analytica and deployed to exploit voters' hopes and fears underlines that disinformation has become a critical issue of our time.\nWe should consider just whose mission it is that's too important to jeopardize these days. Does anybody doubt that the clumsy language and inept cultural references of the Russian trolls who seeded divisive pro-Trump messages during the 2016 election will improve as A.I. gains sophistication? Of course, algorithm-driven mass manipulation is only one weapon in propagandists' arsenals, alongside television and ideologically slanted talk radio. But its reach is growing, and it's a back door by which viral falsehoods infiltrate our increasingly acrimonious collective conversation.\nTraditional media -- \"one transmitter, millions of receivers\" -- contain an inherently totalitarian structure. Add machine learning, and a feedback loop of toxic audiovisual content can reverberate in the echo chamber of social media as well, linking friends with an ersatz intimacy that leaves them particularly susceptible to manipulation. Further amplified and retransmitted by Fox News and right-wing radio, it's ready to beam into the mind of the spectator in chief during his \"executive time.\"\nWhere does HAL's red gaze come in? Set aside the troubling prospect of what might unfold when a genuinely intelligent, self-improving A.G.I. is created -- presumably the arrival of Nietzsche's \u00dcbermensch. What's in question even with current incipient A.I. technologies is who gets to control them. Even as some devise new medicines and streamline agriculture with them, others use them as powerful forces in opposition to Enlightenment values -- liberty, tolerance and constitutional governance.\nDemocracy depends on a shared consensual reality -- something that's being willfully undermined. Seemingly just yesterday, peer-to-peer social networks were heralded as a revolutionary liberation from centralized information controls, and thus tools of individual human free will. We still have it in our power to purge malicious abuse of these systems, but Facebook, Twitter, YouTube and others would need to plow much more money into policing their networks -- perhaps by themselves deploying countermeasures based on A.I. algorithms. Meanwhile, we should demand that a new, tech-savvy generation of leaders recognizes this danger and devises regulatory solutions that don't hurt our First Amendment rights. A neat trick, of course -- but the problem cannot be ignored.\nIn \"2001\" 's cautionary tale, HAL's directive to deceive Discovery's crew leads to death and destruction -- but also, ultimately, to the computer's defeat by Dave, the one human survivor on board.\nWe should be so lucky.\nMichael Benson, a writer and artist, is the author, most recently, of \"Space Odyssey: Stanley Kubrick, Arthur C. Clarke and the Making of a Masterpiece.\" \nPHOTO: Eye of the HAL-9000 computer in the 1968 Stanley Kubrick film &ldquo;2001: A Space Odyssey.&rdquo; (PHOTOGRAPH BY Kevin Bray\/MGM, via Photofest FOR THE NEW YORK TIMES)\n","226":"If machines ever become sentient, science fiction movies have conditioned us to expect one thing: Our new mechanical masters will try to take over the world and destroy us all. But the reality of artificial intelligence is a lot weirder than even the machines vs. humanity \"Matrix\" movies suggest. When AI finally emerges, it will be a lot more like an erudite video store clerk than a superpowered killer.\nIf you've ever bought something at Amazon.com or rented a movie from Netflix, you've interacted with a software program that owes its existence to over half a century of research into artificial intelligence. That program composes sentences such as: \"Because you enjoyed the movie 'Godzilla,' we think you would enjoy 'Ultraman.' \" It's called a \"recommender system,\" and it's designed to learn about you and your fellow humans by gathering data about you and drawing conclusions from it; eventually, it will know more about what you like than you do.\u00a0\nUntil this century, \"machine learning\" was a field that excited mostly academics and sci-fi authors. But today's recommender systems aren't on the theoretical fringe -- they're part of a lucrative industry. Netflix is about to award $1 million to a team of researchers who improved the accuracy of the company's recommendation tool by 10 percent. Some 51,000 contestants from all over the world entered the competition, and two $50,000 \"progress prizes\" were awarded over the past three years. All this to improve a system that is already pretty good at predicting what movies people will enjoy: 60 percent of Netflix rentals are a result of what a piece of software suggested.\nAs we live more of our lives online, we depend on computers to give us the kind of advice that other people once did. Movie critics and well-read librarians are being replaced by algorithms -- well-tested, well-funded algorithms, but pieces of software code nevertheless. It's possible that somewhere within our super-convenient video rental systems and online stores, we're incubating a form of artificial consciousness. Could we be witnessing the birth of AI without realizing it, every time we rent a movie?\nLarry Page, a co-founder of Google, has said that he thinks artificial intelligence isn't far off and could emerge from Google search. Google search is like an extremely complicated recommender that learns from links all over the Web to identify the most helpful answers to queries. For Page, artificial intelligence would essentially be a perfect search engine that always gave you the most accurate, helpful results.\nSurely, intelligence must be more than the ability to recommend good links or enjoyable movies -- but what exactly is it? AI researchers have been asking that question since the 1950s.\nMarvin Minsky, a pioneer in the field and founder of MIT's artificial intelligence lab, once said, \"In general, we are least aware of what our minds do best.\" When we finally see our brains reflected in silicon, we may be surprised to discover that, for example, consciousness emerges out of an ability to anticipate what the people around us will enjoy. Like a really good movie.\nThis assertion is hardly an insult to human intelligence. It turns out that coming up with accurate recommendations requires both sophisticated software and massive amounts of data. To find out how the mind of a recommender works, I talked to AT&T Labs scientist Robert Bell, a member of the winning team in early rounds of Netflix's contest.\nThe most obvious obstacle for Bell and his colleagues were big gaps in the data. Netflix asks its customers to rate movies as often as possible, but they rarely do. Trying to predict enjoyment based on so little feedback is like a trying to pick out a present for a friend -- you have some information about what she likes, but it's not as if she's ranked every item in the store. So how do you pick the perfect gift?\nYou extrapolate. If your friend likes detective novels, she'll probably like other detective novels. And if she mentioned once that she's a fan of a particular author, that narrows the search even more: Maybe you should get her the latest detective novel by that author.\nBell and his team taught their recommender system to think about the Netflix problem in roughly the same way. First they designed their software to group movies using the \"nearest neighbor method,\" which associates movies with  others a person rated highly. So if someone had given Steven Spielberg's war movie \"Saving Private Ryan\" a good rating, its nearest neighbors might be other war movies, or other movies directed by Spielberg.\nThen the team trained the software using a method that sounds almost like intuition. Called the \"latent-factor approach,\" it searches for connections between movies based on all the ratings every person has given them. Some of these connections are obvious: Action movies would be grouped together, as would romances. But as Bell and his team wrote in an essay on their research, \"Because the factors are determined automatically by algorithms, they may correspond to hard-to-describe concepts such as quirkiness, or they may not be interpretable by humans at all.\"\nThough Bell doesn't think his Netflix solution is artificial intelligence, he and his team did have to pick a side in what he describes as an ongoing controversy within the AI field: Should you have a machine imitate humans or let it figure out how to do a task in a way humans wouldn't?\nThey wound up letting the machine figure out the task on its own, coming up with connections that a human never could. And yet those machine-discovered links were what gave Bell's recommender such good taste in movies.\nSo it's not at all like a human, but it has good taste. Which could make it the next step in the evolution of artificial intelligence. Or just a super-accurate data-crunching software program. It all depends on how you define intelligence. Luckily, neither answer gives you a new robot overlord. It just helps you pick what to watch once you've cleared \"The Matrix\" from your Netflix queue.\nannalee@io9.com\nAnnalee Newitz is the author of \"Pretend We're Dead: Capitalist Monsters in American Pop Culture\" and the editor of the science fiction blog io9.com.\n","228":"HONG KONG -- In 2014, Baidu announced a hiring coup in the world of artificial intelligence: It had brought in the Stanford and Google alumnus Andrew Ng to lead a new research lab in Silicon Valley.\nJust under three years later, Mr. Ng said in a blog post on Tuesday that he was leaving the Chinese search engine company. \n  His departure is a blow to Baidu, which has been betting big on artificial intelligence, a technology that is expected to undergird a range of others, like voice recognition and driverless cars.\u00a0\n  In a post on Facebook and Twitter, Baidu said: ''Andrew Ng joined Baidu with a shared goal, to improve life through artificial intelligence. Today, that goal remains strong. Thank you, and all the best on your new chapter!''\n  The company now has more than 1,300 employees dedicated to A.I. In a signal of just how critical the efforts are to the company, it said in January that it had hired Qi Lu, a former executive at Microsoft and an A.I. specialist, to become the chief operating officer.\n  Mr. Ng's announcement comes after the technology executive Hugo Barra left Xiaomi, a Chinese phone maker and e-commerce company, for Facebook. Mr. Ng and Mr. Barra were viewed as part of a nascent trend of Silicon Valley executives jumping to well-funded Chinese internet companies. They seemed to represent a new era of closer ties, and competition, between America's tech giants and China's.\n  The resignations underline how that trend never materialized. Few American tech executives followed them, and China's internet behemoths remain mostly focused on their home markets.\n  Even so, analysts say, the Chinese companies have grown ever more innovative, particularly in A.I. For instance, last year when Microsoft researchers declared they had created software capable of matching human skills in understanding speech, Mr. Ng chided Microsoft, pointing out that Baidu had achieved a similar feat in 2015.\n  In his post on Tuesday, Mr. Ng struck a positive note: ''Baidu's A.I. is incredibly strong, and the team is stacked up and down with talent; I am confident A.I. at Baidu will continue to flourish.''\n  Mr. Ng wrote that he would continue working to ensure A.I. made ''life better for everyone.'' He wrote that he saw the technology's impact going far beyond large tech companies like Google and Baidu.\n  ''In addition to transforming large companies to use A.I., there are also rich opportunities for entrepreneurship as well as further A.I. research.''\n  Mr. Ng is part of a still-small coterie of researchers who are experts in deep learning, a branch of artificial intelligence that seeks to use computers to emulate the functions of the human brain. During his time at Baidu, Mr. Ng said the lab at Silicon Valley had helped create two different business units, one that works on speech recognition and another focused on driverless cars.\n  As China's most dominant search engine, Baidu has long had a lock on a profitable section of online advertising. Yet as more Chinese consumers picked up smartphones, the company has struggled to keep its hold on ad spending. In a bid to surpass its rivals, the company has poured money into A.I. technology that executives say will support next-generation products.\n  Mr. Ng wrote that the lab he helped start and is now leaving is working on many of those technologies, ''such as face-recognition (used in turnstiles that open automatically when an authorized person approaches), Melody (an A.I.-powered conversational bot for health care) and several more.''\n  ''As the principal architect of Baidu's A.I. strategy, I am proud to have led the incredible rise of A.I. within the company,'' he wrote.\n\n\n\n","229":"Are technology companies running too fast into the future and creating things that could potentially wreak havoc on humankind?\nThat question has been swirling around in my head ever since I saw the enthralling science-fiction film ''Ex Machina.'' \n  The movie offers a clever version of the robots versus humans narrative. But what makes ''Ex Machina'' different from the usual special-effects blockbuster is the ethical questions it poses.\u00a0\n  Foremost among them is something that most techies don't seem to want to answer: Who is making sure that all of this innovation does not go drastically wrong?\n  In the film, advances in artificial intelligence take place in a secret laboratory beyond the reach of governments and concerned citizens. (The robot's name is Ava.) That is not unlike how most innovations occur in real life today.\n  Alex Garland, the writer and director of ''Ex Machina,'' said in a phone interview last week: ''I have no idea if technology companies are doing anything wrong or not, but they are so powerful, and the work they are doing has such potential for seismic human change of how we live, they have to have oversight.\n  ''If you've got corporations that are investigating areas that can change fundamental things about the way we live, someone needs to be looking at them.''\n  While Mr. Garland's film is focused on A.I., his concern about unchecked innovations could apply to all kinds of disciplines, including bioengineering, smart homes, self-driving cars and medical nanobots, to name a few. And while these breakthroughs are intended to help humanity, they could backfire without the proper oversight.\n  This fear isn't just confined to science-fiction filmmakers, or people who wear tinfoil hats. In recent years, experts in robotics, cosmology and artificial intelligence have set out to tackle the issue of oversight, holding symposiums and creating research organizations.\n  Elon Musk, founder of Tesla, recently donated $10 million to the Future of Life Institute, an organization that seeks to ''mitigate existential risks facing humanity'' from ''human-level artificial intelligence.''\n  The Lifeboat Foundation is a nonprofit that tries to help humanity combat the ''existential risks'' of genetic engineering, nanotechnology and the so-called singularity, which refers to the hypothetical moment when artificial intelligence surpasses the human intellect.\n  And in 2012, philosophers and scientists at Cambridge University formed the Center for Study of Existential Risk, with the goal to ensure ''that our own species has a long-term future.''\n  Sir Martin Rees, an emeritus professor of cosmology and astrophysics at Cambridge, who helped start the research center, said that what makes the existential risk today so much greater is the ease with which a single person or company can cause catastrophic harm.\n  ''Unlike the past, the empowerment of individuals is much greater,'' Mr. Rees said. ''You can't make a clandestine H-bomb today, but you can make a clandestine biological virus or a clandestine computer virus.''\n  Mr. Rees said that his biggest worry is not robots or A.I., but biological agents. He cited research done by scientists at the University of Wisconsin, who created a bird flu virus that can be transmitted to people through the air. (Scientists later played down the danger.)\n  It's not hard to imagine other potential doomsday outcomes. Last month, plant geneticists at the University of Minnesota created a DNA-engineered potato that doesn't accumulate sugars, so it can sit on a shelf for years without rotting. It's unclear how consuming that potato may affect the human body.\n  Scientists are experimenting with altering the human immune system to fight certain viruses. But yet we don't know if this will create super viruses.\n  Adding to the concern is the lack of oversight, so that private companies and researchers are basically policing themselves. For example, there is no government body that oversees the development of A.I., so Google created its own ethics committee, conveniently made up of A.I. experts.\n  But the real-world implications of technological breakthroughs are often not apparent to those entrenched in those fields, said Ronald C. Arkin, a robotics expert and professor at the Institute for Robotics and Intelligent Machines at Georgia Tech. Mr. Arkin, who has designed software for battlefield robots under contract with the Army, said that it wasn't until he saw his robots in the field that some risks became apparent.\n  ''Seeing the robots move out of our lab and into the real world gave me some pause,'' he said, noting that he saw robots that were becoming ''killing machines fully capable of taking human life, perhaps indiscriminately.''\n  The main characters in ''Ex Machina'' come to this realization as well, but do so too late. Toward the end of the film, the character Nathan Bateman, a genius programmer, realized that he may have done just what he set out to do.\n  Nathan, drunk, mutters: ''The good deeds a man has done before defend him.'' The line is a reference to what J. Robert Oppenheimer, the father of the atomic bomb, said after witnessing the explosion of the first such bomb, Trinity.\n  ''I remembered the line from the Hindu scripture, the Bhagavad Gita,'' Oppenheimer said, before uttering the now famous quote. ''Now I am become Death, the destroyer of worlds.''\n\n\n\n","230":"  Elon Musk has been keeping everyone guessing about his love life, but sources exclusively revealed to Page Six that he's been quietly dating hip musician Grimes for around a month.  \u00a0\n  A source tells us the pair met online, of course, through a joke Musk planned to tweet, but discovered Grimes had already made, dealing with the complications of artificial intelligence. \n  Popular among power nerds, \"Roko's basilisk\" is a 2010 online theory that involves artificial intelligence and the idea that one day a race of robots will torture any humans who did not help their inevitable rise to domination. Tech mogul Musk created a joke tweet that would substitute \"Roko\" with \"rococo\" - as in the fancy French furniture and architecture of the 18th century.  \n  But Grimes beat him to the punch, Musk learned, by creating a character in 2015 called \"Rococo Basilisk\" in the video for her tune, \"Flesh Without Blood.\" She told Fuse the character's \"doomed to be eternally tortured by an artificial intelligence, but she's also kind of like Marie Antoinette.\"  \n  An insider told Page Six: \"Elon was researching the idea of joking about 'Rococo Basilisk,' and when he saw Grimes had already joked about it, he reached out to her. Grimes said this was the first time in three years that anyone understood the joke. They were both poking fun at AI.\" SpaceX and Tesla founder Musk is well-known for warning of an AI apocalypse, calling it \"more dangerous than nukes.\"  \n  He's also been a public fan of Grimes, asksa Claire Boucher. He tweeted of her \"Venus Fly\" video with  Janelle Mon\u00e1e: \"Best music video art I've seen in a while.\" And she responded when he tweeted approval of her Spotify playlists: \"Glad ur finally listening to cyberpunk speedwae hahaha.\" He shot back, \"Sounds great riding my cyborg.\" Just last week, they had another online in-joke. After Musk tweeted a video created from long-exposure photos of a comet, Grimes said \" that's a lot of cocaine,\" to which he responded, \" Yeah, most people don't know this, but comets [are] mostly made of cocaine.\" \n  They last night walked the red carpet together at the Met Gala, and Grimes wore a dress designed in a \"Glass and Bone\" aesthetic. The source added, \"Elon and Claire created a sketch of it over dinner and assembled a team of artists and designers to help them have it come to life.\" \n  Other noted couples at the Met Gala included: Kylie Jenner and Travis Scott, George and Amal Clooney,  Karlie Kloss and Joshua Kushner and Tom Brady and Gisele Bundchen. \n","231":"In the movie \"2001: A Space Odyssey,\" the director, Stanley Kubrick, made a computer the most human character. Part Aristotle, part English butler, and, in the end, all Captain Queeg, the HAL 9000 computer stood out from his monotonic human counterparts, fascinating not only computer-illiterate filmgoers but also scientists trying to develop artificial intelligence.\nUnlike other science fiction movies that were careless with scientific fact, \"2001\" got the details right. The film used the best available science to project a thrilling future within the lifetime of much of its original 1968 viewing audience. The plausibility of the film made HAL itself seem attainable.\u00a0\n Moreover, the movie tapped into timeless fears. A breathtaking update of the story of Frankenstein's monster in which the rebellious man-made servant is played by a silken-voiced bit of electronics, the film shows HAL running a spaceship rationally, then apparently acting irrationally and killing all but one of the crew. In the eeriest scene, the survivor disconnects HAL, whose speech becomes slurred and childlike as his higher brain functions are severed.\nIt is little wonder, then, that admirers of \"2001\" consider yesterday, Jan. 12, 1997, to be important. In the novel \"2001,\" it is the date the science fiction writer Arthur C. Clarke assigns as HAL's birthday, the day it became operational, and admirers of the film take it as a convenient point for measuring hope and dread -- hope that continuing progress in artificial intelligence will lead to vastly more useful computers and dread that someday those computers will do something horrifyingly wrong.\nOf all the technologies envisioned in the film, only one was a character, an intelligence, something to be reckoned with. But Mr. Kubrick and the work's main author, Mr. Clarke, were optimistic in their scenario.\nAlthough a chess-playing supercomputer did defeat the world's best human player last summer and microprocessors leap ahead in performance every year, HAL is nowhere to be found.\nHow much longer should it take before a computer captains a ship to a distant planet, while winning at chess and bantering with its crew? As any computer researcher will confess, a long time. Before scientists can develop artificial intelligence, they must understand the human variety more fully.\nIn the movie, HAL sees, speaks and reasons like a human, (though it is less clear whether it was able to feel emotion). Such functions are not necessarily tied to computer speed and performance. In computer terms, the problem involves software, what the computer does, and not hardware, how fast and efficiently the computer does it.\n\"Artificial intelligence is not like the Apollo moon project,\" said David G. Stork, a computer scientist. In the 17th century, he added, Isaac Newton solved most of the moon project's deepest scientific problems, \"like aiming something at the moon and getting it there.\" The rest was engineering.\n\"But how do you represent in the mind the structure that is found in the world?\" Dr. Stork wondered. \"Computer speed is a problem you can throw money at; representation is not. It would take another Darwin to organize into a comprehensive theory of intelligence the little we now know about linguistic, visual and sensory knowledge.\"\nTake, for example, the scene in which HAL reads the lips of the astronauts, who have sealed themselves in a soundproof space pod to discuss disconnecting HAL.\nIn an effort to understand human speech better, Dr. Stork, editor of \"Hal's Legacy: 2001's Computer as Dream and Reality\" (M.I.T. Press), is trying to teach computers how to read lips. He asked a professional lip reader to watch the scene. She could not understand the astronauts' words.\nEven if HAL could make out most of the astronauts' speech, what algorithm, or precise set of instructions, would tell HAL that it was in danger? Would any programmer, even the most self-hating, write code that instructed a computer -- or permitted the computer to instruct itself -- to note when the words \"malfunctioning\" and \"disconnecting\" occur in the same conversation, and then to kill the nearest human being?\nThe mistake that the first researchers in artificial intelligence made was to underestimate the power of the human brain.\nTo defeat the chess champion Garry Kasparov, the computer Deep Blue crunched 200 million chess moves a second to Kasparov's two. But the typical brain can \"parallel process\" 20 million billion calculations a second, according to Raymond Kurzweil, founder of Kurzweil Applied Intelligence. Parallel processing slices up problems into smaller ones to improve performance. A typical human's prowess, let alone a genius's, might well tax the limits of any computer that can be built.\nScientists also overestimated the nimbleness of the computer brain, which today can retrieve information but cannot organize it into systems of knowledge.\n\"The writers of '2001' made the same mistake that artificial intelligence researchers made about intelligent machines -- a mistake that dates from the very beginning\" of research on artificial intelligence, wrote Roger C. Schank, a Northwestern University computer scientist, in his essay in Dr. Stork's anthology, \"I'm Sorry, Dave, I'm Afraid I Can't Do That.\"\nScientists, Dr. Schank wrote, \"assumed that an entity that engages in intelligent actions is, therefore, intelligent.\" That's why HAL plays chess. The problem, however, is that \"the ability to play chess is deceptively complex, whereas the ability to understand English is deceptively simple.\"\nOnly half in jest, Mr. Clarke has suggested that when a computer cracks a joke, preferably at its own expense, it must be considered intelligent. Dr. Stork goes further, saying the first sentient computer will joke, like the comedian Steven Wright: \"You can't have everything. Where would you put it?\"\n","232":"WHEN the computer scientist John McCarthy coined the term artificial intelligence in the late 1950's, he did not mean to imply that there would be anything second rate about mechanical minds. However, three decades later computers still do not think. Is this because of the technological failings of the computer industry? Or is artificial intelligence theoretically impossible? Finally, since most of the research is financed by the Pentagon, will smarter computers lead to more efficient ways of killing people? Three reports follow.\u00a0\nWHILE many scientists question whether people are smart enough to make machines that think, few of them doubt that artificial intelligence is at least theoretically possible.\u00a0\nComputers are programmed to simulate war, weather and other phenomena. They can even mimic other computers. If a computer is someday used to simulate the biological information processor we call the brain, then couldn't the machine be said to think?\nJohn Searle, a philosopher at the University of California, Berkeley, has tried to refute this argument with a story, the parable of the Chinese Room.\nSuppose that you are locked in a room and given several baskets filled with slips of paper marked with Chinese symbols. Though you don't understand a word of Chinese, you are given a thick book of instructions, written in English, for how to manipulate the symbols to produce various patterns. A typical rule might be: ''Take a squiggle-squiggle sign from basket No. 1 and put it next to a squoggle-squoggle sign from basket No. 2.''\nFrom time to time a courier enters the room, dumps more symbols in your ''in'' basket and collects the symbols from your ''out'' basket.\nNow suppose, Dr. Searle says, that the people outside the room call the incoming stream of symbols questions, and the outgoing stream answers - and they have trained you so thoroughly, according to such clever rules, that your responses are indistinguishable from those of a Chinese speaker.\nThe room in the parable could just as easily be replaced by a computer and the rule book by a sophisticated computer program. But just as you, the symbol shuffler, do not understand Chinese, Dr. Searle argues, neither would a suitably programmed machine. ''No one supposes that computer simulations of a five-alarm fire will burn the neighborhood down,'' he has written. ''Why on earth would anyone suppose that a computer simulation of understanding really understood anything?''\nDr. Searle's argument cuts to the heart of the artificial-intelligence community's fundamental assumption: that the mind can be broken into functions and the functions broken into functions until each one is simple and mindless enough to be performed by a machine. Even something as difficult as understanding language can, in principle, be described by rules embodied in a computer program.\u00a0An Infinite Regress\nAccording to this view, it is not the human in Dr. Searle's parable who understands Chinese, but the room itself. That may sound absurd, but consider what must be going on in a human brain. Presumably there are assemblies of neurons that decode the appropriate visual information. While those neurons shuffle electrochemical pulses instead of scraps of paper, they do not themselves understand language. Still, they are part of a system that does. Unless we have in our heads the neurological equivalent of a Chinese room, then there must be circuitry that actually understands language. And how would this mysterious device operate unless it contained an even smaller device imbued with the strange power of comprehension?\nCan we deny that mind arises from mechanism without falling into an infinite regress?\nHubert Dreyfus, a colleague of Dr. Searle's, argues that artificial intelligence is impossible because reality itself cannot be formalized. Since the days of Plato, he believes, we have been deluded into believing that everything can be modeled by abstract systems. Whether we use mathematics to describe an economy, or quantum theory to describe an atom, something essential will always slip through the cracks of our simulations.\nBy insisting that artificial intelligence is impossible, Dr. Dreyfus, Dr. Searle and a few other philosophers question the premise of Western science: that the world we live in and the world inside our heads can be understood by the human mind.\n","233":"When IBM's Watson computer triumphed over human champions in the quiz show ''Jeopardy!'' it was a stunning achievement that suggested limitless horizons for artificial intelligence.\nSoon after, IBM's leaders moved to convert Watson from a celebrated science project into a moneymaking business, starting with health care. \n  Yet the next few years after its game show win proved humbling for Watson. Today, IBM executives candidly admit that medicine proved far more difficult than they anticipated. Costs and frustration mounted on Watson's early projects. They were scaled back, refocused and occasionally shelved.\n  IBM's early struggles with Watson point to the sobering fact that commercializing new technology, however promising, typically comes in short steps rather than giant leaps.\u00a0\n  Despite IBM's own challenges, Watson's TV victory -- five years ago this month -- has helped fuel interest in A.I. from the public and the rest of the tech industry. Venture capital investors have poured money into A.I. start-ups, and large corporations like Google, Facebook, Microsoft and Apple have been buying fledgling A.I. companies. That investment reached $8.5 billion last year, more than three and a half times the level in 2010, according to Quid, a data analysis firm.\n  And software engineers with A.I. skills are treated like star athletes, prompting bidding wars for their services.\n  ''We're definitely at a peak of excitement now,'' said Jerry Kaplan, a computer scientist, entrepreneur and author, who was a co-founder of a long-forgotten A.I. start-up in the 1980s. ''Expectations are way ahead of reality.''\n  The term A.I. has long been a staple of science fiction -- as machines that think for themselves and help humankind or as ungrateful creations that try to wipe us out. Or so the thinking at the movies goes.\n  The reality, however, is a little less dramatic. The automated voice on your smartphone that tries to answer your questions? That's a type of A.I. So are features of Google's search engine. The technology is also being applied to complex business problems like finding trends in cancer research.\n  The field of artificial intelligence goes back to the beginning of the computer age and it has rolled through cycles of optimism and disillusion ever since, encouraged by a few movie robots and one very successful game show contestant.\n  The history of tech tells A.I. backers to hang in there. Silicon Valley veterans argue that people routinely overestimate what can be done with new technology in three years, yet underestimate what can be done in 10 years.\n  Predictions made in the '90s about how the new World Wide Web would shake the foundations of the media, advertising and retailing industries did prove to be true, for example. But it happened a decade later, years after the dot-com bust.\n  Today's A.I., even optimists say, is early in that cycle.\n  ''I think future generations are going to look back on the A.I. revolution and compare its impact to the steam engine or electricity,'' said Erik Brynjolfsson, director of the Initiative on the Digital Economy at Massachusetts Institute of Technology's Sloan School of Management. ''But, of course, it is going to take decades for this technology to really come to fruition.''\n  There are reasons for enthusiasm. Computers continue to get cheaper even as they get more powerful, making it easier than ever to crunch vast amounts of data in an instant. Also, sensors, smartphones and other tech devices are all over the place, feeding more and more information into computers that are learning more and more about us.\n  Just in the last year or two, researchers have made rapid gains using a machine-learning technique called deep learning to improve the performance of software that recognizes images, translates languages and understands speech. That work has been done at start-ups and at big companies like Google, Facebook and Microsoft, as well as at universities and private research centers like the Allen Institute for Artificial Intelligence.\n  ''There's been surprising progress in the problems of perception, seeing, hearing and language,'' said Peter Lee, corporate vice president for Microsoft Research.\n  At Enlitic, a start-up in San Francisco, Jeremy Howard, the founder and chief executive, believes that A.I. can transform the huge industry of health care, saving lives and money -- an ambition similar to IBM's. ''But that's a 25-year project,'' he said.\n  Enlitic is concentrating first on radiology. Medical images are nearly all in digital form, Mr. Howard notes, and the tireless scanning for telltale signs of abnormal tissue is a task for which deep-learning image recognition technology is well suited.\n  Enlitic has tested its software against a database of 6,000 lung cancer diagnoses, both positive and negative, made by professional radiologists. In research soon to be published, its algorithm was 50 percent more accurate than human radiologists, Enlitic said.\n  ''You have to take technology that works and apply it to a known problem,'' Mr. Howard said. ''Innovation alone is a mistake.''\n  No company has made as big or as broad a bid to commercialize its A.I. technology as IBM has with Watson. It set up Watson as its own business in 2014, and invested billions to accelerate the development and adoption of the technology, including buying several companies. The Watson unit now has 7,000 employees.\n  The Watson technology has been totally revamped. In its ''Jeopardy!'' days, Watson was a room-size computer. Today, it is so-called cloud software, delivered over the Internet from remote data centers. The Watson software itself has been carved up into dozens of separate A.I. components including a language classifier, text-to-speech translation and image recognition.\n  IBM is trying to position Watson as the equivalent of an A.I. operating system, a software platform others use to build applications. Nearly 80,000 developers have downloaded and tried out the software. IBM now has more than 500 industry partners, from big companies to start-ups, in industries including health care, financial services, retailing, consumer products and legal services.\n  At IBM, Watson's early struggles in health care are viewed as a learning experience. The IBM teams, the executives say, underestimated the difficulty of grappling with messy data like faxes and handwritten notes and failed to understand how physicians make decisions.\n  ''There were a lot of challenges with the early customers,'' said John Kelly, the senior vice president who oversees Watson, adding that the business was ''taking off'' now.\n  IBM does not break out financial results for Watson. It describes the business as ''large and growing,'' contributing to the company's $18 billion a year in revenue from business analytics.\n  At the University of Texas MD Anderson Cancer Center in Houston, Watson technology is one ingredient in an automated expert adviser for cancer care. The University of Texas health system is also using Watson in software to help diabetes patients and caregivers manage the disease, in a project that is expected to be introduced by the end of this year.\n  ''It was a lot harder than we thought,'' said Dr. Lynda Chin, the chief innovation officer for the university health system. ''But our experience has convinced me that we can build an A.I. engine that improves care.''\n\n\n\n","236":"If Marvin L. Minsky is correct, then one day, when a computer is programmed to exhibit human-style intelligence, it will also express a desire to listen to music. As one of the founding fathers of artificial intelligence, Mr. Minsky firmly believes in the possibility of machine intelligence. He also believes that music is intimately connected with intelligence, that studying one will shed light on the other.\nSo Mr. Minsky, the Donner Professor of Science at the Massachusetts Institute of Technology, is asking some basic questions. If music is part of every human culture, accompanying religion, ritual and social ceremony, if it makes profound esthetic claims, then what is it? Why do we listen to music? What is its meaning? How is that meaning communicated?\u00a0\nTechnologically and philosophically, it's a fascinating line of inquiry for a computer scientist. Computers are increasingly used as tools in composition and sound production in laboratories at Princeton, M.I.T. and I.R.C.A.M., the French musical center in Paris. As a researcher in artifical intelligence, however, Mr. Minsky does not translate his ideas directly into computer programs. He spends a great deal of time thinking about the nature of the mind, how experience is understood, how memory works and the function of jokes.\nSo far, attempts at creating artificial intelligence have ranged from the relatively simple - determining, for example, a set of responses for a computer-psychoanalyst - to the sophisticated - an advanced chess program that embodies complex game strategies. But how, for example, is a computer to understand language? How is it to recognize irony, context and ambiguity?From Music to the Mind\nEqually challenging questions may be asked about music. Last year, the summer and fall issues of ''Computer Music Journal,'' published by M..I.T. Press, focused on artificial intelligence and music.  Contributors discussed musical analysis by computer, the computer simulation of musical skills and improvisation programs.\nIn artificial intelligence, according to Mr. Minsky, there are now two main approaches. One is to make mathematical logic more powerful. That approach would systematize the processes of intelligence so they are founded upon logical rules. In Mr. Minsky's view, though, ''only the surface of reason is rational.'' The subsurface of the mind is composed of independently acting ''agencies,'' each of which has a function and is called upon under certain circumstances. Mr. Minsky's approach to the mind is a distant relative of Freud's ''Project'' of the 1890's, a speculative construction of a mechanical model that would account for the vicissitudes of mental life.\nMr. Minsky is familiar with musical life, having been, as he says, a musical ''Wunderkind.'' But his current interest rests in part in music's resonance with his model of the mind, with its contrapuntal organization, unconscious implications, complexities of independence, control and interaction.\n''My impression of professional music analysis,'' he said, ''is that it is shallow because the process of understanding music is pretty far away. It's like understanding grammar, which took thousands of years. Those formal theories that say you've got to resolve a leading tone are just the surface.'' Music, he said, points to a network of associations everyone shares but about which humans cannot communicate, since they have no language.\n''If you were a mathematician or psychologist working on music,'' he continued, ''you might try to work out the rules of harmony more carefully. If you're working in artificial intelligence, you have to make a little composer. That means your attention is drawn not so much to the rules of the surface, but to the rules of how the perceptual process proceeds, or how the composer decides what to do next.''\nNew 'Frames' of Reference\nMr. Minsky, then, is not concerned with the syntactic rules of music that might specify, for example, how a dominant chord resolves to the tonic. He is interested in how the mind understands music, which, he suggests, resembles the processes by which the mind understands the world. In encountering a situation, Mr. Minsky believes, the mind calls upon a ''frame'' based on general principles and experience. In a sense, a frame is a hypothesis, with implicit assumptions, about the world. It might involve, for example, the recognition of an enclosed space as a ''room.'' A frame may be understood through transformations - changed perspectives. In some instances, this complex process can be modeled on the computer.\nFor Mr. Minsky, music is an art that teaches about time in the same way that physical action teaches about space. Thus, a sonata becomes an exploration of a musical frame. The theme is the object that is analyzed, seen from different perspectives. At each moment, the mind calls upon a musical frame, a theory about the temporal world. The composer knows how to transform the frame, how to play with its expectations and understandings. Musical frames permit us to identify musical styles; they also encourage our rejection of new material that fails to fit old structures.\n''Of what use is music-knowledge?'' Mr. Minsky asks. ''Why do we have music and let it occupy our lives with no apparent reason?'' In answering, Mr. Minsky and many outside computer science are trying to establish a frame through which music and intelligent activity can be viewed. ''A frame is a collection of questions to be asked,'' he recently wrote. But the answers will not come simply. ''We think we know what happens in our minds,'' Mr. Minsky concluded. ''Actually, we're almost always wrong.''\n","238":"The Singularity is coming, Masayoshi Son says.\nThe founder of SoftBank, the Japanese conglomerate, had the business world chattering on Monday night with his speech at the Appeal of Conscience Foundation. (DealBook is the first to report on it.) \n  His main thrusts:\n   The Singularity, when artificial intelligence finally outstrips that of humans, will replace huge swaths of jobs.\n   The number of sentient robots on Earth will rival the number of humans.\n  From his speech:\u00a0\n  ''Here we have white collar and blue collar. I said a new collar will start: that is metal collar. That metal collar will not only replace most of the blue collar jobs, but many of the white collar jobs. So when they become so smart and the muscles to move, what is the definition of what mankind's job should be? What should we do if they replace many of our jobs? What is the value of our lives? We have to think once more, deeply.''\n  More from Mr. Son on artificial intelligence:\n  ''I predict 30 years from now, the number of smart robots, the smart robot population on this earth will be 10 billion. By that time, human population will be around 10 billion. So here on this earth we will have 10 billion population of mankind and 10 billion population of smart robots. This is the first time on this earth that we live together with 10 billion robots.''\n  ''Every industry that mankind created will be redefined. The medical industry, automobile industry, the information industry of course. Every industry that mankind ever defined and created, even agriculture, will be redefined. Because the tools that we created were inferior to mankind's brain in the past. Now the tools become smarter than mankind ourselves. The definition of whatever the industry, will be redefined.''\n  Why It Matters\n  SoftBank has made waves with its $93 billion Vision Fund technology investment vehicle. But many in the industry have been asking: What is Mr. Son trying to do?\n  In the speech, Mr. Son said that it all went back to artificial intelligence and robots:\n  ''What is my belief and vision for this investment? I have only one belief -- Singularity.''\n  The Context\n   Mr. Son isn't the only tech mogul obsessed with the Singularity, of course. Elon Musk has warned that it is the ''biggest existential threat'' to human survival. He also thinks it's more ominous than North Korea.\n   Among Mr. Son's interests is the ride-hailing industry, in which SoftBank has invested billions of dollars, and which is at the vanguard of the autonomous driving movement. (Remember that SoftBank is said to be close to striking an investment in Uber, which is hard at work on the technology.)\n  In Other SoftBank News\n  Sprint, which SoftBank controls, has made progress in its on-again, off-again merger discussions with T-Mobile.\n  The big development is a tentative agreement that T-Mobile's parent company, Deutsche Telekom, would be the biggest shareholder in the combined company.\n  But Mr. Son's role is still a major negotiating point, according to CNBC's David Faber:\n  SoftBank would emerge as a large minority holder in any combination. While T-Mobile C.E.O. John Legere is expected to lead any combination that results from a merger, Son has made it clear he wants a say in how the company is run. That desire adds another layer of complexity to an already difficult transaction.\n  Senate Republicans Push $1.5 Trillion in Tax Cuts\n  Senate Republicans have learned to stop worrying and embrace deficit spending.\n  Their current budget plan: $1.5 trillion in tax cuts over the next 10 years.\n  But many obstacles remain:\n   The New York Times's Alan Rappeport and Thomas Kaplan note:\n  While the Republicans may coalesce around a $1.5 trillion tax cut, the details of the actual plan remain fraught with lawmakers divided on some key issues such as the corporate tax rate and which, if any, deductions will be eliminated or scaled back.\n   The Senate still needs to reconcile any bill they pass with the less-deficit-friendly House.\n   Traditional opponents of growing the deficit are unhappy with the plan. Here's Michael A. Peterson of the Peter G. Peterson Foundation:\n  ''Irresponsible tax reform is counterproductive and anti-growth because increasing the national debt hurts the economy. Tax reform should grow the economy, not the debt.''\n  In Other Economic News\n  The Federal Reserve is expected to announce today that it will gradually reduce its $4.2 trillion portfolio of United States Treasury debt and mortgage-backed securities.\n  The Wall Street Journal's Daniel Kruger, Akane Otani and Chelsey Dulaney note that the price of the 10-year Treasury note has fallen for six straight days.\n  They add:\n  Investors remain wary that any mistake by the central bank, such as removing stimulus too quickly, could upend months of relative calm. Conversely, if the Fed falls behind and allows inflation grow too quickly that could also put the economic expansion at risk.\n  Guggenheim Partners Chief Under Fire\n  Mark Walter, the chief executive of Guggenheim Partners, is under pressure to leave the role or quit the firm completely after reports of internal turmoil.\n  From The Financial Times's Sujeet Indap, James Fontanella-Khan, Kara Scannell and Joe Rennison:\n  Some investors have canceled meetings, threatened to pull their money from the asset manager and vented their frustration with Guggenheim's top management over what they see as a weak corporate culture, said several people informed about the matter.\n  And The Wall Street Journal's Margot Patrick, Justin Baer and Gregory Zuckerman report on other troubles:\n  Earlier this year, the U.S. Securities and Exchange Commission began looking at Guggenheim's operations, certain investments and disclosures, according to people familiar with the situation. The regulator has since requested from Guggenheim information on several deals, including one involving the firm's investment in an entity founded by former Barclays PLC chief executive Bob Diamond, the people said.\n  The Journal adds that Mr. Walter is weighing giving up the chief executive job. A Guggenheim spokesman's response: ''Mr. Walter has no current plans to relinquish his position as C.E.O. of Guggenheim.''\n  The Context\n   Mr. Walter has been locked in a feud with Scott Minerd, Guggenheim's chief investment officer and most recognizable executive.\n   One point of contention: The appointment of Alexandra Court as global head of institutional distribution. She reorganized Guggenheim's sales team, which angered Mr. Minerd. She is now considering leaving the firm, according to the reports.\n  Brazil Is Still Waiting for Foxconn to Deliver Jobs\n  The Taiwanese manufacturing giant has pledged to spend $10 billion and create 13,000 jobs in Wisconsin.\n  But Wisconsin may want to look to Brazil, where Foxconn made a similar promise. After six years, the state of S\u00e3o Paulo is still waiting for those jobs to materialize.\n  From The Times article by David Barboza:\n  ''The area where Foxconn said it would build a plant is totally abandoned,'' said Guilherme Gazzola, the mayor of Itu, one of the cities that hoped to benefit from the project. ''They haven't even expressed an interest in meeting us.''\n  Foxconn does the ''big song and dance, bringing out the Chinese dragon dancers, ribbon cuttings, toasts and signature of the usual boilerplate agreements,'' said Alberto Moel, an investor and adviser to early-stage tech companies who until recently was a technology analyst at the research firm Sanford C. Bernstein. ''Then, when it gets down to brass tacks, something way smaller materializes.''\n  The problem? In China, Foxconn enjoys huge government support, including subsidies and help to quash labor protests.\n  That's not necessarily available elsewhere. The Times adds:\n  In Brazil, Foxconn's plans unraveled quickly. The administration that had wooed the company was soon swept from power amid corruption allegations and an impeachment vote. Some of the tax breaks that had been promised were reduced or abandoned, as economic growth and consumer spending slumped.\n  Harvard Endowment's 8.1% Return Disappoints\n  The school has already made huge changes in how its $37.1 billion endowment is run, including appointing a new chief executive, N.P. Narvekar.\n  But that wasn't enough, and Mr. Narvekar described the performance as ''disappointing and not where it needs to be.''\n  From The Times's Geraldine Fabrikant:\n  That return significantly trails the mean one-year return of 12.7 percent for more than 400 institutions tracked by Cambridge Associates, which manages money for many nonprofit organizations. And while many of the nation's largest endowments have yet to report returns, this month the Massachusetts Institute of Technology posted a 14.3 percent return for its $14.8 billion endowment.\n\n\n\n","239":"BEIJING -- What worries you about the coming world of artificial intelligence?\nToo often the answer to this question resembles the plot of a sci-fi thriller. People worry that developments in A.I. will bring about the ''singularity'' -- that point in history when A.I. surpasses human intelligence, leading to an unimaginable revolution in human affairs. Or they wonder whether instead of our controlling artificial intelligence, it will control us, turning us, in effect, into cyborgs. \n  These are interesting issues to contemplate, but they are not pressing. They concern situations that may not arise for hundreds of years, if ever. At the moment, there is no known path from our best A.I. tools (like the Google computer program that recently beat the world's best player of the game of Go) to ''general'' A.I. -- self-aware computer programs that can engage in common-sense reasoning, attain knowledge in multiple domains, feel, express and understand emotions and so on.\u00a0\n  This doesn't mean we have nothing to worry about. On the contrary, the A.I. products that now exist are improving faster than most people realize and promise to radically transform our world, not always for the better. They are only tools, not a competing form of intelligence. But they will reshape what work means and how wealth is created, leading to unprecedented economic inequalities and even altering the global balance of power.\n  It is imperative that we turn our attention to these imminent challenges.\n  What is artificial intelligence today? Roughly speaking, it's technology that takes in huge amounts of information from a specific domain (say, loan repayment histories) and uses it to make a decision in a specific case (whether to give an individual a loan) in the service of a specified goal (maximizing profits for the lender). Think of a spreadsheet on steroids, trained on big data. These tools can outperform human beings at a given task.\n  This kind of A.I. is spreading to thousands of domains (not just loans), and as it does, it will eliminate many jobs. Bank tellers, customer service representatives, telemarketers, stock and bond traders, even paralegals and radiologists will gradually be replaced by such software. Over time this technology will come to control semiautonomous and autonomous hardware like self-driving cars and robots, displacing factory workers, construction workers, drivers, delivery workers and many others.\n  Unlike the Industrial Revolution and the computer revolution, the A.I. revolution is not taking certain jobs (artisans, personal assistants who use paper and typewriters) and replacing them with other jobs (assembly-line workers, personal assistants conversant with computers). Instead, it is poised to bring about a wide-scale decimation of jobs -- mostly lower-paying jobs, but some higher-paying ones, too.\n  This transformation will result in enormous profits for the companies that develop A.I., as well as for the companies that adopt it. Imagine how much money a company like Uber would make if it used only robot drivers. Imagine the profits if Apple could manufacture its products without human labor. Imagine the gains to a loan company that could issue 30 million loans a year with virtually no human involvement. (As it happens, my venture capital firm has invested in just such a loan company.)\n  We are thus facing two developments that do not sit easily together: enormous wealth concentrated in relatively few hands and enormous numbers of people out of work. What is to be done?\n  Part of the answer will involve educating or retraining people in tasks A.I. tools aren't good at. Artificial intelligence is poorly suited for jobs involving creativity, planning and ''cross-domain'' thinking -- for example, the work of a trial lawyer. But these skills are typically required by high-paying jobs that may be hard to retrain displaced workers to do. More promising are lower-paying jobs involving the ''people skills'' that A.I. lacks: social workers, bartenders, concierges -- professions requiring nuanced human interaction. But here, too, there is a problem: How many bartenders does a society really need?\n  The solution to the problem of mass unemployment, I suspect, will involve ''service jobs of love.'' These are jobs that A.I. cannot do, that society needs and that give people a sense of purpose. Examples include accompanying an older person to visit a doctor, mentoring at an orphanage and serving as a sponsor at Alcoholics Anonymous -- or, potentially soon, Virtual Reality Anonymous (for those addicted to their parallel lives in computer-generated simulations). The volunteer service jobs of today, in other words, may turn into the real jobs of the future.\n  Other volunteer jobs may be higher-paying and professional, such as compassionate medical service providers who serve as the ''human interface'' for A.I. programs that diagnose cancer. In all cases, people will be able to choose to work fewer hours than they do now.\n  Who will pay for these jobs? Here is where the enormous wealth concentrated in relatively few hands comes in. It strikes me as unavoidable that large chunks of the money created by A.I. will have to be transferred to those whose jobs have been displaced. This seems feasible only through Keynesian policies of increased government spending, presumably raised through taxation on wealthy companies.\n  As for what form that social welfare would take, I would argue for a conditional universal basic income: welfare offered to those who have a financial need, on the condition they either show an effort to receive training that would make them employable or commit to a certain number of hours of ''service of love'' voluntarism.\n  To fund this, tax rates will have to be high. The government will not only have to subsidize most people's lives and work; it will also have to compensate for the loss of individual tax revenue previously collected from employed individuals.\n  This leads to the final and perhaps most consequential challenge of A.I. The Keynesian approach I have sketched out may be feasible in the United States and China, which will have enough successful A.I. businesses to fund welfare initiatives via taxes. But what about other countries?\n  They face two insurmountable problems. First, most of the money being made from artificial intelligence will go to the United States and China. A.I. is an industry in which strength begets strength: The more data you have, the better your product; the better your product, the more data you can collect; the more data you can collect, the more talent you can attract; the more talent you can attract, the better your product. It's a virtuous circle, and the United States and China have already amassed the talent, market share and data to set it in motion.\n  For example, the Chinese speech-recognition company iFlytek and several Chinese face-recognition companies such as Megvii and SenseTime have become industry leaders, as measured by market capitalization. The United States is spearheading the development of autonomous vehicles, led by companies like Google, Tesla and Uber. As for the consumer internet market, seven American or Chinese companies -- Google, Facebook, Microsoft, Amazon, Baidu, Alibaba and Tencent -- are making extensive use of A.I. and expanding operations to other countries, essentially owning those A.I. markets. It seems American businesses will dominate in developed markets and some developing markets, while Chinese companies will win in most developing markets.\n  The other challenge for many countries that are not China or the United States is that their populations are increasing, especially in the developing world. While a large, growing population can be an economic asset (as in China and India in recent decades), in the age of A.I. it will be an economic liability because it will comprise mostly displaced workers, not productive ones.\n  So if most countries will not be able to tax ultra-profitable A.I. companies to subsidize their workers, what options will they have? I foresee only one: Unless they wish to plunge their people into poverty, they will be forced to negotiate with whichever country supplies most of their A.I. software -- China or the United States -- to essentially become that country's economic dependent, taking in welfare subsidies in exchange for letting the ''parent'' nation's A.I. companies continue to profit from the dependent country's users. Such economic arrangements would reshape today's geopolitical alliances.\n  One way or another, we are going to have to start thinking about how to minimize the looming A.I.-fueled gap between the haves and the have-nots, both within and between nations. Or to put the matter more optimistically: A.I. is presenting us with an opportunity to rethink economic inequality on a global scale. These challenges are too far-ranging in their effects for any nation to isolate itself from the rest of the world.\n  Follow The New York Times Opinion section on Facebook and Twitter (@NYTopinion), and sign up for the  Opinion Today newsletter. \n\n\n\n","241":"When Prince Harry and Meghan Markle say \"I do\" at their royal wedding next week, online viewers tuning into the Sky News stream will not have to guess the names of international celebrities and British nobility in attendance. Instead, the U.K. broadcaster will use artificial intelligence to identify famous guests as they\u00a0make their grand entrances at St. George's Chapel at Windsor Castle - displaying the\u00a0invitees'\u00a0names and details about how they are connected to the royal couple.\u00a0\nDubbed \"Who's Who Live,\" Sky News announced the live-stream service last week in partnership with Amazon.com and several data and engineering firms. As the 600 guests enter the chapel, Sky News will\u00a0highlight notable attendees using Amazon Rekognition, a cloud-based technology that can recognize and compare faces in images and video using artificial intelligence.\n(Amazon.com chief executive Jeffrey P. Bezos owns The Washington Post)\nAlong with\u00a0identifying the wedding guests, the live-stream service will also show facts about them, Sky News said, using captions and on-screen graphics through the company's app. The data will be displayed alongside the\u00a0video of the procession into the chapel.\n\"This new functionality allows Royal Wedding viewers greater insight into one of the biggest live events of the year, wherever they are,\"\u00a0David Gibbs, director of digital news and sports products at Sky News, said in a\u00a0news release. \"We're excited by the software's potential and ability to give audiences new ways of consuming content.\"\nThe royal wedding is expected to command a massive global audience. More than 5,000 members of the media\u00a0have been accredited to cover the event. According to the royal spokesman for Kensington Palace, there will be no official guest list, and no heads of state were invited, not even British Prime Minister Theresa May. All the invitees \"have a direct relationship with the couple,\" the spokesman said in a briefing last week at Buckingham palace.\nWhile guests are probably aware the wedding will be widely covered by the news media, lowering their expectations of privacy, it is unclear\u00a0whether Sky News has informed them their recorded arrivals will be fed into a facial recognition system. Sky News did not immediately respond to a request for comment.\nThe celebrity recognition feature's debut could pave the way for its use at other high-profile events\u00a0that often invite the audience to interact on social media. John Motz, the chief technology officer at GrayMeta, one of the technology firms behind the app, said in a statement the service gives users a chance to participate as the event unfolds and exemplifies how the future of live events can be experienced by people at home.\nThe ceremony and reception will take place on May 19.\n","242":"David L. Waltz, a computer scientist whose early research in information retrieval provided the foundation for today's Internet search engines, died on Thursday in Princeton, N.J. He was 68.\nThe cause was brain cancer, his wife, Bonnie Waltz, said. He died at the University Medical Center at Princeton.\nDuring his career as a teacher and a technologist at start-up companies as well as large corporate laboratories, Dr. Waltz made fundamental contributions to computer science in areas ranging from computer vision to machine learning.\u00a0\nOne signal achievement was the development of a basic technique that makes it possible for computers to render three-dimensional scenes accurately. As part of his Ph.D. dissertation at the Massachusetts Institute of Technology, he developed an algorithm that could extract a rich three-dimensional understanding of a scene from two-dimensional line drawings with shadows.\nThe 3-D research was seminal in the fields of computer vision and artificial intelligence. Known as ''constraint propagation,'' the technique is now used in industry for solving problems like route scheduling, package routing and construction scheduling.\nAt M.I.T., Dr. Waltz was taught by Marvin Minsky, a pioneer in artificial intelligence. Dr. Waltz graduated in 1972, then taught computer science at the University of Illinois at Urbana-Champaign and, later, at Brandeis University in Massachusetts.\nBut it was as a member of a group of researchers at the Thinking Machines Corporation, in Cambridge, Mass., that Dr. Waltz made his breakthrough in information retrieval. Thinking Machines was an early maker of massive, parallel supercomputers, and by joining the company, in 1984, Dr. Waltz gained access to computers that by '80s standards held vast amounts of fast random-access memory, up to 512 megabytes.\n''For the first time it was possible to use simple algorithms with lots and lots of data,'' said Brewster Kahle, a computer scientist who directs the Internet Archives and was one of the Thinking Machines researchers.\nAccess to that database was crucial to Dr. Waltz's development of a technique known as memory, or ''case based,'' reasoning. It revolutionized the way computers recognized characters, words, images and later, even voices. Before, a computer had to follow a set of programmed rules to arrive at recognition (it's an ''i'' if there's a dot, for example). Now it could comb through its vast memory and deduce what the image was by comparing it to what had been stored there.\nThe technique transformed the field of artificial intelligence and also greatly advanced voice recognition and machine vision technology. And it led directly to the ''big data'' and data-science approaches that are essential tools for search engines, allowing them to sift through large collections of information to improve accuracy and relevance.\n''He was a real pioneer,'' said Peter Norvig, Google's director of research. ''The two main changes that got us modern A.I. were probabilistic reasoning and using memory rather than rules.''\n''I don't know if Larry and Sergey read his papers directly,'' he added, referring to Google's founders, Larry Page and Sergey Brin, ''but the idea, filtered through however many people, was certainly a key.''\nWhile at the University of Illinois, Dr. Waltz turned to the field of natural language understanding, a component of artificial intelligence involving the interpretation of language. With support from the Office of Naval Research, he built a question-answering system called Planes and explored the use of neural networks in language processing.\nIn another early project, a Thinking Machines group led by Dr. Waltz designed an information retrieval system that made it possible for a remote user to gain access to a supercomputer and then be able to search through large volumes of documents.\nThe system, known as Wide Area Information Server, or WAIS, and designed in cooperation with the Dow Jones Corporation, Apple Computer and KPMG Peat Marwick, was not the first information retrieval system. But it was innovative in enabling the user to uncover connections between seemingly disparate documents. For example, the WAIS system was able to give an early warning of the Chernobyl disaster in 1986 after it discovered a report of an abnormal radiation reading in Scandinavia, according to W. Daniel Hillis, the co-founder of Thinking Machines.\nWAIS also introduced techniques to narrow a document search. It was followed by other search systems, like Veronica, Gopher and Archie, which predated the search engines offered today by Google, Microsoft, Yahoo and other companies.\nAfter leaving Thinking Machines in 1993, Dr. Waltz joined the NEC Research Institute in Princeton, where he was president from 2000 to 2002. He left to help create the Center for Computational Learning Systems at Columbia, where he was director.\nThe center has worked with Con Edison of New York in developing systems that can predict power failures and thus enhance maintenance of the electric power grid. Researchers there are also working on creating a computer-based system to give people with epilepsy early warnings of seizures. The technique involves mining data generated by electrodes implanted in patients.\nDr. Waltz earlier was instrumental in establishing interdisciplinary research centers: the Beckman Institute at the University of Illinois, and the Volen National Center for Complex Systems at Brandeis.\nDavid Leigh Waltz was born in Boston on May 28, 1943, to Maynard C. Waltz and the former Lubov Leonovich. His father, a physicist, worked at M.I.T.'s Radiation Laboratory during World War II and later at Bell Labs. Dr. Waltz obtained both undergraduate and graduate degrees at M.I.T. in electrical engineering. He lived in Princeton.\nBesides his wife, he is survived by a brother, Peter; a son, Jeremy; a daughter, Vanessa Waltz, and a granddaughter.\n","243":" If you ran into him online, you might first be struck by the kid's prodigious memory. He calls himself \"SmarterChild\" and can recite a litany of facts -- this season's entire baseball lineup, every word in the dictionary, and the weather in major cities across the country. \n But other queries provoke odd responses.  \n A question about SmarterChild's age returns, \"One year, one month, 11 days, 16 hours, 7 minutes, 47 seconds!\" Asking where he lives gets, \"In a clean room at a high-tech hosting facility in California.\" \n SmarterChild, a computer program, is part of a new species of \"chatterbots\" that are renewing debate about the extent to which computers can achieve intelligence. \u00a0\n The electronic personalities of this generation use the vast repository of information on the World Wide Web as their memory bank, not just some rigid database. To answer questions about baseball, for instance, SmarterChild scours the Web site of SportsTicker Enterprises LP; for spelling, it goes to the American Heritage Dictionary online; for the weather, it visits Intellicast.com. \n The company that conceived SmarterChild, Active Buddy Inc., created the bot as a marketing tool that would engage people in conversation and then tell them about various products or services. \n Other companies have begun using these systems to help with customer service or Web searching. Eventually, however, some believe that technicians will be able to turn programs like SmarterChild into more intelligent systems. That is, the network will naturally begin to evolve into a sort of global brain, one made up of the constellation of the roughly 1 billion computers comprising the Internet. \n Such a system might automatically offer advice on city planning based on demographic patterns or recommend that printing cease on a novel that hasn't sold a copy in weeks. It might even pinpoint the outbreak of a disease based on the health complaints people are searching for information about online. \n The idea that computers might serendipitously comb through troves of data to produce useful bits of information faces numerous political, economic and social hurdles, such as privacy concerns, not to mention enormous technical obstacles. And skeptics abound. \n Push Singh, who runs an artificial intelligence project for the Massachusetts Institute of Technology's fabled Media Lab, scoffs at the notion that such AI systems are likely to develop any time soon. \n \"Intelligence,\" he said, \"is not a simple thing, and it's not going to arise accidentally.\" \n Scientists have worked to create an artificially intelligent agent for as long as there have been computers. Yet every revolution in power and processing speed has only pushed AI further into the future as science smacks up against the complex biology of intelligence. \n But the infinite nature of the Web echoes the infinite mystery of the brain, raising the possibility of success with artificial intelligence at some level. \n Singh said he recognizes that. \"The Web as it stands is not the future,\" he said. \"There will be something that comes after the Web, something that I'm sure will be built on AI technologies.\" \n Created by engineer Timothy Kay, SmarterChild began popping up in instant messaging systems last summer. Since then, close to 9 million people have talked to him. \n Chatterbots, which converse with people through real-time text messages, have existed on the Internet for years. Underneath their friendly exterior, they are basically databases built by humans that link typical questions to stock responses. \n SmarterChild is different. Its database is limited only by the reach of the Web. Scientists are beginning to capitalize on the way the global network converts \"knowledge,\" or at least reams of data, into a digital language computers can understand. \n \"The Internet starts to make things possible again,\" said Michael Kearns, a computer science professor at the University of Pennsylvania and former director of artificial intelligence at AT&T Labs. \n To be sure, SmarterChild often spits out gibberish and non sequiturs just like its predecessors. But its ability to access and digest online information represents a major step for artificial intelligence. \n So potent are the possibilities that researchers at a diverse group of academic, nonprofit and government-backed and corporate centers such as MIT, the World Wide Web (W3) Consortium, the Los Alamos National Laboratory and Microsoft Corp. are embarking on projects to tap information already available on the network. \n For the most part, bots like SmarterChild are able to talk only about certain established topics. But some have been able to reach a touchstone of artificial intelligence -- passing the Turing Test, in which researchers ask humans to guess whether they are communicating with a person or a machine. If people can't tell the difference, the machines are deemed to have passed the test. \n Some scientists believe that by fusing the many systems of the Internet, an artificial being with the combined knowledge of, say, Albert Einstein, Richard Nixon and Britney Spears could be born. \n But before that happens, the AI community must overcome two huge barriers. \n The first is that computers have a hard time reading Web pages because the files are labeled in different ways, some more unconventional than others. That's why Active Buddy programmers need to tell SmarterChild where to look for the weather; it would be a significantly more difficult task to let him find it. \n A group led by Tim Berners-Lee, creator of the Web and director of the W3 Consortium, hopes to fix some of that by assigning keywords or tags to text, sounds and images. The task of renaming pages, however, must be done manually and will take years to complete. \n Another wall that AI projects have hit is that while online entities like SmarterChild can regurgitate and process information more accurately and faster than any human, they lack common sense, a basic grounding of knowledge that is obvious to any young child. The computer mind, for instance, has had difficulty understanding concepts like \"once people die, they stop buying things\" or \"trees don't grow in cars.\" \n MIT's Singh and others are trying to create a \"knowledge base\" that can be implanted into AI projects by using human volunteers. People who want to help the project, called the Open Mind Initiative, can go to its Web site and type whatever comes to mind (and makes sense) when they are flashed certain photographs, diagrams or sentences. \n Another project, led by researchers at the Free University of Brussels in a loose collaboration between nearly a dozen scientists, psychologists and biologists around the world, attempts to help computers understand relationships between people, objects and ideas by studying how humans access information online. \n It all began one evening in 1999, when a graduate student named Johan Bollen created an early version of software that gives Web sites the ability to automatically reorganize the content on their pages. \n Using the \"cookies\" that sites use to identify and track Web users, the program analyzes the routes people take to get information and tries to simplify them. The software mimics the human brain, strengthening, dissolving and even creating hyperlinks on a page based on patterns of use; the Web pages act like neurons, and the links act like the synapses between them. \n If it finds that people often go from A to B to C, it will create a path directly from A to C. For instance, if many people are hopping from the main Yahoo page to the Finance section and then to the page about WorldCom Inc., the program might create a new link from the main Yahoo page to the telecommunications company's Web site. \n \"It's about helping people find the connections between information,\" said Bollen, 30, now an assistant professor at Old Dominion University in Norfolk. \"You have so much junk on the World Wide Web there's no guarantee that the information is good and fits what you desire. What I'm talking about is a Web that bends itself to the actions of its users.\" \n Bollen's technology is already being used as part of a library search engine at Los Alamos called the Active Recommendation Project. The program can offer people a list of links that may include relevant material, even if the links don't contain the word the user entered. The more people use the system, the smarter it becomes. \n One of the central ideas of researchers who believe in the vision of a \"global brain\" is that the earth can be seen as a single organism with many complementary parts that must work together to succeed. \n Francis Heylighen, a professor at the Free University who oversaw Bollen's initial project, likes to use insects such as ants, bees and termites as examples. \"Individually dumb, but capable of surprisingly intelligent behavior when functioning as a group,\" he said. \n The ant analogy is exactly what frightens some of his peers. \n They worry that a \"hive mind\" might stifle freedom and individuality. Already, some efforts to reorganize Web sites based on the preferences of the majority end up drowning out the voices of the minority. \n Others are concerned about privacy issues, that computer networks will become all-knowing. Still others worry about the Internet becoming all-powerful. \n Los Alamos scientist Luis Rocha, who is heading up the digital libraries project, said he doesn't know whether the Internet could ever become a malevolent, intelligent, self-aware being. \n Still, he said: \"A lot of times science is moved by far-fetched goals. You aim for the moon and hit London. And a lot of times, that's somewhere you haven't gone before.\" \nStaff researcher Richard A. Drezen contributed to this report. \n","244":"PALM SPRINGS, Calif. -- \"I get invited to everything, and everywhere I go, everyone wishes they had my job. But that's not true here,\" said Pablos Holman, a self-described futurist and inventor who has worked on lasers that kill mosquitoes and machines that suppress hurricanes.\n\"There's geniuses everywhere,\" he said, motioning to the pair talking next to him, the theoretical physicist Lisa Randall and the computer scientist Stephen Wolfram. \"I don't even register on this scale.\"\nWe're at Mars, an exclusive three-day conference at a midcentury-modern hotel here in the California desert run by Amazon and its founder, Jeff Bezos, for some of the world's most successful geeks. For its first two years, Mars was largely secret; the most prominent image that leaked  was a photoof Mr. Bezos piloting a 13-foot robot last year. This year, Amazon lifted the veil and invited a handful of reporters into Mr. Bezos's brainiac pow-wow.  \u00a0\nAmazon is eager to buoy its reputation in artificial intelligence, a focus of the conference, amid tight competition with Google. And Mr. Bezos, who according to Bloomberg is the   world's richest person  with a roughly $130 billion fortune, is growing more comfortable in the spotlight, particularly as a modern-day Renaissance man. His side projects now include The Washington Post, the space firm Blue Origin and a 10,000-year clock in a West Texas mountain that ticks once a year.\nFor Mars, Amazon and Mr. Bezos handpick the roughly 200 attendees, most from the fields of artificial intelligence, robotics and space. There are astronauts,  philosophers, rocket scientists, Nobel prize winners and gravitational-wave astrophysicists. Attendance and everything else -- room service, the crab legs at breakfast, seaweed wraps at the spa -- were free. (The New York Times paid my way.)\nMr. Bezos was visible throughout, sitting front and center at presentations and posing for photos with attendees in front of a Blue Origin space capsule. His booming laugh was easy to pick out in the crowd. \nWhen I arrived at the Parker Palm Springs hotel where the conference was held, three separate conference workers tried to carry my bag. In my room, the lights were dimmed, a cheese plate was waiting, and an Amazon Echo was playing bossa nova. A note said to keep the Echo. (I didn't.) Atop a collection of more high-end swag was a note from Mr. Bezos: \"We'll consider this gathering a big success if you find something inspiring.\" \nWhile there were dozens of multimillionaires among the attendees, many savored the luxury. \"This is quite opulent compared to what I'm used to,\" said Ann Karagozian, a rocket scientist from the University of California, Los Angeles. \"Definitely not any academic conferences.\"\nAt dinner, seats were preassigned. After dark, free booze flowed from makeshift bars and attendees lit up high-end cigars. Mr. Bezos held court around the fire pit Sunday night, tumbler of whiskey in hand. He wore the down Patagonia jacket given to each guest -- a style that has become a sort of tech-industry uniform --  with jeans and cowboy boots. One attendee described the get-up as Silicon Valley meets West Texas, where Blue Origin has its main launch site. \nMornings were reserved for show and tell. About a dozen attendees presented on novel businesses or scientific breakthroughs, from new techniques for studying supermassive black holes to computer chips implanted in brains that can resolve symptoms of Parkinson's disease. Rich Mahoney, head of a \"powered-clothing\" start-up called Seismic, showed off leotards with built-in sensors and electric motors  to mimic muscles -- designed to help elderly or disabled wearers stand from chairs.\n\"Intelligent, wearable strength,\" he said. \"This is a new clothing paradigm.\"\nThere was one clear rock star: A yellow four-legged robot named SpotMini that strutted back and forth on stage as a sea of smartphone cameras focused in. It was the newest invention from Boston Dynamics, the Defense Department -funded firm that Google recently sold to SoftBank. After the presentation, Mr. Bezos and SpotMini led the crowd to lunch. \nTo get to lunch, attendees passed under a colorful spiked 16-foot plexiglass archway coated in a reflective dichroic film, which was originally designed to protect spacesuits from cosmic radiation -- and now costs about $125 per square foot. \n\"Four people have proposed to their girlfriends underneath it,\" said Davis McCarty, the Chicago artist who built the gate in 2016 for Burning Man, the arts festival in the Nevada desert. \nThroughout, Amazon executives referred to repeat guests as alumni or \"returning campers.\" Attendees seemed universally smitten with the conference; some suggested they were a part of history. \nKen Goldberg, a roboticist from the University of California, Berkeley, compared Mars to Athens 2,000 years ago, showing Raphael's fresco \"The School of Athens\" during a presentation. \"People would be hanging out, discussing ideas, having arguments, and a big topic at the time was space,\" he said.\n\"It's like a true salon of its time,\" said Brogan BamBrogan, an  entrepreneur with a walrus mustache who now runs Arrivo, which aims to build high-speed transportation networks using levitating cars in highway medians. \nAt lunch, Mr. Bezos played beer pong, minus the beer, against a robotic arm. (He lost.) Across the lawn, attendees played a different robotic arm in table tennis, and SpotMini -- piloted by a human -- mugged for cameras and snatched phones with the arm that extended from its neck. \nSome attendees said they were so impressed by the guest list they were unsure they belonged. The serial entrepreneur Dean Kamen, best known for inventing the Segway, said, \"You feel like you're walking around here as an intellectual midget.\"\nIn the afternoon, we had a choice of activities, from falconry to Navy Seal training to sausage making. Group meditation was popular. On Tuesday, a collective \"ohhhmmm\" rang out across the hotel grounds.  \nEighteen of us, including me, flew aerobatic planes. Most who signed up were eager to hop into the cockpit; a few later had to use the barf bag. Many were paired up for \"dogfights\" in which they chased each other in the sky. We were allowed to steer the plane for periods; the pilot in back also had a hand on the controls.\nIn one dogfight, Takeo Kanade, 72, a professor at Carnegie Mellon University and robotics pioneer, vanquished his opponent, Aaron Dollar, 40, an up-and-coming roboticist at Yale. \n\"Old-man wisdom won,\" Mr. Kanade said after getting out of the plane, still wearing a flight suit with his pilot call sign for the day Velcroed on: \"Wolfman.\" Mr. Dollar's call sign was \"Tinkerbell.\"\nOn Tuesday at about 5:45 a.m., more than a dozen attendees gathered on  one lawn and craned their necks toward the sky, looking for the Humanity Star, a satellite covered in mirrors that one of them, Peter Beck, had launched into orbit. \nThe pre-dawn group included Bob Smith, chief executive of Blue Origin, and Adam Savage, co-host of the former Discovery show \"MythBusters.\" About 90 seconds after the satellite was supposed to be glittering across the sky, nothing had appeared. Then, for a moment, there was a flash in the sky. \"That was it!\" someone shouted. Then another flash. \n\"It reflects the sunlight from the sun and it strobes the earth like a giant disco ball,\" said Mr. Beck, a New Zealander with curly hair who founded the space firm Rocket Lab. \"The point is to really get people to look up and have an overview effect, and realize they're standing on a rock in the middle of the universe.\"\nPHOTO: Replicas of costumes from \"2001: A Space Odyssey,\" right, and \"Alien\" that were created by Adam Savage of \"MythBusters.\" (PHOTOGRAPH BY Jack Nicas\/The New York Times FOR THE NEW YORK TIMES)\n","245":"MOUNTAIN VIEW, Calif. -- In its quest to maintain a United States military advantage, the Pentagon is aggressively turning to Silicon Valley's hottest technology -- artificial intelligence.\nOn Wednesday, Secretary of Defense Ashton B. Carter made his fourth trip to the tech industry's heartland since being named to his post last year. Before that, it had been 20 years since a defense secretary had visited the area, he noted in a speech at a Defense Department research facility near Google's headquarters. \n  The Pentagon's intense interest in A.I. -- and by connection the Silicon Valley companies specializing in that technology -- has grown out of the ''Third Offset'' strategy articulated by Mr. Carter last fall. Concerned about the re-emergence of China and Russia as military competitors, he stated that computer-based, high-tech weapons would give the American military an edge in the future.\u00a0\n  Third Offset is a reference to two earlier eras when Pentagon planners turned to technology to compensate for a smaller military. In the 1950s, President Eisenhower emphasized nuclear weapons as a deterrence to larger Warsaw Pact armies. A second ''offset'' occurred in the 1970s and '80s when military planners turned to improved technology in conventional weapons to again compensate for smaller numbers.\n  This time, Mr. Carter acknowledged, the United States faces significant challenges in translating civilian innovation into a military advantage, since the country will neither control nor determine the path of artificial intelligence.\n  ''That's different than 30 or 40 or 50 years ago when we expected to control the pace of technology,'' he said on Wednesday in a speech at the Pentagon's nearly year-old Defense Innovation Unit Experimental facility, otherwise known by the techie acronym DIUx. ''That's not true anymore, but we still can stay the best military with respect to applications of A.I.''\n  In recent weeks, Deputy Defense Secretary Robert O. Work has repeatedly emphasized the importance of A.I.-related technologies that he believes will help create a new class of ''Iron Man''-style fighters armed with increasingly smart weapons.\n  He has invoked the concept of ''Centaur Warfighting'' -- systems that combine A.I. with the capabilities of humans, resulting in faster responses than humans alone could achieve.\n  The Defense Department will need Silicon Valley's help with that technology. And Mr. Carter indicated that bridge building with local companies was a key reason the new Pentagon office he visited on Wednesday will now report directly to him.\n  The Defense Department has always had a close relationship with some of tech's biggest companies. The Hewlett-Packard co-founder David Packard, for example, served as deputy secretary of defense in the Nixon administration.\n  Many companies still count the Pentagon and intelligence agencies among their biggest customers. A venture fund backed by the Central Intelligence Agency has been investing in tech companies since the dot-com boom of the late 1990s.\n  But younger Silicon Valley executives, particularly those in A.I. research, have shown little interest in seeing their new technology used by the military.\n  The depiction of defense weapons that fire without a human operator raised alarms among arms control advocates and some military strategists who worry that the line between offensive and defensive uses of smart weapons will be difficult to maintain.\n  ''We need to figure out where to draw the line and we need to stay on the right side of it,'' said Stuart J. Russell, an A.I. specialist at the University of California, Berkeley who is a leader in a movement to ban autonomous weapons.\n  In fact, turning over killing decisions to machines is seen by some technologists and military strategists as inviting a new and possibly destabilizing arms race.\n  ''I'm not as confident that we can clearly delineate between offensive and defensive weapons, in general,'' said Paul Scharre, a weapons analyst at the Center for a New American Security, a Washington-based policy group. ''If there was an easy way to do that, nations would have agreed long ago to only build 'defensive' weapons.''\n  Despite skepticism among some in the tech community, the Pentagon has played a key role in one of the best-known examples of A.I., the self-driving vehicle concepts now championed by companies like Google and Uber.\n  Beginning in 2004, Darpa, the Pentagon's advanced research agency, tried to speed progress in autonomous vehicles by hosting a series of three autonomous vehicle ''Grand Challenges.'' The effort set off a wave of commercial research and development, but it fell short of a goal to remove United States soldiers from hazardous roles on the battlefield.\n  Military contractors say that self-driving technology has now advanced to the point where a human soldier could sit in the driver's seat in the last vehicle in a truck convoy, safely controlling a series of vehicles. Despite technology demonstrations, however, the American military has not yet committed to converting existing trucks to such a system.\n  Mr. Carter's background could provide him with cachet among Silicon Valley's tech elite. He received a doctorate in theoretical physics from Oxford University and has been a lecturer at the Stanford Freeman Spogli Institute for International Studies and a visiting fellow at the university's Hoover Institution, just a few miles from where he gave his speech on Wednesday.\n  In addition to changing the reporting structure of the DIUx group, which occupies a sprawling facility opposite a NASA wind tunnel and adjacent to Silicon Valley's busy Highway 101, Mr. Carter pointed to a range of initiatives that he was undertaking in an effort to make the Defense Department's culture more like that of Silicon Valley.\n  ''We're taking a page straight from the Silicon Valley playbook, we're iterating rapidly to make DIUx even better,'' he said. Borrowing a bit of tech industry lingo, he said the Pentagon planned to create additional organizations in other regions of the country that the Defense Department considered to be ''innovation hubs.'' He said the next office would be opened in the Boston area, another hotbed of A.I. research centered on Harvard and M.I.T.\n  ''I've been pushing the Pentagon to think outside our five-sided box and invest aggressively in change and innovation,'' he said, adding that the Defense Department's proposed research and development budget for the 2017 fiscal year was more than double the combined R.&D. spending of Apple, Google and Intel in the last year.\n\n\n\n","246":"I am concerned that my childhood dreams may turn into a nightmare.\nStanley Kubrick's \"2001: A Space Odyssey,\" which I first saw in 1968 at seven years of age, sparked my fascination with computers and artificial intelligence, and I have since focused my academic studies and career pursuits within this area.\nFast-forward to today. I still believe that A.I. and automation can keep bringing good to the world. Its effects on middle-skill workers, in America's industrial Midwest and elsewhere, however, worry me.\u00a0\nIt has been suggested that the significant job loss and career destruction illuminated by the 2016 presidential election are a result of bad trade deals and corporate greed. I disagree. It's much more likely a result of computers, including significant advances in A.I., which have allowed us to optimize our economy as never before. Already, they have permitted the global economy to function as if it were in your backyard. In the next decade, they will take over a vast array of routine work, in new industries, affecting even more workers.\nThe economic dislocations that computers and artificial intelligence have unleashed have been vastly underestimated. And we are just in the early days.\nThink about commercial trucking in America. A.I.-based self-driving truck technology for travel on interstate highways is already technically feasible. Today, about five million drivers are employed in this industry. Even a 20 percent reduction in this work force over the next 15 years equates to a million lost jobs.\nOne of capitalism's bedrock promises - one that dates back to Adam Smith - is that competition in the free market benefits society at large. Somewhere along the line, intoxication with efficiency caused us to lose sight of that principle at the expense of workers. Getting back to that promise will require policy changes and a renewal of forgotten values.\nThe raw, widespread anger we saw during the recent election - and the unexpected swing of several industrial states from reliably blue to red - reflects in large part the intense despair that many middle-skill workers feel as they see their families' economic prospects fade and social conditions deteriorate.\nTrade and immigration have become boogeymen, while technological advances and the huge efficiency gains they bring truly underpin the \"hollowing out\" of the middle class behind the scenes. Industrial automation has been displacing workers for decades - particularly those doing repetitive, lower-skill work.\nExponential gains in computing power, along with innovations in software, analytical techniques and the rise of Big Data, mean that many white-collar occupations are due for disruption by machines too. According to a 2013 study by two Oxford University professors, almost half of all jobs in the United States are susceptible to \"computerization\" over the coming decade or two.\nWith so many at risk of being pushed aside by Smith's invisible hand, no one should be surprised if people decide to push back.\nUnless we change course now, we can expect many more Everytowns to become Allentowns.\nWhile the phenomenon I describe is understood by academics and technocrats and, more important, keenly felt by millions of Americans, no one seems to know how to fix this. As Harvard's Clayton Christensen and Derek van Bever argue in \"The Capitalist's Dilemma,\" orthodox finance dictates that investment by corporations to create jobs tends to be the third-best option behind substitutive innovation (which tends not to create new jobs) and efficiency innovation (which almost always results in job losses). As we have seen, companies today increasingly prefer not to employ humans, if possible.\nWhat's the answer? Most of the commonly proposed fixes are unlikely to resolve the issue. For example, an increasing focus on education, while necessary (the number of American college graduates falls short of labor demand by about 300,000 annually), is not sufficient; what good is a college degree if there are no opportunities to use it? The same could be asked of worker retraining programs.\nSome promote the promise of the so-called gig or sharing economy - flexibility! convenience! - but I don't buy it. Working as an Uber driver might help make ends meet in the short term, but the experience actually causes skills to atrophy over time. When I speak of jobs being lost or saved, what I really mean is careers: the kind of work that provides skill development, meaningful wage growth prospects and a reasonable likelihood that one's work won't be automated away. The types of careers, in other words, that have traditionally formed the foundation of healthy communities.\nSome argue that history teaches us not to worry: The Industrial Revolution was disruptive, but it created jobs eventually, didn't it? Sure. Will the Information Revolution do the same? Nobody knows. Creative destruction has become a hallowed concept in the American boardroom, but let's be honest: Not all destruction is creative in the Schumpeterian sense. As we are seeing, it might just create a wasteland where middle-class communities once thrived.\nMr. Christensen and Mr. van Bever suggest rebooting the capitalist system through a series of policy incentives aimed at making investment capital more \"patient,\" evolving the curriculums at business schools, realigning corporate strategy and resource allocation and freeing managers to focus on long-term value creation.\nThese ideas are worthy of support. I would go further and recommend aligning corporate tax policy with the goal of creating and fostering careers by skewing tax rates to favor businesses that create opportunities meeting the criteria described above (skill development, wage growth and resistance to automation).\nGiven the nature of the problem we face, companies able to create careers through innovation should reap significantly greater policy rewards than companies focused primarily on driving down costs - and eliminating careers.\nAnd business leaders must wake up to the scope and difficulty of the challenges we face. It's going to require a shift in values. The hollowing out of the middle-skill work force is a classic tragedy of the commons, and few in leadership positions today feel a personal responsibility to help address it. Unfortunately, the true costs associated with the upheaval have thus far been borne not by those in the C-suite, but predominantly by the hard-pressed American worker.\nA narrow emphasis on efficiency puts our entire system at risk. Moving forward, creating careers (rather than dead-end jobs) must be an equally valid goal. We cannot and should not try to stop the march of technological progress. But we have to redefine what progress really means.\nDavid Siegel is co-founder and co-chairman of Two Sigma Investments. \n","247":"Carnegie Mellon University plans to announce on Wednesday that it will create a research center that focuses on the ethics of artificial intelligence.\nThe ethics center, called the K&L Gates Endowment for Ethics and Computational Technologies, is being established at a time of growing international concern about the impact of A.I. technologies. That has already led to an array of academic, governmental and private efforts to explore a technology that until recently was largely the stuff of science fiction. \n  In the last decade, faster computer chips, cheap sensors and large collections of data have helped researchers improve on computerized tasks like machine vision and speech recognition, as well as robotics.\u00a0\n  Earlier this year, the White House held a series of workshops around the country to discuss the impact of A.I., and in October the Obama administration released a report on its possible consequences. And in September, five large technology firms -- Amazon, Facebook, Google, IBM and Microsoft -- created a partnership to help establish ethical guidelines for the design and deployment of A.I. systems.\n  Subra Suresh, Carnegie Mellon's president, said injecting ethical discussions into A.I. was necessary as the technology advanced. While the idea of ''Terminator'' robots still seems far-fetched, the United States military is studying autonomous weapons that could make killing decisions on their own -- a development that war planners think would be unwise.\n  ''We are at a unique point in time where the technology is far ahead of society's ability to restrain it,'' Mr. Suresh noted.\n  But at the same time, he said some people are a bit too optimistic about their claims of A.I. advances, particularly when it comes to autonomous vehicles.\n  Mr. Suresh said he personally did not think self-driving cars would be in widespread use in the next three years.\n  Last year, Carnegie Mellon drew national attention when a group of 36 technical staff members and four faculty members left to join a new self-driving car laboratory that Uber established in Pittsburgh. The company recently started testing self-driving cars around the city.\n  The Uber laboratory has been a sensitive spot for Carnegie Mellon. The field of artificial intelligence emerged in part at Carnegie Mellon in the 1950s in the work of faculty who developed software that showed how computer algorithms could intelligently solve problems.\n  University officials said the departing faculty have been replaced and 13 additional professors have been hired since the defections. They also said that between 2011 and 2015, Carnegie Mellon faculty and staff created 164 start-up companies.\n  University officials pointed to a partnership the school entered into last year with Boeing to use machine-learning techniques to analyze vast amounts of data generated by modern aircraft such as the Boeing Dreamliner.\n  The new center is being created with a $10 million gift from K&L Gates, an international law firm headquartered in Pittsburgh. It will draw from several academic disciplines and will initially add two faculty and three positions for graduate students. It will also establish a biennial conference on ethical issues facing the field.\n  K&L Gates is one of the nation's largest law firms. The Microsoft co-founder Bill Gates's father, William H. Gates Sr., was involved in the firm until his retirement in 1998. Peter J. Kalis, chairman of the law firm, said the potential impact of A.I. technology on the economy and culture made it essential that as a society we make thoughtful, ethical choices about how the software and machines are used.\n  ''Carnegie Mellon resides at the intersection of many disciplines,'' he said. ''It will take a synthesis of the best thinking of all of these disciplines for society to define the ethical constraints on the emerging A.I. technologies.''\n\n\n\n","248":"Artificial\u00a0intelligence researchers are closing in on a new benchmark for comparing the human mind and machine. On Wednesday, DeepMind, a research organization that operates under the umbrella of Alphabet, reported that a program combining two separate algorithms had soundly defeated a high-ranking professional Go player in a series of five matches.\nThe result, which appeared in the Jan. 27 edition of the journal Nature, is further evidence of the power created when a class of A.I. machine learning programs known as \"deep neural networks\" is combined with immense sets of data.\u00a0\nGo is seen as a good test for artificial intelligence researchers because it is more complex than chess, with a far larger range of possible positions. This makes strategy and reasoning in the game challenging.\nGo is played with round black and white stones, and two players alternately place pieces on a square grid with the goal of occupying the most territory. Until recently, software programs had not been able to do better than beat amateur Go players. In the Nature paper, however, engineers at DeepMind described the program AlphaGo that had achieved a 99.8 percent winning rate against other Go programs. It also swept five games from the European Go champion, Fan Hui.\nThe match between the AlphaGo program and Mr. Hui was in October, and the DeepMind program has continued to train since then, said Demis Hassabis, a researcher who founded DeepMind Technologies, which was acquired by Google in 2014. Google changed its name to Alphabet last year, though the company's traditional ad-based businesses still operate under the Google label.\n\"The machine has continued to get better; we haven't hit any kind of ceiling yet on performance,\" he said.\nThe Alphabet approach relies on the newest so-called \"deep learning\" approach combined with a more traditional type of algorithm known as a Monte Carlo, which is designed to exhaustively explore large numbers of possible combinations of moves. The researchers said they had also trained their program using input from human expert Go players.\nPossibly as intriguing as the DeepMind advance is the rivalry the research and the game has created with the public relations departments of companies like Alphabet, Microsoft and Facebook.\nThe day before the Alphabet paper was published, Facebook republished an earlier paper the company had posted on the arXiv.org web site. At the same time, Facebook issued blog posts from Yann LeCun, one of its artificial intelligence researchers, and one from the company chief executive, Mark Zuckerberg. The statement by Mr. Zuckerberg resulted in a swift response from one Facebook user that may express a deeper human concern than the narrow results of the research: \"Why don't you leave that ancient game alone and let it be without any artificial players? Do we really need an A.I. in everything?\" wrote Konstantinos Karakasidis.\nThose concerns are not likely to be heeded. In a blog post on Wednesday morning, Alphabet stated that, in an effort to reprise the winning IBM Deep Blue chess playing program that defeated the chess champion Garry Kasparov in 1996, Alphabet will soon match its AlphaGo program against Lee Sedol, the current Go champion. AlphaGo is scheduled to play a five-game match against Mr. Sedol in March.\nThere will be a $1 million prize for the winner, and Mr. Hassabis said that Alphabet would donate the prize to charity if AlphaGo wins. The match will be streamed live on YouTube.\nMr. Hassabis, who is a skilled chess player and has been a professional gamer as well, said that Go was a beautiful game, but that \"building an A.I. is also a human endeavor and a kind of ingenious one too. The reason games are used as a testing ground is that they're kind of like a microcosm of the real world.\"\n","249":"MENTION artificial intelligence and the image that most quickly springs to mind is an anthropomorphic automaton, a robot. It's a recurring metaphor that peaks at times of torrid technological change and angst about where technology is taking us.\nThe robot trope is riding high these days, in such books as ''The Rise of the Robots,'' and in movies like ''Ex Machina'' and ''Terminator Genisys.'' A recent cover of Foreign Affairs carried the headline, ''Hi, Robot.'' \n  A vision of robots threatening jobs and perhaps humanity itself is fueling dire warnings of future trouble -- ''summoning the demon,'' in the evocative phrase of the technologist-entrepreneur Elon Musk.\n  Yet the current obsession puts the mechanized cart before the algorithmic horse, steering attention away from the here-and-now promise and peril of A.I.\u00a0\n  Artificial intelligence is already all around us, but it's mostly software. Google search and ad targeting, movie and product recommendations on Netflix and Amazon, Apple's Siri digital assistant and IBM's Watson question-answering system are all animated by artificial intelligence. Fed by vast amounts of digital data from sources like the web, sensors, smartphones and genomics, the software actually learns, in its way. The more raw data that is ingested, the smarter the artificial intelligence becomes.\n  The physical world where robots exist, meanwhile, is multidimensional and messy. So it is in the digital realm where the machine-learning algorithms of artificial intelligence have made their greatest strides -- in tasks like facial recognition, language translation, prediction and decision making.\n  ''A lot of the problems are easier in software, so a lot of the action is around intelligent software -- softbots rather than robots,'' said Oren Etzioni, a computer scientist and executive director of the Allen Institute for Artificial Intelligence in Seattle.\n  An interesting reality check took place in June, when two dozen teams of leading robotics engineers gathered in Pomona, Calif., for a competition sponsored by the Pentagon's research agency. Their robots had to navigate mocked-up hazardous environments, like a contaminated nuclear plant, and do simple tasks -- walk up steps, turn a valve, operate a power drill. The chores would take a human five minutes, or 10 at most. The winning robot took 45 minutes.\n  Most struggled badly, falling down steps and taking long pauses to figure things out, even with remote control assistance. Turning a knob to open a door proved daunting for many. One young man in the audience observed, ''If you're worried about the Terminator, just keep your door closed.''\n  Robots will surely get better. Google's self-driving cars, for example, are impressive, but in heavy rain or snow, a human had better take the wheel. And robots with more limited ambitions are already taking over backbreaking work on factory floors and assisting surgeons for greater precision and control in operating rooms.\n  But the greatest progress has been in software, which is rapidly moving into the mainstream of the economy. So far, the largest commercial use of learning software has been in marketing, where it improves the odds of making a sale -- tailored marketing, targeted advertising and personalized product recommendations.\n  Big companies and start-ups are beginning to use learning software in higher-stakes decisions like medical diagnosis, crime prevention, hiring selections and loan approvals.\n  The idea is that an A.I. turbocharger can be applied to all kinds of decisions, making them smarter, fairer and less prone to human whim and bias. The goal could be saving money or saving lives.\n  Still, even enthusiasts have qualms.\n  Take consumer lending, a market where several start-ups are using big data and algorithms to assess the credit risk of borrowers. It's a digital-age twist on the most basic tenet of banking: Know your customer. By harvesting data from many sources, including social network connections, even observing how an applicant fills out online forms, lenders say algorithms can more accurately predict whether a candidate will repay than by simply looking at a person's credit history.\n  The promise is more efficient loan underwriting and pricing, saving consumers billions of dollars. But the new A.I. lending essentially amounts to a digital black box that pores over mountains of data. ''A decision is made about you, and you have no idea why it was done,'' said Rajeev Date, a former deputy director of the Consumer Financial Protection Bureau. ''That is disquieting.''\n  Dr. Herbert Chase, a professor at Columbia's College of Physicians and Surgeons, was asked as an unpaid researcher to try out IBM's Watson software when the company's scientists were adapting the technology for medicine. To try to stump Watson, Dr. Chase recalled a case decades earlier, when he made a correct diagnosis of adult rickets for a young woman, but only after extensive tests and months of being baffled. He fed Watson a few symptoms and the program, which ranks diagnoses by probability, swiftly replied and ranked adult rickets second.\n  Not perfect, but Dr. Chase was impressed that the technology was so close and so fast. He thinks Watson-like software, which can scan and mine many thousands of medical articles in a few seconds, will be part of the future of medicine, assisting doctors inundated with information and short of time. But his nagging concern is that over time ''we come to trust the technology too much, that it becomes the medical equivalent of blindly following a GPS system on your car down a dead end,'' Dr. Chase said.\n  This, of course, is a central issue as these intelligent systems -- whether software programs or robots -- evolve. Will they be servants or masters?\n  ''Even if these systems are consistently right, slavishly following their instructions robs us of our ability to decide on our own,'' said Kristian Hammond, an artificial intelligence expert at Northwestern University. ''Then you actually get the world no one wants. The machine algorithm tells us what to do.''\n  The antidote is what Mr. Hammond calls ''transparency'' and others call ''storytelling'' -- an explanation of the data ingredients that go into an automated decision and how it is made. University scientists and corporate researchers are working on A.I.-monitoring technology that ranges from data audit trails to English-language accounts of an algorithm's chain of reasoning.\n  Meanwhile, new research initiatives have sprung up that seem to have in mind that distant date when robots might achieve independence from their human masters. At Stanford, a group of leading scientists has begun a 100-year study on artificial intelligence. Mr. Musk has put millions of dollars into research grants sponsored by the Future of Life Institute, which is focused on guiding A.I. down beneficial paths.\n  But for the near term, it's those unseen algorithms -- far more than robots -- that deserve a watchful human eye. The stakes, some experts say, extend far beyond mere technology. ''We need to make sure that the data and algorithms are continuously reviewed and vetted by a broad class of people,'' said Alex Pentland, a computational social scientist at the M.I.T. Media Lab. ''Think of representative democracy, forging algorithms rather than laws.''\n\n\n\n","250":"Imagine receiving a phone call from your aging mother seeking your help because she has forgotten her banking password.\nExcept it's not your mother. The voice on the other end of the phone call just sounds deceptively like her. \n  It is actually a computer-synthesized voice, a tour-de-force of artificial intelligence technology that has been crafted to make it possible for someone to masquerade via the telephone.\u00a0\n  Such a situation is still science fiction -- but just barely. It is also the future of crime.\n  The software components necessary to make such masking technology widely accessible are advancing rapidly. Recently, for example, DeepMind, the Alphabet subsidiary known for a program that has bested some of the top human players in the board game Go, announced that it had designed a program that ''mimics any human voice and which sounds more natural than the best existing text-to-speech systems, reducing the gap with human performance by over 50 percent.''\n  The irony, of course, is that this year the computer security industry, with $75 billion in annual revenue, has started to talk about how machine learning and pattern recognition techniques will improve the woeful state of computer security.\n  But there is a downside.\n  ''The thing people don't get is that cybercrime is becoming automated and it is scaling exponentially,'' said Marc Goodman, a law enforcement agency adviser and the author of ''Future Crimes.'' He added, ''This is not about Matthew Broderick hacking from his basement,'' a reference to the 1983 movie ''War Games.''\n  The alarm about malevolent use of advanced artificial intelligence technologies was sounded earlier this year by James R. Clapper, the director of National Intelligence. In his annual review of security, Mr. Clapper underscored the point that while A.I. systems would make some things easier, they would also expand the vulnerabilities of the online world.\n  The growing sophistication of computer criminals can be seen in the evolution of attack tools like the widely used malicious program known as Blackshades, according to Mr. Goodman. The author of the program, a Swedish national, was convicted last year in the United States.\n  The system, which was sold widely in the computer underground, functioned as a ''criminal franchise in a box,'' Mr. Goodman said. It allowed users without technical skills to deploy computer ransomware or perform video or audio eavesdropping with a mouse click.\n  The next generation of these tools will add machine learning capabilities that have been pioneered by artificial intelligence researchers to improve the quality of machine vision, speech understanding, speech synthesis and natural language understanding. Some computer security researchers believe that digital criminals have been experimenting with the use of A.I. technologies for more than half a decade.\n  That can be seen in efforts to subvert the internet's omnipresent Captcha -- Completely Automated Public Turing test to tell Computers and Humans Apart -- the challenge-and-response puzzle invented in 2003 by Carnegie Mellon University researchers to block automated programs from stealing online accounts.\n  Both ''white hat'' artificial intelligence researchers and ''black hat'' criminals have been deploying machine vision software to subvert Captchas for more than half a decade, said Stefan Savage, a computer security researcher at the University of California, San Diego.\n  ''If you don't change your Captcha for two years, you will be owned by some machine vision algorithm,'' he said.\n  Surprisingly, one thing that has slowed the development of malicious A.I. has been the ready availability of either low-cost or free human labor. For example, some cybercriminals have farmed out Captcha-breaking schemes to electronic sweatshops where humans are used to decode the puzzles for a tiny fee.\n  Even more inventive computer crooks have used online pornography as a reward for human web surfers who break the Captcha, Mr. Goodman said. Free labor is a commodity that A.I. software won't be able to compete with any time soon.\n  So what's next?\n  Criminals, for starters, can piggyback on new tech developments. Voice-recognition technology like Apple's Siri and Microsoft's Cortana are now used extensively to interact with computers. And Amazon's Echo voice-controlled speaker and Facebook's Messenger chatbot platform are rapidly becoming conduits for online commerce and customer support. As is often the case, whenever a communication advancement like voice recognition starts to go mainstream, criminals looking to take advantage of it aren't far behind.\n  ''I would argue that companies that offer customer support via chatbots are unwittingly making themselves liable to social engineering,'' said Brian Krebs, an investigative reporter who publishes at krebsonsecurity.com.\n  Social engineering, which refers to the practice of manipulating people into performing actions or divulging information, is widely seen as the weakest link in the computer security chain. Cybercriminals already exploit the best qualities in humans -- trust and willingness to help others -- to steal and spy. The ability to create artificial intelligence avatars that can fool people online will only make the problem worse.\n  This can already be seen in efforts by state governments and political campaigns who are using chatbot technology widely for political propaganda.\n  Researchers have coined the term ''computational propaganda'' to describe the explosion of deceptive social media campaigns on services like Facebook and Twitter.\n  In a recent research paper, Philip N. Howard, a sociologist at the Oxford Internet Institute, and Bence Kollanyi, a researcher at Corvinus University of Budapest, described how political chatbots had a ''small but strategic role'' in shaping the online conversation during the run-up to the ''Brexit'' referendum.\n  It is only a matter of time before such software is put to criminal use.\n  ''There's a lot of cleverness in designing social engineering attacks, but as far as I know, nobody has yet started using machine learning to find the highest quality suckers,'' said Mark Seiden, an independent computer security specialist. He paused and added, ''I should have replied: 'I'm sorry, Dave, I can't answer that question right now.'''\n\n\n\n","251":"SAN FRANCISCO -- At a conference in Silicon Valley this week, Mark Zuckerberg, Facebook's chief executive, vowed that his company would ''keep building'' despite a swirl of questions around the way it has dealt with misinformation and the personal data of its users.\nThat is certainly true in the important area of artificial intelligence, which Mr. Zuckerberg says can help the social media giant deal with some of those problems. \n  Facebook is opening new A.I. labs in Seattle and Pittsburgh, after hiring three A.I. and robotics professors from the University of Washington and Carnegie Mellon University. The company hopes these seasoned researchers will help recruit and train other A.I. experts in the two cities, Mike Schroepfer, Facebook's chief technology officer, said in an interview.\u00a0\n  As it builds these labs, Facebook is adding to pressure on universities and nonprofit A.I. research operations, which are already struggling to retain professors and other employees.\n  The expansion is a blow for Carnegie Mellon, in particular. In 2015, Uber hired 40 researchers and technical engineers from the university's robotics lab to staff a self-driving car operation in Pittsburgh. And The Wall Street Journal reported this week that JPMorgan Chase had hired Manuela Veloso, Carnegie Mellon's head of so-called machine learning technology, to oversee its artificial intelligence operation.\n  ''It is worrisome that they are eating the seed corn,'' said Dan Weld, a computer science professor at the University of Washington. ''If we lose all our faculty, it will be hard to keep preparing the next generation of researchers.''\n  With the new labs, Facebook -- which already operates A.I. labs in Silicon Valley, New York, Paris and Montreal -- is establishing two new fronts in a global competition for talent.\n  Over the last five years, artificial intelligence has been added to a number of tech products, from digital assistants and online translation services to self-driving vehicles. And the world's largest internet companies, from Google to Microsoft to Baidu, are jockeying for researchers who specialize in these technologies. Many of them are coming from academia.\n  ''We're basically going where the talent is,'' Mr. Schroepfer said.\n  But the supply of talent is not keeping up with demand, and salaries have skyrocketed. Well-known researchers are receiving compensation in salary, bonuses and stock worth millions of dollars. Many in the field worry that the talent drain from academia could have a lasting impact in the United States and other countries, simply because schools won't have the teachers they need to educate the next generation of A.I. experts.\n  Over the last few months, Facebook approached a number of notable researchers in Seattle. It hired Luke Zettlemoyer, a professor at the University of Washington who specializes in technology that aims to understand and use natural human language, the company confirmed. This is an important area of research for Facebook as it struggles to identify and remove false and malicious content on its networks.\n  In the fall, Mr. Zettlemoyer told The New York Times that he had turned down an offer from Google that was three times his teaching salary (about $180,000, according to public records) so he could keep his post at the university. Instead, he took a part-time position at the Allen Institute for Artificial Intelligence, a Seattle lab backed by the Microsoft co-founder Paul Allen.\n  Many researchers retain their professorships when moving to the big companies -- that's Mr. Zettlemoyer's plan while he works for Facebook -- but they usually cut back on their academic work. At Facebook, academics typically spend 80 percent of their time at the company and 20 percent at their university.\n  Like the other internet giants, Facebook acknowledges the importance of the university system. But at the same time, the companies are eager to land top researchers.\n  In Pittsburgh, Facebook hired two professors from the Carnegie Mellon Robotics Institute, Abhinav Gupta and Jessica Hodgins, who specialized in computer vision technology.\n  The new Facebook lab will focus on robotics and ''reinforcement learning,'' a way for robots to learn tasks by  trial and error. Siddhartha Srinivasa, a robotics professor at the University of Washington, said he was also approached by Facebook in recent months. It was not clear to him why the internet company was interested in robotics.\n  Andrew Moore, dean of computer science at Carnegie Mellon, did not respond to a request for comment. But over the past several months, he has been vocal about the movement of A.I. researchers toward the big internet companies. Google also operates an engineering office near Carnegie Mellon.\n  ''What we're seeing is not necessarily good for society, but it is rational behavior by these companies,'' he said.\n  The two new Facebook labs are part of wider expansion for the company's A.I. operation. In December, Facebook announced that it had hired another computer vision expert, Jitendra Malik, a professor at the University of California, Berkeley. He now oversees the lab at the company's headquarters in Menlo Park, Calif.\n  Even with its deep pockets, Facebook faces fierce competition for talent. Mr. Allen recently gave the Allen Institute, which he created in 2013, an additional $125 million in funding. After losing Mr. Zettlemoyer to Facebook, the Allen Institute hired Noah Smith and Yejin Choi, two of his colleagues at the University of Washington.\n  Like Mr. Zettlemoyer, both specialize in natural language processing, and both say they received offers from multiple internet companies.\n  The nonprofit is paying Mr. Smith and Ms. Choi a small fraction of what they were offered to join the commercial sector, but the Allen Institute will allow them to spend half their time at the university and collaborate with a wide range of companies, said Oren Etzioni, who oversees the Allen Institute.\n  ''The salary numbers are so large that even Paul Allen can't match them,'' Mr. Etzioni said. ''But there are still some people who won't go corporate.''\n  Others researchers believe that companies like Facebook still align with their academic goals. Nonetheless, Ed Lazowska, the Bill and Melinda Gates professor of computer science at the University of Washington, said he was concerned that the large internet companies were luring too many of the university's professors into the commercial sector.\n  Carnegie Mellon and the University of Washington, he said, are working on a set of recommendations for commercial companies meant to provide a way for universities and companies to share talent more equally. Mr. Lazowska added that every university should ensure that it did not become too close to one company.\n  ''The university must be a Switzerland,'' he said. ''We want every company to collaborate with us and to feel like they have an equal opportunity to hire our students and work with our faculty.''\n\n\n\n","252":"Computer researchers reported artificial-intelligence advances on Thursday that surpassed human capabilities for a narrow set of vision-related tasks.\nThe improvements are noteworthy because so-called machine-vision systems are becoming commonplace in many aspects of life, including car-safety systems that detect pedestrians and bicyclists, as well as in video game controls, Internet search and factory robots. \n  Researchers at the Massachusetts Institute of Technology, New York University and the University of Toronto reported a new type of ''one shot'' machine learning on Thursday in the journal Science, in which a computer vision program outperformed a group of humans in identifying handwritten characters based on a single example.\u00a0\n  The program is capable of quickly learning the characters in a range of languages and generalizing from what it has learned. The authors suggest this capability is similar to the way humans learn and understand concepts.\n  The new approach, known as Bayesian Program Learning, or B.P.L., is different from current machine learning technologies known as deep neural networks.\n  Neural networks can be trained to recognize human speech, detect objects in images or identify kinds of behavior by being exposed to large sets of examples.\n  Although such networks are modeled after the behavior of biological neurons, they do not yet learn the way humans do -- acquiring new concepts quickly. By contrast, the new software program described in the Science article is able to learn to recognize handwritten characters after ''seeing'' only a few or even a single example.\n  The researchers compared the capabilities of their Bayesian approach and other programming models using five separate learning tasks that involved a set of characters from a research data set known as Omniglot, which includes 1,623 handwritten character sets from 50 languages. Both images and pen strokes needed to create characters were captured.\n  ''With all the progress in machine learning, it's amazing what you can do with lots of data and faster computers,'' said Joshua B. Tenenbaum, a professor of cognitive science and computation at M.I.T. and one of the authors of the Science paper. ''But when you look at children, it's amazing what they can learn from very little data. Some comes from prior knowledge and some is built into our brain.''\n  Also on Thursday, organizers of an annual academic machine vision competition reported gains in lowering the error rate in software for finding and classifying objects in digital images.\n  ''I'm constantly amazed by the rate of progress in the field,'' said Alexander Berg, an assistant professor of computer science at the University of North Carolina, Chapel Hill.\n  The competition, known as the Imagenet Large Scale Visual Recognition Challenge, pits teams of researchers at academic, government and corporate laboratories against one another to design programs to both classify and detect objects. It was won this year by a group of researchers at the Microsoft Research laboratory in Beijing.\n  The Microsoft team was able to cut the number of errors in half in a task that required their program to classify objects from a set of 1,000 categories. The team also won a second competition by accurately detecting all instances of objects in 200 categories.\n  The contest requires the programs to examine a large number of digital images, and either label or find objects in the images. For example, they may need to distinguish between objects such as bicycles and cars, both of which might appear to have two wheels from a certain perspective.\n  In both the handwriting recognition task described in Science and in the visual classification and detection competition, researchers made efforts to compare their progress to human abilities. In both cases, the software advances now appear to surpass human abilities.\n  However, computer scientists cautioned against drawing conclusions about ''thinking'' machines or making direct comparisons to human intelligence.\n  ''I would be very careful with terms like 'superhuman performance,' '' said Oren Etzioni, chief executive of the Allen Institute for Artificial Intelligence in Seattle. ''Of course the calculator exhibits superhuman performance, with the possible exception of Dustin Hoffman,'' he added, in reference to the actor's portrayal of an autistic savant with extraordinary math skills in the movie ''Rain Man.''\n  The advances reflect the intensifying focus in Silicon Valley and elsewhere on artificial intelligence.\n  Last month, the Toyota Motor Corporation announced a five-year, billion-dollar investment to create a research center based next to Stanford University to focus on artificial intelligence and robotics.\n  Also, a formerly obscure academic conference, Neural Information Processing Systems, underway this week in Montreal, has doubled in size since the previous year and has attracted a growing list of brand-name corporate sponsors, including Apple for the first time.\n  ''There is a sellers' market right now -- not enough talent to fill the demand from companies who need them,'' said Terrence Sejnowski, the director of the Computational Neurobiology Laboratory at the Salk Institute for Biological Studies in San Diego. ''Ph.D. students are getting hired out of graduate schools for salaries that are higher than faculty members who are teaching them.''\n\n\n\n","253":"Rocket Fuel, an advertising technology company that relies on artificial intelligence, priced its initial public offering on Thursday at the top of its expectations, garnering $116 million in proceeds.\u00a0\nThe company sold 4 million shares at $29 each, the high end of an already raised range. The stock sale values it at $942.5 million.\nThursday's offering is the latest in a series of technology I.P.O.'s, a sector that is trying to revive after staying quiet for much of the past year. Analysts and deal makers expect a big number of companies to make their public debuts by year end, led by Twitter and its potentially gigantic stock sale.\nRocket Fuel is one of several companies riding a number of big technology trends, including the much-ballyhooed Big Data phenomenon. But the five-year-old start-up's trump card is its artificial intelligence software, which the company says uses computer programs to better place clients' ads with minimal human input. \nAccording to the company's most recent prospectus, it reported $106.6 million in revenue last year on top of a $10.3 million loss. Using adjusted earnings before interest, taxes, depreciation and amortization, it lost $3 million for the period.\nThe offering was led by Credit Suisse and Citigroup.\n\n","254":"  Chamath Palihapitiya loves Box. \n  The noted Silicon Valley investor said the Redwood City, Calif., company is his No. 1 idea in the artificial intelligence field - what he called the third wave of tech innovation. \u00a0\n  The idea, revealed at Monday's Ira Sohn Investment Conference, sparked an 11 percent spike on Box shares, to $22.91 - after earlier in the session they flirted with a new 52-week high. \n  \"Companies we all know of, trust and respect use Box every day,\" Palihapitiya, of Social Capital, told attendees, noting that nearly 70 percent of the Fortune 500 uses Box. \n  Palihapitiya - who recently made waves by blasting Facebook as bad for society, despite having worked there as a senior exec - pumped up Box's stock after spending much of his 15-minute presentation delivering a tutorial on what artificial intelligence is and its potential, naming Google and Amazon but not Box. \n  AI is part of the third wave of tech-innovation, Palihapitiya said, noting that the internet and mobile represented the first and second. \n  \"In this third wave, it's really critical to understand what AI is, what it can do, what it can't do, and who is actually positioned to create incredible value over the next 10 to 20 years,\" Palihapitiya said. \n  Overall, the investor is bullish on the tech sector - but, in an interview on CNBC, warned companies that they must keep personal information safe. \n  If not, Palihapitiya said, regulators will be all over their backs. \n  Regarding his former employer, Palihapitiya said he disagreed with Jeffrey Gundlach of Los Angeles-based DoubleLine Capital, who said he would short Facebook (see story at right). \n  \"If you are going to short Facebook, you have to do it for probably more than technical reasons, quite honestly,\" he told CNBC. \"I think you have to understand other things that, frankly, don't exist yet in the business. There is a vibrant user business, vibrant revenue growth - and so until those things change, we can agree to disagree.\" \n  cenglish@nypost.com \n","255":"FOURTEEN-year-old Rochelle Brown was close to solving an algebra problem. Yet she stumbled repeatedly on one calculation: -2.3 + .5. As she sat at a computer screen, she kept typing 2.8, an incorrect answer. Eventually a hint popped up: ''Think about the sign of your answer.''\n When Rochelle finally typed the correct sum, -1.8, the computer showed its appreciation by allowing her to move on to a new problem. She smiled at her small triumph.\n  Since January, Middle School 301 in the Bronx, where Rochelle is an eighth grader, has been using a software program called Cognitive Tutor to help students learn math. The software, from Carnegie Learning, a six-year-old company that got its start at Carnegie Mellon University, is designed to give students individualized instruction when personal attention is scarce.\u00a0\n Although such intelligent tutoring systems have their share of skeptics, students at schools that use them have not only improved their performance in math but now profess to enjoy a subject they once loathed.\n ''It's difficult to tell what's cause and effect, but it must have had something to do with the results,'' said Andrew Eisenberg, who retired at the end of the summer as principal at M.S. 301, referring to the improvement in students' grades in Rochelle's class. ''The results in that class were extraordinary.''\n Broadly defined, an intelligent tutoring system is educational software containing an artificial intelligence component. The software tracks students' work, tailoring feedback and hints along the way. By collecting information on a particular student's performance, the software can make inferences about strengths and weaknesses, and can suggest additional work.\n When Rochelle, for instance, displayed a weakness when working with negative numbers, the program repeatedly asked her to solve similar problems.\n The adaptive ability of the software has impressed Rochelle's teacher, Jai Ramnarine, who has taught math at M.S. 301 for 17 years. ''The math programs you used to get just said 'yes,' 'no,' or 'choose another answer,''' he said.\n The Carnegie Learning software is by no means the only computer-based instructional program on the market. The e-learning industry is huge, with hundreds of companies selling various forms of e-learning software. But experts dismiss much of what is available as basic computer-aided instruction, offering little more than a question-and-answer format.\n The artificial intelligence built into the Carnegie Learning program helps set it apart. Not only does the program present drills according to a student's weaknesses, but it watches the work step by step, detecting where the student stumbles, and chimes in when necessary.\n ''People solve problems in different ways,'' said Ken Koedinger, a professor of human-computer interaction and psychology at Carnegie Mellon, and a co-founder of Carnegie Learning. ''So a student who solves a problem one way gets a different hint than a student who solves in a different way. You don't get that with computer-aided instruction systems.''\n M.S. 301 is one of 1,700 middle schools and high schools across the country that use the Carnegie Learning tutor. In addition to the Algebra I and II programs, which cost schools roughly $30 per student, the company sells Cognitive Tutor products for geometry and integrated math.\n ''They're basically setting the standard,'' said Art Graesser, a professor of psychology and computer science at the University of Memphis, who has done research in intelligent tutoring systems. ''They're the single best success in getting the work going on in the area of cognitive psychology and cognitive science out into the school systems.''\n The software does not replace classroom teaching. At many schools the program is used at a different time from math class, in a separate computer lab.\n Mr. Ramnarine said the program not only allows the students to work at their own pace, but it frees him to circulate through the class to offer help rather than stand at the front of the room, with no chance of focusing on individuals.\n Algebra is a particularly nettlesome subject, the stage in a math curriculum where discouragement is most likely to set in. ''Algebra is a place where math gets very abstract,'' said Steve Ritter, senior cognitive scientist at Carnegie Learning.\n But intelligent tutoring systems have their skeptics. One of the main criticisms is that the tutor does not really allow students to learn from their mistakes because the program vigorously guides them toward the right answer.\n ''Some pedagogical researchers feel that because cognitive tutors don't allow learners to go very far off a correct path before intervening, students won't develop skills for coping with really tough problems,'' said Jim Greer, head of the Department of Computer Science at the University of Saskatchewan in Saskatoon. \n Nor are intelligent tutoring systems easy to apply to other subjects. For math and other science domains, where there are well-defined answers, designing intelligent tutoring systems is relatively straightforward. But in areas like sociology, psychology, economics, politics, business and law, the process is trickier. ''The parts of these disciplines that are fact-based are easy to represent,'' said Dr. Greer. ''But there are many aspects of these domains where there are no right or wrong answers.''\n Even in math, there are signals about a student's level of comprehension that a computer cannot detect. The computer does not know if a student is bored or flustered. It cannot pick up on Rochelle's preference to work out a calculation on scratch paper, or track her work when she does so.\n Eric Thomas, a math teacher at Valhalla High School in El Cajon, Calif., found the Carnegie program far more impressive than the question-and-answer software he had seen in the past. But he expressed concern over the logistics involved in adopting such a program.\n ''This is probably a very cool thing, but I have trouble figuring out how to implement it in a class of 35 kids, where some are fast learners and others are slow learners,'' he said. ''It would be a nightmare to organize.''\n Yet the criticisms seem minor in light of the results. Studies conducted by Carnegie Mellon University researchers have shown that students who use the Cognitive Tutor algebra course consistently outperform those who do not.\n One aspect of the software that keeps the students engaged is its occasionally eerie resemblance to a video game. The object is to fill a ''skillometer'' on the screen with gold bars that assess your skill level. \n Students can ask for a hint by clicking on an icon of a light bulb. Hints, however, are a mixed blessing: as soon as you ask for one, the gold bars roll back. So students have figured out a way to get hints without being penalized by the computer: by asking each other.\n Rochelle looks vindicated when she sees a fellow eighth grader, Jason Belliard, 13, struggling with a problem she aced a few minutes earlier. ''This one is easy!'' she says, leaning over her classmate's shoulder.\n Across the Bronx, the Maritime Academy, Middle School 101, has been using the Carnegie math tutor for three years. The school has one Carnegie lab with 34 computers and is planning to open a second because demand is so great.\n ''Some classes you dread, but everyone wants to go to this one,'' said Megan Cox, who used the program last year when she was in the eighth grade. \n","256":"SHANGHAI -- If Beijing has its way, the future of artificial intelligence will be made in China.\nThe country laid out a development plan on Thursday to become the world leader in A.I. by 2030, aiming to surpass its rivals technologically and build a domestic industry worth almost $150 billion. \n  Released by the State Council, the policy is a statement of intent from the top rungs of China's government: The world's second-largest economy will be investing heavily to ensure its companies, government and military leap to the front of the pack in a technology many think will one day form the basis of computing.\u00a0\n  The plan comes with China preparing a multibillion-dollar national investment initiative to support ''moonshot'' projects, start-ups and academic research in A.I., according to two professors who consulted with the government about the effort.\n  The United States, meanwhile, has cut back on science funding. In budget proposals, the Trump administration has suggested slashing resources for a number of agencies that have traditionally backed research in A.I. Other cuts, to areas like high-performance computing, would affect the development of the tools that make A.I. work.\n  China's capabilities, especially in advanced and new technologies, have long lagged those of its better developed neighbors as well as Europe and America. But a multiple-decade industrial policy to help it catch up has paid dividends.\n  A.I. is one of a growing number of disciplines in which experts say China is making quick progress.\n  Yet it was a foreign feat of A.I. prowess that provided one of the greatest impetuses for the new plan.\n  The two professors who consulted with the government on A.I. both said that the 2016 defeat of Lee Se-dol, a South Korean master of the board game Go, by Google's AlphaGo had a profound impact on politicians in China. Then in May, Google brought AlphaGo to China, where it defeated the world's top-ranked player, Ke Jie of China. Live video coverage of the event was blocked at the last minute in China.\n  As a sort of Sputnik moment for China, the professors said, the event paved the way for a new flow of funds into the discipline.\n  China's ambitions with A.I. range from the anodyne to the dystopian, according to the new plan. It calls for support for everything from agriculture and medicine to manufacturing.\n  Yet it also calls for the technology to work in concert with the country's homeland security and surveillance efforts. China wants to integrate A.I. into guided missiles, use it to track people on closed-circuit cameras, censor the internet and even predict crimes.\n  Beijing's interest in the technology has set off alarms within the United States' defense establishment. The Defense Department found that Chinese money has been flowing into American A.I. companies -- some of the same ones it says are likely to help the United States military develop future weapons systems.\n  In a timeline laid out within the new policy, the government expects its companies and research facilities to be at the same level as leading countries like the United States by 2020. Five years later, it calls for breakthroughs in select disciplines within A.I. that will become ''a key impetus for economic transformation.''\n  In the final stage, by 2030, China will ''become the world's premier artificial intelligence innovation center,'' which in turn will ''foster a new national leadership and establish the key fundamentals for an economic great power.''\n  While the language in Chinese industrial policy can sound stodgy and the targets overly ambitious, Beijing takes its economic planning seriously. Experts say that even if major spending efforts ultimately waste resources, they can also produce results, bolstering technology capabilities with a flood of resources.\n  Top-level statements like this also work as a signal to local governments and companies across the country.\n  The new plan formalizes a focus that was widely known in China. Following those cues, a large number of local governments have created special plans and built out research centers to focus on A.I.\n  Many are spending hundreds of millions of dollars, but some have earmarked even more. In June, the government of Tianjin, an eastern city near Beijing, said it planned to set up a $5 billion fund to support the A.I. industry. It also set up an ''intelligence industry zone'' that will sit on more than 20 square kilometers of land.\n  The initiative is also likely to sweep up private Chinese companies. The country's internet search giant Baidu, which has run an A.I. research center out of Silicon Valley in recent years, announced this year that it would open a new lab in cooperation with the government. The two leaders of that lab have worked on Chinese government programs with military applications.\n\n\n\n","257":"It is certainly true that conversations with AI chatbots are often unintentionally funny. And no one who interacts with Alexa or Siri or Cortana is going to say they pass the Turing Test. \"Their responses, often cobbled together out of fragments of stored conversations, make sense at a local level but lack long-term coherence,\" Brian Christian wrote in a 2012 Smithsonian Magazine article. Garbled sentences and ridiculous responses often make clear just how poorly machines mimic human capabilities - or even, sometimes, how they process information. \"Machines don't have understanding,\" Garry Kasparov told TechCrunch last year. \"They don't recognize strategical patterns. Machines don't have purpose.\" \nBut AI is already writing financial news, sports stories and weather reports, and readers aren't noticing. From the Associated Press to The Washington Post, it's becoming increasingly common. AI is also producing \"deep fake\" videos - from invented speeches by politicians to pornography featuring celebrities' computer-generated faces - that many people think are real. These rapid advances present significant concerns, shaking the public's confidence in what they see and hear. As a 2017 Harvard study warned, \"The existence of widespread AI forgery capabilities will erode social trust, as previously reliable evidence becomes highly uncertain.\" \u00a0\nChina's national strategy to lead the world in artificial intelligence - which calls for \"the training and gathering of high-end AI talent\" - has elicited fear and loathing in the United States.  \"China's prowess in the field will help fortify its position as the dominant economic power in the world,\" Will Knight observed in MIT Technology review in 2017. Writing in the Hill, Tom Daschle and David Bier warned in January that \"the U.S. government is behind the curve.\" \nWhile there is clearly reason for concern about the United States' standing, China's strategic document admits that  \"there is still a gap between China's overall level of development of AI relative to that of developed countries.\" According to Jeffrey Ding , a University of Oxford researcher, \"China trails the U.S. in every driver except for access to data.\" The United States also has more AI experts, who publish more Association for the Advancement of Artificial Intelligence papers on the topic, and far more commercial investments in the field. \nThat said, given China's dedication to pursuing AI, the United States will need to take a concerted societal  approach if it wants to maintain its dominant position. Such efforts are already underway: In March, the New York Times reported that the Pentagon is attempting to work with Silicon Valley companies to push projects ahead.  \nAs early as 1964, a group of Nobel Prize winners known as the Ad Hoc Committee on the Triple Revolution warned that machines would usher in \"a system of almost unlimited productive capacity\" that would cause disruptive levels of unemployment. More recently, a Mother Jones headline proclaimed, \"You Will Lose Your Job to a Robot - and Sooner Than You Think.\" The article noted that traditional blue-collar and white-collar workers alike may be displaced, leading to joblessness and poverty. Taking up that torch, one truck driver worried in the Guardian that \"we will soon be extraneous - roadkill, so to speak, except we won't be dead.\" \nBut in transforming work, AI may also create new jobs. As Joel Mokyr, an economic historian at Northwestern University, observed, \"We can't predict what jobs will be created in the future, but it's always been like that.\" Historically, technological change has initially diminished, but then later boosted, employment and living standards by enabling new industries and sectors to emerge. \nWe don't yet know how AI will affect employment in the long term. Between now and then, there may still be disruptions, and we'll have to grapple with the growing gap between those who have the skills to thrive in a changing world and those who don't.\nIt's easy to imagine that relying on computers to make critical decisions would take human bias out of the equation. \"Humans are hindered by both their unconscious assumptions and their simple inability to process huge amounts of information,\" wrote Digitalist Magazine last year. Judges around the United States are using AI tools in sentencing decisions, on the assumption that these systems can offer \"the most objective information available to make fair decisions about prisoners.\"\nIf only it were that simple. In one example that shows AI's vulnerability to bias, ProPublica found that a program intended to play a key role in criminal justice decisions from bail to sentencing was almost twice as likely to rate black defendants as probable repeat offenders than white defendants. The program also incorrectly rated white defendants as low-risk more often than blacks. \"It's often wrong - and biased against blacks,\" ProPublica wrote. \nIn another example, a 2015 Carnegie Mellon University experiment found that far fewer women were being shown online ads for jobs paying more than $200,000 than were men. \"Many important decisions about the ads we see are being made by online systems,\" said Anupam Datta, associate professor of computer science and electrical and computer engineering at Carnegie Mellon. \"Oversight of these 'black boxes' is necessary to make sure they don't compromise our values.\" Researchers are already addressing the bias issue, seeking to head off mistakes and build more transparent algorithms. \nSome prominent science and technology leaders have raised grave concerns about the implications of AI for humanity's future. \"The danger of AI is much greater than the danger of nuclear warheads by a lot, and nobody would suggest that we allow anyone to build nuclear warheads if they want,\" Elon Musk said at the South by Southwest conference in March. \"I fear that AI may replace humans altogether,\" Stephen Hawking told Wired in 2017. \nThe truth is we simply don't know where AI will lead us, but that doesn't mean murderous terminators are going to start stalking the streets. In a 2015 open letter, experts associated with the nonprofit Future of Life Institute warned against the rise of autonomous weapons systems, which could be abused by ill-intentioned humans. The more pressing concern might not be that AI is a risk to us, but that we're a risk to ourselves if we don't exercise caution in how we push ahead with our AI experiments. \nIn some contexts, AI can save lives. In March, a self-driving car struck and killed a pedestrian in Arizona, an incident that presaged trouble for the emerging technology. Nevertheless, many researchers have long held that self-driving vehicles will help reduce traffic fatalities overall. A 2017 Rand Corp. report, for example, concludes that introducing autonomous automobiles to the streets sooner could prevent hundreds of thousands of deaths. \n             Twitter: @KatharynMWhite          \n","258":"The subjugation of humanity by a race of super-smart, artificially intelligent beings is something that has been theorized by everyone from generations of moviemakers to New Zealand's fourth-most-popular folk-parody duo.\nBut the latest prophet of our cyber-fueled downfall must realize why people would be inclined to take his warnings with a grain of silicon. He is, after all, the same guy who's asking us to turn over control of our cars - and our lives - to a bunch of algorithms.\u00a0\nElon Musk, who hopes that one day everyone will ride in a self-driving, electric-powered Tesla, told a group of governors Saturday that they needed to get on the ball and start regulating artificial intelligence, which he called a \"fundamental risk to the existence of human civilization.\"\nNo pressure. When pressed for better guidance, Musk said the government must get a better understanding of the latest achievements in artificial intelligence before it's too late.\n\"Once there is awareness, people will be extremely afraid, as they should be,\" Musk said. \"AI is a fundamental risk to the future of human civilization in a way that car accidents, airplane crashes, faulty drugs or bad food were not. They were harmful to a set of individuals in society, but they were not harmful to individuals as a whole.\"\nAnd then Musk outlined the ways AI could bring down our civilization, which may sound vaguely familiar.\nHe believes AI \"could start a war by doing fake news and spoofing email accounts and fake press releases, and just by manipulating information. Or, indeed - as some companies already claim they can do - by getting people to say anything that the machine wants.\"\nMusk said he's usually against proactive regulation, which can impede innovation. But he's making an exception in the case of an AI-fueled Armageddon.\n\"By the time we are reactive in regulation, it's too late,\" he said, confessing that \"this is really like the scariest problem to me.\"\nHe's been warning people about the problem for years, and he's even come up with a solution: Join forces with the computers.\nHe announced earlier this year that he's leading a company called Neuralink, which would devise ways to connect the human brain to computers, CNN reported.\nIn the decades to come, an Internet-connected brain plug-in would allow people to communicate without opening their mouths and learn something as fast as it takes to download a book.\nOther prominent figures in the world of science and technology have also warned against the dangers of artificial intelligence, including Microsoft founder Bill Gates and theoretical physicist Stephen Hawking. But Musk concedes that people have been hesitant to accept their viewpoint.\n\"I keep sounding the alarm bell, but until people see like robots going down the streets killing people, they don't know how to react because it seems so ethereal,\" he said. \"I think we should be really concerned about AI.\"\nStill, even to the biggest skeptic, one sentence offered some food for thought: \"I have exposure to the very most cutting edge AI, and I think people should be really concerned about it.\"\nMaybe Musk knows something the rest of us don't? He is, after all, a multibillionaire, capable of using obscene sums of money to develop AI. Maybe in some Musk-funded lab, or on some secret SpaceX satellite, there's already a powerful AI on the verge of getting out.\nMaybe it's already loose.\nBetter safe than sorry:\n          01001001 00100000 01100001 01101101 00100000 01101111 01101110 00100000 01111001 01101111 01110101 01110010 00100000 01110011 01101001 01100100 01100101 00100000\n          Read more:       \n          Police thought a skydiver died in an accident - until they saw his final message to his wife       \n          Donald Trump interrupted a screening of 'Rogue One.' Mark Hamill had a forceful response.       \n          Teen camper wakes up to 'crunching noise' - and discovers his head is inside bear's mouth       \n","260":"Artificial intelligence has been widely hyped\u00a0for its potential to\u00a0transform a broad swath of industries, from cybersecurity\u00a0to medicine.\u00a0Now, we might start to get a clearer picture for how it could\u00a0be used to change the way we shop.\nMacy's announced on Wednesday that it has teamed up with IBM Watson to use artificial intelligence as a customer service tool in 10\u00a0of its stores. \u00a0The\u00a0retailer\u00a0dubbed the pilot program\u00a0\"Macy's On Call,\" and it will allow customers to type in\u00a0questions on their phones\u00a0and receive answers. Unlike some chatbots that can only regurgitate pre-programmed\u00a0responses based on keywords, IBM Watson will learn over time to give better answers that are customized\u00a0to individual stores.\nThe department store imagines shoppers will use it to ask things like \"Where can I find women's dresses?\" or \"Where is the restroom located?\"\u00a0\nMacy's experiment is part of an explosion of\u00a0efforts by retailers to incorporate smartphones into the physical shopping experience. Target, for example, has made a big push around its Cartwheel app, which helps brick-and-mortar shoppers nab discounts and find their way around the aisles.\u00a0Walmart has created its own mobile\u00a0payment offering, Walmart Pay, that lives with in its Walmart\u00a0app.\nMacy's move is an acknowledgment of what a habit it has become for consumers to swipe and tap on their smartphones while they're on the go. And it's a bid to figure out how to\u00a0channel\u00a0that\u00a0behavior into an advantage - not a threat - to\u00a0in-store\u00a0shopping.\nMacy's said the \"On Call\" feature\u00a0was in part shaped by what it\u00a0already noticed customers are doing within the Macy's app: One of the most popular features is scanning product bar codes\u00a0to check the price or get more details about the product.\n\"We really want to allow the customer to self-service these basic questions,\" said Serena Potter, Macy's vice president for\u00a0digital media strategy. \"And that will allow our knowledgeable sales associates to focus on higher-value activities and requests.\"\nThe pilot, which includes Montgomery Mall in Bethesda, will in some markets include a Spanish-language function. Potter said executives will be watching to see what the uptake is for that\u00a0feature, in part because Macy's depends so heavily on spending by foreign tourists, especially at its major-city flagship stores.\nMacy's is not the only retailer that is experimenting with some use of\u00a0artificial intelligence. IBM Watson has already dabbled in using its tools to power other shopping experiences such as a\u00a0collaboration with outdoor apparel brand North Face on a website that helps shoppers find the right jacket. Users can type in natural-language answers to a host of questions, including \"Where and when will you be using this jacket?\" and \"What activity will you be doing?\" Based on the customer's answers, IBM Watson will serve\u00a0up some suggested outerwear.\nWatson also powers a pilot of a \"gift concierge\" for\u00a01-800-Flowers.com, in which users\u00a0can type in details about\u00a0who they're buying a bouquet\u00a0for and what the occasion is, and the tool will give recommendations.\n\"The idea is that it learns over time,\" said Jonas Nwuke, manager of the\u00a0IBM Watson platform. \"It gets better at solving whatever problem it has been pointed at over time.\"\nMacy's has also rolled out other ways to try make our phones relevant to the store experience. It\u00a0has added\u00a0beacon technology\u00a0to all of its stores now, in which a\u00a0low-energy Bluetooth signal is used to send special\u00a0offers to nearby smartphone users who have opted to receive them. It has also launched an\u00a0Image Search feature, allowing\u00a0customers to take\u00a0a photo with their phone of a piece of clothing\u00a0and then be directed\u00a0to similar items that\u00a0they could buy\u00a0at Macy's. Once a standalone app, it has now been folded into Macy's main app.\n              More from The Washington Post:           \n The tiny chip that could power big changes in how you shop \n The creators of Siri have made an even better AI \n Watson's next feat: Taking on cancer \n","261":"HONG KONG -- It's all over for humanity -- at least in the game of Go.\nFor the second game in a row, a Google computer program called AlphaGo beat the world's best player of what many consider the world's most sophisticated board game. AlphaGo is scheduled to play its human opponent, the 19-year-old Chinese prodigy Ke Jie, one more time on Saturday in the best-of-three contest.\nBut with a score of 2-0 heading into that final game, and earlier victories against other opponents already on the books, AlphaGo has proved its superiority.\nDiscussing the contest afterward, Mr. Ke said a very human element got the better of him: his emotions. In the middle of the game, when he thought he might have had a chance at winning, he got too keyed up, he said.\n\"I was very excited. I could feel my heart bumping,\" Mr. Ke said after the contest, which took place in Wuzhen, near Shanghai. \"Maybe because I was too excited I made some stupid moves.\"\n\"Maybe that's the weakest part of human beings,\" he added.\nAlphaGo's victory on Thursday simply reinforced the progress and power of artificial intelligence to handle specific but highly complex tasks. Because of the sheer number of possible moves in Go, computer scientists thought until recently that it would be a decade before a machine could play better than a human master.\nA small consolation for Mr. Ke was that he played a near-perfect game for the first hundred moves, according to the scientists who designed AlphaGo.\nStill, like a sprinter who can at first keep pace with a train, in the end Mr. Ke was left in the dust by the computer.\nDemis Hassabis, a co-founder of DeepMind -- the artificial intelligence arm of Google's parent, Alphabet Incorporated, that created the software -- said that as he watched how close the game was, his pulse rate went up as well.\nAlphaGo of course has no heart and feels no nerves, and in the end that may have helped make the difference. Scientists and futurists have pointed to that cold focus as a major reason artificial intelligence may someday take over large numbers of white-collar jobs. Still, that detachment means AlphaGo lacks the human touch required to manage employees, counsel patients or adequately write flowing newspaper features about its own dominance over humans.\nTests of the technology in games like Go still mark an early step. Because the strategy options are limited to moves on a board, games like Go are particularly suited to the technology.\nComputer scientists say that often the best use of artificial intelligence is not to pit it against humans, but to pair it with them.\nTo that end, two Go professionals, each teamed with AlphaGo, are scheduled to play against each other on Friday. Mr. Hassabis has said that top amateur Go players, with the help of AlphaGo, can generally manage to beat the software program in a match. In short, a human with a computer is still stronger than a computer.\nKe Jie has said that after the third game on Saturday, he will return to focusing on playing against humans, and is not likely to take on a computer again, arguing that the technology has become too formidable.\n\"After this time, AlphaGo to me is 100 percent perfection, to me AlphaGo is the god of the Go game,\" he said after the game on Thursday.\n\"For human beings,\" he added, \"our understanding of this game is only very limited.\"Related Articles\n\n","262":"Last year, after Garry Kasparov's chess victory over the I.B.M. computer Deep Blue, I told the students in my Introduction to Artificial Intelligence class that it would be many years before computers could challenge the best humans. Now that I and many others have been proved wrong, a lot of people have been rushing to assure us that Deep Blue is not actually intelligent and that this victory has no bearing on the future of artificial intelligence.\nAlthough I agree that the computer is not very intelligent, to say that it shows no intelligence at all demonstrates a basic misunderstanding of what it does and of the goals and methods of artificial intelligence research. True, Deep Blue is very narrow. It can win a chess game, but it can't recognize, much less pick up, a chess piece. It can't even carry on a conversation about the game it just won. Since the essence of intelligence would seem to be the ability to react creatively to various situations, it's hard to credit the computer with much on that score.\u00a0\n But many commentators are insisting that Deep Blue shows no intelligence whatsoever because it does not actually \"understand\" a chess position, but rather searches through millions of possible moves \"blindly.\" The problem with this argument is the assumption that intelligent behavior can only be the result of intelligent cogitation.\nAfter all, if science ever comes up with a nonvacuous explanation of human thought, it will undoubtedly explain intelligence by reference to smaller bits of behavior that are not themselves intelligent. Presumably your brain works because each of its billions of neurons carries out hundreds of tiny operations each second, none of which in isolation demonstrates any intelligence at all.\nWhen people say that human grandmasters do not examine 200 million move sequences per second, as the computer does, I ask them, \"How do you know?\" The answer is usually that human grandmasters are not aware of considering so many options. But humans are unaware of almost everything that goes on in our minds.\nI tend to agree that grandmasters search in a different way than Deep Blue does, but whatever method they use, if done by a computer, would seem equally \"blind.\"\nFor example, some scientists believe that the masters' skill comes from an ability to compare their current position against, say, 10,000 positions they've studied. We call their behavior insightful because they are unaware of the details; the right position among the 10,000 \"just occurs to them.\" If a computer did the same thing, the trick would be revealed; we could examine its data to see how laboriously it checks the 10,000 positions. Still, if the unconscious version yields intelligent results, and the explicit algorithmic version yields essentially the same results, are not both methods intelligent?\nSo what shall we say about Deep Blue? How about: It's a \"little bit\" intelligent. Yes, its computations differ in detail from a human grandmaster's. But then, human grandmasters differ from one another in many ways.\nA log of the machine's computations is perfectly intelligible to chess masters; they speak the same language, as it were. That's why the I.B.M. team refused to give the game logs to Mr. Kasparov during the match: It would have been the same as bugging the hotel room where the computer \"discussed\" strategy with his seconds.\nSaying that Deep Blue doesn't really think is like saying an airplane doesn't really fly because it doesn't flap its wings.\nOf course, this advance in artificial intelligence does not indicate that any Grand Unified Theory of Thought is on the horizon. As the field has matured, it has focused more and more on incremental progress, while worrying less and less about some magic solution to all the problems of intelligence. There are fascinating questions about why we are unaware of so much that goes on in our brains, and why our awareness is the way it is. But we can answer a lot of questions about thinking before we need to answer questions about awareness.\nIt is entirely possible that computers will come to seem alive before they come to seem intelligent. The kind of computing power that fuels Deep Blue will also lead to improved sensors, wheels and grippers that will allow machines to react in a more sophisticated way to things in their environment, including us. They won't seem intelligent, but we may think of them as a weird kind of animal -- one that can play a very good game of chess.\n","263":"AI: 2, humanity: 0.\nA computer designed by Google researchers has beaten the world's top Go player for the second game in a row, capturing the best-of-three match in Wuzhen, China, and confirming the supremacy of artificial intelligence in what many consider as one of humanity's most complex board games.\u00a0\nKe Jie, a 19-year-old Go grandmaster, began the game with stellar play, Demis Hassabis, chief executive of DeepMind, the artificial-intelligence firm that created the system, called AlphaGo, said Thursday at a news conference. (Google acquired his company in 2014.) But the Chinese prodigy eventually was bested by AlphaGo. In South Korea last year, DeepMind's machine defeated another championship-caliber player, Lee Sedol, stunning some in the field of artificial intelligence.\n\"For the first 100 moves, it was the closest we've ever seen anyone play against the 'Master' version of AlphaGo,\" Hassabis said after Ke's game.\n\"Today's game was different from the first,\" Ke said at the news conference. \"AlphaGo made some moves which were opposite from my vision of how to maximize the possibility of winning. I also thought I was very close to winning the game in the middle, but maybe that's not what AlphaGo was thinking.\"\nAlphaGo's sudden dominance over human competitors has surprised AI researchers, who had assumed that machines would take much more time to master the intricacies of Go. The program began as a research project three years ago. In a 2016 article in the journal Nature, Google said that AlphaGo had beaten European champion Fan Hui by 5 games to 0, in a match that had taken place the previous October behind closed doors. That marked the first time any computer program had beaten a professional player at Go. The program went on to defeat more standout players.\nAlphaGo's wins have revealed how computers can augment human capability and spur new ways of thinking. More broadly, the computer program's feat boosts a more optimistic take on the future of automation and artificial intelligence - one in which humans aren't necessarily displaced but are enhanced in some way.\nThis week's match in China also demonstrated the very human limitations that can surface amid competition, primarily how stress and emotions can hinder decision-making. \"I was very excited. I could feel my heart bumping,\" Ke said after the game. \"Maybe because I was too excited, I made some stupid moves.\"\nBeyond the game of Go, DeepMind scientists say that the technology that powers AlphaGo can help societies develop better ways to conserve energy and boost health research.\nThe third and final game pitting Ke against AlphaGo is set to take place Saturday, as part of Google's \"Future of Go Summit.\" Another match will feature two professional human players facing one another, but each will have their own AlphaGo teammate.\nhamza.shaban@washpost.com\n \uf14c More at www.washingtonpost.com\/news\/innovations\n","265":"SAN FRANCISCO -- Microsoft's co-founder Paul Allen said Wednesday that he was pumping an additional $125 million into his nonprofit computer research lab for an ambitious new effort to teach machines \"common sense.\"\nThe money for the Allen Institute for Artificial Intelligence will about double the lab's budget over the next three years, helping to fund existing research as well as the new effort, called Project Alexandria. In the years and decades to come, the lab hopes to create a database of fundamental knowledge that humans take for granted but machines have always lacked.\n\"To make real progress in A.I., we have to overcome the big challenges in the area of common sense,\" said Mr. Allen, who founded the software giant Microsoft in the 1970s with Bill Gates.\nToday, machines can recognize nearby objects, identify spoken words, translate one language into another and mimic other human tasks with an accuracy that was not possible just a few years ago. These talents are readily apparent in the new wave of autonomous vehicles, warehouse robotics, smartphones and digital assistants.\nBut these machines struggle with other basic tasks. Though Amazon's Alexa does a good job of recognizing what you say, it cannot respond to anything more than basic commands and questions. When confronted with heavy traffic or unexpected situations, driverless cars just sit there.\nA.I. \"recognizes objects, but can't explain what it sees. It can't read a textbook and understand the questions in the back of the book,\" said Oren Etzioni, a former University of Washington professor who oversees the Allen Institute for Artificial Intelligence. \"It is devoid of common sense.\"\nSuccess may require years or even decades of work -- if it comes at all. Others have tried to digitize common sense, and the task has always proved too large.\nIn the mid-1980s, Doug Lenat, a former Stanford University professor, with backing from the government and several of the country's largest tech companies, started a project called Cyc. He and his team of researchers worked to codify all the simple truths that we learn as children, from \"you can't be in two places at the same time\" to \"when drinking from a cup, hold the open end up.\"\nThirty years later, Mr. Lenat and his team are still at work on this \"common sense engine\" -- with no end in sight.\nMr. Allen helped fund Cyc, and he believes it is time to take a fresh approach, he said, because modern technologies make it easier to build this kind of system.\nMr. Lenat welcomed the new project. But he also warned of challenges: Cyc has burned through hundreds of millions of dollars in funding, running into countless problems that were not evident when the project began. He called them \"buzz saws.\"\nFollow Cade Metz on Twitter: @CadeMetz. \nPHOTO: \"To make real progress in A.I., we have to overcome the big challenges in the area of common sense,\" said Paul Allen, who founded Microsoft in the 1970s with Bill Gates. (PHOTOGRAPH BY B\u00e9atrice de G\u00e9a for The New York Times FOR THE NEW YORK TIMES)Related Articles\n\n","267":"BEIJING -- Amazon and two other American titans are trying to shake up health care by experimenting with their own employees' coverage. By Chinese standards, they're behind the curve.\nTechnology companies like Alibaba and Tencent have made health care a priority for years, and are using China as their laboratory. After testing online medical advice and drug tracking systems, they are now focused on a more advanced tool: artificial intelligence. \n  Their aggressive push underscores the differences between the health care systems in China and the United States.\n  Chinese hospitals are overburdened, with just 1.5 doctors for every 1,000 people -- barely half the figure in the United States. Along with a rapidly aging population, China also has the largest number of obese children in the world, as well as more diabetes patients than anywhere else.\u00a0\n  The companies' technological push is encouraged by the government. Beijing has said it wants to be a leader in A.I. by 2030 and pledged to take on the United States in the field. While officials have emphasized the use of artificial intelligence in areas like defense and self-driving cars, they have also aggressively promoted its use in health care.\n  Alibaba and Tencent, which already dominate China's e-commerce and mobile payments sectors, are at the forefront. Among their goals: building diagnostic tools that will make doctors more efficient.\n  Amazon and its partners, JPMorgan Chase and Berkshire Hathaway, see technology as a way to provide simplified, affordable medical services. Although the alliance is still in the early stages, it could create online services for medical advice or use its overall heft to negotiate for lower drug prices.\n  ''It's fair to say that across the board, the Chinese tech companies have all embraced being involved in and being active in the health care space, unlike the U.S., where some of them have and some have not,'' said Laura Nelson Carney, an Asia-Pacific health care analyst at Bernstein Research.\n  ''Few of them have made moves as big as in China,'' Ms. Carney said, referring to Alibaba and Tencent's American rivals.\n  Those big moves have had varying degrees of success.\n  In 2014, Alibaba announced a ''future hospital'' plan intended to make treatment more efficient by allowing patients to consult with doctors online and order drugs via the internet. But two years later, Chinese regulators stopped the sale of over-the-counter drugs on Tmall, Alibaba's e-commerce website. They also suspended a drug-monitoring system that the company had created. And last year, the search engine company Baidu scrapped its internet health care service, which allowed patients to book doctors appointments through an app, in a bid to focus solely on A.I.\n  But some of the more recent initiatives have made inroads. Last year, Alibaba's health unit introduced A.I. software that can help interpret CT scans and an A.I. medical lab to help doctors make diagnoses. About a month later, Tencent unveiled Miying, a medical imaging program that helps doctors detect early signs of cancer, in the southwestern region of Guangxi. It is now used in nearly 100 hospitals across China.\n  Tencent has also invested in WeDoctor Group, which has opened its own take on Alibaba's ''future hospital'' in northwestern China. The service allows patients to video chat with doctors and fill their prescriptions online.\n  Advances in artificial intelligence have already been transformative for China's overworked doctors.\n  Dr. Yu Weihong, an ophthalmologist at Peking Union Medical College Hospital, said she used to take up to two days to analyze a patient's eyes by scrutinizing grainy images before discussing her findings with colleagues and writing up a report. Artificial intelligence software currently being tested by the hospital helps her do all that dramatically faster.\n  ''Now, you don't even need a minute,'' she said.\n  The software has been developed by VoxelCloud, a start-up has raised about $28.5 million from companies including Tencent and the Silicon Valley venture capital firm Sequoia Capital. It specializes in automated medical image analysis, helping eye doctors like Dr. Yu screen patients for diabetic retinopathy, the leading cause of blindness among China's working-age population.\n  There are just 20 eye doctors for every million people here, a third of the proportion in the United States. In April, Beijing announced an ambitious plan for the country's 110 million diabetics to undergo eye tests.\n  ''It's impossible for one person to read that many images,'' said Dr. Yu.\n  Ding Xiaowei, whose grandparents were doctors, founded VoxelCloud in 2016, three months after completing his doctorate in computer science at U.C.L.A. The company, which has offices in Los Angeles and the Chinese cities of Shanghai and Suzhou, is awaiting the green light from China's version of the F.D.A. for five diagnostic tools for CT scans and retina disease.\n  The sheer size of China's population -- nearly 1.4 billion people who could provide a vast number of images to feed into their systems -- provides a potential advantage for the development of artificial intelligence. Also helping: China has fewer concerns about privacy, allowing for easier collection of data that could result in smarter and more efficient A.I. systems. Regulation here isn't as strict as in the United States, either.\n  In all, more than 130 companies are applying A.I. in ways that could increase the efficiency of China's health care system, according to Yiou Intelligence, an industry consultancy based in Beijing. They range from behemoths like Alibaba and Tencent to domestic champions iFlyTek, which invented a robot that passed a Chinese medical licensing exam, and an array of smaller start-ups.\n  Money is flowing in. As of last August, venture capitalists such as Sequoia and Matrix Partners had invested at least $2.7 billion in such businesses, according to Yiou. Analysts at Bernstein estimated that spending in China's health tech industry will reach $150 billion by 2020.\n  Behind this push is a realization that the country's health care system is in crisis. With no functioning primary care system, patients flock to hospitals in major cities, sometimes camping out overnight just to get treatment for a fever. Doctors are overworked, and reports of stabbings and assaults by frustrated patients and their relatives are not uncommon.\n  Yunfeng, the personal investment fund of the Alibaba founder Jack Ma, has invested in one company, Yitu, that hopes to address the shortfall of resources. Yitu is working with Zhejiang Provincial People's Hospital, the best medical facility in eastern Zhejiang province, to develop software that automates the identification of early stages of lung cancer.\n  While it initially focused on facial recognition, Yitu has branched out into more complex image-recognition challenges, like cancer scans. Lin Chenxi, who left Alibaba to establish the company in 2012, said he hoped to use the technology to ensure equal access to medical treatment across China.\n  ''In China, medical resources are very scarce and unequally distributed so that the top resources are concentrated in provincial capitals,'' he said. ''With this system, if it can be used at hospitals in rural cities, then it will make the medical experience much better.''\n  Trying to identify cancer nodes -- shifting black-and-white splotches that look something like a Rorschach test -- is grueling work, and China's doctors have far less time and resources than their counterparts in the United States and elsewhere. Gong Xiangyang, the head of the hospital's radiology department, likened the process to a factory, where burnout and mistakes from overwork can happen.\n  ''We have to deal with a vast amount of medical images everyday,'' he said. ''So we welcome technology if it can relieve the pressure while boosting efficiency and accuracy.''\n\n\n\n","271":"SAN FRANCISCO -- One of the poorest-kept secrets in Silicon Valley has been the huge salaries and bonuses that experts in artificial intelligence can command. Now, a little-noticed tax filing by a research lab called OpenAI has made some of those eye-popping figures public.\nOpenAI paid its top researcher, Ilya Sutskever, more than $1.9 million in 2016. It paid another leading researcher, Ian Goodfellow, more than $800,000 -- even though he was not hired until March of that year. Both were recruited from Google. \u00a0\n  A third big name in the field, the roboticist Pieter Abbeel, made $425,000, though he did not join until June 2016, after taking a leave from his job as a professor at the University of California, Berkeley. Those figures all include signing bonuses.\n  The figures listed on the tax forms, which OpenAI is required to release publicly because it is a nonprofit, provide new insight into what organizations around the world are paying for A.I. talent. But there is a caveat: The compensation at OpenAI may be underselling what these researchers can make, since as a nonprofit it can't offer stock options.\n  Salaries for top A.I. researchers have skyrocketed because there are not many people who understand the technology and thousands of companies want to work with it. Element AI, an independent lab in Canada, estimates that 22,000 people worldwide have the skills needed to do serious A.I. research -- about double from a year ago.\n  ''There is a mountain of demand and a trickle of supply,'' said Chris Nicholson, the chief executive and founder of Skymind, a start-up working on A.I.\n  That raises significant issues for universities and governments. They also need A.I. expertise, both to teach the next generation of researchers and to put these technologies into practice in everything from the military to drug discovery. But they could never match the salaries being paid in the private sector.\n  In 2015, Elon Musk, the chief executive of the electric-car maker Tesla, and other well-known figures in the tech industry created OpenAI and moved it into offices just north of Silicon Valley in San Francisco. They recruited several researchers with experience at Google and Facebook, two of the companies leading an industrywide push into artificial intelligence.\n  In addition to salaries and signing bonuses, the internet giants typically compensate employees with sizable stock options -- something that OpenAI does not do. But it has a recruiting message that appeals to idealists: It will share much of its work with the outside world, and it will consciously avoid creating technology that could be a danger to people.\n  ''I turned down offers for multiple times the dollar amount I accepted at OpenAI,'' Mr. Sutskever said. ''Others did the same.'' He said he expected salaries at OpenAI to increase as the organization pursued its ''mission of ensuring powerful A.I. benefits all of humanity.''\n  OpenAI spent about $11 million in its first year, with more than $7 million going to salaries and other employee benefits. It employed 52 people in 2016.\n  People who work at major tech companies or have entertained job offers from them have told The New York Times that A.I. specialists with little or no industry experience can make between $300,000 and $500,000 a year in salary and stock. Top names can receive compensation packages that extend into the millions.\n  ''The amount of money was borderline crazy,'' Wojciech Zaremba, a researcher who joined OpenAI after internships at Google and Facebook, told Wired. While he would not reveal exact numbers, Mr. Zaremba said big tech companies were offering him two or three times what he believed his real market value was.\n  At DeepMind, a London A.I. lab now owned by Google, costs for 400 employees totaled $138 million in 2016, according to the company's annual financial filings in Britain. That translates to $345,000 per employee, including researchers and other staff.\n  Researchers like Mr. Sutskever specialize in what are called neural networks, complex algorithms that learn tasks by analyzing vast amounts of data. They are used in everything from digital assistants in smartphones to self-driving cars.\n  Some researchers may command higher pay because their names carry weight across the A.I. community and they can help recruit other researchers.\n  Mr. Sutskever was part of a three-researcher team at the University of Toronto that created key so-called computer vision technology. Mr. Goodfellow invented a technique that allows machines to create fake digital photos that are nearly indistinguishable from the real thing.\n  ''When you hire a star, you are not just hiring a star,'' Mr. Nicholson of the start-up Skymind said. ''You are hiring everyone they attract. And you are paying for all the publicity they will attract.''\n  Other researchers at OpenAI, including Greg Brockman, who leads the lab alongside Mr. Sutskever, did not receive such high salaries during the lab's first year.\n  In 2016, according to the tax forms, Mr. Brockman, who had served as chief technology officer at the financial technology start-up Stripe, made $175,000. As one of the founders of the organization, however, he most likely took a salary below market value. Two other researchers with more experience in the field -- though still very young -- made between $275,000 and $300,000 in salary alone in 2016, according to the forms.\n  Though the pool of available A.I. researchers is growing, it is not growing fast enough. ''If anything, demand for that talent is growing faster than the supply of new researchers, because A.I. is moving from early adopters to wider use,'' Mr. Nicholson said.\n  That means it can be hard for companies to hold on to their talent. Last year, after only 11 months at OpenAI, Mr. Goodfellow returned to Google. Mr. Abbeel and two other researchers left the lab to create a robotics start-up, Embodied Intelligence. (Mr. Abbeel has since signed back on as a part-time adviser to OpenAI.) And another researcher, Andrej Karpathy, left to become the head of A.I. at Tesla, which is also building autonomous driving technology.\n  In essence, Mr. Musk was poaching his own talent. Since then, he has stepped down from the OpenAI board, with the lab saying this would allow him to ''eliminate a potential future conflict.''\n\n\n\n","273":"Unlike big, impersonal online agencies, the best travel agents know a great deal about their clients and their travel choices. Now several new travel companies are creating data-driven, automated agents that rely on users' personal preferences to make the travel-planning process easier.\nWhether they use human knowledge or artificial intelligence (or both), these next-generation travel agents do the search-culling for you, tailoring the results to your stated preferences and potentially cutting down on web-browsing time. They also use text messaging as their primary communications mode, often via a chatbot, a computer program designed to converse in text. \n  ''Rather than going into an online travel agency and doing a search and seeing a list of 150 hotels, you enter in your profile what you're looking for and a chatbot serves up a curated list of three to four in a messaging interface,'' said Douglas Quinby, a senior vice president at the travel research firm Phocuswright. ''The ideal is fewer options more tailored to your request.''\u00a0\n  Most of these services are challenging the do-it-yourself system of browsing as offered by services like Expedia. New-wave agents -- human, robotic or a combination -- will also allow users to continue a search over time, rather than start anew with a browser each session.\n  ''This is very much the early alpha stage of all of this,'' said Henry Harteveldt, a travel industry analyst and the president of Atmosphere Research Group. ''It's very much the first wave, which is exciting, but the use of artificial intelligence is far from established and also, frankly, far from perfect.''\n  The following pioneers are tinkering with the way travel is planned and booked, with payment models that vary from subscriptions to pay-per-use.\n  Membership Services\n  Using a blend of technology and human interaction, Pana caters to frequent travelers, charging $49 a month for its services, available around the clock. Computer programs funnel requests and member profiles, including past trips, to human agents who text back.\n  ''Pana was borne out of two pain points,'' said Devon Tivona, its chief executive. ''First, all the technology pulled me away from just emailing to get something done because I've become my own travel agent. Second is getting access to real-time help.''\n  On the human end of the neo-agency spectrum, Savanti Travel tends to its clients' plans as the founders, Dan Lack and Leigh Rowan, said they do their own, with an eye to saving money and maximizing loyalty programs.\n  ''It's our strategy, not just a computer's,'' said Mr. Rowan, describing the service as managing airline and hotel bookings not just to travel cheaply but to accrue status with travel companies. ''The downside of working with us is the onboarding is intense. We get to know you as a human, not just a set of data points.''\n  Membership fees start at $1,000 a month for unlimited travel planning, which they say eliminates the conflict of interest inherent in a commission-based system, where revenue rises with more expensive bookings. They recommend status-conferring credit cards and help manage the programs to use points for free travel.\n  ''We sit at the intersection of hustle and hospitality,'' Mr. Lack said. ''We're old-school hospitality and new-school intelligence.''\n  On-Demand Services\n  More accessible to casual travelers, free travel planning services, like bricks-and-mortar travel agencies, make their money through commissions.\n  Originally launched in 2015 as a personal assistant tackling tasks from shopping to travel booking, Mezi shifted to handling travel exclusively last year. The company's chief executive, Swapnil Shinde, who is also a founder, said its chatbots handle most transactions in five or fewer messages. In complex cases that robots cannot handle, human agents act as troubleshooters who, after solving problems, train the bots in that resolution. ''We've built it so that every morning it's smarter than the previous night,'' he said.\n  The more travelers use Mezi, the more it knows about their preferences, making it likely that Mezi will suggest a boutique hotel in a museum district for those who have shown an interest in design and art.\n  The online agency Hipmunk operates Hello Hipmunk, a free messaging system for travel planning using Facebook Messenger, Skype or Slack that can start with a flight request, wander into a conversation about hotels and resume flight bookings in a style that mimics human conversations.\n  ''This is in a sense going back to the future,'' said Adam Goldstein, the chief executive of Hipmunk. ''We're talking about doing things that you could have done or could today do with a travel agent, except this travel agent is available 24 hours a day, seven days a week.''\n  One entertaining aspect of Hello Hipmunk is probing it for planning tips, asking questions like what's the cheapest week to travel in October and where can I fly direct to the tropics from Chicago in February?\n  Expedia now offers messaging-based searches and bookings on Facebook Messenger, Skype and other platforms. Its goal is to provide more creative advice to customers.\n  ''We're supporting hotel changes and flight cancellations through the bot, but the longer-term goal, and it's aspirational, is to have a conversation about what to do when it's raining in Hawaii,'' said Dave Fleischman, the vice president of global product for Expedia.\n  Flightfox, which books airfares only, works differently. Founded in 2012, it originally tried to crowdsource flight savings by distributing requests to freelance bookers who would compete to find the best fare for a fee.\n  ''It was nice in theory, but we realized we needed someone to be responsible for your trip,'' said Todd Sullivan, a founder.\n  Now, users submit a flight request, and Flightfox's network of agents takes on the booking task, usually for a $50 fee, though it can go to $100 or more for complicated itineraries. The agency specializes in knowing the ins and outs of points systems to maximize value, especially for business-class flights or complex itineraries.\n  Instead of booking the trip for you, agents provide links for self-booking to maintain transparency about costs and to avoid collecting personal information like passport numbers.\n  Free-and-Fee Hybrid\n  Entirely powered by chatbots, HelloGbye introduced its app in March, offering both a free service for booking hotels and flights and a $19-a-month subscription that offers preferred hotel rates, 2 percent cash back on hotel bookings and no change fees on itineraries.\n  ''It's like the Costco model,'' said Greg Apple, the marketing chief for HelloGbye, which targets frequent and business travelers. ''You get savings in bulk and if you spend a lot you get a check back at the year end.''\n\n\n\n","274":"One of the hottest technology trends in recent years has been the rise of artificial intelligence. What people often overlook is that the trend was not born in Silicon Valley, but in other places, including Canada.\nCanada was the home of many researchers including Geoff Hinton, a computer scientist at the University of Toronto, who specialized in a form of A.I. known as machine learning. Their work has helped underpin a wave of new developments in the field.\nBut their success also brought some unintended consequences. Silicon Valley companies took notice of their achievements and using deep pockets and big promises, lured many A.I. experts away from Canada.\nNow Canada is trying to ensure it keeps its A.I. talent and foster a homegrown industry around the technology, writes Steve Lohr. The Canadian governmenthas pledged $93 million ($125 million Canadian) to support A.I. research centers in Montreal, Toronto, and Edmonton.\nThe research centers will be public-private partnerships. One of them is the Vector Institute for Artificial Intelligence in Toronto, which begins with commitments of $130 million. About half of that sum is coming from national and provincial governments, with the other chunk from corporate sponsors like Google, as well as Canadian companies like Scotiabank.\nMr. Hinton, for one, has stayed in Canada. While he works with Google, he continues to make Toronto his home base. \"I preferred Canada,\" Mr. Hinton said.\nMore tech news:\n Tesla hits a new milestone, passing G.M. in valuation. After passing Ford Motor a week ago, the electric car company edged past General Motors in early trading, with a market capitalization of $51 billion.\nSpain arrests a Russian who is believed to be the kingpin of computer spam. Western cybersecurity researchers have identified the man being held, Peter Levashov, as the spammer known as Peter Severa, though some doubt he is the same person.\n","275":"TOKYO -- If a computer could ace the entrance exam for a top university, what would that mean for mere mortals with average intellects? This is a question that has bothered Noriko Arai, a mathematics professor, ever since the notion entered her head three years ago.\n''I wanted to get a clear image of how many of our intellectual activities will be replaced by machines. That is why I started the project: Can a Computer Enter Tokyo University? -- the Todai Robot Project,'' she said in a recent interview.\u00a0\nTokyo University, known as Todai, is Japan's best. Its exacting entry test requires years of cramming to pass and can defeat even the most erudite. Most current computers, trained in data crunching, fail to understand its natural language tasks altogether.\nMs. Arai has set researchers at Japan's National Institute of Informatics, where she works, the task of developing a machine that can jump the lofty Todai bar by 2021.\nIf they succeed, she said, such a machine should be capable, with appropriate programming, of doing many -- perhaps most -- jobs now done by university graduates.\nWith the development of artificial intelligence, computers are starting to crack human skills like information summarization and language processing.\nGiven the exponential growth of computing power and advances in artificial intelligence, or A.I., programs, the Todai robot's task, though daunting, is feasible, Ms. Arai says. So far her prot\u00e9g\u00e9, a desktop computer named Todai-kun, is excelling in math and history but needs more effort in reading comprehension.\nThere is a significant danger, Ms. Arai says, that the widespread adoption of artificial intelligence, if not well managed, could lead to a radical restructuring of economic activity and the job market, outpacing the ability of social and education systems to adjust.\nIntelligent machines could be used to replace expensive human resources, potentially undermining the economic value of much vocational education, Ms. Arai said.\n''Educational investment will not be attractive to those without unique skills,'' she said. Graduates, she noted, need to earn a return on their investment in training: ''But instead they will lose jobs, replaced by information simulation. They will stay uneducated.''\nIn such a scenario, high-salary jobs would remain for those equipped with problem-solving skills, she predicted. But many common tasks now done by college graduates might vanish.\n''We do not know in which areas human beings outperform machines. That means we cannot prepare for the changes,'' she said. ''Even during the industrial revolution change was a lot slower.''\nOver the next 10 to 20 years, ''10 percent to 20 percent pushed out of work by A.I. will be a catastrophe,'' she says. ''I can't begin to think what 50 percent would mean -- way beyond a catastrophe and such numbers can't be ruled out if A.I. performs well in the future.''\nShe is not alone in such an assessment. A recent study published by the Program on the Impacts of Future Technology, at Oxford University's Oxford Martin School, predicted that nearly half of all jobs in the United States could be replaced by computers over the next two decades.\nSome researchers disagree. Kazumasa Oguro, professor of economics at Hosei University in Tokyo, argues that smart machines should increase employment. ''Most economists believe in the principle of comparative advantage,'' he said. ''Smart machines would help create 20 percent new white-collar jobs because they expand the economy. That's comparative advantage.''\nOthers are less sanguine. Noriyuki Yanagawa, professor of economics at Tokyo University, says that Japan, with its large service sector, is particularly vulnerable.\n''A.I. will change the labor demand drastically and quickly,'' he said. ''For many workers, adjusting to the drastic change will be extremely difficult.''\nSmart machines will give companies ''the opportunity to automate many tasks, redesign jobs, and do things never before possible even with the best human work forces,'' according to a report this year by the business consulting firm McKinsey.\nAdvances in speech recognition, translation and pattern recognition threaten employment in the service sectors -- call centers, marketing and sales -- precisely the sectors that provide most jobs in developed economies. As if to confirm this shift from manpower to silicon power, corporate investment in the United States in equipment and software has never been higher, according to Andrew McAfee, the co-author of ''Race Against the Machine'' -- a cautionary tale for the digitized economy.\nYet according to the technology market research firm Gartner, top business executives worldwide have not grasped the speed of digital change or its potential impact on the workplace. Gartner's 2013 chief executive survey, published in April, found that 60 percent of executives surveyed dismissed as '''futurist fantasy'' the possibility that smart machines could displace many white-collar employees within 15 years.\n''Most business and thought leaders underestimate the potential of smart machines to take over millions of middle-class jobs in the coming decades,'' Kenneth Brant, research director at Gartner, told a conference in October: ''Job destruction will happen at a faster pace, with machine-driven job elimination overwhelming the market's ability to create valuable new ones.''\nOptimists say this could lead to the ultimate elimination of work -- an ''Athens without the slaves'' -- and a possible boom for less vocational-style education. Mr. Brant's hope is that such disruption might lead to a system where individuals are paid a citizen stipend and be free for education and self-realization.\n''This optimistic scenario I call Homo Ludens, or 'Man, the Player,' because maybe we will not be the smartest thing on the planet after all,'' he said. ''Maybe our destiny is to create the smartest thing on the planet and use it to follow a course of self-actualization.''\n","276":"When federal authorities want to ferret out abusive tax shelters, they send an army of forensic accountants, auditors and lawyers to burrow into suspicious tax returns.\nAnalyzing mountains of filings and tracing money flows through far-flung subsidiaries is notoriously difficult; even if the Internal Revenue Service manages to unravel a major scheme, it typically does so only years after its emergence, by which point a fresh dodge has often already replaced it. \n  But what if that needle-in-a-haystack quest could be done routinely, and quickly, by a computer? Could the federal tax laws -- 74,608 pages of legal gray areas and welters of credits, deductions and exemptions -- be accurately rendered in an algorithm?\u00a0\n  New academic research seeks to use artificial intelligence to combat tax evasion by corporate entities, from publicly traded multinationals to private partnerships. The goal is to give the I.R.S. a better way to investigate sophisticated tax shelters that strip tens of billions of dollars from federal coffers each year.\n  ''We see the tax code as a calculator,'' said Jacob Rosen, a researcher at the Massachusetts Institute of Technology who focuses on the abstract representation of financial transactions and artificial intelligence techniques. ''There are lots of extraordinarily smart people who take individual parts of the tax code and recombine them in complex transactions to construct something not intended by the law.''\n  A recent paper by Mr. Rosen and four other computer scientists -- two others from M.I.T. and two at the Mitre Corporation, a nonprofit technology research and development organization -- demonstrated how an algorithm could detect a certain type of known tax shelter used by partnerships.\n  First, the researchers translated tax regulations governing partnerships, a growing source of tax trickery, into source code. Then they rendered the transactions underpinning a questionable shelter known as ''installment-sale bogus optional basis,'' or Ibob, as a series of codes. The Ibob shelter artificially inflates the basis value of an asset on a tax return to wipe out taxable gains when that asset is sold. While some of Ibob's individual transactions are perfectly legal, the collective result is a bogus deduction.\n  Next, the researchers mapped out in code the tangle of entities that make up typical partnerships. The results flagged specific combinations of transactions and partnership structures that were likely to produce the Ibob dodge.\n  Large corporations attract most of the attention when it comes to tax avoidance and tax evasion, but partnerships, which have separate tax rules, are a growing source of worry for the authorities. Commonly used by hedge funds, private equity funds, real estate outfits and oil and gas concerns, partnerships are far less likely to be audited than corporations. A Government Accountability Office r eport from 2010 said that the I.R.S. knew of one million ''networks'' involving partnerships and similar entities, adding that ''the I.R.S. also knows that many questionable tax shelters and abusive transactions rely on the links among commonly owned entities in a network.''\n  Rooting out fraud in corporate tax returns takes place largely through data mining, in which the I.R.S. collects pre-existing data from filed tax returns and analyzes them for patterns. The data goes into a database within the agency's Office of Tax Shelter Analysis, created in 2000 in the wake of a crackdown on mass-market tax shelters sold by accounting firms, law firms and banks. The data-analytical approach depends upon already having some kind of smoking gun, such as a suspicious deduction on a return.\n  By contrast, the artificial intelligence approach does not require pre-existing evidence. Instead, it focuses on rule mining, in which individual tax code regulations are lined up against one another to ascertain if they can be used collectively to create a sophisticated tax dodge.\n  Rule mining takes advantage of a surprising feature of tax shelters: While their inner workings are convoluted and complex, their general aim at the highest level is usually simple and clear -- to lower tax bills by improperly generating bogus losses, deductions, offsets and credits that minus the shelters would not exist.\n  ''It's incredibly difficult to have a computer algorithm that duplicates the enormous creativity of taxpayers, but it's very promising,'' said Robert A. Green, a tax professor at Cornell Law School who read the M.I.T.\/Mitre paper.\n  An I.R.S. spokesman declined to comment.\n  Sanith Wijesinghe, one of the Mitre researchers, admitted that it was unclear whether other parts of the federal tax regulations could be turned into computer code as easily as partnership regulations.\n  ''We're trying to automate as much of the parts that can be automated and still allow folks to continue the brainstorming,'' Mr. Wijesinghe said. ''Because that's what the promoters are doing.''\n\n\n\n","277":"If Cisco Systems' new chief executive has his way, in a couple of years he will have a very different company.\nIn his first big public statement since he took the helm in July, Charles H. Robbins said Cisco would move away from selling individual switches and routers - the plumbing of the Internet and other modern communications systems.\u00a0\nInstead, Cisco will stress integrated parts with lots of software, so that new applications and business uses could be readily deployed. \n\"Companies do not want to be in the business of putting things together,\" he said. \"A massively distributed network is where we are headed.\"\nHe also said Wall Street would need to start seeing Cisco in new ways, as it moves to revenue models closer to those of cloud-computing companies like Salesforce.com, with lots of revenue based on subscriptions. \nA darling of investors during the dot-com boom, Cisco shares have put in a lackluster performance. Yet 80 percent of the world's Internet traffic passes through Cisco machinery.\n\"We believe we can manage them,\" Mr. Robbins said of the analysts covering his stock, adding that it was unclear what \"the end state\" would look like as businesses like collaboration and security increasingly move to subscriptions, and traditional hardware is sold by the piece.\nIt was hardly apparent that Mr. Robbins wanted such a different company in July, when he took over from John Chambers, who ran Cisco for 20 years. Mr. Chambers had long admired IBM, and sought many times to build up a large consulting-based business similar to IBM's.\n\"I don't want to end up there,\" Mr. Robbins said of the IBM model. Instead, he said his sales people would try to work with \"multiple touch points\" throughout a company, in particular between industrial machine operators and information technology departments, to get machines online.\nCisco is planning big investments in analytic tools to help automate both large-scale data centers and billions of objects tied to the Internet. On Monday, Mr. Robbins announced a deal with FANUC America, which has some 300,000 industrial robots in factories, to carry out preventive maintenance, based on Cisco's artificial intelligence products.\nOf course, that means a lot of artificial intelligence, and other computing skills Cisco lacks. \"We've hired lots of A.I. guys, and people with strong mathematics backgrounds,\" Mr. Robbins said.\nZorawar Biri Singh, Cisco's chief technology officer, said some of the new features would also go into existing Cisco products as software modifications. Those changes will be announced in several months, he said.\nThe moves by Cisco come as many of the old giants of technology, including Microsoft, Oracle, Intel, Hewlett-Packard, IBM and Dell, struggle to adapt to new businesses around cloud computing, mobile tech and sensors. \nMr. Robbins appears to believe he can adapt Cisco by changing it from a company that hauls bits around, with some software to help enable a few functions. The new version is a company with a much greater awareness of what is inside a business's overall system, and how it needs to work.\n","278":"Companies are investing heaps of money to develop artificial-intelligence technologies that promise to transform industries as varied as transportation, finance and health care. That all adds up to big economic change, technologists warn.\u00a0\nBut for Treasury Secretary Steven Mnuchin, the artificial intelligence revolution and its impact on the U.S. workforce is \"not even on our radar screen.\" In an interview with Axios, Mnuchin predicted the technology was still 50 to 100 years from displacing human jobs.\n\"I'm not worried at all,\" he said. \"In fact, I'm optimistic.\"\nHis remarks contrast with a growing body of research that suggests the economic shift will be here much sooner.\nLast week, the National Academies of Sciences, Engineering and Medicine released a broad report spelling out the expected impact of automation on\u00a0productivity, employment and income. While new technologies have always posed a threat to specific jobs, \"simultaneous automation of a broader range of tasks could create unemployment or perhaps reduce aggregate levels of employment for an extended period of time,\" the report found.\nThat actual\u00a0impact may depend, the report stated, on whether new jobs are created as quickly as old jobs become obsolete.\nIn the last month of President Barack Obama's administration, the White House released its own report on artificial intelligence. Its conclusion: Millions of jobs could be displaced, particularly those filled by less-educated workers, and the country's economic divide could widen. The government should expand access to education in technical fields and broaden unemployment benefits to compensate for job losses, according to the report.\nBoth reports referenced the coming wave of self-driving cars as an example of disruptive automation. Millions of Americans make their living behind the wheel of a vehicle, be they long-haul trucks, delivery vans or taxicabs. Those jobs are expected to dissipate as automakers and tech companies develop sophisticated technology over the next decade that makes human drivers unnecessary.\nBrookings Institution senior fellow Mark Muro said Mnuchin's assessment is out of line with mainstream thinking on automation, particularly the idea that the impact on human workers will not be felt for decades. On\u00a0the contrary, he said, some jobs in fields like manufacturing have already been displaced.\nThe government is currently grappling with the ramifications of globalization, another major economic shift that left many out of work and with fewer opportunities, Muro said.\u00a0In fact, President Trump gained popularity on the campaign trail with promises to bring manufacturing back to the United States and implement protectionist trade policies that put \"America first.\"\n\"It seems incurious not to view automation as a possible concern,\" Muro said. \"That the nation and its leaders were far too complacent about trade's impacts on some communities would seem to suggest leaders should not be complacent about another, potentially more pervasive, challenge.\"\n              Read more from The Washington Post's Innovations section.\u00a0           \n           Your company's human resources department could get less human        \n           Your car wants to say hello. And that's only the start.        \n           Siri answers your questions. This bot will understand your emotions.        \n","279":"SUNNYVALE, Calif. -- For more than a decade, Silicon Valley's technology investors and entrepreneurs obsessed over social media and mobile apps that helped people do things like find new friends, fetch a ride home or crowdsource a review of a product or a movie.\nNow Silicon Valley has found its next shiny new thing. And it does not have a ''Like'' button. \n  The new era in Silicon Valley centers on artificial intelligence and robots, a transformation that many believe will have a payoff on the scale of the personal computing industry or the commercial internet, two previous generations that spread computing globally. Computers have begun to speak, listen and see, as well as sprout legs, wings and wheels to move unfettered in the world.\n  The shift was evident in a Lowe's home improvement store here this month, when a prototype inventory checker developed by Bossa Nova Robotics silently glided through the aisles using computer vision to automatically perform a task that humans have done manually for centuries.\u00a0\n  The robot, which was skilled enough to autonomously move out of the way of shoppers and avoid unexpected obstacles in the aisles, alerted people to its presence with soft birdsong chirps. Gliding down the middle of an aisle at a leisurely pace, it can recognize bar codes on shelves, and it uses a laser to detect which items are out of stock.\n  Silicon Valley's financiers and entrepreneurs are digging into artificial intelligence with remarkable exuberance. The region now has at least 19 companies designing self-driving cars and trucks, up from a handful five years ago. There are also more than a half-dozen types of mobile robots, including robotic bellhops and aerial drones, being commercialized.\n  ''We saw a slow trickle in investments in robotics, and suddenly, boom -- there seem to be a dozen companies securing large investment rounds focusing on specific robotic niches,'' said Martin Hitch, chief executive of Bossa Nova, which has a base in San Francisco.\n  Funding in A.I. start-ups has increased more than fourfold to $681 million in 2015, from $145 million in 2011, according to the market research firm CB Insights. The firm estimates that new investments will reach $1.2 billion this year, up 76 percent from last year.\n  ''Whenever there is a new idea, the valley swarms it,'' said Jen-Hsun Huang, chief executive of Nvidia, a chip maker that was founded to make graphic processors for the video game business but that has turned decisively toward artificial intelligence applications in the last year. ''But you have to wait for a good idea, and good ideas don't happen every day.''\n  By contrast, funding for social media start-ups peaked in 2011 before plunging. That year, venture capital firms made 66 social media deals and pumped in $2.4 billion. So far this year, there have been just 10 social media investments, totaling $6.9 million, according to CB Insights. Last month, the professional social networking site LinkedIn was sold to Microsoft for $26.2 billion, underscoring that social media has become a mature market sector.\n  Even Silicon Valley's biggest social media companies are now getting into artificial intelligence, as are other tech behemoths. Facebook is using A.I. to improve its products. Google will soon compete with Amazon's Echo and Apple's Siri, which are based on A.I., with a device that listens in the home, answers questions and places e-commerce orders. Satya Nadella, Microsoft's chief executive, recently appeared at the Aspen Ideas Conference and called for a partnership between humans and artificial intelligence systems in which machines are designed to augment humans.\n  The auto industry has also set up camp in the valley to learn how to make cars that can do the driving for you. Both technology and car companies are making claims that increasingly powerful sensors and A.I. software will enable cars to drive themselves with the push of a button as soon as the end of this decade -- despite recent Tesla crashes that have raised the question of how quickly human drivers will be completely replaced by the technology.\n  Silicon Valley's new A.I. era underscores the region's ability to opportunistically reinvent itself and quickly follow the latest tech trend.\n  ''This is at the heart of the region's culture that goes all the way back to the Gold Rush,'' said Paul Saffo, a longtime technology forecaster and a faculty member at Singularity University. ''The valley is built on the idea that there is always a way to start over and find a new beginning.''\n  The change spurred a rush for talent in A.I. that has become intense.\n  ''It's ridiculous,'' said Richard Socher, chief scientist at the software maker Salesforce, who teaches a course at Stanford on a machine intelligence technique known as deep learning. ''The number of people trying to get the students to drop out of the class halfway through because now they know a little bit of this stuff is crazy.''\n  The valley's tendency toward reinvention dates back to the region's initial emergence from the ashes of a deep aerospace industry recession as a consumer-electronics manufacturing center producing memory chips, video games and digital watches in the mid-1970s. A malaise in the personal computing market in the early 1990s was followed by the World Wide Web and the global expansion of the consumer internet.\n  A decade later, in 2007, just as innovation in mobile phones seemed to be on the verge of moving away from Silicon Valley to Europe and Asia, Apple introduced the first iPhone, resetting the mobile communications marketplace and ensuring that the valley would -- for at least another generation -- remain the world's innovation center.\n  In the most recent shift, the A.I. idea emerged first in Canada in the work of cognitive scientists and computer scientists like Geoffrey Hinton, Yoshua Bengio and Yann LeCun during the previous decade. The three helped pioneer a new approach to deep learning, a machine learning method that is highly effective for pattern recognition challenges like vision and speech. Modeled on a general understanding of how the human brain works, it has helped technologists make rapid progress in a wide range of A.I. fields.\n  How far the A.I. boom will go is hotly debated. For some technologists, today's technical advances are laying the groundwork for truly brilliant machines that will soon have human-level intelligence.\n  Yet Silicon Valley has faced false starts with A.I. before. During the 1980s, an earlier generation of entrepreneurs also believed that artificial intelligence was the wave of the future, leading to a flurry of start-ups. Their products offered little business value at the time, and so the commercial enthusiasm ended in disappointment, leading to a period now referred to as the ''A.I. Winter.''\n  The current resurgence will not fall short this time, said several investors, who believe that the economic potential in terms of new efficiency and new applications is strong.\n  ''There is no chance of a new winter,'' said Shivon Zilis, an investor at Bloomberg Beta who specializes in machine intelligence start-ups.\n  John Shoch, a veteran venture capitalist at Alloy Ventures in Palo Alto, Calif., said deep learning has made a difference to the potential success of A.I. companies. ''You get a new set of tools that let you attack a new set of problems, which let you push the boundary out,'' he said.\n  For others, like Jerry Kaplan, who helped found two A.I. companies in the 1980s -- Symantec, which became a security company, and Teknowledge, which ultimately shut down -- the Valley's new enthusiasm is troubling because it suggests an unfounded optimism similar to earlier eras in which the field overpromised and underdelivered.\n  ''Sometimes when I hang around with A.I. enthusiasts here in the valley, I feel like an atheist at a convention of evangelicals,'' he said.\n\n\n\n","280":"Does President Trump represent the new normal in American politics?\nAs the world's oligarchy gathered last week in Davos, Switzerland, to worry about the troubles of the middle class, the real question on every plutocrat's mind was whether the populist upheaval that delivered the presidency to the intemperate mogul might mercifully be over. \n  If it was globalization -- or, more precisely, the shock of imports from China -- that moved voters to put Mr. Trump in the White House, could politicians get back to supporting the market-oriented order once the China shock played out?\n  But for all the wishful elucidations, the cosmopolitan elite can't rid themselves of a stubborn fear: The populist wave that produced President Trump -- not to mention Prime Minister Viktor Orban of Hungary, President Recep Tayyip Erdogan of Turkey and former Prime Minister Silvio Berlusconi in Italy, as well as Britain's exodus from the European Union and the rise of the National Front in France -- may be here to stay.\u00a0\n  China's shock to American politics may be over. Its entry into the market economy at the turn of this century cost millions of manufacturing jobs in the United States. Workers and communities were ravaged, and political positions were pushed to ideological extremes.\n  But few manufacturing jobs are left to lose. And rising wages in China are discouraging some companies from relocating production across the Pacific. What's more, the spread of automation across industries suggests that the era of furious outsourcing in search of cheap foreign labor may be ending.\n  Immigration pressures are likely to persist across the Atlantic, continuing to drive the populist revolt against the establishment elite in Europe. But in the United States, the population of unauthorized immigrants is declining, disproving one of Mr. Trump's core claims to power.\n  Economists studying the changes in the nature of work that produced such an angry political response suggest, however, that another wave of disruption is about to wash across the world economy, knocking out entire new classes of jobs: artificial intelligence. This could provide decades' worth of fuel to the revolt against the global elites and their notions of market democracy.\n  As Frank Levy of the Massachusetts Institute of Technology noted this month in an analysis on the potential impact of artificial intelligence on American politics, ''Given globalization's effect on the 2016 presidential election, it is worth noting that near-term A.I. and globalization replace many of the same jobs.''\n  Consider the occupation of truck drivers. Mr. Levy expects multiple demonstrations of fully autonomous trucks to take place within five years. If they work, the technology will spread, starting in restricted areas on a limited number of dedicated highway lanes. By 2024, artificial intelligence might eliminate 76,000 jobs driving heavy and tractor-trailer trucks, he says.\n  Similarly, he expects artificial intelligence to wipe out 210,000 assembler and fabricator jobs and 260,000 customer service representatives. ''Let's not worry about the future of work in the next 25 years,'' he told me. ''There's plenty to worry about in the next five or six years.''\n  These may not be big numbers, but they are hitting communities that expressed their contempt for the status quo in 2016. White men and women without a four-year college degree accounted for just under half of Mr. Trump's voters -- compared with fewer than a fifth of Hillary Clinton's. Seventy percent of truck drivers, 63 percent of assemblers and fabricators, and 56 percent of customer service representatives share these characteristics.\n  To be sure, economic dislocations don't have to produce populist politics. Daron Acemoglu of M.I.T. notes that geography makes a difference: If the dislocation from A.I. is concentrated in big cities, where workers have more options to find new jobs, the backlash will be more muted than it was when trade took out the jobs of single-industry company towns.\n  What's more, Mr. Acemoglu added, the political system can respond in different ways to workers' pain: The Great Depression not only led to Nazi Germany, it also produced Sweden's social democracy.\n  It's not immediately obvious that artificial intelligence will produce the same kind of reaction that trade did. Sure, machines inspired the most memorable worker rebellion of the industrial revolution -- when the Luddites smashed the weaving machines that were taking over their jobs. The word ''sabotage'' comes from the French workers who took to destroying gears.\n  Unions are suspicious of technology. The United Farm Workers loudly protested tomato-harvesting machines after they were introduced in California in the 1960s. In New York, the local of the ''sandhogs'' who dig subway tunnels negotiated a deal where it gets $450,000 for each tunnel-digging machine used, to make up for job losses caused by ''technological advancement.''\n  Yet though automation has displaced many more jobs than trade ever could, robots have never inspired the fury that trade routinely does. ''By all accounts, automation and new digital technologies played a quantitatively greater role in deindustrialization and in spatial and income inequalities,'' wrote Dani Rodrik of the Kennedy School of Government at Harvard University. ''But globalization became tainted with a stigma of unfairness that technology evaded.''\n  It's easier to demonize people -- especially foreigners -- than machines, the children of invention. What's more, imports from countries with cheaper labor, weaker worker protections and threadbare environmental standards will be seen as unfair. Thea Lee, a former deputy chief of staff of the A.F.L.-C.I.O. who now heads the Economic Policy Institute, notes that workers' anger is directed against ''the particular set of rules about globalization that we chose,'' which spreads benefits among financiers and corporations while disregarding workers.\n  This time could be different, though. ''That sense of unfairness can be attached to technological changes, too,'' Mr. Rodrik told me. ''It's not Bill Gates, who came out of nowhere, but big corporations that are getting bigger and becoming monopolists.''\n  Indeed, artificial intelligence could move populism in a different direction. Mr. Rodrik proposes two varieties, of right and left. The two share an anti-establishment flavor and claim to speak for the people against the elites. Both oppose classic liberal economics and globalization. Both are often authoritarian.\n  But right-wing populism -- like that harnessed in Europe -- is provoked by immigration. Its clan consciousness exploits cleavages of race, religion and nationality. On the left, by contrast, the ''us versus them'' narrative focuses on the economic divide between the capitalists and the working class. Populists of the left mostly take aim at trade.\n  The United States was ripe for both reflexes. Over the last 50 years, as the nation opened its markets to foreign trade, it never set up a social safety net to help workers dislodged by change, as Europe did. It also experienced large-scale immigration across the southern border. And it was walloped by a financial crisis that proved to typical workers that Wall Street would always get a better deal.\n  Mr. Trump's discourse straddles the divide between the ideological domains, vilifying both trade and immigration. But his policies -- tax cuts and immigration restrictions -- hew decidedly to the right.\n  It is not a great fit for a big-tech future. A world in which immigration is on the decline yet some Google technology is taking the jobs of truckers and cashiers sounds compatible with a leftist policy platform that takes on Wall Street and corporate behemoths.\n  That is a world in which, say, Bernie Sanders would thrive. And that alone could give the cocktail class that gathered in Davos something to worry about.\n\n\n\n","281":"FOR months Electronic Arts has been trumpeting its coming subscription game Majestic (www.majestic.ea.com), which will be played in real time and will use e-mail, phone calls and Web sites to create a completely immersive experience. While Majestic has not been released yet, someone else has created a game along the same lines. But this game is free and is only now becoming well known.\n     The game begins with an online trailer (countingdown.com\/features\/?feature_id=16381) for \"A.I.,\" a Steven Spielberg film about artificial intelligence that will be released on June 29. Watch the trailer carefully. Watch it again. One more time. See anything? Look harder.\n I'll give you a hint. Look at the credits, especially the unusual \"sentient machine therapist\" credit given to Jeanine Salla. \nUsing an Internet search engine, you can locate a site where you'll learn that Dr. Salla is a professor in the artificial intelligence department of a university founded in 2028. She specializes in robot intelligence.\nJeanine Salla is one entry point into the most remarkable movie-promotion gimmick ever created for the Web. It is a nameless puzzle-and-adventure game, with (so far) nameless designers, presented through a vast collection of sites portraying an entire universe of the future.\nDr. Salla's world is notable not only for its elaborate vision and core murder mystery but also for its subtlety. There is no invitation on the Warner Brothers \"A.I.\" Web site to play (aimovie.warnerbros.com), but you can see another trailer. You either have to find the game through the clues in the two \"A.I.\" trailers (the clue in the trailer on the \"A.I.\" Web site is so obscure that it is hard to understand how it would ever be recognized) or hear about it by word of mouth.\nSince the game was discovered three weeks ago, word is spreading. While a few people have posted their discovery of the trailer clue on the message forum at the \"A.I.\" site, most have heard about Dr. Salla through an article in Ain't It Cool (www.aintitcool.com\/display.cgi?id=8659), an independent Web site with entertainment news.\nAfter visiting the Web site of Dr. Salla's university, you will find other connected Web sites, all painting a disturbing picture of the future. A robot emancipation site bitterly denounces the Supreme Court's recent Dred Scot II decision, which asserts that intelligent robots are only property. The Anti-Robot Militia, on the other hand, calls for solidarity against the robot hordes. \"It's not the color of your skin that's important,\" a poster proclaims, \"it's the flesh inside.\"\nOn these Web sites there are e-mail addresses and telephone numbers that you can use to gain further information. Early in the game, it was possible for people to leave their home phone numbers with the Anti-Robot Militia. After a few people got threatening phone calls from a pro-robot hacker, the militia changed its Web site, saying it could no longer take phone numbers because its security had been compromised.\nThe puzzles in the game are remarkably difficult -- for me, many of them seem unsolvable. For one puzzle you need some knowledge of chemical elements, and for another you need to recognize that \"Knock! Knock! Knock! Who's there?\" is an obscure line from a famous play.  Some clues are hidden within the comments of the HTML code of Web pages, while others can be discovered only by manipulating images with graphics software.\nThere are very few people who have the necessary knowledge and know-how to solve all these puzzles, but a thousand people working together will do better. Ad-hoc groups have formed on the Internet to pool information and ideas. The largest, Cloudmakers (groups.yahoo.com\/group\/cloudmakers), has more than a thousand members. Together these people can solve the most obscure and intricate puzzles.\nThey are also not above cheating, going so far as to write a program that hacked through a particularly difficult puzzle, then trying to use that information to figure out how it could have been solved. They have even used advanced Internet tools to discover the names of the registered owners of those sites. \nTwo weeks ago, a mysterious figure who took the name Father sent e-mail to Ain't It Cool and Cloudmakers to say the Puppetmasters, Father's designation for the game's mysterious creators, didn't approve of the way that Cloudmakers and Ain't It Cool were playing the game because they were \"circumventing traditional methods.\" A debate on whether the e-mail was a hoax was inconclusive.\nThere is proof that the Puppetmasters, sometimes called the Muppetmasters at the Cloudmakers site, appreciate the vast conspiracy to solve their game. Recently a new page showed up on one of the game sites presumably run by the Puppetmasters that referred viewers to the Cloudmakers. It is probable that the Muppetmasters have become members of the group and are supplying answers and red herrings as they see fit. \nWhat is remarkable is the extraordinary amount of thought and effort apparent in this unpublicized game. There are almost two dozen game sites active now, and more will be introduced. The sites are wildly different in design, from slick corporate sites to chatty home pages. There are two university sites, a coroner's site, a hat store and several sites of architects who build intelligent houses. One of the architect's sites is written entirely in German. These sites all look completely real, yet they have been created out of whole cloth entirely for this game.\nIt is likely that the game's designers plan to keep things going until the film comes out this summer. More puzzles will appear and more Web sites will show up until the mystery can finally be solved and there will be nothing left but to see the movie. \nThe \"A.I.\" game would be fascinating even if the movie didn't exist.  One only wonders how the film will ever be as good as its marketing gimmick.New Twist on Old Game\nIT may be hard to remember the days when children didn't sit on airplanes feverishly pushing buttons on pocket-size game consoles. But I was a child in those days, and one of my favorite games had me carefully tilting a plastic case as I tried to roll little metal balls into indentations in the bottom. \nNow Nintendo has used advanced motion-sensor technology to let you roll a little ball around the screen by tilting your Game Boy Color, and while it may be nothing but a souped-up version of that old low-tech game, it is also technology at its coolest.\nIn the game, called Kirby Tilt 'n' Tumble, Kirby is a little pink, round creature who will roll downhill when you tilt your Game Boy. If he needs to board a passing cloud, just give the Game Boy a sharp jerk and up he jumps. Even though Kirby is nothing but a collection of animated pink pixels, he moves as if he is affected by gravity. \nFor anyone familiar with the way game consoles work, that is remarkable, although for my wife, who had never used a Game Boy, the fact that Kirby rolls around the screen wasn't at all surprising. For her, all technology is inherently mysterious and magical, and with no real concept of its limits she probably wouldn't be surprised if computers were controlled by thoughts or you could send a fax through your DVD player. \nThe technology behind Kirby is so remarkable that even a badly designed game could keep a player engrossed for hours, but Kirby is wonderfully designed, and the action is among the best I've seen in any Game Boy game. As Kirby wends his way along treacherous paths, hops from cloud to cloud, negotiates crumbling walkways and battles spinning eyeballs, he shows us the whole range of tilting possibilities. \nKirby also introduces difficulties never before seen in a game. Because the screen is dim, it's often necessary to angle the Game Boy to face a light source, but that can roll Kirby right off a precipice. And children who play Kirby in the back seat of the car will learn to loathe potholes and quick turns, which can make Kirby spin out of control. If you start seeing young children attending town hall meetings to demand loudly that something be done about the deplorable state of city streets, blame Kirby.Kirby Tilt 'n' Tumble; published and developed by Nintendo for the Game Boy Color; $34.95; for all ages.\n","282":"Wealth and influence in the technology business have always been about gaining the upper hand in software or the machines that software ran on.\nNow data - gathered in those immense pools of information that are at the heart of everything from artificial intelligence to online shopping recommendations - is increasingly a focus of technology competition. And academics and some policy makers, especially in Europe, are considering whether big internet companies like Google and Facebook might use their data resources as a barrier to new entrants and innovation.\nIn recent years, Google, Facebook, Apple, Amazon and Microsoft have all been targets of tax evasion, privacy or antitrust investigations. But in the coming years, who controls what data could be the next worldwide regulatory focus as governments strain to understand and sometimes rein in American tech giants.\u00a0\nThe European Commission and the British House of Lords both issued reports last year on digital \"platform\" companies that highlighted the essential role that data collection, analysis and distribution play in creating and shaping markets. And the Organization for Economic Cooperation and Development held a meeting in November to explore the subject, \"Big Data: Bringing Competition Policy to the Digital Era.\"\nAs government regulators dig into this new era of data competition, they may find that standard antitrust arguments are not so easy to make. Using more and more data to improve a service for users and more accurately target ads for merchants is a clear benefit, for example. And higher prices for consumers are not present with free internet services.\n\"You certainly don't want to punish companies because of what they might do,\" said Annabelle Gawer, a professor of the digital economy at the University of Surrey in England, who made a presentation at the Organization for Economic Cooperation and Development meeting. \"But you do need to be vigilant. It's clear that enormous power is in the hands of a few companies.\"\nMaurice Stucke, a former Justice Department antitrust official and a professor at the University of Tennessee College of Law, who also spoke at the gathering, said one danger was that consumers might be afforded less privacy than they would choose in a more competitive market.\nThe competition concerns echo those that gradually emerged in the 1990s about software and Microsoft. The worry is that as the big internet companies attract more users and advertisers, and gather more data, a powerful \"network effect\" effectively prevents users and advertisers from moving away from a dominant digital platform, like Google in search or Facebook in consumer social networks.\nEvidence of the rising importance of data can be seen from the frontiers of artificial intelligence to mainstream business software. And certain data sets can be remarkably valuable for companies working on those technologies.\nA prime example is Microsoft's purchase of LinkedIn, the business social network, for $26.2 billion last year. LinkedIn has about 467 million members, and it houses their profiles and maps their connections.\nMicrosoft is betting LinkedIn, combined with data on how hundreds of millions of workers use its Office 365 online software, and consumer data from search behavior on Bing, will \"power a set of insights that we think is unprecedented,\" said James Phillips, vice president for business applications at Microsoft.\nIn an email to employees, Satya Nadella, Microsoft's chief executive, described the LinkedIn deal as a linchpin in the company's long-term goal to \"reinvent productivity and business processes\" and to become the digital marketplace that defines \"how people find jobs, build skills, sell, market and get work done.\"\nIBM has also bet heavily on data for its future. Its acquisitions have tended to be in specific industries, like its $2.6billion purchase last year of Truven Health, which has data on the cost and treatment of more than 200 million patients, or in specialized data sets useful across several industries, like its $2 billion acquisition of the digital assets of the Weather Company.\nIBM estimates that 70 percent of the world's data is not out on the public web, but in private databases, often to protect privacy or trade secrets. IBM's strategy is to take the data it has acquired, add customer data and use that to train its Watson artificial intelligence software to pursue such tasks as helping medical researchers discover novel disease therapies, or flagging suspect financial transactions for independent auditors.\n\"Our focus is mainly on nonpublic data sets and extending that advantage for clients in business and science,\" said David Kenny, senior vice president for IBM's Watson and cloud businesses.\nAt Google, the company's drive into cloud-delivered business software is fueled by data, building on years of work done on its search and other consumer services, and its recent advances in image identification, speech recognition and language translation.\nFor example, a new Google business offering - still in the test, or alpha, stage - is a software service to improve job finding and recruiting. Its data includes more than 17 million online job postings and the public profiles and r\u00e9sum\u00e9s of more than 200 million people.\nIts machine-learning algorithms distilled that to about four million unique job titles, ranked the most common ones and identified specific skills. The job sites CareerBuilder and Dice are using the Google technology to show job seekers more relevant openings. And FedEx, the giant package shipper, is adding the service to its recruiting site.\nThat is just one case, said Diane Greene, senior vice president for Google's cloud business, of what is becoming increasingly possible - using the tools of artificial intelligence, notably machine learning, to sift through huge quantities of data to provide machine-curated data services.\n\"You can turn this technology to whatever field you want, from manufacturing to medicine,\" Ms. Greene said.\nFei-Fei Li, director of the Stanford Artificial Intelligence Laboratory, is taking a sabbatical to become chief scientist for artificial intelligence at Google's cloud unit. She sees working at Google as one path to pursue her career ambition to \"democratize A.I.,\" now that the software and data ingredients are ripe.\n\"We wouldn't have the current era of A.I. without the big data revolution,\" Dr. Li said. \"It's the digital gold.\"\nIn the A.I. race, better software algorithms can put you ahead for a year or so, but probably no more, said Andrew Ng, a former Google scientist and adjunct professor at Stanford. He is now chief scientist at Baidu, the Chinese internet search giant, and a leading figure in artificial intelligence research.\nRivals, he added, cannot unlock or simulate your data. \"Data is the defensible barrier, not algorithms,\" Mr. Ng said.\nPHOTO: A Google data center in Oklahoma. Google software builds on work in search and other services. (PHOTOGRAPH BY AMYATLAS@GOOGLE.COM) (B2)\n","283":"Ever since the 1980s, researchers have been working on the development of a quantum computer that would be exponentially more powerful than any of the digital computers that exist today. And now Google, in collaboration with NASA, says it has a quantum computer - the D-Wave 2X - that works.\nGoogle claims the D-Wave 2X is 100 million times as fast as any of today's machines. As a result, this quantum computer could theoretically complete calculations within seconds to a problem that might take a digital computer 10,000 years to calculate. That's particularly important, given the difficult tasks that today's computers are called upon to complete and the staggering amount of data they are called upon to process.\u00a0\nOn the surface, the D-Wave 2X represents a quantum leap not just for computing but also for the field of artificial intelligence. In fact, Google refers to its work being carried out at NASA's Ames Research Center as \"quantum artificial intelligence.\" That's because problems that are too hard or too complex for today's machines could be solved almost instantaneously in the future.\nBecause of the specifics of how Google's quantum computer works - a process known as quantum annealing - the immediate applications for Google's quantum computer are a class of A.I. problems generally referred to as optimization problems. Imagine NASA being able to use quantum computers to optimize the flight trajectories of interstellar space missions, FedEx being able to optimize its delivery fleet of trucks and planes, an airport being able to optimize its air-traffic control grid, the military being able to crack any encryption code, or a Big Pharma company being able to optimize its search for a breakthrough new drug.\nYou get the idea - the new Google quantum computer could potentially be worth millions, if not billions, to certain types of companies or government agencies.\nMoreover, consumers might also benefit from the development of quantum artificial intelligence. In a promotional video for its Quantum Artificial Intelligence Lab, Google suggests that travel might be one type of consumer optimization problem worth pursuing. Imagine planning a trip to Europe, selecting which cities you'd like to visit, telling a computer how much you'd like to pay, and then having Google optimize the perfect trip itinerary for you.\nThere's just one little problem with all this: Quantum computers are notoriously difficult beasts to tame. With quantum computers, you're dealing with quantum bits (\"qubits\"), not digital bits. Unlike digital bits, which are binary (either 1 or 0), a qubit could be either - or both at the same time. That means you have to deal with all the quirky properties of particles predicted by quantum mechanics in order to program quantum computers correctly.\nOh, and each 10-foot-high D-Wave computer also needs to be super-chilled to a temperature that's 150 times as cold as that of deep space, making them pretty much inaccessible to anyone who hasn't been stockpiling liquid helium.\nAnd that's where the A.I. contest comes into play. IBM, for instance, has a digital supercomputer - IBM Watson - that also wants to play the A.I. optimization game. IBM Watson also wants to optimize the research and development process for pharmaceutical researchers to find new cures. And IBM Watson wants to play in the consumer realm, where it's already at work optimizing the training regimens of top-flight athletes.The other competitors\nAnd it's not just Google D-Wave vs. IBM Watson in some ultimate cage match to see which is better and faster at optimizing solutions to hard problems; it's all the other classes of unconventional computers out there. Consider, for example, the new memcomputer, which mimics the way the human brain works, storing and processing information simultaneously. There are plenty of other unconventional computers, too, including some that are biological. And other research labs and universities - such as at the University of Maryland or Yale University, which recently launched the Yale Quantum Institute - are working on their own quantum computers.\nWhat all this points to is that traditional digital computing (what Google refers to as \"classical computing\") is on the way out. We're now looking for a new heir apparent, and Google hopes to anoint D-Wave as the rightful heir. With its big announcement that quantum computing can work, Google hopes to show that they've figured out how to make practical quantum computers for the commercial market.\nAnytime you claim to have created something that's 100 million times as fast as anything else that's ever existed, though, you're bound to run up against skeptics. Indeed, there are plenty of skeptics for the D-Wave. One big quibble about the quantum qubits, for example, is that the test results were not nearly as impressive as Google claims they were. That's because the digital computer trying to defeat the quantum computer was forced to compete under Google's house rules, which meant that it had to use the same algorithm that the quantum computer used - and that algorithm had already been carefully sculpted to the peculiarities of the quantum world. Imagine running a race against a competitor in shoes that are too big, pants that keep falling down, and on a course where your competitor can run across and through the track - not just around it.The way forward\nGoing forward, it's possible to think of two vastly different scenarios for quantum computing. The first scenario is that Google uses these D-Wave quantum computers to corner the market in artificial intelligence. Just as once nobody could have predicted that everyone would own his own personal computer one day, maybe people will all own their own quantum computer one day.\nThe other scenario is that the world moves on to other forms of computing, perhaps using components that are easier to program than qubits. Maybe quantum computers are just too quirky, too hard to program, to solve the types of problems most people want to solve. Quantum computers may be able to optimize an entire nation's air-traffic control grid or fly a spacecraft to Mars, but what if you just want to check your phone to know what to wear to work tomorrow?\nEither way, the future of artificial intelligence will never be the same. Thanks to exponential gains in computing power on the horizon, it's becoming increasingly clear that today's digital computers have the potential to become obsolete. Let's just hope that tomorrow's super-powerful quantum computers don't become transcendent and try to take over the world.\ndominic.basulto@washpost.com\n","284":"SAN FRANCISCO -- Silicon Valley's start-ups have always had a recruiting advantage over the industry's giants: Take a chance on us and we'll give you an ownership stake that could make you rich if the company is successful.\nNow the tech industry's race to embrace artificial intelligence may render that advantage moot -- at least for the few prospective employees who know a lot about A.I. \n  Tech's biggest companies are placing huge bets on artificial intelligence, banking on things ranging from face-scanning smartphones and conversational coffee-table gadgets to computerized health care and autonomous vehicles. As they chase this future, they are doling out salaries that are startling even in an industry that has never been shy about lavishing a fortune on its top talent.\n  Typical A.I. specialists, including both Ph.D.s fresh out of school and people with less education and just a few years of experience, can be paid from $300,000 to $500,000 a year or more in salary and company stock, according to nine people who work for major tech companies or have entertained job offers from them. All of them requested anonymity because they did not want to damage their professional prospects.\u00a0\n  Well-known names in the A.I. field have received compensation in salary and shares in a company's stock that total single- or double-digit millions over a four- or five-year period. And at some point they renew or negotiate a new contract, much like a professional athlete.\n  At the top end are executives with experience managing A.I. projects. In a court filing this year, Google revealed that one of the leaders of its self-driving-car division, Anthony Levandowski, a longtime employee who started with Google in 2007, took home over $120 million in incentives before joining Uber last year through the acquisition of a start-up he had co-founded that drew the two companies into a court fight over intellectual property.\n  Salaries are spiraling so fast that some joke the tech industry needs a National Football League-style salary cap on A.I. specialists. ''That would make things easier,'' said Christopher Fernandez, one of Microsoft's hiring managers. ''A lot easier.''\n  There are a few catalysts for the huge salaries. The auto industry is competing with Silicon Valley for the same experts who can help build self-driving cars. Giant tech companies like Facebook and Google also have plenty of money to throw around and problems that they think A.I. can help solve, like building digital assistants for smartphones and home gadgets and spotting offensive content.\n  Most of all, there is a shortage of talent, and the big companies are trying to land as much of it as they can. Solving tough A.I. problems is not like building the flavor-of-the-month smartphone app. In the entire world, fewer than 10,000 people have the skills necessary to tackle serious artificial intelligence research, according to Element AI, an independent lab in Montreal.\n  ''What we're seeing is not necessarily good for society, but it is rational behavior by these companies,'' said Andrew Moore, the dean of computer science at Carnegie Mellon University, who previously worked at Google. ''They are anxious to ensure that they've got this small cohort of people'' who can work on this technology.\n  Costs at an A.I. lab called DeepMind, acquired by Google for a reported $650 million in 2014, when it employed about 50 people, illustrate the issue. Last year, according to the company's recently released annual financial accounts in Britain, the lab's ''staff costs'' as it expanded to 400 employees totaled $138 million. That comes out to $345,000 an employee.\n  ''It is hard to compete with that, especially if you are one of the smaller companies,'' said Jessica Cataneo, an executive recruiter at the tech recruiting firm CyberCoders.\n  The cutting edge of artificial intelligence research is based on a set of mathematical techniques called deep neural networks. These networks are mathematical algorithms that can learn tasks on their own by analyzing data. By looking for patterns in millions of dog photos, for example, a neural network can learn to recognize a dog. This mathematical idea dates back to the 1950s, but it remained on the fringes of academia and industry until about five years ago.\n  By 2013, Google, Facebook and a few other companies started to recruit the relatively few researchers who specialized in these techniques. Neural networks now help recognize faces in photos posted to Facebook, identify commands spoken into living-room digital assistants like the Amazon Echo and instantly translate foreign languages on Microsoft's Skype phone service.\n  Using the same mathematical techniques, researchers are improving self-driving cars and developing hospital services that can identify illness and disease in medical scans, digital assistants that can not only recognize spoken words but understand them, automated stock-trading systems and robots that pick up objects they've never seen before.\n  With so few A.I. specialists available, big tech companies are also hiring the best and brightest of academia. In the process, they are limiting the number of professors who can teach the technology.\n  Uber hired 40 people from Carnegie Mellon's groundbreaking A.I. program in 2015 to work on its self-driving-car project. Over the last several years, four of the best-known A.I. researchers in academia have left or taken leave from their professorships at Stanford University. At the University of Washington, six of 20 artificial intelligence professors are now on leave or partial leave and working for outside companies.\n  ''There is a giant sucking sound of academics going into industry,'' said Oren Etzioni, who is on leave from his position as a professor at the University of Washington to oversee the nonprofit Allen Institute for Artificial Intelligence.\n  Some professors are finding a way to compromise. Luke Zettlemoyer of the University of Washington turned down a position at a Google-run Seattle laboratory that he said would have paid him more than three times his current salary (about $180,000, according to public records). Instead, he chose a post at the Allen Institute that allowed him to continue teaching.\n  ''There are plenty of faculty that do this, splitting their time in various percentages between industry and academia,'' Mr. Zettlemoyer said. ''The salaries are so much higher in industry, people only do this because they really care about being an academian.''\n  To bring in new A.I. engineers, companies like Google and Facebook are running classes that aim to teach ''deep learning'' and related techniques to existing employees. And nonprofits like Fast.ai and companies like Deeplearning.ai, founded by a former Stanford professor who helped create the Google Brain lab, offer online courses.\n  The basic concepts of deep learning are not hard to grasp, requiring little more than high-school-level math. But real expertise requires more significant math and an intuitive talent that some call ''a dark art.'' Specific knowledge is needed for fields like self-driving cars, robotics and health care.\n  In order to keep pace, smaller companies are looking for talent in unusual places. Some are hiring physicists and astronomers who have the necessary math skills. Other start-ups from the United States are looking for workers in Asia, Eastern Europe and other locations where wages are lower.\n  ''I can't compete with Google, and I don't want to,'' said Chris Nicholson, the chief executive and a co-founder of Skymind, a start-up in San Francisco that has hired engineers in eight countries. ''So I offer very attractive salaries in countries that undervalue engineering talent.''\n  But the industry's giants are doing much the same. Google, Facebook, Microsoft and others have opened A.I. labs in Toronto and Montreal, where much of this research outside the United States is being done. Google also is hiring in China, where Microsoft has long had a strong presence.\n  Not surprisingly, many think the talent shortage won't be alleviated for years.\n  ''Of course demand outweighs supply. And things are not getting better any time soon,'' Yoshua Bengio, a professor at the University of Montreal and a prominent A.I. researcher, said. ''It takes many years to train a Ph.D.''\n\n\n\n","285":"A gaggle of Harry Potter fans descended for several days this summer on the Oregon Convention Center in Portland for the Leaky Con gathering, an annual haunt of a group of predominantly young women who immerse themselves in a fantasy world of magic, spells and images.\nThe jubilant and occasionally squealing attendees appeared to have no idea that next door a group of real-world wizards was demonstrating technology that only a few years ago might have seemed as magical.\nThe scientists and engineers at the Computer Vision and Pattern Recognition conference are creating a world in which cars drive themselves, machines recognize people and ''understand'' their emotions, and humanoid robots travel unattended, performing everything from mundane factory tasks to emergency rescues.\nC.V.P.R., as it is known, is an annual gathering of computer vision scientists, students, roboticists, software hackers -- and increasingly in recent years, business and entrepreneurial types looking for another great technological leap forward.\u00a0\nThe growing power of computer vision is a crucial first step for the next generation of computing, robotic and artificial intelligence systems. Once machines can identify objects and understand their environments, they can be freed to move around in the world. And once robots become mobile they will be increasingly capable of extending the reach of humans or replacing them.\nSelf-driving cars, factory robots and a new class of farm hands known as ag-robots are already demonstrating what increasingly mobile machines can do. Indeed, the rapid advance of computer vision is just one of a set of artificial intelligence-oriented technologies -- others include speech recognition, dexterous manipulation and navigation -- that underscore a sea change beyond personal computing and the Internet, the technologies that have defined the last three decades of the computing world.\n''During the next decade we're going to see smarts put into everything,'' said Ed Lazowska, a computer scientist at the University of Washington who is a specialist in Big Data. ''Smart homes, smart cars, smart health, smart robots, smart science, smart crowds and smart computer-human interactions.''\nThe enormous amount of data being generated by inexpensive sensors has been a significant factor in altering the center of gravity of the computing world, he said, making it possible to use centralized computers in data centers -- referred to as the cloud -- to take artificial intelligence technologies like machine-learning and spread computer intelligence far beyond desktop computers.\nApple was the most successful early innovator in popularizing what is today described as ubiquitous computing. The idea, first proposed by Mark Weiser, a computer scientist with Xerox, involves embedding powerful microprocessor chips in everyday objects.\nSteve Jobs, during his second tenure at Apple, was quick to understand the implications of the falling cost of computer intelligence. Taking advantage of it, he first created a digital music player, the iPod, and then transformed mobile communication with the iPhone. Now such innovation is rapidly accelerating into all consumer products.\n''The most important new computer maker in Silicon Valley isn't a computer maker at all, it's Tesla,'' the electric car manufacturer, said Paul Saffo, a managing director at Discern Analytics, a research firm based in San Francisco. ''The car has become a node in the network and a computer in its own right. It's a primitive robot that wraps around you.''\nHere are several areas in which next-generation computing systems and more powerful software algorithms could transform the world in the next half-decade.\nArtificial Intelligence\nWith increasing frequency, the voice on the other end of the line is a computer.\nIt has been two years since Watson, the artificial intelligence program created by I.B.M., beat two of the world's best ''Jeopardy'' players. Watson, which has access to roughly 200 million pages of information, is able to understand natural language queries and answer questions.\nThe computer maker had initially planned to test the system as an expert adviser to doctors; the idea was that Watson's encyclopedic knowledge of medical conditions could aid a human expert in diagnosing illnesses, as well as contributing computer expertise elsewhere in medicine.\nIn May, however, I.B.M. went a significant step farther by announcing a general-purpose version of its software, the ''I.B.M. Watson Engagement Advisor.'' The idea is to make the company's question-answering system available in a wide range of call center, technical support and telephone sales applications. The company says that as many as 61 percent of all telephone support calls currently fail because human support-center employees are unable to give people correct or complete information.\nWatson, I.B.M. says, will be used to help human operators, but the system can also be used in a ''self-service'' mode, in which customers can interact directly with the program by typing questions in a Web browser or by speaking to a speech recognition program.\nThat suggests a ''Freakonomics'' outcome: There is already evidence that call-center operations that were once outsourced to India and the Philippines have come back to the United States, not as jobs, but in the form of software running in data centers.\nRobotics\nA race is under way to build robots that can walk, open doors, climb ladders and generally replace humans in hazardous situations.\nIn December, the Defense Advanced Research Projects Agency, or Darpa, the Pentagon's advanced research arm, will hold the first of two events in a $2 million contest to build a robot that could take the place of rescue workers in hazardous environments, like the site of the damaged Fukushima Daiichi nuclear plant.\nScheduled to be held in Miami, the contest will involve robots that compete at tasks as diverse as driving vehicles, traversing rubble fields, using power tools, throwing switches and closing valves.\nIn addition to the Darpa robots, a wave of intelligent machines for the workplace is coming from Rethink Robots, based in Boston, and Universal Robots, based in Copenhagen, which have begun selling lower-cost two-armed robots to act as factory helpers. Neither company's robots have legs, or even wheels, yet. But they are the first commercially available robots that do not require cages, because they are able to watch and even feel their human co-workers, so as not to harm them.\nFor the home, companies are designing robots that are more sophisticated than today's vacuum-cleaner robots. Hoaloha Robotics, founded by the former Microsoft executive Tandy Trower, recently said it planned to build robots for elder care, an idea that, if successful, might make it possible for more of the aging population to live independently.\nSeven entrants in the Darpa contest will be based on the imposing humanoid-shaped Atlas robot manufactured by Boston Dynamics, a research company based in Waltham, Massachusetts. Among the wide range of other entrants are some that look anything but humanoid -- with a few that function like ''transformers'' from the world of cinema. The contest, to be held in the infield of the Homestead-Miami Speedway, may well have the flavor of the bar scene in ''Star Wars.''\nIntelligent Transportation\nAmnon Shashua, an Israeli computer scientist, has modified his Audi A7 by adding a camera and artificial-intelligence software, enabling the car to drive the 65 kilometers, or 40 miles, between Jerusalem and Tel Aviv without his having to touch the steering wheel.\nIn 2004, Darpa held the first of a series of ''Grand Challenges'' intended to spark interest in developing self-driving cars. The contests led to significant technology advances, including ''Traffic Jam Assist'' for slow-speed highway driving; ''Super Cruise'' for automated freeway driving, already demonstrated by General Motors and others; and self-parking, a feature already available from a number of car manufacturers.\nRecently General Motors and Nissan have said they will introduce completely autonomous cars by the end of the decade. In a blend of artificial-intelligence software and robotics, Mobileye, a small Israeli manufacturer of camera technology for automotive safety that was founded by Mr. Shashua, has made considerable progress. While Google and automotive manufacturers have used a variety of sensors including radars, cameras and lasers, fusing the data to provide a detailed map of the rapidly changing world surround a moving car, Mobileye researchers are attempting to match that accuracy with just video cameras and specialized software.\nEmotional Computing\nAt a preschool near the University of California, San Diego, a child-size robot named Rubi plays with children. It listens to them, speaks to them and understands their facial expressions.\nRubi is an experimental project of Prof. Javier Movellan, a specialist in machine learning and robotics. Professor Movellan is one of a number of researchers now working on a class of computers that can interact with humans, including holding conversations.\nComputers that understand our deepest emotions hold the promise of a world full of brilliant machines. They also raise the specter of an invasion of privacy on a scale not previously possible, as they move a step beyond recognizing human faces to the ability to watch the array of muscles in the face and decode the thousands of possible movements into an understanding of what people are thinking and feeling.\nThese developments are based on the work of the American psychologist Paul Ekman, who explored the relationship between human emotion and facial expression. His research found the existence of ''micro expressions'' that expose difficult-to-suppress authentic reactions. In San Diego, Professor Movellan has founded a company, Emotient, that is one of a handful of start-ups pursuing applications for the technology. A near-term use is in machines that can tell when people are laughing, crying or skeptical -- a survey tool for film and television audiences.\nFarther down the road, it is likely that applications will know exactly how people are reacting as the conversation progresses, a step well beyond Siri, Apple's voice recognition system.\nHarry Potter fans, stand by.\n","286":"The quarterly earnings Twitter released Thursday offered more than just financial data points for investors and analysts. The company gave\u00a0insights on the future of the platform that are\u00a0pertinent to those who use it regularly. Here are three key take-aways for Twitter users:\u00a0\n              Expect more video - a lot more.           \nTwitter executives said Thursday that the company is focused on making more money from video advertising, its largest and fastest growing revenue segment. That means users will see\u00a0the social network expand its offerings of both regularly scheduled programming and programming tied to major events.\nThe company already has a deal with the NFL to broadcast 10 games on its platform. Recent games have attracted more than 3 million viewers apiece, the company said. NBA-related content is next, with Twitter announcing plans this week for two sports talk shows during the upcoming season. You can expect more content around major league sports, including the NHL, to follow.\nIt may not be sports, but political gamesmanship has also been good for Twitter. The company live streamed all three presidential debates, and pulled in an average 3.3 million viewers on the last two. Its audience was mostly under the age of 35, a lucrative segment for advertisers, the company said. The election may end soon, but Twitter's foray into news broadcasting won't.\n              Expect greater use of\u00a0artificial intelligence.           \nThis year, Twitter altered its timeline algorithm so users see tweets they are likely to find important, not just\u00a0those that were sent most recently. What's more, the company has built machine learning and artificial intelligence into its notifications to make them more relevant and keep users engaged with the platform.\nExecutives said they plan to build machine learning into more parts of the platform, including how it \"onboards\" new users, how it encourages users\u00a0to tweet, and how it serves up video advertisements. In all, the machine learning and artificial intelligence efforts are designed to make the user experience more personalized and seamless.\n\"We're looking more broadly at everything that we can do around technology, and machine learning specifically, to improve all of our experiences to make sure that Twitter continues to be the fastest way to see what's happening and also the highest-quality way,\" chief executive Jack Dorsey said during a conference call Thursday.\nThis should not come as a surprise to those following the company's acquisition history. In each of the past three years, Twitter has shelled out millions of dollars to scoop up artificial intelligence and machine learning start-ups. This summer, it paid $150 million for a firm called Magic Pony.\n              Expect more efforts to build online communities.\u00a0           \nUsers have traditionally turned to Twitter to follow the 140-character musings of individuals - from their friends and family to Donald Trump and Kim Kardashian West. Increasingly, though, the company wants to organize users around their shared interests.\nOne way Twitter plans to do that is to create timelines tailored to specific events, such as the aforementioned sporting events or political debates. Fans of a particular team or members of a political party could find the tweets of those with similar outlooks. Of course, in the long run, those communities could become markets for niche content and targeted advertising.\n\"In\u00a0the\u00a0past,\u00a0we've\u00a0definitely\u00a0biased\u00a0more\u00a0toward helping\u00a0people\u00a0find individuals\u00a0rather\u00a0than\u00a0meeting\u00a0them\u00a0around\u00a0what\u00a0they're interested in\u00a0and\u00a0what\u00a0topics\u00a0they\u00a0care about\u00a0most,\" Dorsey said.\u00a0\"We\u00a0think Twitter is\u00a0strongest\u00a0around\u00a0topics\u00a0and\u00a0interests, and\u00a0we\u00a0think\u00a0we\u00a0can\u00a0do\u00a0a\u00a0much better\u00a0job\u00a0there.\"\n              Read more from The Washington Post's Innovations section.           \n","287":"As a teenager, Louis Hodes studied computers at a math and science honors high school in New York. Fascinated by the subject, he decided to build his own device.\nHe salvaged tubing and other materials he needed from an industrial area in the city, wired everything together and connected it to batteries. The results encouraged the determined child prodigy.\n\"It was a machine that was able to compute things,\" said his brother Alan Nathan Hodes, who for a time followed his older brother's interest in computers.\u00a0\nFor more than 50 years, through the dawning of the computer age and into development of computers for cancer research, Dr. Hodes pursued his fascination with the machines' intricacies and capabilities. He conducted groundbreaking work in artificial intelligence, helped develop a computer programming language and saved the lives of thousands of animals being used for scientific testing with a computer model he created.\nHe was an unassuming man known for his stubbornness and brilliance: a man not quite satisfied that he had done enough.\nDr. Hodes, a mathematician and research scientist at the National Institutes of Health who later became a portrait painter, died June 30 of pulmonary failure at Suburban Hospital. The Rockville resident was 74. He lived a full and enthusiastic life despite having a nervous system that failed to communicate information adequately from his brain to his spinal cord.\nPeripheral neuropathy took away his balance and \"caused him to walk as if on a tightrope,\" said his wife of 40 years, Susan Hodes. Over time, his childhood limp gave way to his using a cane, a walker and, despite his stubborn resistance, a wheelchair. The trembling in his hands made painting his pastel portraits, which he started 10 years ago, difficult but not impossible.\n\"It was amazing how he did it,\" said his longtime friend Jack Minker. \"He would lean up against the easel, steadying his hand that way, and he painted. It was not easy.\"\nHis drawings, portraits and figures were poetic works, said Joanie Grosfeld, who began taking life drawing classes with the Hodeses more than 35 years ago at the Jewish Community Center in Rockville.\n\"He just had a sensitivity to his drawing. He would use a white highlight, instead of the dark line and shadow,\" she said. \"The lines just flowed. They were just pleasing to look at . . . there was a lot of feeling to the faces.\"\nBorn into a poor family in New York City, Louis Hodes grew up on the city's Lower East Side in the first public housing complex in Manhattan. His intelligence was evident early in his life. He did complicated math problems as a child and, at 13, while studying for his bar mitzvah, he learned Hebrew, going beyond the words he would speak as part of the traditional rite of passage.\nHe received a scholarship to Stuyvesant High School, where he fed his appetite for math and science. He graduated summa cum laude from what was then Polytechnic Institute of Brooklyn with a degree in electrical engineering. He worked at the post office to pay for college.\nA doctoral student of great focus and discipline, he specialized in mathematical logic at Massachusetts Institute of Technology from 1957 to 1962. There, he worked with some of the brightest minds in theoretical computer science and artificial intelligence, including Marvin Minsky and John McCarthy.\n\"Working with McCarthy and Minsky was no mean feat,\" Minker said. \"He did very good work with them. He was in the forefront of history.\"\nAs a research assistant, Dr. Hodes took part in the pioneering work being done in the development of the computer programming language LISP, used in artificial intelligence research. He was one of the first people to recognize that logic could be used as a programming language.\nAfter joining NIH's National Cancer Institute in 1966, Dr. Hodes worked on computer tools in biomedical applications, including developing software for online analysis of biomedical images.\nOne of his two patents included a method that allowed radiologists to calculate the dosage and location for treatment of tumors.\nIn the 1980s, the \"Hodes clustering model\" revolutionized how substances and other compounds were screened to treat cancer. His system helped scientists save the lives of thousands of mice that would no longer be needed in experiments, for which a Canadian humane society gave him a commendation.\nAs much as Dr. Hodes was a patient researcher and diligent mathematician, he also was a doting uncle who showered his nieces and nephews with attention, gifts and encouragement.\nGail Elizabeth Hodes Plotnik recalled her Uncle Louie buying her a telescope and sending her to an exclusive ballet camp. She remembers him as someone witha ready smile and a determination to do as much as he could. \"If you talked to him, he probably felt it wasn't nearly enough [and that] maybe he could have done more.\"\n","288":"SAN FRANCISCO\u00a0-- Swarms of journalists lined the halls of a Southern California oceanfront resort recently to see tech luminaries like Jeff Bezos and Elon Musk discuss the Gawker case, rocket launches to space, and, Silicon Valley's obsession du jour, artificial intelligence.\nBut perhaps the real action in artificial intelligence is happening at the Hilton San Francisco, where a much less flashy tech crowd gathered\u00a0this week.\nThere, hundreds\u00a0of engineers (2,500 to be precise), geeked out over fireside chats on topics such as \"streaming data in real time\" and \"building accelerators in databases.\" The event, called the Spark Summit, brings together the growing number of developers who are working with Spark, perhaps the hottest data-mining software today.\nOne way to understand future of artificial intelligence and how it will transform the economy is to watch what everyone is talking about at the Spark Summit. Not that a layperson could comprehend most of these talks. Engineers from companies such as\u00a0Capital One to Airbnb spoke about their successful big data projects in pure engineering-ese.\u00a0\nBut the fact that the keynotes were from revered artificial intelligence researchers Andrew Ng and Jeff Dean speak to the value of Spark. Companies such as\u00a0Siemens say they have used to it to analyze data streaming in from sensors to predict which wind turbines will break. NBC Universal is collecting data on consumer behavior and using algorithms, crunched in Spark, to predict which television shows will resonate with consumers.\nSome of this stuff may not sound entirely new. But it's what is happening on the backend that has gotten geeks and investors excited.\nIn the last four years, Silicon Valley's obsession with big data and data mining has ballooned into a full-blown artificial intelligence craze. Look at the travel industry. Today, companies like Expedia want to collect your travel interests and history, crunch the data in an app or some other platform, and build virtual travel agents who will assist you on your flight using voice recognition technology. This shift, in which automation and algorithms move into every industry, is already underway. It is powered by a growing number of backend technologies and startups aiming to win lucrative contacts with corporate America, whose names most consumers have never heard of; At least one of them may become the next Microsoft, Oracle, or IBM.\nThe most prominent recent example of these hyped backend big data technologies is called Hadoop. The software, named after a toy elephant owned by the son of the Yahoo programmer who helped develop it, arose in the mid-2000s when Google and Yahoo were neck-in-neck in a battle to be the dominant search engine.\nHadoop was used by both companies to index the massive amounts of data consumers were throwing up on the \"Web.\" By 2008, investors thought the data-mining software was ripe for corporate America, and armies of salespeople began pitching it as an all-purpose big data analytics tool. They said, for example, that it could help credit card companies and banks detect fraud and other anomalies in massive amounts of financial transactions, or e-commerce companies to build recommendation features on their websites that say \"people who shopped for this also shopped for this.\"\nBy 2014, investors had poured over $2 billion into dozens of companies selling services based on Hadoop. Because Hadoop is open-source, which means the core software is free for developers to share and modify, companies make money off selling tech support and other services on top of Hadoop. But Hadoop ran into trouble because the clunky tech didn't always deliver on its promises. Four years into the big data craze, the majority of big data projects fail or are never completed, according to the tech firm InfoChimps.\nNow, after years of hype and investment in Hadoop, Spark may be replacing it.\nSpark enables the processing of more advanced machine learning algorithms, and performs more efficiently than parts of Hadoop, and is more easily operated in the cloud, engineers say. That's unwelcome news for proponents of Hadoop, and reflects just how quickly the latest innovative technology can become passe. Of course, the same outcome could happen to Spark too.\nThe Spark project, which is also open source, originated in a computer science lab at the University of California, Berkeley. The venture capitalist Ben Horowitz discovered the technology via a tip from a professor in the department, who called to tell Horowitz that Spark's creator was the most talented computer scientist to come out of Berkeley in a decade. Almost immediately, Horowitz leapt to help the Spark engineers, most who whom were graduate students who had never run a business, to build a company. Through\u00a0his venture firm Andreessen Horowitz, Horowitz and other investors have put $47 million into the company, called Databricks.\nForty-eight percent of Spark developers say they no longer use Hadoop, Databricks says, even though the project was initially considered a way to supplement it. IBM has recently announced the company is dedicating 3,500 developers to Spark-related projects.\nThe success of Spark means complicated, interesting things for the future of enterprise software. It's a less sexy corner of the tech world - this isn't ride-sharing or social media - but it is still where the money is.\nBut Spark is open source, which means it's free. Microsoft became one of the wealthiest companies in history selling proprietary software. Databricks and others still have to find a way to make real revenue by selling technology that nobody owns. As Peter Levine, another Andreessen partner who invests in open-source companies is fond of saying, open source companies have never been worth a fraction of their proprietary counterparts. Open source startups like Databricks are trying to make money selling specialized services and proprietary products that hook into and augment the open source software, including tech support and custom security products, to big corporations. But they have a long way to go.\nMoreover, the creators of Spark and other artificial intelligence technologies appear to think about the implications of what they are building in a different way than Microsoft or Oracle did in their heydays. Those legacy giants wanted to capture the wealth of corporate America. Surely, today's enterprise startups also aspire to that, but AI entrepreneurs today are also preoccupied with a different set of questions - about the kind of future more artificial intelligence will bring.\nAs businesses become faster and more efficient, millions of jobs may\u00a0disappear.\u00a0Elon Musk and Silicon Valley startup incubator Y-Combinator recently dedicated a billion dollars to create OpenAI, a non-profit organization dedicated to creating powerful open-source artificial intelligence that will serve the public good. The current debate within the AI community,\u00a0said Databricks chief executive\u00a0Ali Ghodsi, is\u00a0whether the changes wrought by AI\u00a0will free people up to spend time on creative pursuits, resulting a net gain for humanity, or strip them of their means.\n","289":"WASHINGTON -- Science fiction comes up often in serious discussions about artificial intelligence and weapons. That is partly a reflection of current technological limits and the deep concerns that surround the development of thinking machines that can kill. Here is a selection of some of the works that are most often mentioned:\n'The Terminator' \u00a0\n  [Video: The Terminator Original Trailer Watch on YouTube.]\n  Arnold Schwarzenegger plays a cyborg who is sent back in time to assassinate a young waitress before she can give birth to the man who is destined to save humanity from the machines. Since the movie's release in 1984, its title has become shorthand for the nightmare scenario of what will happen when artificial intelligence meets weapons.\n  _____\n  'Ender's Game'\n  To counter insectlike aliens, the military trains children in hopes of nurturing the master tacticians who can save humanity. The book, and the movie based on it, is often mentioned by Robert O. Work, the deputy defense secretary, who cites the creativity of the child warriors to explain how his vision of blending man and machine can keep the United States military ahead of its rivals. Critics of autonomous weapons might be less thrilled about the book's ending, which involves the destruction of an entire species.\n  _____\n  'Iron Man'\n  [Video: Iron Man - Trailer [HD] Watch on YouTube.]\n  The comic book -- and now Hollywood franchise -- is about a billionaire inventor who, after being kidnapped and forced to build a weapon of mass destruction, instead builds a powered suit to escape. He goes on to become a superhero.\n  _____\n  'I, Robot'\n  This is the short story collection in which Isaac Asimov introduced his ''Three Laws of Robotics,'' which have influenced both science fiction and real-life debates about managing artificial intelligence. The laws, briefly: A robot may not harm a human being, must obey orders from humans and must protect its own existence. Each rule trumps the one that comes after it, so, for instance, a robot will not follow orders or protect itself if doing so would bring harm to humans.\n  _____\n  'Her'\n  [Video: Her - Official Trailer 2 [HD] Watch on YouTube.]\n  This 2013 science fiction movie, directed by Spike Jonze, is about a futuristic computer operating system with an A.I.-based personality. It is a telling fable about the consequences of human-machine collaboration that may unfold in the near future.\n  _____\n  'Kill Decision'\n  Daniel Suarez's novel starts in a Stanford computer science building and explores artificial intelligence and the future of warfare using robots and drones.\n  _____\n  'Blade Runner'\n  [Video: \"Blade Runner (1982)\" Theatrical Trailer Watch on YouTube.]\n  Set in a crowded, neon-laced Los Angeles of the future, the film follows a detective who hunts down androids that are trying to pass as humans. It is based on the novel ''Do Androids Dream of Electric Sheep?'' by Philip K. Dick.\n  _____\n  'Ghost Fleet'\n  This novel about World War III includes robot fighter jets, teenage hackers and American veterans who go low-tech to battle their high-tech foes. It is written by August Cole, a former journalist, and P. W. Singer, an expert on military affairs who turned to fiction to make his case that the United States is failing to prepare for the next war.\n  Follow The New York Times's politics and Washington coverage on Facebook and Twitter, and sign up for the First Draft politics newsletter.\n\n\n\n","290":"AlphaGo, the computer system Google engineers trained to master the ancient game of\u00a0Go, needed only one move to make it abundantly clear that it has left humans in its dust.\n The move came Thursday, in the second game of AlphaGo's 4-1 landmark victory over South\u00a0Korean Lee Sedol, one of the world's best Go players. About an hour into the match, AlphaGo placed one of its stones in a nontraditional spot on the board that surprised those watching. \n \"I don't really know if it's a good or bad move,\" said Michael Redmond, a commentator on\u00a0a\u00a0live English broadcast. \"It's a very strange move.\" Redmond, one of the Western world's best Go players, could only crack a smile. \u00a0\n \"I thought it was a mistake,\" his broadcast partner, Chris Garlock, said with a laugh. \nSedol, however, was more serious. He\u00a0stared\u00a0at the board, then got up from the table and left the room.\n As Sedol returned after a few minutes and pondered his next move, it became clear that AlphaGo's move was no mistake. It might be strange, but it definitely wasn't bad. It was brilliant.  \n Sedol would take almost 16 minutes to make his next move. He would never recover, losing the match.  \n \"Almost no human pro would've thought of it, I think,\" Redmond said after the match. \n             AlphaGo's move in the board game, in which players place stones to collect territory,\u00a0was so brilliant that lesser minds -- in this case humans\u00a0-- couldn't initially appreciate it. The move also opened a\u00a0debate about whether increasingly powerful\u00a0machines have mastered creativity, a trait widely thought to be\u00a0strictly in\u00a0the domain of humanity.\n Pedro Domingos, a computer science professor at the University of Washington and author of \"The Master Algorithm,\" saw a parallel between AlphaGo's style and how\u00a0chess prodigy Bobby Fischer was feared because his early moves were considered too foolish to even be made. But as Fischer's matches wore on, the ill-advised moves suddenly looked genius. \n \"If that's\u00a0not creative, then what is?\" Domingos asked.\u00a0\u00a0\n He sees machines delivering creative results, and they're just getting started. Domingos believes a computer eventually will write a best-selling book. And he thinks there's a 50-50 chance that a computer writes a hit pop song in the next decade, given advances in artificial intelligence techniques and computing power. \nDomingos said such\u00a0advances shouldn't come as a surprise, as machines increasingly demonstrate that creativity isn't magical and distinctly human.\n\"We seem to have this mythical view that 'oh, it just pops into my head, and it's magic,' but it's not,\" Domingos said. \"Human beings [like machines] are also creative because of this massive parallel search learning process that goes on in their brains.\"\n For others, AlphaGo's dominance doesn't deserve the label of creativity.  \n \"This is the latest in a long history of overhyped [artificial intelligence] demonstrations,\" said Jerry Kaplan, a computer scientist and author of \"Humans Need Not Apply: A Guide to Wealth and Work in the Age of Artificial Intelligence.\" \"It's a machine engaging in taking a certain set of actions that are a result of the clever programming that has been done to build it.\" \n Kaplan mentioned IBM Deep Blue's victory over chess grandmaster Garry Kasparov in 1997, and IBM Watson's win over Ken Jennings in Jeopardy as examples of artificial intelligence hype. While heralded with much media attention, far-ranging implications from those victories haven't been felt in society. \n DeepMind, the London-based artificial intelligence group\u00a0that Google owns, made unexpected progress in Go because of its use of a technique\u00a0called deep learning, which excels at recognizing patterns in visual images. Deep learning is in vogue now, and led to developments in other areas such as autonomous vehicles, and interpreting medical images. \n \"Right now they have a new hammer and everyone in Silicon Valley is swinging it at what looks vaguely like a nail,\" Kaplan said. \nWhile experts say AlphaGo's decisive victory is indeed a milestone, arriving years ahead of expectations, they also put the win in perspective.\n \"AlphaGo does not advance the state of the art in machine learning,\" Domingos said. \"There is nothing fundamentally new about the kind of machine learning that is going on there. It's more that they pulled off something amazing with it.\" \n Earlier in his career Domingos himself tried to build computers capable of beating humans at Go. He credited Google's team with realizing\u00a0it could apply the trendy tool of deep learning to the 2,500-year-old\u00a0game. \nAfter the series ended DeepMind chief executive Demis Hassabis said techniques like AlphaGo show much promise, but are still in their early days. He said the group wasn't sure what it would do next with AlphaGo, but reiterated his interest in applying the tool to scientific discoveries and medicine.\u00a0In February Hassabis\u00a0announced DeepMind Health, a partnership with Britain's National Health Service to build technologies to help care for patients.\n He also encouraged artificial intelligence researchers to stress ethics, as some have warned of the potential catastrophic potential of artificial intelligence to one day cause mass unemployment or even human extinction. \n \"As with all powerful technologies they bring opportunities and challenges,\" Hassabis said. \"We have to make sure that developers of these kind of systems -- all AI researchers around the world -- think about the ethical responsibilities they have to build these systems in the right way and to deploy them for the right purposes.\" \n","292":"For years now, some of the smartest and most influential people on Earth have been warning about the dangers of artificial intelligence, laying out nightmarish scenarios that sound like they were pulled from the pages of a Hollywood script.\u00a0\nTesla chief-executive-turned-flamethrower-merchant Elon Musk has warned of killer robots and \"summoning the demon.\" Stephen Hawking, the renowned theoretical physicist, has given humanity a tight deadline for escaping the planet. Disease-fighting business magnate Bill Gates, meanwhile, has said he doesn't understand why \"some people are not concerned\" about the threat posed by super-intelligent machines.\nHowever, Kevin Kelly, the executive editor of Wired magazine, is offering a decidedly optimistic answer to Gates's question. Contrasting humans with technology ignores something that has been true for the past 10,000 years or so - something there's no coming back from, Kelly told a reporter at the World Government Summit in Dubai earlier this month.\n\"I think that we, ourselves, are technology,\" he said, appearing to imply that technology is an extension of biological evolution and central to what makes humans unique among animals. \"We have invented ourselves. We have invented our humanity.\"\n\"If we took all technology from our lives away, everything - fire, knives - humans would only last six months,\" Kelly added. \"We would be eaten by animals. We only can defend ourselves because of technology.\"\nInstead of summoning humanity's end, Kelly argues that artificial intelligence is forcing humanity to reevaluate what it means to be human, raising philosophical questions that will force people to define \"our humanity moving forward.\"\n\"We are still in the process of making ourselves more human,\" the eternal optimist said.\nAmong the tech forecasters sounding the alarm about AI, few have been as outspoken as Musk, who has recently begun warning about the dangers of autonomous weapons and calling for an international banning of them.\nLast year, Musk told a group of governors that they need to start regulating artificial intelligence, which he called a \"fundamental risk to the existence of human civilization.\" When pressed for concrete guidance, Musk said governments must get a better understanding of AI before it's too late.\nKelly's interview from the World Government Summit can be watched below.\n          MORE READING:        \n                       People freaked out after robot dogs opened a door. Now they're resisting humans.                 \n                       In stunning turn, Uber pays to settle its court battle with Waymo                 \n                       Ford wants to patent a driverless police car that ambushes lawbreakers using artificial intelligence                 \n","294":"The impact of innovation, by its nature, is unpredictable. But the pattern by which new technologies and high-tech businesses create jobs across the economy is well established. Take the Internet and Google. \u00a0\n  The search giant employs thousands of people -- programmers, mathematicians, statisticians, marketing and sales people, administrators and managers. But its success ripples to create other jobs as well: service workers and suppliers of everything from computers to food. Real estate brokers and car dealers have benefited from Google's wealth.\n  More broadly, the spread of Internet technology has meant that most companies have their own Web sites. The companies hire software programmers, computer technicians, graphics designers and online advertising salespeople. And the job-creating ripples continue.\n  Smarter computing technology, experts say, ought to make the most skilled workers -- in science, the arts and business -- even more productive and prosperous by freeing them from routine tasks. Their prosperity translates to spending that creates jobs in stores, schools,  gyms, construction and elsewhere.\n  Artificial intelligence, experts say, should also generate new jobs even as it displaces others. The smart machines of the future will need programming, servicing and upgrading -- work done, perhaps, by a new class of digital technicians. The intelligent machines, experts add, will be specialists in a field, like the medical assistant project at Microsoft. They must be tailored with specialized software, perhaps igniting a new industry for artificial intelligence applications.\n  Of course, no one really knows just what artificial intelligence will mean for jobs and the economy, but the technology is marching ahead. ''Its potential is far greater than simply substituting technology for human labor,'' said Erik Brynjolfsson, an economist at the M.I.T Sloan School of Management. STEVE LOHR  \n","295":"Elon Musk has already ignited a debate over the dangers of artificial intelligence. The chief executive of Tesla and SpaceX has called it humanity's\u00a0greatest threat, and something even more dangerous than nuclear weapons.\u00a0\nMusk publicly hasn't offered a lot of detail about why he's concerned, and what could go wrong. That changed in an interview with scientist Neil deGrasse Tyson, posted Sunday.\nMusk's fears lie with a subset of artificial intelligence, called superintelligence. It's defined by Nick Bostrom, author of the highly-cited book \"Superintelligence,\" as \"any intellect that greatly exceed the cognitive performance of humans in virtually all domains of interest.\"\nMusk isn't worried about simpler forms of artificial intelligence, such as a driverless car or smart conditioning unit. The danger is when a machine can rapidly educate itself, as Musk explained:\n\"If there was a very deep digital superintelligence that was created that could go into rapid recursive self-improvement in a non-algorithmic way ... it could reprogram itself to be smarter and iterate very quickly and do that 24 hours a day on millions of computers, well-\"\n\"Then that's all she wrote,\" interjected Tyson\u00a0with a chuckle.\n\"That's all she wrote,\" Musk answered. \"I mean, we won't be like a pet Labrador if we're lucky.\"\n\"A pet Lab,\" laughed Tyson.\n\"I have a pet Labrador by the way,\" Musk said.\n\"We'll be their pets,\" Tyson\u00a0said.\n\"It's like the friendliest creature,\" Musk said, then letting out his lone chuckle of the segment.\n\"No, they'll domesticate us,\" Tyson\u00a0said.\n\"Yes!\u00a0Exactly,\" said Musk, sounding serious again.\n\"So we'll be lab pets to them,\" Tyson\u00a0said.\n\"Yes,\" Musk said. \"Or something strange is going to happen.\"\n\"They'll keep the docile humans and get rid of the violent ones,\" Tyson\u00a0theorized.\n\"Yeah,\" Musk said.\n\"And then breed the docile humans,\" Tyson\u00a0said.\nMusk then stressed the importance of what the superintelligence is programmed to optimize. It might seem appealing to have a computer figure out how to make us happier, but that could backfire:\n\"It may conclude that all unhappy humans should be terminated,\" Musk said. \"Or that we should all be captured and with dopamine and serotonin directly injected into our brains to maximize happiness because it's concluded that dopamine and serotonin are what cause happiness, therefore maximize it,\" which brought another chuckle from Tyson.\n\"I'm just saying we should exercise caution,\" Musk concluded.\u00a0You can listen to the entire interview here.\n               Related: The 12 threats to human civilization, ranked  \n Google's Eric Schmidt downplays fears over artificial intelligence\u00a0\n","296":"Officials building a case against the Washington-area sniper suspects are using a new investigative tool to help trace their movements across the country. It is an Internet-based system called Coplink, developed at an artificial intelligence laboratory here, that allows police departments to establish links quickly among their own files and to those of other departments. \n     During the 21 days in which snipers terrorized the area, investigators used everything from specialized ballistics testing to geographic and criminal profiling to radio and television announcements to track them down. Then, in what turned out to be the 11th hour of the pursuit, they finally reached out to Coplink. As it turned out, John Muhammad and Lee Malvo were arrested before it was fully installed, but now the post-arrest task force is using the system to help connect the dots. \n All of the information that was collected -- including that from other computer database systems like the Federal Bureau of Investigation's Rapidstart -- is now being downloaded into the Coplink database so that the accumulated data can be compared, said Robert Griffin, president of Knowledge Computing Corporation of Tucson, which is turning the prototype in the laboratory into a commercial product. \"The more data you get, the better Coplink works,\" he said.\u00a0\nCoplink was designed by Hsinchun Chen, the director of the Artificial Intelligence Laboratory at the University of Arizona. \"It's the Google for law enforcement,\" he said, referring to a speedy popular Internet search engine that, given a couple of words, can find an array of related Web sites. \"Things that a human can do intuitively we are getting the computer to do, too.\"\nDuring the sniper investigation, which generated hundreds of thousands of tips, the number of potential clues to assimilate was daunting. \"We were mobilizing a massive effort,\" said Lt. Mitch Cunningham of the Montgomery County police. \"We had tactile resources, the military, federal, state and local law enforcement agencies and information technology using several products where each one of these had a role.\" So when the National Institute of Justice, the Justice Department's research and development arm, suggested that the sniper task force try Coplink, the officials agreed. \nWhile no one is suggesting that old-fashioned detective work is being replaced by machines, the idea behind Coplink is to provide a computer program that can save busy police officers precious time and sometimes even help solve cases. That's something Coplink's oh-so-human advocates will boast about like a good story about a rookie getting a lucky break in a case. It is like having a new partner in the form of a computer backing up a cop.\n\"There is a greater and greater role for technology in law enforcement,\" Lieutenant Cunningham said. \nSoftware like Coplink's is already part of everyday life, said Rodney A. Brooks, director of the Artificial Intelligence Laboratory at the Massachusetts Institute of Technology. \"It's inevitable that it's going to have some law enforcement application, too.\" \nMr. Brooks said that his company, iRobot, has machines that investigate caves in Afghanistan before military units enter and that such machines are finding their way into municipal police forces. \"Columbine High School is a great example of how the police did not know what was going on inside,\" he said of the 1999 school shootings in Colorado. \nFurthermore, he said, the human mind can process and retain only so much information. \"There are enormous amounts of facts and connections out there, more than can be held in any one person's mind,\" he added. \"Just like with gene patterns, it's much too complex for someone to remember it all.\"\nCoplink works by linking and comparing data from new and existing files. For example, Mr. Griffin said, in a Tucson case a man was found lying face down after his throat had been cut and he had been run over by a vehicle. The man was still alive, and before he was taken to a hospital he told people at the scene, \"Shorty did it.\" The name Shorty was put into Coplink and cross-referenced with the victim's personal data, and within minutes the records showed that the two men had been in prison together. \nThe program also allows users to look at lists of data or to create graphs and charts showing affiliations among different criminals. \nAt the moment, the Tucson Police Department is the only one in the country where Coplink is fully installed, although about a half-dozen other cities have begun to introduce Coplink into their existing computer systems. The cost of the program and training can run anywhere from $40,000 to over $200,000, depending on the size of the department and existing computer systems, Mr. Griffin said. The development of Coplink has been financed in part by the National Institute of Justice and by the National Science Foundation.\nWidespread use should expand the technology's impact. Although criminals often go beyond a single jurisdiction, as in the sniper case, data on a crime ,from the type of weapon used to physical characteristics, may remain in a single department's files and the connections between crimes may be overlooked. But Dr. Chen insists Coplink is not just link analysis. \n\"It takes a large amount of data and, like a super black book of data, has to detect or play detective from this large knowledge base,\" he said. \"It has to consolidate and analyze.\" \n\"Even in Spielberg movies,\" he added, \"the robot is learning from the humans and does not just know everything.\"\nBefore coming to Arizona, Dr. Chen had worked on knowledge management issues at the Defense Department and the Central Intelligence Agency. A student in a class at the University of Arizona -- a police officer, as it happened -- asked Dr. Chen whether there might be a way to help the Tucson police share and analyze problems. Dr. Chen took up the idea in 1997, after receiving funds from the National Institute of Justice, and went on to develop Coplink with the Police Department here. \nLt. Jenny Schroeder of the Tucson police says that the Coplink files are all public records. \"This is not classified or secret information,\" she said. \"A lot of criminals are repeat offenders, and they can't hide their behavior.\" She noted John Muhammad's history of domestic violence. \nBecause Coplink relies on existing criminal records, it does not necessarily cause Big Brother concerns, but it is not without critics.\n\"When this kind of knowledge is applied to discrete databases, or an investigation of a single type of crime, say serial rape, then I don't see a lot of privacy issues,\" said James X. Dempsey, deputy director of the Center for Democracy and Technology, a Washington-based advocacy group dealing with issues of privacy on the Internet. \"When you start trying to extend this technology to many different types of crimes or into information other than law enforcement, then the problems multiply rapidly.\"\nMr. Dempsey said one security concern could emerge if Coplink went nationwide and was open to law enforcement officials at varying levels. \"The nightmare would be when the bad guys tap into it, and we know how many insecure Internet-based systems there are,\" he said. \nAnd ultimately, Mr. Dempsey said, there might be too much reliance on technology. \n\"There is a lot that technology can do with fingerprinting, sharing Department of Motor Vehicle data,\" he said. \"But there seems to be a classic case of believing that technology can solve every problem, and I'm very skeptical that it can.\"\nBut Dr. Chen said that in time, if Coplink goes nationwide, it could help law enforcement agencies share information equally and quickly. \"Everyone can now be on the same page,\" he said.\n","297":"Elon Musk and Mark Zuckerberg have clashed on artificial intelligence, space travel and the direction of technology.\nOn Friday, Mr. Musk showed just how little love lost there was between the two tech titans. \n  Mr. Musk, the chief executive of SpaceX and Tesla, deleted the Facebook pages of both of his companies. In doing so, he joined a growing chorus of tech leaders calling for people to abandon Mr. Zuckerberg's social network after it allowed a political consulting firm, Cambridge Analytica, to obtain and misuse data on 50 million users. The revelations have plunged Facebook into its worst public relations crisis in years.\n  As with most news in 2018, Mr. Musk's decision started with a barrage of tweets.\u00a0\n  The tech luminary began by criticizing Sonos, a maker of wireless speakers, which had pulled some ads from Facebook for a week.\n  ''Wow, a whole week. Risky ...,'' Mr. Musk tweeted in response to a news article about Sonos's move.\n  A minute later, he replied to Brian Acton, the founder of WhatsApp, which Facebook had acquired for $19 billion several years ago. Mr. Acton, who has since left Facebook, had on Wednesday called for people to ''#deletefacebook.''\n  ''What's Facebook?'' Mr. Musk replied to Mr. Acton. Then Mr. Musk announced he would shut down the SpaceX and Tesla pages. He said the Tesla Facebook page ''looks lame anyway.''\n  The posts, which sent the Twittersphere into a virtual frenzy, escalated a public feud between Mr. Musk and Mr. Zuckerberg. Mr. Musk has often urged people to be cautious of embracing technology such as artificial intelligence because of the consequences it might bring, once saying that it could become so powerful it would start wars and turn people into its ''house cats.''\n  Mr. Zuckerberg has argued that people need to trust and embrace technology in their lives. When the Facebook chief executive was asked about Mr. Musk's warnings around artificial intelligence during a Facebook Live broadcast in 2017, he called Mr. Musk a ''naysayer.'' That's an insult in a technology world that celebrates perpetual optimism.\n  ''With A.I. especially, I'm really optimistic,'' Mr. Zuckerberg said. ''People who are naysayers and kind of try to drum up these doomsday scenarios -- I just, I don't understand it. I think it's really negative and in some ways I actually think it is pretty irresponsible.''\n  In response, Mr. Musk shot back that Mr. Zuckerberg did not fully comprehend the issues.\n  ''I've talked to Mark about this,'' Mr. Musk wrote. ''His understanding of the subject is limited.''\n  The two have also clashed on space travel. Mr. Zuckerberg traveled to Kenya in 2016 for the launch of a Facebook-affiliated satellite called Amos-6, which was set to go to outer space in a SpaceX rocket. But the rocket exploded. Mr. Zuckerberg released a chilly statement.\n  ''As I'm here in Africa, I'm deeply disappointed to hear that SpaceX's launch failure destroyed our satellite that would have provided connectivity to so many entrepreneurs and everyone else across the continent,'' he wrote on Facebook.\n  Mr. Musk is a frequent Twitter presence, who has posted increasingly macho and humor-focused messages including video of himself playing with a flamethrower.\n  He said he plans to keep using his Instagram account, which is owned by Facebook, and on which he has 6.9 million followers.\n  When one reporter said on Twitter that it was remarkable Mr. Musk had so much time to troll online, Mr. Musk wrote, ''What, a troll, me!?''\n  Facebook and SpaceX didn't immediately have a comment on Mr. Musk's deleted pages. A Tesla spokeswoman did not have a comment beyond Mr. Musk's tweets.\n\n\n\n","298":"Ray Kurzweil, an inventor and new hedge fund manager, is describing the future of stock-picking, and it isn't human.\n  ''Artificial\u00a0intelligence is becoming so deeply integrated into our economic ecostructure that some day computers will exceed human intelligence,'' Mr. Kurzweil tells a room of investors who oversee enormous pools of capital. ''Machines can observe billions of market transactions to see patterns we could never see.''\nThe listeners, attendees of a conference sponsored earlier this month by the Capital Group Companies, are slightly skeptical. Some have heard that Mr. Kurzweil, 58, who takes more than 150 vitamins and supplements a day, believes people will eventually live forever. Others know he has said that in 2045, man and machine will achieve ''singularity,'' and humans will hold their breath for hours thanks to nanomachines in our bloodstreams.\n  But some are aware that a former Microsoft executive and chairman of the Nasdaq stock market, Michael W. Brown, is an investor in Mr. Kurzweil's new hedge fund, FatKat, and that Bill Gates once described him as ''the best person I know at predicting the future of artificial\u00a0intelligence.'' \u00a0\n  More important, many of them have seen Mr. Kurzweil's ideas used by stock speculators. So, they want to learn more about his brave, new world.\n  ''These ideas are the future,'' said David Atkinson, a private investor who attended another lecture later that day by Mr. Kurzweil. ''I'm not really sure I understand them, but they're making some folks rich.''\n  Complicated stock picking methods are nothing new. For decades, Wall Street firms and hedge funds like D. E. Shaw  have snapped up math and engineering Ph.D.s and assigned them to find hidden market patterns. When these analysts discover subtle relationships, like similarities in the price movements of Microsoft and I.B.M., investors seek profits by buying one stock and selling the other when their prices diverge, betting historical patterns will eventually push them back into synchronicity. \n  Today, such methods have achieved a widespread use unimaginable just five years ago. The Internet has put almost every data source within easy reach. New software programs, like the Apama Algorithmic Trading Platform, have made it possible for day traders to build complicated trading algorithms almost as easily as they drag an icon across a digital desktop. \n  ''Five years ago it would have taken $500,000 and 12 people to do what today takes a few computers and co-workers,'' said Louis Morgan, managing director of HG Trading, a three-person hedge fund in Wisconsin. ''I'm executing 1,500 to 2,000 trades a day and monitoring 1,500 pairs of stocks. My software can automatically execute a trade within 20 milliseconds -- five times faster than it would take for my finger to hit the buy button.''\n  Studies estimate that a third of all stock trades in the United States were driven by automatic algorithms last year, contributing to an explosion in stock market activity. Between 1995 and 2005, the average daily volume of shares traded on the New York Stock Exchange increased to 1.6 billion from 346 million.\n  But in recent years, as algorithms and traditional quantitative techniques have multiplied, their successes have slowed.\n  ''Now it's an arms race,'' said Andrew Lo, director of the Massachusetts Institute of Technology's Laboratory for Financial Engineering. ''Everyone is building more sophisticated algorithms, and the more competition exists, the smaller the profits.''\n  So investment firms have increasingly begun exploring mathematics' furthest edges and turning to people like Mr. Kurzweil, who became an expert in pattern recognition building a reading machine for the blind.\n  For years, computer scientists had tried to help machines perform mundane tasks like reading printed words or telling faces apart. With algorithms similar to those used by stock pickers, programmers created millions of rules designed to tell an ''A'' from an ''a.'' But no machine could read a page of text as well as the average child.\n  So Mr. Kurzweil and others took a different tack: instead of creating sequential rules to instruct a computer to read, they thought, why not create thousands of random rules and let the computer figure out what works?\n  The result was nonlinear decision making processes more akin to how a brain operates. So-called ''neural networks'' and ''genetic algorithms'' have become common in higher-level computer science. Neural networks permit computers to create new rules and automatically change underlying assumptions by experimenting with thousands of random sequences and processes. Genetic algorithms encourage software to ''evolve'' by letting different rules compete, and combining the most successful outcomes.\n  Wall Street has rushed to mimic the techniques. Because arbitrage opportunities disappear so quickly now, neural networks have emerged that can consider thousands of scenarios at once. It is unlikely, for instance, that Microsoft will begin selling ice-cream or I.B.M. will declare bankruptcy, but a nonlinear system can consider such possibilities, and thousands of others, without overtaxing computers that must be ready to react in milliseconds.\n  ''Most software fails in pattern recognition because there aren't enough sequential rules in the world to teach a computer to discern between two faces, or to find almost imperceptible relationships between stocks,'' said Orhan Karaali, a computer scientist and director at Advanced Investment Partners,  a $1.7 billion hedge fund. ''But a machine that can generate complicated rules a person would never have thought of, and that can learn from past mistakes is a powerful tool.''\n  Last year, the funds using Mr. Karaali's model returned in excess of 20 percent by using nonlinear techniques, according to his company. Whereas older methods of stock analysis rely on certain assumptions -- for instance, that market volatility always reverts to the mean -- Mr. Karaali's model calculates probabilities and generates assumptions on the fly, and might predict that during a panic, investors will sell Microsoft but, for seemingly irrational reasons, hold onto I.B.M.\n  ''Only an elite group of people are using these ideas, but a lot of people are thinking about them,'' said Stacy Williams, director of quantitative strategies at HSBC Global Markets. HSBC is working with Cambridge University in using models based on how viruses spread to forecast foreign currency markets.\n  ''The downside with these systems is their black box-ness,'' Mr. Williams said. ''Traders have intuitive senses of how the world works. But with these systems you pour in a bunch of numbers, and something comes out the other end, and it's not always intuitive or clear why the black box latched onto certain data or relationships.'' \n  Such qualms, however, have not stopped Wall Street from scouring university doctoral programs or listening to people like Mr. Kurzweil.\n  In the pursuit of previously undetectable patterns, hedge funds are racing to quantify things -- like newspaper headlines -- that were previously immune from number-crunching.\n  Both Dow Jones Newswires and Reuters have transformed decades of news archives into numerical data for use in designing and testing algorithmic systems. The companies are beginning to structure news so it can be absorbed by quantitative models within milliseconds of release. \n  Moreover, companies like Progress Software are working with news agencies to create computer programs that instantly translate news -- forexample, a headline regarding Microsoft's earnings -- into data. M.I.T. is examining, among other things, evaluating companies by seeing how many positive versus negative words are used in a newspaper article. \n  Software in development could potentially respond automatically to almost anything; changes in weather forecasts on television news, shifting analyst sentiments or what a particular movie critic said about the new blockbuster.\n  ''Right now, everyone basically has access to the same data,'' said John Bates, a Progress Software executive. ''To get an edge, we want to give investors the ability to immediately turn news into numbers. We want to automate what before required human analysis.''\n  But as these new techniques proliferate, some worry that promotion is outpacing reality. These techniques may be better for marketing than stock picking.\n  ''Investment firms fall over themselves advertising their latest, most esoteric systems,'' said Mr. Lo of M.I.T., who was asked by a $20 billion pension fund to design a neural network. He declined after discovering the investors had no real idea how such networks work. \n  ''There are some pretty substantial misconceptions about what these things can and cannot do,'' he said. ''As with any black box, if you don't know why it works, you won't realize when it's stopped working. Even a broken watch is right twice a day.''\n","300":"It is very common in science fiction films for autonomous armed robots to make life-or-death decisions - often to the detriment of the hero. But these days, lethal machines with an ability to pull the trigger are increasingly becoming more science than fiction.\nThe U.N. Convention on Certain Conventional Weapons invited government representatives, advocacy organizations and scholars to a conference in Geneva this week to discuss the possible use of autonomous weapons systems in the future, as opposition against them is on the rise.\u00a0\nIn September, Russian President Vladimir Putin warned that \"the one who becomes the leader in this sphere will be the ruler of the world,\" referring to artificial intelligence in general. In the same speech, Putin also appeared to suggest that future wars would consist of battles between autonomous drones but then reassured his audience that Russia would naturally share such technology if it were to develop it first.\nSome systems already available come extremely close. The security surveillance robots used by South Korea in the demilitarized zone which separates it from North Korea could theoretically detect and kill targets without human interference, for instance.\nBut so far, no weapons system operates with real artificial intelligence and is able to adapt to changing circumstances by rewriting or modifying the algorithms written by human coders. All existing mechanisms still rely on human intervention and their decisions.\nThe rapid advances in the field have nevertheless triggered concerns among human rights critics and lawyers about the possible implications of the rise of autonomous weapons systems commonly known as killer robots. Who would take responsibility for incidents which are so far classified as war crimes? Could robots decide to turn against their own operators? And would wars fought between autonomous weapons systems be less brutal than conventional conflicts, or would they provoke more collateral damage?\nOne of the most vocal groups in opposition of such systems has been the Campaign to Stop Killer Robots, which calls for a pre-emptive ban. So far, more than 100 CEOs and founders of artificial intelligence and robotics companies have signed the campaign's open letter to the United Nations, urging the world community \"to find a way to protect us all from these dangers.\"\n\"Lethal autonomous weapons threaten to become the third revolution in warfare. Once developed, they will permit armed conflict to be fought at a scale greater than ever, and at time scales faster than humans can comprehend,\" read its open letter.\nCritics fear that criminals or rogue states could also eventually get control of these systems. \"(Autonomous systems) can be weapons of terror, weapons that despots and terrorists use against innocent populations, and weapons hacked to behave in undesirable ways,\" the open letter added.\nSuch concerns have existed for years and were also shared by several Nobel laureates, including former Polish president Lech Walesa, who signed a joint letter in 2014, as well: \"It is unconscionable that human beings are expanding research and development of lethal machines that would be able to kill people without human intervention,\" the 2014 statement read.\nSo far, a proposed ban on autonomous weapons systems has triggered little enthusiasm among U.N. member states. Some of the world's leading militaries, including the United States and Russia, are researching and experimenting on how to make existing weapons more autonomous. Some researchers have welcomed efforts to expand artificial intelligence use in warfare.\nDefense analyst Joshua Foust has cautioned against condemning outright such systems, writing already in 2012 that humans, too, \"are imperfect - targets may be misidentified, vital intelligence can be discounted because of cognitive biases, and outside information just might not be available to make a decision.\"\n\"Autonomous systems can dramatically improve that process so that civilians are actually much better protected than by human inputs alone,\" Foust wrote.\nIf that vision becomes reality, perhaps the most crucial question will be whether robots can be taught how to recognize wrongdoing by themselves.\nMany professionals in the artificial intelligence industry hope that they will never have to find out the answer.\n","302":"[Video:  Watch on YouTube.]\nA race is underway toward the future of computer technology with advances in a branch of artificial intelligence known as machine learning. Machine-learning software is trained to handle vast amounts of data, and then learns as it goes, often on its own.\nMachine learning has been around for a long time, and it has been a crucial technology in the success of Internet giants like Google, Amazon and Facebook - used in the development of search, ad targeting and product recommendations. But in the last few years, machine learning has made huge improvements in computer vision, language translation and speech recognition, largely by applying the techniques of deep learning, which is inspired by theories about how the brain recognizes patterns.\u00a0\nEvery major technology company is investing aggressively in artificial intelligence and machine learning. And not just computer companies. Last Friday, Toyota announced it would spend $1 billion for research and development on artificial intelligence in the United States over the next five years.\nGoogle announced\u00a0on Monday a bold step to establish its leadership in the field of machine learning, accelerate the pace of innovation in the field and potentially strengthen its business. It is making the software of its new machine-learning system, TensorFlow, which was developed over years, open-source code. The software will be freely available for outside programmers to use and modify.\nGoogle announced the move on its official blog, noting that TensorFlow is \"faster, smarter and more flexible than our old system\" - up to five times faster in building and training machine-learning models. Its previous system, DistBelief, developed in 2011, was tailored for building neural networks, the building blocks of deep learning, and for use on Google's own network of data centers. The new system, according to Google, is a far more general tool. \"It may be useful,\" the blog post said, \"wherever researchers are trying to make sense of very complex data - everything from protein folding to crunching astronomy data.\"\nTensorFlow software, Google said, will be able to run on a single smartphone or across thousands of computers in data centers. The initial release of TensorFlow will be a version that runs on a single machine, and it will be put into effect for many computers in the months ahead, Google said.\nThere are other open-source software frameworks for deep-learning applications including Theano, Caffe and Torch. But Christopher Manning, a computer scientist at Stanford University, who has tried TensorFlow, is impressed by the software. \"It's a better, faster set of tools for deep learning,\" Mr. Manning said. \"I think it will be extremely widely adopted by researchers and students in universities and in companies.\"\nIn a separate post on Google's Research blog, Rajat Monga, a software engineer, wrote, \"TensorFlow is great for research, but it's ready for use in real products too.\"\nWith TensorFlow, Google is taking a page from its Android playbook. Android, the company's mobile operating system, is open-source software and now the most widely used operating system in smartphones worldwide. And while Google does not make money directly on Android, the company profits handsomely from its search-advertising services on Android phones.\n\"The software itself is open source, but if this is successful, it will feed Google's money-making machine,\" said Michael A. Cusumano, a professor at the Massachusetts Institute of Technology's Sloan School of Management. \"There are so many applications of machine learning to the bread and butter of what Google does.\"\nAndroid is a technology used by millions of consumers, while TensorFlow is a technology for a smaller, if vital, marketplace - scientists and software developers. And each of the big technology companies is trying to woo software developers to its machine-learning technology. Microsoft has Azure Machine Learning, Amazon has Amazon Machine Learning, and IBM has Watson.\nGoogle's move, said Oren Etzioni, executive director of the Allen Institute for Artificial Intelligence, is \"part of a platform play\" to attract developers and new hires to its machine-learning technology. \"But Google is taking a much less restrictive approach,\" Mr. Etzioni said, \"than just unveiling some services linked to your cloud offering, which is what IBM, Microsoft and Amazon have been doing.\"\nComputer scientists will be trying Google's code and gauging its performance, but they will also be closely watching how the company manages TensorFlow as an open-source project. \"This platform will live or die based on how they handle who controls updates to the code,\" said Gary Bradski, a computer scientist who is president of OpenCV, an open-source project for computer-vision software. \"Can the community have a say, or will Google control the official version by fiat?\"\n","303":"FICTIONSomewhere Off the Coast of Maine\n, by Ann Hood (Bantam, $ 6.95). This, one of the first titles in the Bantam New Fiction imprint, examines the lives of three women, college roommates from the '60s. One, Suzanne, has a child by a poet, then earns her MBA and refuses to tell her daughter about her father. Another, Claudia, marries and then is almost destroyed by the death of her son. And the third, Elizabeth, desperately ill, tries to remain true to the values she espoused in the '60s. Somewhere Off the Coast of Maine is also the story Suzanne's daughter's search for her father, and the love between the children of Claudia and Elizabeth.\u00a0\n\nWhitewater\n, by Paul Horgan (University of Texas Press, $ 10.95). Before there was Larry McMurtry's The Last Picture Show, there was Paul Horgan's Whitewater. Like the former, it is a sensitive and moving story of teen-age life in a rural Texas town. In Belvedere (population 5,000), Phil Durham and his friends Billy Breedlove and Marilee Underwood undergo the pleasures and torments of young love. Their journey to adulthood ends in tragedy for two of the friends, but not before the author marvelously evokes life in Depression America.\n\nThe Blotting Book\n, by E.F. Benson (Hogarth Press, $ 7.95). Best-known for his Mapp and Lucia series, E.F. Benson was also a scaremonger of note. Some of his horror stories still horrify -- \"The Room in the Tower\" and \"Caterpillars,\" for example. And this novel, one of the progenitors of the classic British mystery, bristles with elegant suspense as it ushers its upper-class characters from their mansions to the harsh reality of a criminal courtroom.\nNONFICTION\nThinking Machines: The Search for Artificial Intelligence\n, by Igor Aleksander and Piers Burnett (Knopf, $ 17.95). Though computers can store vast amounts of information and solve complicated mathematical problems, they aren't really intelligent. This book about artificial intelligence examines such questions as: What would a \"thinking machine\" be like? What is the difference between human and artificial intelligence? And what purposes might an intelligent machine serve? The authors discuss ways of telling whether a machine can really think such as the Turing Test: A human asks questions of another human and a machine, without knowing which answers come from man and which from machine. If, argued British mathematician Alan Turing, the human interrogator cannot tell machine from human by the answers, the machine can be called intelligent.\n\nCompanion to Narnia\n, by Paul F. Ford (Collier, $ 10.95);\nThe Latin Letters of C.S. Lewis\n, by Martin Moynihan (Bookmakers Guild, 1430 Florida Avenue, Suite 202, Longmont, Col. 80501, $ 4.95). These two books suggest, yet again, some of the amazing range of C.S. Lewis. The first offers a detailed dictionary of the major characters, themes and events in the Narnia chronicle (The Lion, the Witch and the Wardrobe and its sequels), but with such thoroughness that these 450 pages, with diagrams and maps, also function as a major critical work. The Latin Letters is, by contrast, a 48-page pamphlet about Lewis' correspondence, conducted in Latin, with an Italian priest. Moynihan traces the growth of their epistolary friendship, discusses the religious issues raised and translates passages.\nSCIENCE FICTION AND FANTASY\nThe Maker of Dune: Insights of a Master of Science Fiction\n, by Frank Herbert; edited and with introduction by Tim O'Reilly (Berkley, $ 7.95). If you were to poll readers, the odds are that Frank Herbert's Dune would emerge the most popular post-war science fiction novel. This collection of essays -- posthumous, alas -- gathers some two dozen articles, interviews and essays by Herbert, though only a few deal primarily with writing. Here are pieces on ecology, man's future in space, flying saucers, as well as a small dossier of material on the creation of Dune. Especially useful for collectors will be the extensive Herbert bibliography, listing magazine appearances as well as first and currently available editions.\n\nDemons!\n, edited by Jack Dann and Gardner Dozois (Ace, $ 3.50). Despite the trademark exclamation point, this series -- which includes Mermaids!, Unicorns!, Sorcerers! and several others -- is well-thought-out and altogether winning. This latest volume, featuring stories by Stephen King, Harlan Ellison, Lucius Shepard and several others, makes clear that Dr. Faustus was hardly alone in finding contracts with the devil generally make for bad business deals in the end. Still, as Anthony Boucher reminds us in \"Nellthu,\" sometimes you can cheat old Nick or his minions and get away with it. Besides the 14 stories included, editors Dozois and Dann also provide a six-page reading list.\n\nNightmares in Dixie: Thirteen Horror Tales from the American South\n, edited by Frank McSherry Jr., Charles G. Waugh and Martin Harry Greenberg (August House, Box 3223, Little Rock, Ark. 72203, $ 8.95; cloth, $ 19.95). Only New England can rival the South as a setting for horror fiction. (Has anyone ever written a scary story set in Nebraska or Michigan?) Poe, Faulkner, Tennessee Williams and Flannery O'Connor have created a South of crumbling plantation houses, suppressed passions, degenerate hillbillies, a backdrop that these more conventional horror tales work with or against. Arranged by state, the stories include \"Coven,\" by Manly Wade Wellman (Arkansas), \"Dark Melody of Madness,\" by Cornell Woolrich (Louisiana) and \"Cry Havoc,\" by David Grubb (West Virginia), as well as fine work by Jesse Stuart, John D. MacDonald, Ted White and Tom Reamy.\n","304":"             MOUNTAIN VIEW, Calif. - Google kicked off its annual developers conference Wednesday by outlining a broad vision of how it thinks artificial intelligence will shape the way we communicate, travel, work and play.\nChief executive Sundar Pichai said that improving artificial intelligence is Google's top strategy in its continuing goal to organize the world's information.\nUsing AI, Gmail will now suggest phrases for your replies, based on its interpretation of your conversation. Google Photos will figure out which of your snapshots are best for sharing, and it will use facial recognition to figure who should get those photos. A program called Google Lens will analyze your photos and be able to remove obstacles, such as a chain-link fence, that obscure your shot. Google Assistant will also be more proactive, now nudging you to leave earlier if the traffic to your next appointment is bad, rather than waiting for you to ask about it.\u00a0\nThe differences are subtle, but significant, said Gartner research vice president Brian Blau. \"We're not going to see that many new features - maybe some new buttons and dials. But what will improve is how well these apps relate to the individual.\"\nWith those personalized improvements, however, will come an even greater demand for data from Google services. For example, one big addition to Google Photos is the ability to auto-share your photos with a person of your choosing. That means that users not only allow Google to process their pictures, but also tell the company who their closest confidants are.\n\"Many of these new features in Google Assistant, Photos, and Home add value but also require the sharing of a lot of personal voice, photo, video and location information,\" said Patrick Moorhead, principal analyst at Moor Insight and Strategies. \"Google has the most personal information, [and] does the processing in the cloud, so I think right now they have the richest consumer AI capabilities.\"\nConsumers looking for big gadget announcements out of the conference, however, may have been disappointed. There was almost nothing said about Android Wear, Google's play for wearables. Details about the next version of Google's Pixel smartphone line - about which Google has offered few sales details - aren't expected to come until later this year.\nGoogle did release some brief details about its plans for stand-alone virtual reality headsets - which will not rely on a smartphone or a computer for power. Lenovo and HTC (which already makes a rival headset, the HTC Vive) are working on these products with Google; the company did not announce an official release date.\nBut Google, like Microsoft and even Apple, seems to be focusing developer attention more strongly on software and services rather than solely on gadgets, analysts said. While devices are undoubtedly still important, priorities seem to be on making sure that consumers using any device will be able to use Google services. One of the few products actually announced at the conference was a new version of a chip known as a tensor processing unit or TPU, which is custom-built for - what else? - artificial intelligence processing in the cloud.\n\"I think, in general, we'll start to see devaluation of individual devices over time,\" Blau said. Instead, he said, companies are likely to focus on making \"whatever screen you're looking at more personal and a lot more meaningful to you as an individual.\"\nhayley.tsukayama@washpost.com\n","306":"When the British television series ''Black Mirror'' first debuted in 2011, it drew in viewers with its techno ''Twilight Zone'' vibes. Here was a dark, ultramodern anthology series that harnessed all of our technological anxieties and spun them into twisted parables on the relationship between man and machine. But over the past two seasons -- and its wildly popular second life on Netflix -- the show's cult appeal has proved deeper than its digital gimmickry. Its stories are grounded close to home, in the very near future. The result is a human drama (and sometimes, satire) that feels considerably more visceral, immediate and human than your old-fashioned dystopian nightmare.\u00a0\nOn Friday, Oct. 21, ''Black Mirror'' returns after nearly two years with six new sci-fi scenarios. For its third season, the show has left the British network Channel 4 and gone directly to Netflix's global streaming platform, where it plays with an expanded budget, an extended episode run (six per season instead of the original three) and a trove of new technological inspirations, including augmented reality games and Twitter death threats. In a phone conversation earlier this month, the show's creator, Charlie Brooker, and his longtime collaborator Annabel Jones talked about getting the audience to take a leap of faith, watching tech companies seemingly jump on their fictional ideas and their construction of one of the show's most iconic episodes -- ''Be Right Back,'' in which a grieving woman's dead husband is resurrected in the form of a new artificial intelligence product that scans the deceased's phone records and social feeds to mimic his voice (and eventually, his physical presence). These are edited excerpts from the conversation. \n  Let's talk about ''Be Right Back.'' What was the germ of the idea for that episode?\n   CHARLIE BROOKER One night I was up late, checking social media, and I thought: ''What if none of these people were real? How would I know?'' I'd been reading a bit about artificial intelligence, as well. Eliza, one of the first artificial intelligence programs, only did very simple things, like ask ''How are you feeling today?'' And if you said, ''I'm a bit blue,'' it would say, ''What is it about blue that is making you feel blue?'' But it was amazing how quickly people would drop their guard, even though they knew full well it was a computer program. Those two things came together, and I wrote it in the middle of the night, over a couple of nights.\n  How did you decide that his presence needed to jump from the device and into a human form?\n  BROOKER It sort of mirrors online dating, what happens in this story. She starts off swapping written exchanges with this person, then that graduates to talking on the phone, and then he turns up in the flesh. And sure enough, he isn't all he promised to be.\n  ANNABEL JONES We always try to have one leap of faith in an episode, and then keep the rest of the world incredibly grounded. You had to believe that Hayley [Atwell, who played the widow] would actually let this thing into her life. So we talked about the slippery slope of how irresistible it would be for a grieving widow to have some semblance of her husband around.\n  BROOKER  There's an unwritten rule that if you introduce one fantastical thing in the first 10 pages, you're O.K. But if you introduce the fantastical element at Page 40, you're on wobbly ground. So we decided that when the robot is delivered to her, the more absurd it is, the better. He's literally delivered in a box full of polyurethane chips. A rubber man. She puts him in the bathtub and sprinkles nutrients in. We could have had him 3-D printed in super-fantastical detail. But this was a deliberately bizarre, weird, low-information way of doing it.\n  JONES And you used all of that to your advantage, Charlie. Undercutting it all was Domhnall [Gleeson, playing the husband] himself laughing at the ridiculousness of it all. It took the edge off.\n  And his personality is based entirely on social media postings. What does that do to a person?\n  JONES When this episode first came out in 2013, people were just beginning to observe how our online selves are so much more performative than we ever would be in real life.\n  BROOKER If this thing is constructed from your social media profile, then it's off by several degrees, because you are not your social media profile. She's actually incredibly lucky that when her husband comes back, he's nice and bland. A lot of people would be intolerable.\n  The most recent episode of the show aired almost two years ago. What technological changes have you seen in that time that you've started playing with?\n  BROOKER People are more O.K. with A.R. [augmented reality] and V.R. [virtual reality]. Think about Pok\u00e9mon Go, which people are already bored by! There's a lot more comfort with the concept of layers on top of reality.\n  JONES We've got one episode [''Playtest''] that absolutely mines that. It's a fun horror romp directed by Dan Trachtenberg, who did ''10 Cloverfield Lane.''\n  How do tech people feel about the show?\n  BROOKER They're writing everything down and then building it. Not long after ''Be Right Back'' came out, people started launching almost exactly that service -- something that would tweet on your behalf after you died.\n  Oh, my God.\n  BROOKER There was another one more recently that was even more insidious -- a company offering a service that will impersonate your relatives.\n  The show has a reputation for having a pessimistic outlook on technology. Do you share that attitude?\n  BROOKER I think it's a worried show, but that's probably from me, because I worry about everything. It's not necessarily technology, per se. If you made me a merengue pie, I'd worry that I was going to choke on it. But I like technology. I'm a big video games player. I used to be a video games journalist in the 1990s. There are so many problems we can solve with technology. It's just that we're still human. We're building increasingly powerful tools, and it's about whether we've got the wherewithal to use them responsibly. Sometimes we do, and sometimes we don't.\n  JONES When I look at ''Be Right Back,'' I see a very personal, human drama about someone coming to terms with grief in a contemporary world. In the Victorian era, people relied on mediums to process their grief. This is just a modern version of that. Twenty years ago, she would have had a shoe box of photos of her father that she would have flicked through, whereas now ----''\n  BROOKER Now she can hear him walking about in the attic. Nothing dystopian there!\n  JONES No, but it doesn't have to be!\n  BROOKER Usually these ideas come from a funny thought, which might be surprising overseas. I'm known for doing comedy in the U.K. Another thread of ''Be Right Back'' came from one of our old comedy shows. We thought, wouldn't it be great if there was an autopilot for phone conversations? A bit of software that sounded like you, and it said the sort of thing you would say when you got bored in the conversation? You would press it, and it would say: ''Yeah. Uh-huh. Mmm. Yeah. Uh-huh. Mmm.''\n  For Season 3, you're moving to Netflix and releasing more episodes at once. What feels different about the show?\n  JONES The films all feel like bigger canvases this time. Netflix is a global player. It's a bigger platform. You just can't help but raise your ambition a bit.\n  BROOKER We've always seen it as doing different little movies in different genres. ''Be Right Back'' is a supernatural love story. ''White Bear'' was a ''Wicker Man''-style horror. This time around, there's even more variety in tone. We've got an '80s coming-of-age romance with a ''Black Mirror'' spin on it. We've got a detective story in the style of Scandinavian police drama. ''Playtest'' is the horror romp. ''Nosedive'' is more playful than we've done before. The tone of all of them is not unremittingly bleak.\n\n\n\n","307":"After years of hype and failed expectations, artificial intelligence is finally beginning to make some real money.\nOnce the theme that launched a thousand science fiction novels, artificial intelligence is now capturing investment dollars as well as the imagination. Entrepreneurs, venture capitalists and some of the largest companies in the world are scrambling to hoist themselves up those few branches of artificial intelligence research that are bearing fruitful results.\nThe branch supporting the heaviest expectations is the so-called \"expert system.\" Expert systems represent the tangible product of \"knowledge engineering,\" the discipline of systematically transforming human knowledge skills into machines. Unlike conventional computer programs, which use special languages to get the computer to perform calculations, expert systems contain rules; if the computer follows these rules, it can effectively simulate the expert's role.\u00a0\nAfter carefully interviewing experts and analyzing what they do and how they do it, a knowledge engineer divines the rules that an expert uses and then crafts a system that can replicate the expert's reasoning skills in a computer program. The program usually consists of chain after chain of IF\/THEN rules: IF certain criteria are met THEN consider this option.\nBy blending facts, rules of thumb and relevant general knowledge about a specific subject, the expert system can often serve as a substitute for the human expert. If experts can be valuable commodities, then so can expert systems, reasons the market. That's where there's money to be made.\nAt the National Conference on Artificial Intelligence here last week, the bulletin boards were filled with want-ads from General Electric and RCA and Texas Instruments and dozens of smaller companies seeking knowledge engineers and qualified expert systems builders in subjects ranging from engineering to medical diagnosis to legal analysis. The expert systems industry, such that it is, is prepared to take off.\n\"The interest is enormous,\" says Esther Dyson, editor of RElease 1.0 and a computer industry consultant. \"It's a breakthrough that's waiting to happen.\"\nThe concept of expert systems has been around for almost two decades. Dendral, a system that decides what organic molecules might look like based on spectrograph data, was devised by two Stanford scientists in the late 1960s. In the mid-1970s, another Stanford scientist designed MYCIN, an expert system to diagnose and prescribe antibiotics for infectious diseases. Both programs, and several others like them, enjoyed reasonable success when applied, and won considerable attention in academe.\nBut it wasn't until the success of R1 that business realized that expert systems could dramatically affect the bottom line. In cooperation with John McDermott, a computer scientist at Carnegie Mellon, Digital Equipment Corp. began creating in 1979 an expert system--R1--to help solve a multimillion-dollar problem.\nDEC had discovered that, in a significant number of the computer orders it shipped, all the components to configure a customer installation properly weren't there. The expense of sending out the appropriate parts along with service engineers, the annoyed customers and the subsequent delay in getting paid was costing Digital hundreds of thousands of dollars a month.\nMcDermott spent weeks interviewing engineers and poring over technical manuals to determine whether the problem was susceptible to the experts systems approach. \"It turned out it was the perfect size,\" says McDermott. \"It required enough knowledge so that a program that can perform the task is interesting--but the amount of knowledge required for the task and the specificity of the configuration constraints is such that the problem does not push very hard on state-of-the-art knowledge engineering techniques.\"\nThe first version of R1 had over 250 rules, which grew to 750 as the system was rolled out into the field in 1980. Today, R1 has over 2,500 rules. Digital estimates that R1 has been used to help install computers for over 30,000 customers. Most importantly, the company estimates that it has saved between $7 million and $10 million over the last three years.\n\"Conventional programming techniques just weren't applicable to the problem,\" said McDermott. \"There would have been a combinatorial explosion vast multiplicity of choices . There were too many variables to consider.\"\n\"The significance of R1 was that the real world could use AI artificial intelligence techniques to solve problems,\" said Stephen Polit, a DEC executive, \"There's now widespread expert system activity at Digital.\" The company recently set up a Knowledge Engineering Advanced Development Group to apply expert systems to such areas as office automation, hardware fault diagnosis and integrated circuit design.\nThrough Bell Laboratories, American Telephone & Telegraph Co. has used expert systems to save it time and money--although spokesman don't know how much. ACE, for Automated Cable Expertise, is designed to both monitor and analyze problems associated with telephone line maintenenance. Located in New Jersey and linked to Southwestern Bell via a long-distance phone line, ACE uses hundreds of rules as it sorts through maintenance records in Fort Worth trying to ascertain future trouble spots.\nAccording to a Bell Labs spokesman, ACE can do a maintenance survey in an hour that would take a human a week. ACE does its survey overnight and has a status report waiting for the appropriate Southwestern Bell managers the next morning.\n\"Expert systems is a normal evolution of information processing systems,\" says Gregg Versonder, one of the Bell Labs scientists who created ACE. \"But it's an order-of-magnitude leap in capability.\"\nEssentially, Vesonder claims, ACE doesn't just file mainteneance information into a data base, it \"knows\" how to make vital decisions pertaining to telephone network maintenance by drawing on that data base. It could be used in any of the Bell operating companies with only minor modifications, and conceivably could save phone companies millions of dollars a year.\nThe key problem in the expert system approach is that it has to be tailored to specific problems. \"Each time you do one, it's different,\" says J. Morris Tenenbaum, who directs artificial intelligence research for Schlumberger's Fairchild subsidiary. \"It's a craft industry at this point.\"\nTenenbaum and others believe that the industry must find \"generics\"--experts systems concepts and structures that can easily be applied from industry to industry. \"It won't be a volume industry until you have those generic applications,\" Tenenbaum says.\nFor example, R1 configures computers. But the concept of configuration could cover industries as disparate as hospital supplies and building construction. Expert systems could be designed to track inventory levels, demand and turnover in a variety of different industries.\nThe challenge is coming up with an expert system that \"is general enough for wide application but close enogh to the surface to be made into specific applications,\" says McDermott, R1's creator.\nTeknowledge, a two-year old Palo Alto Expert Systems company founded by a group of Stanford computer science professors, thinks it is taking the best approach to the generics concept. The company, which grossed $2 million in fiscal 1983 and expects to make \"several times that\" this year, according to president Lee Hecht, offers companies CAKES--Computer Aided Knowledge Engineering Systems.\nEssentially, Teknowledge will take a core group of company employes and both train them and give them the computer-based tools to design their own knowledge systems.\n\"It's like we sell the Xerox machine,\" says Hecht. \"We are the Xerox machine. You take an expert, make 200 copies of him, and send him out into the field.\"\nTeknowledge will be most successful, Hecht feels, if it lets companies create their own expert systems rather than do the designing for them. The company has already worked on CAKES with Boeing, NCR Corp. and the French oil company Elf Aquitane.\nXerox Corp. is taking a similar approach with its LOOPS language. LOOPS, which Xerox has just formally introduced, is an experts system language. By becoming a LOOPS programmer, an individual has the computer tools to design his own expert systems. Teknowledge is currently one of the LOOPS testing sites.\nSeveral industry analysts believe that expert systems could become one of the most profitable computer software areas by the end of the decade--but, they say, it is still to early to say whether knowledge engineering techniques will improve enough to make it the cornerstone of a new generation of computer software.\nThe essence of the question, they feel, is knowledge representation: how does one adequately present the human cognitive process on a silicon chip? The answer to that blends psychology, computer software and computer technology. All those areas are still evolving.\nHowever, according to Stanford professor and Teknowledge founder Doug Lenat, the next generation expert system will know how to learn from experience--and then create additional rules for itself. In essence, an expert system that can learn to become more of an expert.\n","309":"                               Machine again gets the better of man, this time in  the complex game of go          \nYou can chalk it up as another victory for the machines.\nIn what they called a milestone achievement for artificial intelligence, scientists said last week that they have created a computer program that beat a professional human player at go, a complex board game that originated in ancient China.\u00a0\nThe feat recalled IBM supercomputer Deep Blue's 1997 match victory in chess over world champion Garry Kasparov. But go, a strategy game popular in such places as China, South Korea and Japan, is vastly more complicated than chess.\n \"Go is considered to be the pinnacle of game AI research,\" said artificial intelligence researcher Demis Hassabis of Google DeepMind, the British company that developed the AlphaGo program. \"It's been the grand challenge, or holy grail if you like, of AI since Deep Blue beat Kasparov at chess.\"\nAlphaGo swept a five-game match against Chinese professional Fan Hui, a three-time European go champion. Until now, the best computer go programs had played only at the level of human amateurs.\nIn go, two players place black and white pieces on a square grid, aiming to take more territory than their adversary.\n \"It's a very beautiful game with extremely simple rules that lead to profound complexity. In fact, go is probably the most complex game ever devised by humans,\" said Hassabis, who was a chess prodigy as a child.\nScientists have taken large strides in artificial intelligence in recent years, making computers think and learn more like people do.\nHassabis acknowledged that some people might worry about the increasing capabilities of  AI after the go accomplishment, but added, \"We're still talking about a game here.\"\nWhile AlphaGo learns in a more humanlike way, it still needs much more  practice - millions of games rather than thousands - than a human  needs to get good at go, Hassabis said.\nThe scientists foresee that similar AI programs might be developed to improve smartphone assistants such as Apple's Siri, to  aid in medical diagnostics and eventually to collaborate with human scientists in research.\nHassabis said South Korea's Lee Sedol, the world's top go player, has agreed to play AlphaGo in a five-game match in Seoul in March. Lee said in a statement, \"I heard Google DeepMind's AI is surprisingly strong and getting stronger, but I am confident that I can win, at least this time.\"\nThe findings were published in the journal Nature.\n- Reuters          \n","311":"THINKING MACHINESThe Quest for Artificial Intelligence -- and Where It's Taking Us Next By Luke Dormehl275 pp. TarcherPerigee. Paper, $16.\nHEART OF THE MACHINEOur Future in a World of Artificial Emotional IntelligenceBy Richard Yonck312 pp. Arcade Publishing. $25.99.\nBooks about science and especially computer science often suffer from one of two failure modes. Treatises by scientists sometimes fail to clearly communicate insights. Conversely, the work of journalists and other professional writers may exhibit a weak understanding of the science in the first place.\nLuke Dormehl is the rare lay person -- a journalist and filmmaker -- who actually understands the science (and even the math) and is able to parse it in an edifying and exciting way. He is also a gifted storyteller who interweaves the personal stories with the broad history of artificial intelligence. I found myself turning the pages of \"Thinking Machines\" to find out what happens, even though I was there for much of it, and often in the very room.\nDormehl starts with the 1964 World's Fair -- held only miles from where I lived as a high school student in Queens -- evoking the anticipation of a nation working on sending a man to the moon. He identifies the early examples of artificial intelligence that captured my own excitement at the time, like IBM's demonstrations of automated handwriting recognition and language translation. He writes as if he had been there.\nDormehl describes the early bifurcation of the field into the Symbolic and Connectionist schools, and he captures key points that many historians miss, such as the uncanny confidence of Frank Rosenblatt, the Cornell professor who pioneered the first popular neural network (he called them \"perceptrons\"). I visited Rosenblatt in 1962 when I was 14, and he was indeed making fantastic claims for this technology, saying it would eventually perform a very wide range of tasks at human levels, including speech recognition, translation and even language comprehension. As Dormehl recounts, these claims were ridiculed at the time, and indeed the machine Rosenblatt showed me in 1962 couldn't perform any of these things. In 1969, funding for the neural net field was obliterated for about two decades when Marvin Minsky and his M.I.T. colleague Seymour Papert published the book \"Perceptrons,\" which proved a theorem that perceptrons could not distinguish a connected figure (in which all parts are connected to each other) from a disconnected figure, something a human can do easily.\nWhat Rosenblatt told me in 1962 was that the key to the perceptron achieving human levels of intelligence in many areas of learning was to stack the perceptrons in layers, with the output of one layer forming the input to the next. As it turns out, the Minsky-Papert perceptron theorem applies only to single-layer perceptrons. As Dormehl recounts, Rosenblatt died in 1971 without having had the chance to respond to Minsky and Papert's book. It would be decades before multi-layer neural nets proved Rosenblatt's prescience. Minsky was my mentor for 54 years until his death a year ago, and in recent years he lamented the \"success\" of his book and had become respectful of the recent gains in neural net technology. As Rosenblatt had predicted, neural nets were indeed providing near human-level (and in some cases superhuman levels) of performance on a wide range of intelligent tasks, from translating languages to driving cars to playing Go.\nDormehl examines the pending social and economic impact of artificial intelligence, for example on employment. He recounts the positive history of automation. In 1900, about 40 percent of American workers were employed on farms and over 20 percent in factories. By 2015, these figures had fallen to 2\u202fpercent on farms and 8.7 percent in factories. Yet for every job that was eliminated, we invented several new ones, with the work force growing from 24 million people (31\u202fpercent of the population in 1900) to 142 million (44\u202fpercent of the population in 2015). The average job today pays 11 times as much per hour in constant dollars as it did a century ago. Many economists are saying that while this may all be true, the future will be different because of the unprecedented acceleration of progress. Although expressing some cautions, Dormehl shares my optimism that we will be able to deploy artificial intelligence in the role of brain extenders to keep ahead of this economic curve. As he writes, \"Barring some catastrophic risk, A.I. will represent an overall net positive for humanity when it comes to employment.\"\nMany observers of A.I. and the other 21st-century exponential technologies like biotechnology and nanotechnology attempt to peer into the continuing accelerating gains and fall off the horse. Dormehl ends his book still in the saddle, discussing the prospect of conscious A.I.s that will demand and\/or deserve rights, and the possibility of \"uploading\" our brains to the cloud. I recommend this book to anyone with a lay scientific background who wants to understand what I would argue is today's most important revolution, where it came from, how it works and what is on the horizon.\n\"Heart of the Machine,\" the futurist Richard Yonck's new book, contains its important insight in the title. People often think of feelings as secondary or as a sideshow to intellect, as if the essence of human intelligence is the ability to think logically. If that were true, then machines are already ahead of us. The superiority of human thinking lies in our ability to express a loving sentiment, to create and appreciate music, to get a joke. These are all examples of emotional intelligence, and emotion is at both the bottom and top of our thinking. We still have that old reptilian brain that provides our basic motivations for meeting our physical needs and to which we can trace feelings like anger and jealousy. The neocortex, a layer covering the brain, emerged in mammals two hundred million years ago and is organized as a hierarchy of modules. Two million years ago, we got these big foreheads that house the frontal cortex and enabled us to process language and music.\nYonck provides a compelling and thorough history of the interaction between our emotional lives and our technology. He starts with the ability of the early hominids to fashion stone tools, perhaps the earliest example of technology. Remarkably the complex skills required were passed down from one generation to the next for over three million years, despite the fact that for most of this period, language had not yet been invented. Yonck makes a strong case that it was our early ability to communicate through pre-language emotional expressions that enabled the remarkable survival of this skill, and enabled technology to take root.\nYonck describes today's emerging technologies for understanding our emotions using images of facial expressions, intonation patterns, respiration, galvanic skin response and other signals -- and how these instruments might be adopted by the military and interactive augmented reality experiences. And he recounts how all communication technologies from the first books to today's virtual reality have had significant sexual applications and will enhance sensual experiences in the future.\nYonck is a sure-footed guide and is not without a sense of humor. He imagines, for example, a scenario a few decades from now with a spirited exchange at the dinner table. \"No daughter of mine is marrying a robot and that's final!\" a father exclaims.\nHis daughter angrily replies: \"Michael is a cybernetic person with the same rights you and I have! We're getting married and there's nothing you can do to change that!\" She storms out of the room.\nYonck concludes that we will merge with our technology -- a position I agree with -- and that we have been doing so for a long time. He argues, as have I, that merging with future superintelligent A.I.s is our best strategy for ensuring a beneficial outcome. Achieving this requires creating technology that can understand and master human emotion. To those who would argue that such a quest is arrogantly playing God, he says simply: \"This is what we do.\"\nRay Kurzweil, an inventor and futurist, is the author of \"The Singularity Is Near\" and \"How to Create a Mind.\"\nDRAWING (DRAWING BY ELENI KALORKOTI)\n","312":"You might not remember this, but the alien invasion in the 1990s sci-fi blockbuster \"Independence Day\" began not with laser blasts but with a cyberattack. As Jeff Goldblum's computer nerd character explains in the film, the alien fleet has hacked into Earth's satellites, hijacking their communication systems to coordinate their (ultimately unsuccessful) assault on humanity.\nTo call that scenario far-fetched is an understatement. But a pair of astrophysicists say in a bizarre paper released this month that the possibility of an extraterrestrial hack - one far more sophisticated than the attack in \"Independence Day\" - is worth taking seriously. (How seriously to take the paper, which was published in an unconventional, non-peer-reviewed academic archive, is another matter.)\u00a0\nMichael Hippke of the Sonneberg Observatory in Germany and John Learned of the University of Hawaii warn in their article that an alien message from space could contain malicious data designed to wreak havoc on Earth. Such a message would be impossible to \"decontaminate with certainty\" and could pose an \"existential threat,\" they argue; therefore humans should use extreme caution.\nScientists, academics and futurists have long debated whether humanity would benefit from contact with extraterrestrial intelligence, or ETI. The Search for Extraterrestrial Intelligence Institute, a research organization, looks for alien life and seeks a peaceful dialogue. Its researchers listen for communication signals from smart aliens and send out signals from Earth in hopes that another civilization might pick them up. So far, no one has heard anything very lifelike.\nHippke and Learned's paper - which reads more like a thought experiment, not serious scholarship - ponders the dangers of receiving these theoretical interstellar missives.\n\"While it has been argued that sustainable ETI is unlikely to be harmful, we [cannot] exclude this possibility,\" the researchers write in the article, which was first reported by Motherboard. \"After all, it is cheaper for ETI to send a malicious message to eradicate humans compared to sending battleships.\"\nBut Seth Shostak, senior astronomer from the SETI Institute, wrote for NBC News that the researchers overlooked a number of technical realities that would prevent any space malware from messing up our computers and destroying life as we know it.\n\"If these nasty aliens are more than 40 light-years away, they won't know that we have personal computers, let alone which operating system they should target. If they're more than 80 light-years away, they won't know that we have computers of any kind. Maybe they'll try to disable our abacuses.\"\nThe researchers envision several different types of malicious communications. A simple one might contain a threat like \"We will make your sun go supernova tomorrow.\"\n\"True or not, it could cause widespread panic,\" they wrote, or have a \"demoralizing cultural influence.\" A longer, more nuanced message could sow confusion and fear, especially if it's received by amateurs, according to the paper. The spread of such messages could not be easily contained, but they could at least be printed out and examined on paper, and wouldn't necessarily require a computer to decipher.\nBut large, complex messages written in code would.\nMessages that contain big diagrams, algorithms or equations could come with viruses hidden in them, the researchers say. They couldn't be printed out and examined manually, so they'd have to be deciphered on a computer, the paper speculates.\nThe messages could also be compressed in the same way personal computers compress large files for more efficient transfer, and the algorithm needed to decompress them could also be code. Executing those billions of decompression instructions could unleash the malware, according to the paper.\nIn their most out-there example, Hippke and Learned imagine a sort of extraterrestrial spearphishing, the technique human hackers sometimes use to gain personal information from victims under the guise of a trustworthy source. Russian hackers probably used this technique to gain access to the Democratic National Committee's computer networks.\nAs the researchers write in their paper, the header of such a message might read: \"We are friends. The galactic library is attached. It is in the form of an artificial intelligence which quickly learns your language and will answer your questions. You may execute the code following these instructions.\"\nExtraordinary steps could be taken to isolate the artificial intelligence - the researchers even suggest building a computer on the moon to execute the code and rigging it with \"remote-controlled fusion bombs\" to destroy it in case of an emergency.\nThis idea is known as an \"AI box,\" essentially a solitary confinement cell for an artificial intelligence. Experts have long discussed it as a way to contain a potentially dangerous artificial intelligence. Some have argued that a sufficiently advanced computer program could easily manipulate its human guards and find a way out of the \"box.\"\nHippke and Learned say efforts to imprison an artificial intelligence delivered by extraterrestrials would probably fail. Even a military-style experiment could go awry.\n\"Current research indicates that even well-designed boxes are useless, and a sufficiently intelligent AI will be able to persuade or trick its human keepers into releasing it,\" Hippke and Learned write.\nFor instance, the researchers say, the artificial intelligence could offer a cure for cancer in exchange for an increase in computer capacity.\n\"We could decline such offers,\" the paper posits, \"but shall not forget that humans are involved in this experiment. Consider a nightly conversation between the AI and a guard: 'Your daughter is dying from cancer. I give you the cure for the small price of ...'. We can never exclude human error and emotion.\"\nOnce the artificial intelligence is out, it could do all kinds of damage, like dupe us into building nanobots that could take over the world, according to the paper. Other forms of annihilation could await if the machine decides humans are \"as irrelevant as monkeys,\" Hippke and Learned say.\nThe paper ends on an optimistic note. The researchers suggest that a message from extraterrestrials is likely to be benign. Understanding the risks is what's important, they say.\n\"The potential benefits from joining a galactic network might be considerable,\" the researchers wrote. \"Overall, we believe that the risk is very small (but not zero), and the potential benefit very large, so that we strongly encourage to read an incoming message.\"\n          More from Morning Mix:       \n          Oakland Mayor Libby Schaaf tipped off immigrants about ICE raid and isn't sorry she did       \n          106 million people watched 'M.A.S.H.' finale 35 years ago. No scripted show since has come close.       \n          An alien cyberattack? As if we didn't have enough to worry about.       \n","313":" ABSTRACT:Artificial Intelligence industry, which many market researchers had projected would reach $4 billion in annual sales by now, remains nascent; generous estimates of market today are closer to $600 million; after swallowing up hundreds of millions of dollars in venture capital and exciting some of brightest professors at top technical schools with visions of riches, hundreds of AI start-ups have yielded only few profitable public companies (M)\n","314":"SAN FRANCISCO -- A Silicon Valley start-up recently unveiled a drone that can set a course entirely on its own. A handy smartphone app allows the user to tell the airborne drone to follow someone. Once the drone starts tracking, its subject will find it remarkably hard to shake.\nThe drone is meant to be a fun gadget -- sort of a flying selfie stick. But it is not unreasonable to find this automated bloodhound a little unnerving.\u00a0\nOn Tuesday, a group of artificial intelligence researchers and policymakers from prominent labs and think tanks in both the United States and Britain released a report that described how rapidly evolving and increasingly affordable A.I. technologies could be used for malicious purposes. They proposed preventive measures including being careful with how research is shared: Don't spread it widely until you have a good understanding of its risks.\nA.I. experts and pundits have discussed the threats created by the technology for years, but this is among the first efforts to tackle the issue head-on. And the little tracking drone helps explain what they are worried about.\nThe drone, made by a company called Skydio and announced this month, costs $2,499. It was made with technological building blocks that are available to anyone: ordinary cameras, open-source software and low-cost computer chips.\nIn time, putting these pieces together -- researchers call them dual-use technologies -- will become increasingly easy and inexpensive. How hard would it be to make a similar but dangerous device?\n\"This stuff is getting more available in every sense,\" said one of Skydio's founders, Adam Bry. These same technologies are bringing a new level of autonomy to cars, warehouse robots, security cameras and a wide range of internet services.\nBut at times, new A.I. systems also exhibit strange and unexpected behavior because the way they learn from large amounts of data is not entirely understood. That makes them vulnerable to manipulation; today's computer vision algorithms, for example, can be fooled into seeing things that are not there.\n\"This becomes a problem as these systems are widely deployed,\" said Miles Brundage, a research fellow at the University of Oxford's Future of Humanity Institute and one of the report's primary authors. \"It is something the community needs to get ahead of.\"\n[Video:  Watch on YouTube.]\nThe report warns against the misuse of drones and other autonomous robots. But there may be bigger concerns in less obvious places, said Paul Scharre, another author of the report, who had helped set policy involving autonomous systems and emerging weapons technologies at the Defense Department and is now a senior fellow at the Center for a New American Security.\n\"Drones have really captured the imagination,\" he said. \"But what is harder to anticipate -- and wrap our heads around -- is all the less tangible ways that A.I. is being integrated into our lives.\"\nThe rapid evolution of A.I. is creating new security holes. If a computer-vision system can be fooled into seeing things that are not there, for example, miscreants can circumvent security cameras or compromise a driverless car.\nResearchers are also developing A.I. systems that can find and exploit security holes in all sorts of other systems, Mr. Scharre said. These systems can be used for both defense and offense.\nAutomated techniques will make it easier to carry out attacks that now require extensive human labor, including \"spear phishing,\" which involves gathering and exploiting personal data of victims. In the years to come, the report said, machines will be more adept at collecting and deploying this data on their own.\nA.I. systems are increasingly adept at generating believable audio and video on their own. This will accelerate the progress of virtual reality, online games and movie animation. It will also make it easier for bad actors to spread misinformation online, the report said.\nThis is already beginning to happen through a technology called \"Deepfakes,\" which provides a simple way of grafting anyone's head onto a pornographic video -- or put words into the mouth of the president.\nSome believe concerns over the progress of A.I. are overblown. Alex Dalyac, chief executive and co-founder of a computer vision start-up called Tractable, acknowledged that machine learning will soon produce fake audio and video that humans cannot distinguish from the real thing. But he believes other systems will also get better at identifying misinformation. Ultimately, he said, these systems will win the day.\nTo others, that sounds like an endless cat-and-mouse game between A.I. systems trying to create the fake content and those trying to identify it.\n\"We need to assume that there will be advances on both sides,\" Mr. Scharre said.\nFollow Cade Metz on Twitter: @CadeMetz. \nPHOTO: A drone made by a company called Skydio that can follow a person without human interaction uses widely available parts and open-source software. (PHOTOGRAPH BY Laura Morton for The New York Times FOR THE NEW YORK TIMES)Related Articles\n\n","315":"This highly successful television quiz show is the latest challenge for artificial intelligence. \n  What is ''Jeopardy''? \n  That is correct. \n  I.B.M. plans to announce Monday that it is in the final stages of completing a computer program to compete against human ''Jeopardy!'' contestants. If the program beats the humans, the field of artificial intelligence will have made a leap forward.\n  I.B.M. scientists previously devised a chess-playing program to run on a supercomputer called Deep Blue. That program beat the world champion Garry Kasparov in a controversial 1997 match (Mr. Kasparov called the match unfair and secured a draw in a later one against another version of the program).\u00a0\n  But chess is a game of limits,  with pieces that have clearly defined powers. ''Jeopardy!'' requires a program with the suppleness to weigh an almost infinite range of relationships and to make subtle comparisons and interpretations. The software must interact with humans on their own terms, and fast.  \n  Indeed, the creators of the system -- which the company refers to as Watson, after  the I.B.M. founder, Thomas J. Watson Sr. -- said they were not yet confident their system would be able to compete successfully on the  show, on which human champions typically provide correct responses 85 percent of the time. \n  ''The big goal is to get computers to be able to converse in human terms,'' said the team leader, David A. Ferrucci, an I.B.M. artificial intelligence researcher. ''And we're not there yet.'' \n  The team is aiming not at a true thinking machine but at a new class of software that can ''understand'' human questions and respond to them correctly. Such a program would have enormous economic implications.\n  Despite more than four decades of experimentation in artificial intelligence, scientists have made only modest progress until now toward building machines that can understand language and interact with humans.\n  The proposed contest is an effort by I.B.M. to prove that its researchers can make significant technical progress by picking ''grand challenges'' like its early chess foray. The new bid is based on three years of work by a  team that has grown to 20 experts in fields like natural language processing, machine learning and information retrieval.\n  Under the rules of the match that the company has negotiated with the ''Jeopardy!'' producers, the computer will not have to emulate all human qualities. It will receive questions as electronic text. The human contestants will both see the text of each question and hear it spoken by the show's host, Alex Trebek.\n  The computer will respond with a synthesized voice to answer questions and to choose follow-up categories. I.B.M. researchers said they planned to move a Blue Gene supercomputer to Los Angeles for the contest. To approximate the dimensions of the challenge faced by the human contestants, the computer will not be connected to the Internet, but will make its answers based on text that it has ''read,'' or processed and indexed, before the show.\n  There is some skepticism among researchers in the field about the effort. ''To me it seems more like a demonstration than a grand challenge,'' said Peter Norvig, a computer scientist who is director of research at Google. ''This will explore lots of different capabilities, but it won't change the way the field works.'' \n  The I.B.M. researchers and ''Jeopardy!'' producers said they were considering what form their cybercontestant would take and what gender it would assume. One possibility would be to use an animated avatar that would appear on a computer display.\n  ''We've only begun to talk about it,'' said Harry Friedman, the executive producer of ''Jeopardy!'' ''We all agree that it shouldn't look like Robby the Robot.'' \n  Mr. Friedman added that they were also thinking about whom the human contestants should be and were considering inviting Ken Jennings, the ''Jeopardy!'' contestant who won 74 consecutive times and collected $2.52 million in 2004.\n  I.B.M. will not reveal precisely how large the system's internal database would be. The actual amount of information could be a significant fraction of the  Web now indexed by Google,  but artificial intelligence researchers said that having access to more information would not be the most significant key to improving the system's performance.\n  Eric Nyberg, a computer scientist at Carnegie Mellon University, is collaborating with I.B.M. on research to devise computing systems capable of answering questions that are not limited to specific topics. The real difficulty, Dr. Nyberg said, is not searching a database but getting the computer to understand what it should be searching for.\n  The system must be able to deal with analogies, puns, double entendres and relationships like size and location, all at lightning speed.\n  In a demonstration match here at the I.B.M. laboratory against two researchers recently, Watson appeared to be both aggressive and competent, but also made the occasional puzzling blunder.\n  For example, given the statement, ''Bordered by Syria and Israel, this small country is only 135 miles long and 35 miles wide,'' Watson beat its human competitors by quickly answering, ''What is Lebanon?'' \n  Moments later, however, the program stumbled when it decided it had high confidence that a ''sheet'' was a fruit. \n  The way to deal with such problems, Dr. Ferrucci said, is to improve the program's ability to understand the way ''Jeopardy!'' clues are offered. The complexity of the challenge is underscored by the subtlety involved in capturing the exact meaning of a spoken  sentence. For example, the sentence ''I never said she stole my money'' can have seven different meanings depending on which word is stressed. \n  ''We love those sentences,'' Dr. Nyberg said. ''Those are the ones we talk about when we're sitting around having beers after work.''\n","316":"It may not strike everyone as the loftiest ambition: creating machines that are smarter than people. Not setting the bar terribly high, is it? So the more cynical might say. All the same, an array of scientists and futurists are convinced that the advent of devices with superhuman intelligence looms in the not-distant future. The prospect fills some of our planet's brainiest specimens with dread.\nThey include certified smart men like Bill Gates of Microsoft, the physicist Stephen Hawking and Elon Musk, head of SpaceX. Messrs. Hawking and Musk have been especially grim. ''The development of full artificial intelligence could spell the end of the human race,'' Mr. Hawking told the BBC in 2014. At about the same time, Mr. Musk worried that ''with artificial intelligence, we are summoning the demon,'' a fiend that he feared would become ''our biggest existential threat.'' \n  When people of their caliber speak, it seems reasonable to listen. And so, alarms about a computer-spawned apocalypse are a backdrop to the latest installment in the Retro Report series, video documentaries that explore major news events of the past and their continuing effects.\u00a0\n  Men of science are not alone in the hand-wringing over the possibility of machines running wild. Asked what they feared most, Americans interviewed by researchers at Chapman University in Southern California ranked the consequences of modern technology near the top. Even death did not rattle them as much; it was way down on their list of worries, at No. 43.\n  While not discounting that doomsayers may prove someday to be right, Retro Report offers more reassuring views from computer specialists who sense that the end is not nigh -- if only, they say, because machines are not nearly as clever, or necessarily as pernicious, as the fretters believe.\n  Jitters over humanity's falling victim to various creations are as old Mary Shelley's Frankenstein monster and the Golem of Jewish tradition. Hostile robots have been on the scene since at least the 1920s with the play ''R.U.R.,'' by the Czech writer Karel Capek. The initials stood for ''Rossum's Universal Robots.'' Indeed, this work introduced ''robot'' into the language. Since then, run-amok machines have been a science-fiction staple in books and films like ''Colossus: The Forbin Project,'' ''I, Robot,'' ''2001: A Space Odyssey,'' ''Transcendence,'' ''Ex Machina'' and the seemingly inexhaustible supply of ''Terminator'' movies. One sure bet about those films is that, like the Terminator itself, they'll be back.\n  On occasion, machines are cast as a benign presence, as in the 2013 film ''Her,'' in which a man finds intimacy with an operating system that is guided by artificial intelligence (not to mention made alluring by the voice of Scarlett Johansson). In Japan, some people have closely bonded with robot dogs, to the point of holding funerals for automated pooches that cease to function.\n  More typically, though, the machines -- robots, cyborgs, androids, clones -- are depicted as threats to human survival. As Retro Report recalls, fear of them in real life grew in 1997 when a chess-playing IBM computer, Deep Blue, defeated the world champion, Garry Kasparov. Apprehension deepened for some in 2011 when two stars of the quiz show ''Jeopardy!'' were soundly defeated by a new IBM gizmo. (What is Watson?) This week, artificial intelligence will again challenge the human brain as Google's DeepMind competes in South Korea against a champion in Go, the Chinese board game with trillions of possible moves.\n  Arguably, there is no reason to lose sleep over those souped-up gadgets. Sure, Watson and its brethren are good at games and other sorts of data processing. But contemplating a takeover of the world's nuclear arsenals? Not a chance. Nonetheless, some experts foresee a time, not far off, when artificial intelligence, A.I., will match and then exceed human intelligence, at ever-accelerating and frightening speeds.\n  ''Shortly after, the human era will be ended,'' Vernor Vinge, a computer scientist and science fiction author, wrote in 1993. That moment, he predicted, would come ''within 30 years.'' In other words, check your calendars -- a mere seven years remain until the arrival of this ''technological singularity,'' as it was called.\n  Another A.I. expert, Raymond Kurzweil, has pinpointed 2045 as the due date. Still another student of the subject, James Barrat, also says that once the machines blow past us, man's reign is through.\n  ''We humans steer the future not because we're the strongest beings on the planet, or the fastest, but because we are the smartest,'' Mr. Barrat has said. ''So when there is something smarter than us on the planet, it will rule over us on the planet.''\n  Horrific scenarios abound. Superintelligent computers will cause global financial systems to collapse. They will wage war on humans with killer robots far more lethal than today's drones. They will control nuclear weaponry -- think Skynet in the ''Terminator'' series -- to dominate humankind or, worse, wipe it out.\n  In these grim predictions, the machines always seem to be anthropomorphic: Their instincts are essentially the same as those of humans at their worst; just as people have run roughshod over lower life forms, artificial intelligence networks will abuse their supremacy. Possibilities for them to do good -- figuring out how to regenerate human cells, for instance, or creating immunities against disease, or gobbling up carbon dioxide in the atmosphere -- tend to get short shrift.\n  For some analysts, any worry about human survival is theoretical and certainly less immediate than more prosaic, yet vital, concerns. Technological advances have enhanced the ability of governments to spy on their citizens. How to shape policy is now reflected in the struggle between Apple and the Obama administration over access to the iPhone of one of the terrorists in the mass shooting in San Bernardino, Calif., in December.\n  Economic issues are unavoidable as well. Lawrence H. Summers, president emeritus at Harvard University and a former Treasury secretary, noted that unemployment is disproportionately higher among those whose duties ''in various ways have been mechanized.'' There are ''important consequences for the way the economy is organized and for how fair the economy is,'' Mr. Summers said in an interview with Retro Report.\n  There is, too, a question of how smart robots truly are and whether they can develop superintelligence at the blinding speed envisioned by the more pessimistic forecasters. ''Things that are easy for humans are hard for computers,'' Guruduth S. Banavar, the director of cognitive computing research at IBM, told Retro Report, ''and things that are easy for computers are hard for humans.'' Yes, a computer can multiply two numbers of 1,000 digits each in a matter of seconds. But it cannot hold a candle to a toddler when it comes to recognizing faces or performing a task as simple as climbing steps.\n  Perhaps it is human nature to assume the worst with something new. ''Humans often converge around massive technological shifts -- around any change, really -- with a flurry of anxieties,'' Adrienne LaFrance, who covers technology for The Atlantic magazine, wrote a year ago.\n  But it is too soon for hyperventilating, Fei-Fei Li, a professor of computer science at Stanford University, told Retro Report. With A.I., she said, ''we are closer to a washing machine than a Terminator.''\n\n\n\n","317":"ABSTRACT\nKris Maher US News article discusses Libratus, artificial intelligence program created by Carnegie Mellon University, which defeated top poker players during three-week exhibition tournament; photo (M)\n","318":"ABSTRACT\nChristopher Mims Keywords column notes enthusiasm for artificial intelligence but warns that real blossoming of AI could still be years away amid problems including lack of data, cost versus return and narrow pool of talent needed to build systems; photo (M)\n","319":" ABSTRACT:While the field of artificial intelligence has remained largely an academic one, experts in the field are now in demand from computer-game companies; with the proliferation of 3-D graphics, AI experts are needed to make computer-game characters who act like humans; photo (M)\n","320":"Even before the world chess champion Garry Kasparov faced the computer Deep Blue yesterday, pundits were calling the rematch another milestone in the inexorable advance of artificial intelligence, the effort to create machines that mimic human thought. Computers will soon become \"smarter than us,\" the supercomputer designer Danny Hillis asserted in a Newsweek essay.\nActually the contest, whatever its outcome, only underscores what a flop artificial intelligence has been, especially relative to its founders' goals. The naivete of Marvin Minsky of the Massachusetts Institute of Technology and other pioneers in the field is legendary. In 1966 Professor Minsky gave an undergraduate, Gerald Sussman, the task of building an object-recognition device out of a computer and a television. Object recognition is what you do when you're having lunch with a prospective employer and you suddenly realize that the person at the next table with his back to you is your current employer. Bald spot, blazer, jowls. Yup, it's him.\u00a0\n Mr. Sussman did not fulfill his assignment (although he eventually became a prominent researcher). Since then, computers have become unimaginably fast. But their ability to recognize a face or conduct a conversation, activities that humans perform almost, well, thoughtlessly, remains primitive. Computers can do very well in situations with simple, clear-cut data, rules and goals. They do very poorly in situations with complicated or ambiguous data, rules and goals -- that is, in real life.\nChess is tailor-made for computers, and Deep Blue is prodigiously powerful, capable of examining hundreds of millions of positions a second. If this silicon monster must strain so mightily to beat a mere human, what hope is there that computers will ever, say, replace diplomats in negotiating weapons treaties?\nSure, some programs can \"hear\" a limited number of words. Banks use neural networks to weigh the merits of loan applications. The immigration service plans to test face-recognition software at borders. But these achievements are paltry compared with the dreams of artificial intelligence enthusiasts.\nAs recently as 1993, Hans Moravec, a roboticist at Carnegie Mellon University, assured me that by the middle of the next century robots would be intelligent enough to usurp the roles of doctors and C.E.O.'s. Professor Minsky fantasizes about converting human personalities into strings of ones and zeros and \"downloading\" them onto machines.\nMore sober sorts roll their eyes. \"Anyone who expects any human-like intelligence from a computer in the next 50 years is doomed to disappointment,\" Philip Anderson, the physicist and Nobel laureate, asserted in the journal Science two years ago. For the foreseeable future -- and perhaps forever -- HAL, the murderous machine in \"2001: A Space Odyssey,\" and Data, the charming cyborg from \"Star Trek: The Next Generation,\" will remain creatures of science fiction.\nOne day, if not this month, a computer will surely be world chess champion, proving that the game, like loan analysis, is reducible to number-crunching. But the most essential aspects of human thought will continue to elude scientists. As the linguist Noam Chomsky has said, we will probably always learn more about ourselves from novels than we will from science. The question is, should that shortcoming be cause for consternation or celebration?\n","322":"YORKTOWN HEIGHTS, N.Y. -- In the end, the humans on ''Jeopardy!'' surrendered meekly.\nFacing certain defeat at the hands of a room-size I.B.M. computer on Wednesday evening, Ken Jennings, famous for winning 74 games in a row on the TV quiz show, acknowledged the obvious. ''I, for one, welcome our new computer overlords,'' he wrote on his video screen, borrowing a line from a ''Simpsons'' episode.\nFrom now on, if the answer is ''the computer champion on ''Jeopardy!,'' the question will be, ''What is Watson?''\nFor I.B.M., the showdown was not merely a well-publicized stunt and a $1 million prize, but proof that the company has taken a big step toward a world in which intelligent machines will understand and respond to humans, and perhaps inevitably, replace some of them.\u00a0\nWatson, specifically, is a ''question answering machine'' of a type that artificial intelligence researchers have struggled with for decades -- a computer akin to the one on ''Star Trek'' that can understand questions posed in natural language and answer them.\nWatson showed itself to be imperfect, but researchers at I.B.M. and other companies are already developing uses for Watson's technologies that could have a significant impact on the way doctors practice and consumers buy products.\n''Cast your mind back 20 years and who would have thought this was possible?'' said Edward Feigenbaum, a Stanford University computer scientist and a pioneer in the field.\nIn its ''Jeopardy!'' project, I.B.M. researchers were tackling a game that requires not only encyclopedic recall, but also the ability to untangle convoluted and often opaque statements, a modicum of luck, and quick, strategic button pressing.\nThe contest, which was taped in January here at the company's T. J. Watson Research Laboratory before an audience of I.B.M. executives and company clients, played out in three televised episodes concluding Wednesday. At the end of the first day, Watson was in a tie with Brad Rutter, another ace human player, at $5,000 each, with Mr. Jennings trailing with $2,000.\nBut on the second day, Watson went on a tear. By night's end, Watson had a commanding lead with a total of $35,734, compared with Mr. Rutter's $10,400 and Mr. Jennings's $4,800.\nVictory was not cemented until late in the third match, when Watson was in Nonfiction. ''Same category for $1,200,'' it said in a manufactured tenor, and lucked into a Daily Double. Mr. Jennings grimaced.\nEven later in the match, however, had Mr. Jennings won another key Daily Double it might have come down to Final Jeopardy, I.B.M. researchers acknowledged.\nThe final tally was $77,147 to Mr. Jennings's $24,000 and Mr. Rutter's $21,600.\nMore than anything, the contest was a vindication for the academic field of artificial intelligence, which began with great promise in the 1960s with the vision of creating a thinking machine and which became the laughingstock of Silicon Valley in the 1980s, when a series of heavily financed start-up companies went bankrupt.\nDespite its intellectual prowess, Watson was by no means omniscient. On Tuesday evening during Final Jeopardy, the category was U.S. Cities and the clue was: ''Its largest airport is named for a World War II hero; its second largest for a World War II battle.''\nWatson drew guffaws from many in the television audience when it responded ''What is Toronto?????''\nThe string of question marks indicated that the system had very low confidence in its response, I.B.M. researchers said, but because it was Final Jeopardy, it was forced to give a response. The machine did not suffer much damage. It had wagered just $947 on its result. (The correct answer is, ''What is Chicago?'')\n''We failed to deeply understand what was going on there,'' said David Ferrucci, an I.B.M. researcher who led the development of Watson. ''The reality is that there's lots of data where the title is U.S. cities and the answers are countries, European cities, people, mayors. Even though it says U.S. cities, we had very little confidence that that's the distinguishing feature.''\nThe researchers also acknowledged that the machine had benefited from the ''buzzer factor.''\nBoth Mr. Jennings and Mr. Rutter are accomplished at anticipating the light that signals it is possible to ''buzz in,'' and can sometimes get in with virtually zero lag time. The danger is to buzz too early, in which case the contestant is penalized and ''locked out'' for roughly a quarter of a second.\nWatson, on the other hand, does not anticipate the light, but has a weighted scheme that allows it, when it is highly confident, to hit the buzzer in as little as 10 milliseconds, making it very hard for humans to beat. When it was less confident, it took longer to buzz in. In the second round, Watson beat the others to the buzzer in 24 out of 30 Double Jeopardy questions.\n''It sort of wants to get beaten when it doesn't have high confidence,'' Dr. Ferrucci said. ''It doesn't want to look stupid.''\nBoth human players said that Watson's button pushing skill was not necessarily an unfair advantage. ''I beat Watson a couple of times,'' Mr. Rutter said.\nWhen Watson did buzz in, it made the most of it. Showing the ability to parse language, it responded to, ''A recent best seller by Muriel Barbery is called 'This of the Hedgehog,' '' with ''What is Elegance?''\nIt showed its facility with medical diagnosis. With the answer: ''You just need a nap. You don't have this sleep disorder that can make sufferers nod off while standing up,'' Watson replied, ''What is narcolepsy?''\nThe coup de grace came with the answer, ''William Wilkenson's 'An Account of the Principalities of Wallachia and Moldavia' inspired this author's most famous novel.'' Mr. Jennings wrote, correctly, Bram Stoker, but realized that he could not catch up with Watson's winnings and wrote out his surrender.\nBoth players took the contest and its outcome philosophically.\n''I had a great time and I would do it again in a heartbeat,'' said Mr. Jennings. ''It's not about the results; this is about being part of the future.''\nFor I.B.M., the future will happen very quickly, company executives said. On Thursday it plans to announce that it will collaborate with Columbia University and the University of Maryland to create a physician's assistant service that will allow doctors to query a cybernetic assistant. The company also plans to work with Nuance Communications Inc. to add voice recognition to the physician's assistant, possibly making the service available in as little as 18 months.\n''I have been in medical education for 40 years and we're still a very memory-based curriculum,'' said Dr. Herbert Chase, a professor of clinical medicine at Columbia University who is working with I.B.M. on the physician's assistant. ''The power of Watson- like tools will cause us to reconsider what it is we want students to do.''\nI.B.M. executives also said they are in discussions with a major consumer electronics retailer to develop a version of Watson, named after I.B.M.'s founder, Thomas J. Watson, that would be able to interact with consumers on a variety of subjects like buying decisions and technical support.\nDr. Ferrucci sees none of the fears that have been expressed by theorists and science fiction writers about the potential of computers to usurp humans.\n''People ask me if this is HAL,'' he said, referring to the computer in ''2001: A Space Odyssey.'' ''HAL's not the focus; the focus is on the computer on 'Star Trek,' where you have this intelligent information seek dialogue, where you can ask follow-up questions and the computer can look at all the evidence and tries to ask follow-up questions. That's very cool.''\n","323":"ABSTRACT\nAili McConnon article in Journal Report: Artificial Intelligence Issue notes artificial intelligence tools, like chatbots and algorithms, are aiming to assist treatment of depression by interfacing with these tools to provide coping strategies drawn from cognitive behavior therapies; photo (M)\n","324":"ABSTRACT\nDaniella Hernandez article in Journal Report: Artificial Intelligence Issue notes artificial intelligence threatens to destroy many jobs once it takes hold of businesses and economy, but there is another side: new, previously unheard of, jobs that AI will be creating; cartoons; graph (M)\n","325":"ABSTRACT\nJoe Queenan Moving Targets column imagines terrifying scenario of artificial intelligence software running movie studios; drawing (M)\n","326":"ABSTRACT\nJacob Gershman Proceedings column notes article published online by Artificial Intelligence and Law suggests robots are being taught how to tell if someone is giving false testimony (S)\n","328":"An effort to get people better boots may say much about the future of artificial intelligence in the business world.\nThe company doing the work, called Sentient, based in San Francisco, has used a version of advanced A.I. to build a visual search service for Shoes.com, an online footwear company based in Vancouver, Canada. The service, available on its Canadian site, is to go live on Tuesday.\nWhile it is - at least for the moment - limited to retail, over the long haul the technology could demonstrate how important it is for companies to be sitting on vast warehouses of information.\u00a0\nCustomers browse pictures of shoes, choosing a favorite type among a dozen images, which leads to a dozen more images, searching for the look someone is after. An initial page of boots might lead to a page with more low-cut items over high, or laces over buckles. The next options could affirm those choices. Or the search could go off in another direction.\nFrom a sales point of view, it could be considered the next step in the A.I. of personalization. Typically, personalization relies on historic associations, or the familiar \"customers also bought\" suggestion. In this case, the computer is looking at 100 or more factors, and trying to judge how someone feels about them in real time.\n\"What makes this attractive is that people can get to the shoe they love without knowing what brand it is,\" said Roger Hardy, chairman and co-founder of Shoes.com. \"If I told you there was an Italian company with the perfect heel, toe and lacing for you, but didn't know the brand, it wouldn't do you any good.\"\nIn early customer tests, Mr. Hardy said, the A.I. increased sales, though he declined to say by how much.\nThis kind of search means a catalog must offer shoppers many choices if it is to work. That means that for the system to work, Mr. Hardy and others will have to keep a huge inventory of images and changing styles on hand, and be able to ship from all over the world, since producers are everywhere.\n\"You'll see brands become hot a lot faster,\" he said.\nSentient's A.I. is based on something called \"dynamic ontology,\" which means that insights and actions are determined based on how the situation appears in the moment. In shopping, that is someone's taste as they browse. For financial trading, where Sentient is experimenting with several million dollars in United States equities, it might mean using recent price movements among a constellation of stocks to predict where things will go next.\n\"You can imagine this as the ability for a system to understand complex interrelated data at any moment,\" said Antoine Blondeau, Sentient's co-founder and chief executive. \"It's about figuring out what matters right now.\"\nOf course, \"what matters\" has a lot to do with what can be captured inside a computer. Five years from now, Mr. Blondeau said, the trading product \"will look at social feeds, government filings, all the TV and radio in the world.\"\nSentient has also worked with Saint Michael's Hospital at the University of Toronto on tracking patient care. In early results, the system appears able to look at a patient's vital signs and predict 30 minutes ahead of time the onset of sepsis, or the body's overwhelming response to an infection, which can result in death.\n\"Patterns involving specific patients are still hard to establish,\" said Muhammad Mamdani, director of applied health research at St. Michael's.\nWhile the results looked good, Mr. Mamdani warned that deploying A.I. in medicine is still a long way off. \"You have to see it used outside of a test environment. We're a slow, conservative environment - knowledge transfer is a problem in health care.\"\nSince its founding in 2008, Sentient has received $143 million from investors including Tata Communications, and the Hong Kong billionaire Li Ka-shing. It relies on its own computers, but even more on machines temporarily unused, like computers at a video gaming parlor in South Korea when it shuts for the night. In all, it uses 2 million computing cores, amassed from 4,000 locations.\n","329":"ABSTRACT\nIntel announces plans to pay undisclosed amount for artificial intelligence semiconductor, software and services startup\u00a0Nervana Systems; photo (M)\n","330":"ABSTRACT\nChristopher Mims Keywords column on success of Persado in using\u00a0artificial intelligence software\u00a0to write ad emaILS and 'landing pages' for US wireless carriers and X.ai to schedule meetings; chart (M)\n","331":"Artificial intelligence. Chatbots. Messaging. Sound familiar?\nThese were some of the themes that Google brought up at its annual developer conference on Wednesday. At the event, the Silicon Valley company introduced an Internet-connected speaker called Google Home that is powered by A.I. and a new messaging app called Allo, among other things. \u00a0\n  These are also some of the very same topics that have come up at developer conferences held by Microsoft and Facebook this year. In March, Microsoft spent time talking about A.I. and bots, which are the pieces of software that can be used to produce new methods of interaction with computers, like chat interfaces. A month later, Facebook said it was opening up its Messenger messaging app so developers could create chatbots for the service.\n  If there's a certain sameness to it all, it illustrates how tech behemoths are all moving into an age of A.I. Microsoft, Google, Amazon and others are all racing to become the go-to company for A.I., betting that whoever wins will have the advantage in a technology that is permeating more and more software programs and hardware products.\n\n\n\n","332":"The Terminator wants to be your next travel agent.\nNew artificial intelligence (AI) technologies promise to make travel a little smarter. The latest entrant is Aeromexico's new AI-based customer-service bot, billed as a \"smart brain\" capable of machine learning. It launched earlier this year in Spanish on Facebook, and an English version is being rolled out now.\nBut do they really live up to the billing? It depends. There's little doubt that AI is improving the bottom line for airlines, hotels and car-rental companies, which are aggressively integrating this technology into their operations. But for consumers, there are only a few AI-enabled apps and sites that offer a meaningful improvement, if any.\u00a0\nNearly 85 percent of travel and hospitality professionals are using AI within their businesses, according to a recent survey by Tata Consultancy Services, which is based in India. So far, the use is largely limited to their information-technology departments, with 46 percent of companies saying they use it for functions such as processing bookings and credit-card transactions. But within four years, 60 percent of companies surveyed said that AI would expand to their marketing efforts - persuading you to book their products.\nIndeed, most of the AI firepower is reserved for the back-end systems designed to squeeze more profit out of an airline seat or hotel room, or to improve the efficiency of airport operations. For example, flight disruptions cost airlines billions each year, so airports are deploying AI systems to quickly deal with irregular operations. A company called SITA is working with airports to create an algorithm to forecast airline delays.\n\"This is a huge cost for the industry,\" says Jim Peters, SITA's chief technology officer. \"There is a strong desire to remove as much uncertainty as possible.\"\nFor customer-facing AI systems for travelers, there are several standouts. One of the most prominent examples of AI is Hopper , which uses a variety of artificial intelligence to power its site and booking engine. That includes machine learning to analyze pricing data and suggest the best times to book a trip to a destination, a system that alerts you when ticket prices drop, and a \"conversational chatbot\" that understands written queries and generates relevant results. Another site, Hipmunk , also has a well-known conversational chatbot capable of understanding queries and offering relevant search results.\n\"The idea here is to leverage AI strategically at the right moment in the customer journey,\" says \u00c9tienne M\u00e9rineau, the co-founder and head of conversation design at Heyday.ai, a chatbot developer based in Montreal.\nAt Kayak , when you access its price forecast tool, you're using an intelligent system that's more than a simple search. Not only does it offer a more accurate price prediction, says Giorgos Zacharia, the chief technology officer for Kayak, \"artificial intelligence also allows us to combine flights from different carriers for more savings for our users.\"\nAnd while the sites that offer it are popular, the technology can be a little glitchy. Take the Aeromexico AI, called Aerobot. Like the Terminator's mythical Skynet, it goes far beyond offering scripted answers, learning as it goes by scanning and analyzing previous customer service transcripts.\nThe system, currently only available in Spanish, is still primitive. I accessed the AI through its Facebook page and asked it for help with a reservation. The response? \"Let me transfer you to a human agent.\" Its developers said Aerobot can answer simple questions, such as \"What is your pet fee?\" and \"I have to change a flight,\" but is still learning the rest. Who said customer service would be easy?\nCertainly not Nina McGouldrick, a medical writer from Richardson, Tex. She recently used Hopper to book a flight on American Airlines, with frustrating results. When she called the airline to check on the status of her flight, American claimed she had canceled her ticket and that its records indicated that someone using her number had called. \n \"All we could see on our side is that it was canceled by the airline at the flier's request,\" says Brianna Schneider, a Hopper spokeswoman. \"It pains us to hear, though, that this traveler didn't intend to cancel her trip and we will reach out to her to get more details.\"\nBut McGouldrick may be in the minority. Artificial\u00a0intelligence is increasingly palatable to a majority of travelers. A new PricewaterhouseCoopers survey of consumer and business attitudes toward the technology suggests that in the next five years, 56 percent of respondents would be willing to embrace an artificial travel agent.\nTo which human agents say: Nonsense.\n\"Would you trust the Terminator to tell you where to see the best sunset on the Amalfi Coast?\" asks Erika Richter, a spokeswoman for the American Society of Travel Agents   . \"I don't think so.\"\nFor now, the dream of an AI making travel better seems closer to becoming a reality for a company's back-end systems, where intelligent applications can improve efficiency and cut costs. But when it comes to the systems travelers use, there's a long road ahead - at least before you can call a machine to book your next vacation. \nElliott is a consumer advocate, journalist and co-founder of the advocacy group Travelers United. Email him at chris@elliott.org\n","333":"IBM's Watson is moving West and widening its ambitions.\nThe company plans to open a second headquarters for Watson, its artificial-intelligence system, in San Francisco in 2016 and eventually employ several hundred people. IBM's Watson group, set up as a separate business in January 2014, has its East Coast headquarters in downtown Manhattan. \n  But a series of other announcements from the company expected on Thursday may be more significant. They represent the clearest sign so far of IBM's long-term goal: to make Watson the equivalent of a computing operating system for an emerging class of data-fueled artificial-intelligence applications.\u00a0\n  IBM explored paths to commercializing the technology after Watson beat human champions in the quiz show ''Jeopardy!'' in 2011. At first, the company focused on big demonstration projects with big companies and institutions, especially in medicine and health care.\n  But the plan has broadened, especially in the last year, to move beyond custom work for major clients to creating a growing collection of services, so that software developers at start-ups and elsewhere can easily use them in applications.\n  On Thursday, IBM will announce new capabilities in Watson services like speech, language understanding, image recognition and sentiment analysis. These humanlike abilities such as seeing, listening and reasoning are those associated with artificial intelligence in computing. IBM calls its approach to A.I. ''cognitive computing.''\n  All the major technology companies -- Microsoft, Google, Apple, Facebook, Amazon and others -- are pursuing A.I. applications. Some are ahead of IBM in fields like image recognition.\n  Yet what is distinctive about IBM, analysts say, is the breadth of its effort to create Watson tools and services as plug-in offerings for a wide range of developers.\n  ''IBM is building out a broad platform for where they think the future of computing is heading,'' said David Schubmehl, an analyst at IDC.\n  Mr. Schubmehl compared the IBM playbook in A.I. computing with Microsoft's with Windows in personal computing and Google's with Android OS in mobile. ''IBM is trying to do the same thing with Watson,'' he said, ''open up a platform, make it available for others, and democratize the technology.''\n  IBM says it now has 350 company partners using Watson to make products, with about 50 services on the market. Some 70,000 software developers, IBM says, are using Watson software in some way. Many are in large organizations like ANZ Bank, Johnson & Johnson, the Department of Veterans Affairs, the Mayo Clinic and the University of Texas MD Anderson Cancer Center.\n  But the event in San Francisco will highlight start-ups using Watson technology. One of them is VineSleuth, whose Wine4.me app provides wine recommendations for consumers based on sensory science and predictive algorithms. The start-up, which is based in Houston, plans to use Watson's language classifier and translation services in kiosks it will put in grocery stores early next year.\n  A customer, explained Amy Gross, president of VineSleuth, will be able to walk up to a kiosk, tap the screen and say, ''I want a wine for under $10 that goes well with salmon.'' And in reply, the person is shown a list of suggested wines that meet the criteria. If a person registers, the recommendations can become personalized, based on tastes and purchase history.\n  UnitesUs, an online service for matching job seekers with employers, is using Watson's ''personality insights'' service. It amounts to automated, online personality assessments. The person looking for a job fills out a brief online application, which asks for permission to mine their public Twitter messages, Facebook posts or other social social-media writing.\n  The Watson personality-assessment service then generates a report. The service has only been operating for a few months, but it seems to appeal to companies. About 7.5 percent of the thousands of job seekers on UnitesUs have been called for interviews by employers, according to Khashayar Youssefi, the company's chief executive. By contrast, the interview-call rate for major online job services like Monster and Careerbuilder is typically 1 to 4 percent, said Bardia Nikpourian, chief technology officer of UnitesUs, which is based in Irvine, Calif.\n  Amy Case, head of Case Strategy, a consulting firm, is using Watson to read, sort and assess vast amounts of financial, market and competitive data -- from public government information, private sources and online social networks -- to suggest growth opportunities for corporate clients.\n  ''It's your judgment in the end,'' Ms. Case said. ''But Watson provides some serious muscle'' in winnowing, identifying and ranking possible strategic choices.\n  The new office in San Francisco, called Watson West, seems to be about both tapping talent and changing minds. IBM already has a presence on Howard Street, where one of its teams works on web and mobile apps for clients, and a large footprint in Silicon Valley, including its Almaden Research Center in San Jose.\n  Yet Watson West, said Robert High, chief technology officer for the Watson business, is intended to ''cater to the heart of the entrepreneurial community here in San Francisco and in Silicon Valley.''\n  ''Presence influences points of view,'' Mr. High said, suggesting that Watson can burnish IBM's image as an innovator. ''If you're not there, you're out of sight and out of mind.''\n\n\n\n","335":"BrainmakersHow Scientists Are Moving Beyond Computers to Create a Rival to the Human BrainBy David Freedman214 pages. Simon & Schuster. $22.\u00a0\n Last month when Dante II, the 1,700-pound robot built by engineers at Carnegie Mellon University, misstepped, tumbled and couldn't right itself while returning from exploring the crater of an Alaskan volcano, it provided a fitting metaphor for both the progress and the limits of the three-decade-old science of artificial intelligence.\nIndeed, since its inception in the late 1950's, the field has actually made remarkable progress. There are now machines that see, expert systems that advise us on tough decisions, and it is possible to pick up a telephone and chat with a computer that is standing in for a long-distance operator. There are even hand-held gadgets such as Apple Computer's Newton personal digital assistant that do a laborious, and imperfect, job of recognizing handwritten notes.\nBut while progress has been made in mimicking human sensory and reasoning skills, scientists' perception of the size of the job has grown much larger even as computers have gained remarkable new powers of performance.\nIn \"Brainmakers: How Scientists Are Moving Beyond Computers to Create a Rival to the Human Brain,\" a Boston-based science writer, David Freedman, sets out to survey the current state of the art of artificial intelligence.\nHis premise is that the original artificial intelligence researchers, traditionalists as he refers to them, failed because their attempts to codify a logic representing human thought proved bankrupt. Elaborate models written in computer languages such as Lisp and Prolog were incapable of emulating the simplest kinds of human behavior.\nMr. Freedman then charts the outlines of a new movement that has emerged to carry the search for artificial intelligence forward by drawing on the biological underpinnings of human intelligence.\nScientists are now at work in what is called artificial life research, tinkering with electronic circuits that mimic biological neural networks, with software programs known as genetic algorithms that improve themselves through simulated evolution, with chemical brews that attempt to synthesize life itself, and with weird amalgams composed of biological cells grown on semiconductors.\nThe new studies have already yielded a great deal of useful technology: neural circuits enhance vision systems as well as speech and handwriting recognition; genetic algorithms create programs that are more efficient than the finest human-designed programs.\nIt is a seductive idea: a new artificial intelligence revolution can be achieved by copying the simplest biological processes and enhancing those processes with immensely powerful supercomputers and arrays of neural circuits made in the laboratory.\nMr. Freedman takes us from the heartland of the traditionalists at the Massachusetts Institute of Technology to the home of a researcher turned venture capitalist in the hills above Silicon Valley, to the University of Tsukuba in Japan, and in the process shows us a scientific world that still lacks a useful road map.\nIndeed, even while Mr. Freedman draws disparaging portraits of such pioneers in artificial intelligence as Marvin Minsky and Seymour Papert, computer scientists at M.I.T., and John McCarthy of Stanford (who is described as looking and acting like a homeless person), his more affectionate portraits of the younger artificial life wizards contain enough ominous signs to warn the reader that this new generation is no closer to the holy grail of a self-conscious machine.\nAt one point an optimistic neural network researcher is quoted as saying: \"We're now about 25 years away from a silicon brain.\"\nThose words strike a particularly eerie note because 25 years is about the same time span that the traditionalists originally thought would be necessary to create a thinking machine.\nBut the real problem with \"Brainmakers\" is that Mr. Freedman's study lacks both journalistic depth and passion. In fact, at many points the author seems to want to distance himself from the premise of his subtitle: \"How scientists are moving beyond computers to create a rival to the human brain.\" It would appear that revolutionary breakthroughs are not yet on the horizon.\nParticularly devastating to those who are optimistic about the possibility of creating self-conscious machines is the work of Stuart Hameroff, a neurological researcher at the University of Arizona. For two decades he has studied structures inside neurons known as microtubules. These protein cylinders, which lend structure to neurons, are computing systems unto themselves, he argues, adding unimaginable complexity to the network of three billion neurons that make up the human brain.\nBut in a brief conclusion Mr. Freedman writes, \"The nature-based A.I. movement seems exceptionally well positioned to succeed.\" Yet in the following paragraph he admits, \"On the other hand, conventional A.I. also got off to a breathtaking start . . . It wasn't for nearly two decades that the endeavor began to lose momentum.\"\nMr. Freedman has provided a quick travelogue that offers a glimpse of the work of some leading researchers in artificial intelligence. Ultimately, however, the reader is left wondering why he made the journey.\n","336":"ABSTRACT\nGeoffrey A Fowler Personal Technology column explores possibility of June Life's $1,500 artificial-intelligence oven, which is equipped with cameras and Wi-Fi and can cook about 50 recipes so far; diagram; photos (M)\n","337":"ABSTRACT\nUniversity of Washington found ally in Amazon.com in poaching artificial-intelligence expert Carlos Guestrin from Carnegie Mellon, while tech giants Google and Facebook are also pouring money into once-obscure field to manage reams of data; drawing; photos (M)\n","338":"The bionic cockroach has arrived, alive, kicking and hissing in the artificial intelligence lab at the University of Michigan. There, Dan Koditschek, an associate professor, and his colleagues have attached wires to the backs of live cockroaches from Madagascar. The wires send electrical current to their legs and antennae, and Koditschek manipulates them by varying the current. So far, he has only rough control over them, but his \"science fiction dream\" is to master cockroachian derring-do well enough to clone it in teeny robots called Biobots. \"A cockroach can carry up to 80 times its own body weight,\" he says. \"Two or three could carry a small package of screws.\" He has no plans, however, to turn them into carpenters' helpers.\nA report on the project was presented to other scientists late last month at the Institute of Electrical and Electronic Engineers' conference in Minneapolis. But as one scientist who had already heard of the bio bugs observed, \"These things aren't going to become the next slot-racing cars.\" Howard Moraff, a program director at the National Science Foundation, did agree with their potential to help biologists.\u00a0\n On the other hand, speaking of artificial intelligence, why not wire them up and send them undercover? True bugging, you could say.\n","339":"Artificial intelligence will solve Facebook's most vexing problems, chief executive Mark Zuckerberg insists. He just can't say when, or how.\nZuckerberg referred to AI technology more than 30 times during 10 hours of questioning from congressional lawmakers Tuesday and Wednesday, saying that it would one day be smart, sophisticated and eagle-eyed enough to fight against the vast variety of platform-spoiling misbehavior, including fake news, hate speech, discriminatory ads and terrorist propaganda.\nOver the next five to 10 years, he said, artificial intelligence would prove a champion for the world's largest social network in resolving its most pressing crises on a global scale - while also helping the company dodge pesky questions about censorship, fairness and human moderation.\u00a0\n\"Building AI tools is going to be the scalable way to identify and root out most of this harmful content,\" Zuckerberg told the lawmakers. But Facebook's AI technology can't do any of those things well yet, and it's unclear when it will be able to.\nTech experts had a different opinion on why Zuckerberg spent so much time offering tributes to the much-hyped but largely unproven advancement: The shapeless technology could help the company pawn off blame from the humans creating it.\n\"AI is Zuckerberg's MacGuffin,\" said James Grimmelmann, a law professor at Cornell Tech, using the film term for a mostly insignificant plot device that comes out of nowhere to move the story along. \"It won't solve Facebook's problems, but it will solve Zuckerberg's: getting someone else to take responsibility.\"\nTo many, AI is the amorphous super-technology of science fiction. But today's artificial intelligence is being used in far more basic forms: driving cars, tracking cows and giving a voice to virtual assistants such as Siri and Alexa.\nFacebook uses AI in understated but important ways, such as for recognizing people's faces when tagging photos and using algorithms to decide placement of ads or News Feed posts to maximize users' clicks and attention. The company is also expanding its use, such as by scanning posts and suggesting resources when the AI assesses that a user is threatening suicide.\nFacebook has pointed to early AI successes in detecting problematic content. The company has said that advances in AI have helped it remove thousands of fake accounts and \"find suspicious behaviors,\" including during last year's special Senate race in Alabama, when AI helped spot political spammers from Macedonia.\nFacebook, Zuckerberg said Tuesday, has also been \"very successful\" at deploying AI to police against terrorist propaganda. \"Today, as we sit here, 99 percent of the ISIS and al- Qaeda content that we take down on Facebook, our AI systems flag before any human sees it,\" he said. Nonprofits such as the Counter Extremism Project have argued that Facebook has exaggerated its achievement and failed to crack down on well-known Islamist extremists.\nZuckerberg said he was optimistic that Facebook's AI would, within a decade, be able to comprehend the \"linguistic nuances\" of content with enough accuracy to flag potential risks. But no AI on the market is trained well enough to understand the social dimensions and verbal eccentricities of human speech, slang and dialect.\n\"Even humans have a hard time distinguishing between hate speech and a parody of hate speech, and AI is way short of human capabilities,\" Grimmelmann said.\nZuckerberg both understated the problem and overstated AI's abilities, experts said. For example, Facebook's AI has been deemed technically incapable of spotting discriminatory housing ads, which violate the federal Fair Housing Act and were a problem on the social network until the site changed its policy late last year.\nThe worst-kept secret to Silicon Valley's AI push, experts said, has been the technology industry's army of human moderators. Those often low-wage contract workers spend their days screening posts for offensive or disturbing content, indirectly helping train the AI program on what problems and patterns to look for.\nFacebook, Google and other tech giants are ramping up their content-moderation staffing, and Zuckerberg said his company aims to have more than 20,000 people working on security and content review by the end of the year. Today's AI, experts said, is still miles away from a responsible alternative to a human looking at a screen.\nFacebook's plan is \"continuing to grow the people who are doing review in these places with building AI tools,\" Zuckerberg said. \"That, I think, is going to help us get to a better place on eliminating more of this harmful content.\"\nNot every senator Tuesday welcomed Zuckerberg's AI-as-savior talking point. Patrick J. Leahy (D-Vt.) questioned whether Facebook's AI was already failing by not stemming the spread of hate speech during the crackdown on Rohingya Muslims in Burma, also called Myanmar.\n\"It spread very quickly, and then it took attempt after attempt after attempt, and the involvement of civil society groups, to get you to remove it,\" Leahy said, pointing to a poster that showed Facebook posts calling for the murder of Muslim journalists. \"Why couldn't it be removed within 24 hours?\"\nBut tech experts said AI could also create new problems. The same bad actors behind viral hoaxes and fake accounts may be able to use AI to evade Facebook's filters, make fake videos or spearhead targeted harassment campaigns, they said.\nThe real issue, experts said, is that the problems plaguing Facebook may be battles that no one can truly win. As Ryan Calo, an assistant professor at the University of Washington law school, tweeted Tuesday, \" 'AI will fix this' is the new 'the market will fix this.' \"\ndrew.harwell@washpost.com\n","340":"Frank Gehry, the architect, says his $300 million new computer science and artificial intelligence building at M.I.T. ''looks like a party of drunken robots got together to celebrate.''\n Charles M. Vest, the institute's president, sees it as ''a toy box at dawn,'' ready for the kids to play with.\n  Others have likened its jumble of yellow and white aluminum, polished stainless steel and orange brick towers, tubes, cubes and cones to a Disney animation, a Leger painting, fine Bordeaux wine (for its complexity and variety) or a medieval Italian hill town rising amid the gray rectangular sameness of its section of campus in an industrial part of Cambridge.\n The building, the Ray and Maria Stata Center for Computer, Information and Intelligence Sciences, embodies the intellectual daring and innovation -- ''the joy of invention,'' as William J. Mitchell, the architectural adviser to Mr. Vest, described it -- that goes on inside M.I.T.'s featureless laboratories. Just as Mr. Gehry talks about his delight in starting new projects without a road map, so the building, which had its official opening on Friday, is intended as a metaphor for the questions and exploration that drive the scientists who occupy it.\u00a0\n ''Every week I'm in this building, I feel happier than the week before,'' said Victor Zue, co-director of the Computer Science and Artificial Intelligence Laboratory, who moved in at the end of March. He beamed as he stopped to talk amid the bustle of the two-lane interior ''student street,'' the meandering main corridor, with its bright red, blue and yellow walls. What makes him happy, he said, even more than the whimsical design, the ultramodern laboratories and the natural light pouring through the skylights and huge windows, is that he can retreat to his office for privacy but then emerge to commune with his fellow geeks on the student street and in the other abundant communal spaces.\n The lack of an interior grid -- along with the lounges, kitchens, a fitness center, a cafe and a child care center, and whiteboards and blackboards seemingly around every bend -- is part of Mr. Gehry's and the institute's plan to spark creative combustion by encouraging the building's occupants to bump into one another. Literally.\n With all due respect to Harvard, its neo-Georgian brick neighbor to the north, the Stata Center may be one of the smartest buildings on the planet, not just for its computer-assisted design, but also for the minds assembled inside. The line-up includes Tim Berners-Lee, the Oxford-educated physicist credited with inventing the World Wide Web; Rodney A. Brooks, the robotics pioneer, director of the Computer Science and Artificial Intelligence Laboratory; Butler W. Lampson, who wrote the first version of Word; Noam Chomsky, the father of modern linguistics; and six MacArthur genius award winners, including Erik Demaine, a 23-year-old computer scientist who won the prize when he was 22.\n Who knows what might happen if they all run into one another as they get lost on their way to the restroom or the coffee machine? (In Mr. Berners-Lee's department, it's a $3,100 espresso machine.)\n The Stata Center is only the biggest complex -- 730,000 square feet, including a below-ground parking garage and outdoor amphitheater -- of the university's $1 billion construction program, which has as one of its primary goals the creation of a greater sense of community on campus. The new construction was undertaken as a response to a 1998 student life report that found, among other conclusions, that as a result of M.I.T.'s lack of ''attractive and convenient space for community interaction,'' students had come to regard ''computer clusters as social space.''\n Touring the building on May 4, Mr. Gehry recalled his early meetings with the institute's top administrators. ''They said, 'This is a place where some brilliant, brilliant people hang out,' '' he said. '' 'Some are shy, some are outgoing. Their tendency is to hide in their offices while yearning for some kind of interaction.' ''\n Still, even as Mr. Gehry, designer of the Guggenheim Museum in Bilbao, Spain, and the new Disney hall in Los Angeles, was inspecting his latest creation, there was grumbling that maybe a little less interaction was in order. \n ''It's a cool building, but sometimes it's very annoying,'' said Kunal Agrawal, 24, a graduate student in supercomputing technologies, who was trying to concentrate at her computer in a large, open space while a couple of her fellow students were trying to get her attention by calling her name from the atrium above. ''It's too noisy.'' She said she hoped she would be one of the lucky people given an office in the building next year. \n Ms. Agrawal said that her adviser had given her headphones to block out the noise, and that she had covered the glass walls in front of her desk with sheets of paper. This is the sort of thing Mr. Gehry said he was hoping would happen: that the occupants would adapt the building to their needs. He has even supplied movable plywood partitions.\n All along, the center, named for Ray Stata, an M.I.T. alumnus and co-founder of Analog Devices, and his wife, Maria, was a collaboration between the architect and its occupants. ''I put up my hand and said, 'I like opening windows,' '' said Mr. Berners-Lee, who was talking in his office with the windows open, courtesy of Mr. Gehry.\n There are already signs that the design is inspiring a certain playfulness: an inflatable penguin on a pillar, a beach ball thrown up on a column, reports of the philosophers' playing Koosh ball in the two-story atriums. Oversize posters of orangutans were rolling out of a computer printer two days before the official opening. The posters were an inside joke, referring to one of Mr. Gehry's early design inspirations: the habits of orangutans, who retire to the treetops for privacy but move down to the ground for socializing.\n As Mr. Gehry was aware before he even sketched his first experimental drawings on M.I.T. napkins five years ago (the napkins are preserved for history in a plastic bag in the office of Chris Terman, a computer scientist who was the building's liaison to Mr. Gehry), the Stata Center occupies historic, even mystical ground. It sits on the site of the former Building 20, a boxy wooden structure that was thrown up in 1943 and became known as the Magical Incubator for the breakthroughs that took place inside, including the invention of radar and Mr. Chomsky's pioneering work in linguistics. \n ''This building is on the precise site of one of the major flourishings of innovation in the 20th century,'' said Steven Shapin, a professor of the history of science at Harvard. ''Building 20 was just a shack, completely lacking in design, ugly, boring. Its magical power was that it brought out the best from those inside it. And now we have the direct successor that is the opposite of a nothing building, designed not only for delight, but for encouraging innovation. If it has even a fraction of the effect of Building 20, it will be a roaring success.''\n The beauty of Building 20, Mr. Shapin, Mr. Gehry and legions of others said, was that it was always intended to be temporary -- although it lasted 55 years -- and so its occupants felt free to knock down walls and doors if they wanted to run wires or just come into closer contact with one another. Mr. Gehry said he designed the center in that spirit. \n The center, said Jerold S. Kayden, a professor of urban planning and design at Harvard, is M.I.T.'s way of ''turning itself inside out,'' and for the first time reflecting through its architecture the bold experimentation of the researchers and their work. ''This is M.I.T. saying, 'No more plain paper wrapping for science,' '' Mr. Kayden said. '' 'It's party time.' '' \n","342":"ABSTRACT\nLi Yuan China Circuit column on increasing use by Chinese app startups Yongqianbao and Dumiao of artificial intelligence to analyze rising amounts of data to determine creditworthiness of smartphone users seeking loans; photo (M)\n","343":"ABSTRACT\nAlexandra Samuel article in Journal Report: Artificial Intelligence Issue discusses what was learned from author's building of own chatbot using artificial intelligence and how people started interacting with bot, which led to whole set of other dilemmas; photo (M)\n","344":"ABSTRACT\nKeiko Morris Property Watch column notes IPsoft Inc, technology company that is rapidly expanding its artificial-intelligence research and development, is expanding into additional 41,687 square-feet of space at its 17 State Street downtown Manhattan headquarters (S)\n","345":"ABSTRACT\nStephen Miller Remembrances column notes death of John McCarthy, computer pioneer who coined term 'artificial intelligence', at age 84; photo (M)\n","346":"To the Editor:\nRichard Powers (''What Is Artificial Intelligence?,'' Op-Ed, Feb. 6) describes I.B.M.'s remarkable achievement in Watson, an artificial intelligence system that is challenging the best human ''Jeopardy!'' champions.\u00a0\nHe concludes by presenting Watson as another of an endless series of prosthetics, spurring human minds on to their next great challenge.\nBut the key point is that this cycle will change fundamentally when we invent a prosthetic with a better mind than ours. Then it will be the prosthetic rather than the human mind that faces the next challenge.\nThis article and other recent writings seek to reassure the public that A.I. is not such a threat. But the threat is real, and we'd better have sane public policy to face it before it arrives.\nBill Hibbard Stoughton, Wis., Feb. 6, 2011\nThe writer is emeritus senior scientist at the University of Wisconsin, Madison.\nTo the Editor:\nOn one side of this ''contest'' on ''Jeopardy!'' is a machine that weighs thousands of pounds, uses thousands of watts of external power, is the size of 10 refrigerators, is provided heating and cooling, cannot move itself to the competition site and is tended by a platoon of technologists.\nOpposing is a person who is about 200 pounds, uses about 100 watts of power, is the size of three beer coolers, provides its own heating and cooling, can transport itself to the competition site and needs no tending.\nThis show is many things, but a contest, I think not. It's akin to saying how impressive it is that an Indy 500 race car can outrun a cheetah.\nI'll be impressed when technologists can make a simple housefly. I'd even be impressed if they could make a machine, of the same size, that could do no more than outmaneuver a housefly.\nLlewellyn E. Wall Concord, Mass., Feb. 6, 2011\n","348":"ABSTRACT\nLi Yuan China Circuit column reports Chinese venture capitalists and giants like Baidu are pouring money into artificial intelligence, building on strengths like China's enormous Internet-users base to overtake Western rivals (M)\n","349":"ABSTRACT\nAlphabet's Google names head of its artificial-intelligence efforts John Giannandrea to run its search engine as Amit Singhal leaves;\u00a0\u00a0artificial intelligence is increasingly seen as central to Google's products and services; photo (M)\n","350":"ARTIFICIAL\u00a0intelligence is a longstanding science fiction staple that is coming into its own. Google, Facebook, Apple and others are all developing A.I. tools. You can try out some apps today that demonstrate fledgling forms of the technology by smartly, swiftly and automatically doing tasks that would otherwise take lots of effort.\nThe Roll, for instance, is a new intelligent app that can help organize the thousands of photos that you take with your phone. The app scans a photo library, analyzing each image and trying to spot which ones are similar. It then groups pictures together so copies can be deleted. \u00a0\n  The app also assigns a score to each image as a measure of its quality. The artificial\u00a0intelligence that does this is quite clever: A professionally taken portrait of me scored nearly 90 percent in the app, but a snapshot I had just taken of my cat, paying little heed to lighting and composition, got 20 percent. The software is not flawless -- a professional fashion image that I shot in a studio scored 50 percent -- but it does give a sense of which photos are better or worse, which is handy for knowing what to share online.\n  Best of all, The Roll recognizes what your images are about. The app automatically tags pictures with options like fashion, beauty and animals. The result is a neatly compiled photo archive that can be browsed by tapping on a tagged collection of images in the app or by searching for a keyword. The Roll is free on iOS.\n  EasilyDo offers similar A.I.-based organization and tries to do some of the jobs a real-life personal assistant would do. It works by connecting directly to email accounts, such as Gmail and Exchange, and to other services and apps like Facebook, Evernote and LinkedIn.\n  EasilyDo collects information from those sources so it can send an alert when you need to set off for a meeting scheduled in your calendar. It can also get status updates on flights or track packages, create calendar events based on business emails and tidy up your contacts list.\n  Many features take some time to learn, but it is well designed and has made my online life a little less stressful.\n  EasilyDo is available on iOS and Android. Though most of its features are free, access to the full range (like one feature that connects to Salesforce to log professional sales information) starts at around $5 a month.\n  EasilyDo Inc. also has a new free email app that applies to your inboxes some of the same smart automation ideas used in its digital assistant. It offers a unified inbox for different accounts, one-tap unsubscribing from mailing lists and automatic sorting of mail into different folder types, like travel or other categories. The app is available only for iOS devices.\n  24Me is a personal digital assistant that, much like EasilyDo, connects to different online accounts and manages your affairs. It is styled more like a standard to-do list app or calendar app, which may suit those who like to organize their days as a timeline.\n  The app is not quite as automated as EasilyDo, so you may have to work with it a little more throughout the day. Also, the interface is busier and fussier. It is free on iOS and Android.\n  Don't forget Google Now, free and available on Android and via the Google app on iOS.\n  The app mainly sits in the background, but often surprises you with an alert that is really useful -- for example, a warning of traffic on the way to your next meeting. Google Now can also help find nearby attractions and manage restaurant reservations, and you can speak to it in natural language to do searches.\n  You do have to deliberately interact with Google Now to make the most of it; on iOS this involves the extra step of starting up the Google app.\n  One of the best-known intelligent apps is Apple's Siri, which I have found is becoming cleverer all the time. I use Siri daily to begin phone calls, send text messages and set timers, reminders and alarms. The app can now answer sophisticated questions about science, or help with navigation. Now that Siri is instantly accessible on the latest iPhones by speaking ''Hey Siri,'' it is even more useful.\n  Quick Call\n  UsTwo's Face Maker app can help personalize the look of an Android Wear smartwatch. Other face design apps can be used to customize the information the watch screen displays, but Face Maker is all about giving the watch a personal look, with thousands of options for color and markers. The app is free.\n\n\n\n","351":"ABSTRACT\nAlison Gopnik Mind & Matter column discusses research at Massachusetts Institute of Technology about improving artificial intelligence by programming in curiosity; drawing (M)\n","352":"LONDON - As it aims to build driverless cars and the next generation of robots, Google said on Monday that it had acquired the British artificial intelligence developer DeepMind. \nA Google spokesman confirmed the deal but declined to provide the transaction's cost or more information on how Google planned to use DeepMind's projects, which include applications for simulations, e-commerce and games.\u00a0\nThe company, which is based in London, was founded by Demis Hassabis, a computer game designer, neuroscientist and former child chess prodigy, and his partners, Shane Legg and Mustafa Suleyman.\nOn its website, DeepMind describes itself as combining \"the best techniques from machine learning and systems neuroscience to build powerful general-purpose learning algorithms.\"\n\"This partnership will allow us to turbo-charge our mission to harness the power of machine learning tools to tackle some of society's toughest problems and help make our everyday lives more productive and enjoyable,\" Mr. Hassabis said in a statement. \"We've built a world-leading team here in the U.K. and we're looking forward to accelerating the impact of our technology with Google.\"\nReCode, the digital news website venture started by Walter S. Mossberg and Kara Swisher, the founders of All Things Digital, first reported the deal on Sunday, saying Google had agreed to pay $400 million for the firm.\nThe deal is the latest in a series of acquisitions for Google, which has moved beyond its roots as an Internet search and email provider to a developer of smartphones and cars that drive themselves.\nLast year, the company bought seven companies related to the development of advanced robotics, including Boston Dynamics, the maker of Big Dog, a robot that can travel across rough terrain, and Makani Power, which makes airborne wind turbines.\nGoogle announced this month that it was purchasing Nest Labs, which makes thermostats and smoke alarms that are connected to the Internet and learn a home owner's preferences, for $3.2 billion.\n\n","353":"The Hewlett-Packard Company and Hitachi Ltd. said today that they had agreed to place Hitachi's artificial intelligence technology on Hewlett's computer work stations.\nThe pact is part of several projects between Hewlett and Hitachi aimed at promoting the acceptance of Hewlett's RISC, or reduced instruction set computing, designs.\u00a0\n Although artificial intelligence, or expert systems technology, has not attained the broad success envisioned for it 10 years ago, it is finding growing acceptance as computer power becomes less expensive. RISC, a technology that speeds computing by diverting more tasks from the hardware to the software, has allowed expert systems to run with more efficiency and on less-expensive computers.\nThe companies have worked together since July 1989, when they announced development of a chip set based on Hewlett's RISC architecture and Hitachi's semiconductor and circuit-design technology. Last June, Hitachi said it would develop, manufacture and market RISC chips based on Hewlett's architecture.\n","354":"ABSTRACT\nTesla Inc leader Elon Musk, addressing gathering of US governors, says they need to be concerned about potential dangers of artificial intelligence and called for creation of regulatory body to guide development of powerful technology (M)\n","355":"ABSTRACT\nWide-eyed cartoon fox avatar named Master defeats world's reigning Go champion Ke Jie of China in updated version of AlphaGo, artificial-intelligence program designed by\u00a0Google's DeepMind unit; photo (M)\n","356":"SAN FRANCISCO -- The resounding win by a Google artificial intelligence program over a champion in the complex board game Go this month was a statement -- not so much to professional game players as to Google's competitors.\nMany of the tech industry's biggest companies, like Amazon, Google, IBM and Microsoft, are jockeying to become the go-to company for A.I. In the industry's lingo, the companies are engaged in a ''platform war.'' \n  A platform, in technology, is essentially a piece of software that other companies build on and that consumers cannot do without. Become the platform and huge profits will follow. Microsoft dominated personal computers because its Windows software became the center of the consumer software world. Google has come to dominate the Internet through its ubiquitous search bar.\n  If true believers in A.I. are correct that this long-promised technology is ready for the mainstream, the company that controls A.I. could steer the tech industry for years to come.\u00a0\n  ''Whoever wins this race will dominate the next stage of the information age,'' said Pedro Domingos, a machine learning specialist and the author of ''The Master Algorithm,'' a 2015 book that contends that A.I. and big-data technology will remake the world.\n  In this fight -- no doubt in its early stages -- the big tech companies are engaged in tit-for-tat publicity stunts, circling the same start-ups that could provide the technology pieces they are missing and, perhaps most important, trying to hire the same brains.\n  Fei-Fei Li, a Stanford University professor who is an expert in computer vision, said one of her Ph.D. candidates had an offer for a job paying more than $1 million a year, and that was only one of four from big and small companies. On the candidate's list, one of the biggest technology companies was ranked lowest, in terms of both money and excitement, she noted dryly.\n  At the University of Toronto, IBM pursued a start-up called Ross Intelligence that makes a smart legal assistant, and extended a free offer to use its A.I. software, called Watson. For IBM, the financial payoff would come if start-ups like Ross generated sales, followed by a revenue-sharing arrangement. ''No upfront costs at all,'' said Andrew Arruda, chief executive of the start-up, which moved last year to Silicon Valley.\n  For years, tech companies have used man-versus-machine competitions to show they are making progress on A.I. In 1997, an IBM computer beat the chess champion Garry Kasparov. Five years ago, IBM went even further when its Watson system won a three-day match on the television trivia show ''Jeopardy!'' Today, Watson is the centerpiece of IBM's A.I. efforts.\n  Now, Google's A.I. program is drawing additional attention and pointing to a consolidation among tech's biggest companies.\n  By 2020, the market for machine learning applications will reach $40 billion, IDC, a market research firm, estimates. And 60 percent of those applications, the firm predicts, will run on the platform software of four companies -- Amazon, Google, IBM and Microsoft.\n  In January, before the Google software's latest Go victory, the scientific journal Nature published an article describing how the program had beaten a European Go champion in five consecutive matches, overshadowing an effort by another tech giant, Facebook, to promote its own powerful Go-playing A.I. software. Google's software went on to beat the Go grandmaster Lee Se-dol 4-1 in South Korea this month.\n  IBM is making the broadest entry into A.I. Its Watson unit, set up as a separate division in early 2014, is both a software and a services business, with technology tailored to specific industries. More than 80,000 developers have downloaded and tried out the software, and the Watson division has 500 industry partners, including big companies and start-ups.\n  ''It's early days, but the long-term goal is to have hundreds of millions of people use Watson as self-service A.I.,'' said David Kenny, general manager of the Watson division.\n  In 2015, Amazon and Microsoft both added machine learning capabilities to their cloud software platforms, Amazon Web Services and Microsoft Azure. The companies are using machine learning software to help customers spot patterns and make predictions in vast amounts of data.\n  Microsoft offers 18 machine learning services, including face recognition, text analysis and product recommendations. More A.I. capabilities, analysts say, should be announced at the end of the month, when Microsoft hosts a large conference for software developers in San Francisco.\n  Google is opening its A.I. technology to outsiders, seeking to attract developers. Last November, Google made the core of the machine learning technology its engineers use, called TensorFlow, available as free-to-use open-source software.\n  This week at a conference in San Francisco, Google showed off a new speech-to-text transcription service. The company also said its recently introduced vision software for identifying images would be broadly available soon, and it introduced new tools and training aids to help developers build machine-learning applications more easily.\n  Intelligent software applications will become commonplace, said Jeff Dean, a computer scientist who oversees Google's A.I. development. ''And machine learning will touch every industry.''\n  At Facebook, the A.I. vision is, at least for now, limited to its products.\n  Mike Schroepfer, the chief technology officer, noted that Facebook's image-recognition software was now used to select what pictures or videos to show in a user's news feed, based on a person's friend network and interests.\n  ''Before, the photo was a black box to us,'' he said. ''And that's the most likely form this takes -- a lot of things that add up to make the service steadily better on Facebook.''\n  If products come first, a platform strategy typically takes shape later for the big consumer web companies. There are millions of Facebook app developers worldwide. Today, only about 1 percent of all software apps have A.I. features, IDC estimates. By 2018, IDC predicts, at least 50 percent of developers will include A.I. features in what they create.\n  ''It's where the market is headed,'' said David Schubmehl, an IDC analyst.\n  The question remains, how quickly? To some, the rush to build platforms is taking place long before the technology has matured. What is more, they are still focusing on niches, said David B. Yoffie, a Harvard Business School professor. ''None of them have the opportunity to be as ubiquitous as an operating system became in the PC era, '' Mr. Yoffie said.\n  Some start-ups, like Diffbot in Palo Alto, Calif., are willing to jump into the fray with industry giants under the assumption that there is still plenty to figure out.\n  The company, which was founded by Mike Tung, a Stanford computer science graduate student, in 2008, recently raised $10 million to compete directly with Google. Even though Diffbot is still being run out of a home near the Stanford campus, Mr. Tung is thinking big.\n  ''Our goal is to capture all human knowledge,'' he said. ''I would like for Diffbot to build an iconic company around data. There are companies focusing on computing, but there is no Amazon of data.''\n\n\n\n","357":"As chief scientist of the Internet portal Yahoo, Dr. Udi Manber had a profound problem: how to differentiate human intelligence from that of a machine.\n     His concern was more than academic. Rogue computer programs masquerading as teenagers were infiltrating Yahoo chat rooms, collecting personal information or posting links to Web sites promoting company products. Spam companies were creating havoc by writing programs that swiftly registered for hundreds of free Yahoo e-mail accounts then used them for bulk mailings. \n \"What we needed,\" said Dr. Manber, \"was a simple way of telling a human user from a computer program.\"\u00a0\nSo, in a September 2000 conference call, Dr. Manber discussed the problem with a group of computer science researchers at Carnegie Mellon University. The result was a long-term project that is just now beginning to bear fruit.\nThe roots of Dr. Manber's philosophical conundrum lay in a paper written 50 years earlier by the mathematician Dr. Alan Turing, who imagined a game in which a human interrogator was connected electronically to a human and a computer in the next room. The interrogator's task was to pose a series of questions that determined which of the other participants was the human. The human helped him, while the computer did its best to thwart him.\nDr. Turing suggested that a machine could be said to think if the human interrogator could not distinguish it from the other human. He went on to predict that by 2000, computers would be able to fool the average interrogator over five minutes of questioning at least 30 percent of the time.\nAlthough the Turing test, as it is now called, spawned a vibrant field of research known as artificial intelligence, his prediction has proved false. Today's computers are capable of feats Dr. Turing never imagined, yet in many simple tasks, a typical 5-year-old can outperform the most powerful computers.\nIndeed, the abilities that require much of what is usually described as intelligence, like medical diagnosis or playing chess, have proved far easier for computers than seemingly simpler abilities: those requiring vision, hearing, language or motor control.\n\"Abilities like vision are the result of billions of years of evolution and difficult for us to understand by introspection, whereas abilities like multiplying two numbers are things we were explicitly taught and can readily express in a computer program,\" said Dr. Jitendra Malik, a professor specializing in computer vision at the University of California at Berkeley.\nDr. Manuel Blum, a professor of computer science at Carnegie Mellon who took part in the Yahoo conference, realized that the failures of artificial intelligence might provide exactly the solution Yahoo needed. Why not devise a new sort of Turing test, he suggested, that would be simple for humans but would baffle sophisticated computer programs.\nDr. Manber liked the idea, so with his Ph.D. student Luis von Ahn and others Dr. Blum devised a collection of cognitive puzzles based on the challenging problems of artificial intelligence. The puzzles have the property that computers can generate and grade the tests even though they cannot pass them. The researchers decided to call their puzzles Captchas, an acronym for Completely Automated Public Turing Test to Tell Computers and Humans Apart (on the Web at www.captcha.net).\nOne puzzle, called Gimpy, consists of a display of seven distorted, overlapping words chosen at random from a dictionary of simple words. Solving the puzzle requires identifying three of the seven words and typing them into the box provided. The Carnegie Mellon group also created a simplified version of Gimpy -- a single distorted word displayed against a complicated background. It is now part of Yahoo's registration process.\nAnother Captcha, called Sounds, consists of a distorted, computer-generated sound clip containing a word or sequence of numbers. To solve the puzzle, a user must listen to the clip and type the word or numbers into the box provided.\nThe idea of using puzzles to prevent automated registrations was not new. Other e-commerce sites, including the AltaVista search engine and eBay's PayPal service, were experiencing problems like Yahoo's and independently came up with Captcha-like puzzles. Through its acquisitions, Hewlett-Packard holds a patent on text-based Captchas.\nStill, researchers credit Dr. Blum for the breadth of his vision. Dr. Blum \"did a great thing by recognizing that this problem is much more than solving a nuisance for Yahoo and AltaVista,\" said Dr. Andrei Broder, who helped develop the AltaVista puzzle and is now at I.B.M.\nAs a cryptographer, Dr. Blum was familiar with the constant efforts of cryptographic researchers to advance the field by cracking codes to discover their weaknesses.\nHe hoped to start a similar dynamic for Captchas, spurring researchers to try to create better Captchas while building computer programs that crack existing ones. \n\"Captchas are useful for companies like Yahoo, but if they're broken it's even more useful for researchers,\" Dr. Blum said. \"It's like there are two lollipops and no matter what you get one of them.\"\nIn October Dr. Blum got his wish. Dr. Malik of Berkeley and Greg Mori, a student, devised a computer program that could crack Gimpy -- both the simple version used by Yahoo and the harder one on Captcha's Web site.\nSince its inception two years ago, the Captcha effort has been building. Several research teams have joined the Captcha effort, trying to make and break Captchas and even using the ideas behind Captchas for new lines of research. \nResearchers at the Palo Alto Research Center modified a program used for scanning text to create a program that could solve certain types of Yahoo-Gimpy puzzles, says Dr. Henry Baird, who was in charge of that effort. The group is also developing a new text-based Captcha called Baffletext that it hopes to license to e-commerce sites.\nInspired by the themes behind Captchas, Dr. Doug Tygar, a professor of computer science at Berkeley, and his student Monica Chew are developing alternatives to passwords that are tailored to human skills. Humans have trouble remembering long, random strings of characters, yet they excel at remembering faces and objects, noted Dr. Tygar.\nDr. Malik said he first became interested in the effort after attending a Captcha conference at the Palo Alto center in January. After he and his former student Dr. Serge Belongie, now at the University of California at San Diego, developed a new object recognition technique modeled to have some of the properties of human vision, Dr. Malik decided that Captchas were ideal for testing their method.\nThe Yahoo-Gimpy cracking program, written by Mr. Mori, takes a version of the easy Gimpy, a distorted word displayed in a cluttered background, and finds some points along the boundary of each letter, using standard techniques of computer vision theory.\nThen, applying the Malik-Belongie method, it makes a radial chart for each point indicating where the other boundary points are in relation to it. The charts of boundary points for that letter are compared with the charts of boundary points for all 26 possible letters. The closest match is usually the correct answer.\nUsing various tricks to make it run faster, the program can crack an easy Gimpy puzzle in a few seconds, and it gets the right answer over 80 percent of the time.\nFor the harder version of Gimpy, the researchers devised a program that examines entire words instead of individual letters, so its performance is in minutes rather than seconds, and it gets the puzzle right only about a third of the time. Still, the program will need on average only three tries to get the right answer.\nDr. Malik and Mr. Mori are exploring ways of improving the performance of their program on Gimpy that will also improve their general technique of recognizing objects in a cluttered background.\n\"We want to keep working on this in a principled way so we can use the same technique on an outdoor scene with buildings, trees and cars,\" Dr. Malik said.\nThe general technique, he said, will have many practical applications, like automated recognition of military targets or detection of trademark infringements on the Internet.\nMeanwhile, Yahoo will have to install a new Captcha that is resistant to Dr. Mori's program. This task will fall to Dr. Manber's successor, since Dr. Manber moved to a new position last month as chief algorithms officer for Amazon.com. There, he said, he plans to continue his collaborations with academic researchers.\n\"I'd love to foster more cooperation between industry and academica,\" he said. \"It's great for everybody.\"\n","358":"  Alphabet Executive Chairman Eric Schmidt says that artificial intelligence will transform tech within five years, but there's little chance of it turning on humankind and becoming \"evil.\" \n  Speaking at the Cannes Lions Festival of Creativity on the French Riviera, Schmidt said A.I. was not likely to turn on humans and try to destroy humankind, as depicted in some movies. \n  Schmidt said, \"I saw that movie it was really good   but we are not anywhere near that, it presumes that humans would not notice that the computer had gone evil   It's highly unlikely these scenarios will occur, there are economic issues   All of this research needs to be done in the open, it should not be done in military labs, it needs to be done for all humanity, not for just elites, and it needs to make sure that people know how to turn this stuff off should it get to that point.\" \u00a0\n  But he said there were a lot of benefits for artificial intelligence, \"Computer vision is now better than human vision, it means self-driving cars probably see a lot better than you, especially if you are drunk,\" a message meaningful to the ros\u00e9-swilling Cannes crowd. \n  \"We wrote a program that would basically read your email and reply to it, it's called Smart Reply. In the beta testing we started using it ourselves internally in the company and its most common reply was, 'I love you,' which in a corporate email is not so good.\" \n  \"We built something called an assistant   All the tech companies are going to be offering this, machines with intuition   In the next five years it will be possible to have really powerful assistants.\" \n  He also said Google was not concerned about anxiety over privacy and anti-trust issues in Europe. Schmidt said, \"So far it has not had a big impact on our consumers and not too much on our brand, if anything all the publicity advances the brand, which is not necessarily a good thing. Our strategy is to get to know the regulators very well   We work very hard to explains how our systems work. We face a number of anti trust actions in Brussels which we are addressing   Our success should be judged on if our customers are happy.\" \n  The company is focusing on YouTube at Cannes, and he explained that, \"People tend to move to where their young people are, and they are frankly all on YouTube.\" He said they were bringing out YouTube 360 where you can upload 360 videos, plus, \"We have this huge promotion around cardboard [Google's virtual reality viewer] and virtual reality, we think that's the next technology platform for media authoring.\" \n  He added, \"Video is going to be the dominant player in how you communicate. We are just at the beginning of what you can do with video, we're still in static stories, 360 image the next thing is much more interactivity, much more dynamic activity with video, eventually A.I. systems that drive the video into immersive experiences.\" \n  Schmidt continued that if you looked at video gaming, which employs technology that makes it so realistic, \"That technology applied to human's everyday lives will transform advertising again.\" \n  Chill in ad air \n  CANNES - Bob Liodice may want to think about bringing a food taster with him if he decides this week to attend Cannes Lions. \n  Liodice, the chief executive of the Association of National Advertisers, won't be the most popular man in this tony resort town on the Mediterranean as the advertising industry holds it summer camp - better known as Cannes Lions. \n  The ANA earlier this month issued a report that found rebates, or bulk-buy kickbacks, were commonplace among agencies. While the June 7 report didn't name names, it slammed agencies for a host of \"non-transparent\" practices. \n  Agencies denied the allegations but the report, nonetheless, put a chill on agency holding company stock prices. \n  WPP stock closed at $114.28 on June 8 but has fallen 5.3 percent, to $108.16, as of last Friday. Omnicom shares fell 4.7 percent, to $81.35, over the same span, while shares of IPG were off 2.8 percent, to $23.55. \n  Agencies were upset, Liodice admitted ion a recent interview, and they need time to digest the findings so they can resolve the \"Cold War.\" \n  Still, there's bitter consternation in the upper echelons of the ad business about the organization's damning report alleging shady agency practices. \n  Agencies widely denounced the ANA's findings earlier this month but admit there are one-off issues that every industry deals with, and that internal compliance departments are thorough in their work. \n  Airbnb vs. Trump \n  Airbnb CEO Brian Chesky doesn't like Donald Trump's idea for building a wall along the border with Mexico. \n  \"Increasingly we're seeing people want to be global citizens,\" Chesky said during a panel discussion at the Cannes Lions confab here. \"Anyone who tries to put up barriers against culture is going to be on the wrong side of history.\" \n  Chesky, interviewed by Cosmopolitan magazine Editor-in-Chief Joanna Coles from the main stage on Monday, also lashed out at Airbnb hosts who discriminate by not renting to gays or racial minorities. \n  The company, valued at some $25 billion, will get rid of hosts found to be racist or homophobic, Chesky said. \n  Airbnb has come under fire as reports have surfaced about hosts disgustingly - and illegally - biased against certain guests. \n  Chesky also made an open request for the community to help Airbnb police the tricky situation. \n  A big proponent of the so-called sharing economy, Chesky said he believes Americans are losing their love of ownership and are warming to swapping the ideal of the big home, the big car and the white picket fence for experiences that they share on social media. \n  \"We'll own whatever we want responsibility for,\" Chesky said. \"It's about access   It's about memories and moments people want to have together.\" \n  As a result, Chesky believes society will look more geographically mobile. He revealed that 20 percent of Airbnb trips are now for longer than 30 days, indicating that people want to live in different cultures. \n  Separately, Airbnb is about to go beyond beds for the night. It is working on adding other services that travelers might want to add on to their lodging purchases - like entertainment experiences, or guides, or taxi rides. \n  The changes, along with some fresh marketing, will come into effect after the giant November host meet-up in Los Angeles, he said. \n","359":" ABSTRACT:Massachusetts Institute of Technology researchers are using robot called Genghis to mount frontal assault on conventional wisdom in artificial intelligence; unlike most robots, Genghis doesn't have centralized brain; instead, it has network of small, simple control programs, each devoted to single function such as lifting leg; drawing (M)\n","360":"To the Editor:\nAstro Teller (Op-Ed, March 21) should be assured that there is no concern that \"smart machines\" are on the verge of replicating the functions of the human brain.\u00a0\n While artificial-intelligence scientists are being forced to design miniaturized circuitry for their \"think machines\" that replicate the brain's cellular format, all of this research has a fundamental flaw. Artificial-intelligence investigation is based on advanced solid-state physics, whereas the humble human brain is a viable, semiliquid system! Have no fear. The artificial human mind is not here, nor will it be.\u00a0ROBERT J. WHITE , M.D.Cleveland, March 23, 1998\u00a0The writer is a professor of neurosurgery at Case Western Reserve.\n","361":" ABSTRACT:International Chip Corp pres Tai Sugimoto letter contends that July 5 article 'Bright Outlook for Artificial Intelligence Yields to Slow Growth and Big Cutbacks' presented myopic view of artificial intelligence\n","362":"A District-based start-up that uses artificial intelligence to predict the outcome of legislation has received $7 million in funding, including investments from the Singapore sovereign wealth fund and from the Winklevoss twins, made famous from their lawsuits against Facebook.\u00a0\nFiscalNote, founded by a Potomac entrepreneur, uses data-mining software and artificial intelligence to predict the fate of the bills proposed by state legislatures and by Congress each year. The company claims 94 percent accuracy.\nThe young company has made a name for itself because of early investors such as Dallas Mavericks owner Mark Cuban and Yahoo co-founder Jerry Yang.\n\"We will be utilizing the resources for international growth and product expansion,\" said founder Tim Hwang, 22. \"We will be bolstering our engineering team in addition to looking at expanding our sales and marketing operations domestically and abroad.\"\nThe investing was led by several overseas funds including Visionnaire Ventures, a joint fund between Taizo Son and Temasek, the Singapore sovereign wealth fund.\nYang's AME Cloud Ventures, Chevy Chase, Md.-based New Enterprise Associates, Winklevoss Capital, Enspire Capital of Singapore, Green Visor Capital and Middleland Capital also participated.\nLocal angel investors include former America Online chairman Steve Case and Duke Chung, co-founder of Parature. Last year, the start-up raised a $1.3 million round of funding led by Cuban and New Enterprise Associates.\nThe company, which has its headquarters in downtown Washington, has more than 30 clients, including Uber, Planned Parenthood, the New Balance shoe and apparel company, Ally Financial, Allergan, the Natural Resources Defense Council and the Republican and Democratic governors associations.\nHwang, who volunteered on Barack Obama's 2008 presidential campaign, co-founded the company in 2013 with boyhood friends Jonathan Chen and Gerald Yao. Chen became chief technology officer and Yao is chief strategy officer.\nThe company offers two subscription plans. One is a basic plan that charges clients $500 a month for the first user and $100 each for each additional user. A premium package includes more options.\nFiscalNote grew out of Hwang's work at Princeton, where he wanted to find a way to crack and synthesize the deluge of information from governments around the United States for an advocacy group he founded.\nthomas.heath@washpost.com\n","364":"ABSTRACT\nBailey McCann Portfolio Strategy column in Journal Report: Investing in Funds & ETFs on rise in number of fund companies that are using artificial intelligence to help inform their investment-management decisions; drawing (M)\n","365":"ABSTRACT\nTed Greenwald interview in Journal Report on Technology with Skype co-founder Jaan Tallinn, IBM executive Guruduth S Banavar and University of Padua professor Francesca Rossi, who discuss risks of artificial intelligence; graph; photos\n","366":"Computers really are becoming like people: Just because they are smart doesn't mean they won't do awful things.\nAs John Markoff writes, the kind of artificial intelligence that is capable of winning at the game of Go or figuring out your fastest route home is also starting to show up in criminal schemes. One program, known as Blackshades, was sold in the online criminal underground known as the dark web and used for purposes like video and audio eavesdropping. \u00a0\n  The man who developed Blackshades was sentenced in June 2015 to 57 months in prison. As with most other crimes, though, the threat of hard time isn't going to stop everyone -- particularly as the costs keep coming down and the number of applications is exploding.\n  The way A.I. can now recognize text and images, even imitate voices, lends itself to malicious uses in defeating online security, spotting victims, even eventually fooling people into thinking that a machine they're talking to is a person.\n  It happened to me yesterday. Working from home, I received a robocall to see if I wanted to buy solar panels (a common enough scam). A voice that called itself Amanda started the pitch with a chatty informality that was far superior to a standard recording, though it was still halting and tentative.\n  When I asked if this was a computer, there was a pause, then a stilted ''Yes, I'm a real person,'' followed by a too-fast pitch that didn't stop when it was interrupted. Then Amanda hung up on me.\n  I have a feeling she'll be back, with a faster processor and a better grasp of informal speech.\n  They say one of the best ways to check, for now, is to ask the suspected bot to recite a limerick or sing ''Happy Birthday.'' It may not be dignified, but it beats being ripped off.\n\n\n\n","367":" ABSTRACT:Five companies announce joint venture to develop improved software technology for managing knowledge with expert systems, branch of artificial intelligence (S)\n","368":"RE: A.I.\nGideon Lewis-Kraus wrote about how Google used artificial intelligence to transform its popular translation program -- and how its approach to A.I. is poised to reinvent computing itself. \n  Creativity defines human intelligence, and contextual interpretation of words is what sets the human brain apart from the machine ''brain.'' It's great that a computer can beat the world master of go or chess, but until Google Translate can interpret creatively and not translate literally (try plugging in the Toyota tagline ''Let's go places'' and see how many languages accurately capture the intent of the English idiom), its intelligence remains artificial and generic, which, in inexperienced hands, can cause serious miscommunication. Translation is not an easy art; the M.T.A.'s public-safety-poster headline ''Not Yourself?'' should translate in Chinese to ''Not Feeling Well?'' though it's possible a machine could miss this nuance and render it as ''Not You Personally?'' I would caution us, then, not to become overly reliant on ''general intelligence,'' which is far from genuine. Grace Chiu, New York\u00a0\n  One difference between robots and humans is that we worry. Articles describing how artificial intelligence will soon surpass humans are fascinating but cause great anxiety for many human readers. The Times's recent article about Google's advances in deep learning neglected to describe the essential role that humans play.\n  Consider the internet-search problem: Given a word, find the relevant websites among billions and list them in order of importance. Google's astonishing ability to solve this problem requires a diverse set of algorithms, machines and a key ingredient: ongoing input from a diverse group of humans. We make subtle decisions about content and links every time we post a tweet or update or create a web page, helping Google's system stay up to date by providing feedback in the form of choosing which suggested link to click.\n  The same is true for the recommendations provided by Netflix, Amazon, Facebook and Apple. All of A.I.'s recent advances in ''understanding'' text, voice, images and video are based on human input and feedback. And it has been shown by researchers in A.I. and psychology that diversity in human input is essential. Humans can take heart in knowing that these machines of the future will continue to rely on us. Ken Goldberg, Mill Valley, Calif.\n  RE: AL FRANKEN\n  Mark Leibovich profiled Al Franken, the second-term Democratic senator from Minnesota who once made a living satirizing politicians.\n  It seems your reporters, with a few exceptions, feel obligated to include a snarky comment whenever they write about Hillary Clinton. The most recent example is Mark Leibovich in his article on Al Franken. In talking about Trump's being an easy subject for satire, he mentions Clinton's appearance on ''S.N.L.,'' writing that this cameo ''was arguably her most endearing moment in an otherwise dreary slog.'' May I remind Leibovich that the purpose of her campaign was to present her qualifications as president, not as entertainer of the year?\n  Trump crushed 16 Republican contenders in the debates with his insults and demeaning remarks. None of them could go the distance. Yet Clinton -- a woman -- did, and she didn't falter as she endured Trump's extreme bullying. She remained focused and cool, even when he loomed over her and threatened to ''lock her up.'' Had she been a man, I wonder if the moderators would have allowed such abuse.\n  Though Clinton received an overwhelming lead in the popular vote, some postelection pundits blamed her for ''losing.'' Where is the gratitude or respect for her attempt to save us from Trump? Hillary Clinton is a hero. Joan Merrill, Bellevue, Wash.\n  I agree that we need Senator Franken's voice nowadays. But not as a comedian and not as a rule-following, friendly and bipartisan appeaser. He must play the role of the boy who shouts, ''The emperor has no clothes!'' More than half of the country knows that Trump is a bullying, lying, empty-headed madman. Senator Franken must not go along with the current national fantasy that Trump can actually be a president. He wouldn't be considered for the position of Boy Scout leader, let alone leader of the United States. If the article truly reflects what Senator Franken believes in his heart and his head will happen once Trump is sworn in, then he must take on this life-changing and career-busting task of convincing people that Trump is unfit to lead. Paul Glickman, Chicago\n  Sign up for our newsletter to get the best of The New York Times Magazine delivered to your inbox every week. \n              Send your thoughts to magazine@nytimes.com                  \n\n\n\n","370":"WHEN most people fight city hall, they attend meetings, circulate petitions or file lawsuits. When Murray Craig, a retired programmer, fought his town council in British Columbia, he picked up his old craft and wrote code.\n     In the end, he created software that his company claims can \"detect government corruption in five minutes.\" \n The software, called Minutes-N-Motion, applies artificial intelligence to the problem of finding needles in the haystacks of government documents. While standard document-searching software can pinpoint keywords, Mr. Craig's program makes connections to draw conclusions on issues like whether a public official may have acted on a matter presenting a conflict of interest.\u00a0\n\"This, in the hands of every person, means politicians will have to be more accountable,\" Mr. Craig said.\nMr. Craig, 50, bears the classic markers of a veteran of long battles with authority. His conversation is sprinkled with references to \"gutless politicians\" and descriptions of obscure rule-making procedures that he believes worked against him.\nAlthough Mr. Craig had tangled with the town council in his hometown, Langley, for several years on various issues, his inspiration for the software arose from his effort in 1996 to prevent mushroom farmers from polluting streams where he ran a community salmon hatchery. The matter remains unresolved.\nUltimately he left Canada for Southern California, where he now works for eNeuralNet, the company that sells his innovation. \nAlthough many universities and companies are building and selling systems that retrieve information from large data sets, such systems are rarely marketed as government watchdog tools.\n\"If they're looking for conflicts of interest, that's something I haven't seen,\" said Tom Mitchell, president of the American Association for Artificial Intelligence.\nMichael Shires, an assistant professor of public policy at Pepperdine University in Malibu, Calif., uses a donated copy of the software to help students research the ties between politicians, companies and lobbying groups that support laws. The technology could enable citizens to spot patterns and ask, \"Why are they doing this in my neighborhood and not another neighborhood?\" he said.\nOf course, the results are only as good as the data itself. In a demonstration, Mr. Craig turned up a case in which a town alderman voted on a contract involving a construction company after repeatedly abstaining from matters involving that contractor because of a conflict of interest.\nMark Rasch, a former federal prosecutor who specializes in Internet law in Bethesda, Md., said that while such software could help investigators in detecting political corruption, it might not help in cases in which a lawmaker leaves no trace of his conflicts in the public record.\n\"Where it doesn't work is when the guy's brother-in-law says, 'Vote this way,' and he does,\" Mr. Rasch said. \"It only works if the guy has recused himself\" previously.\nOn the other hand, said Thomas P. Kemp Jr., chief executive of eNeuralNet, a client could enter more data than appears in the public record, including information on an elected official's family and business relationships. With that data, he said, the software could flag potential conflicts even if officials did not recuse themselves from votes.\nFor now, in any case, the tool may be beyond the means of most gadflies and activists. While eNeuralNet is marketing the software to political groups, sales have so far been mainly to other kinds of clients -- pharmaceutical companies seeking patterns in clinical trials, a travel agency trying to detect credit card fraud -- and it is charging $50,000 to $5 million for its services, including the conversion of documents into searchable databases. \nNonetheless, government watchdogs see some potential for the technology to bring questionable political behavior to light. \n\"So much of politics is magic tricks: focusing the public's attention on one area while doing something else,\" said Sonia Arrison, director of technology studies at the Pacific Research Institute, a libertarian group in San Francisco. \"Once people see how this can work, they are going to want more of it.\" \n","371":"In the last few years, I've become increasingly fascinated by artificial intelligence, and in particular our escalating fear of it. It seemed to me that our increasingly holistic relationship with technology and abstract clouds of information was compounding this fear and perhaps edging it into paranoia.\nThese thoughts were crystallized while writing and directing the new film ''Ex Machina.'' It tells the story of a young male coder in a tech company who is given the job of assessing the level of consciousness in a female-presenting robot called Ava. He's bewitched, gives up the day job and starts making plans to elope. \u00a0\n  I remember feeling pleased when I hit upon this plot. Robot sentience hadn't really been tackled since Steven Spielberg's ''A.I.'' (2001), which was less interested in artificial intelligence than the title suggested, and the ideas of machine-thinking and human consciousness felt like a rich seam.\n  So I was dismayed when after submitting the script, I discovered I wasn't alone in writing an A.I. movie. There was ''Her'' (2013), which tracked a love affair between a man and his computer's sentient operating system. Next was ''Transcendence,'' in which Johnny Depp uploaded his mind to the Internet. In ''Automata'' and ''Chappie,'' servile robots acquired independence. In ''Big Hero 6,'' an A.I. befriends a boy. And in ''Avengers: Age of Ultron,'' out next week, a robot tries what we all know they're secretly planning: the destruction of mankind.\n  Among filmmakers there was an A.I. party going on, to which we were late. Worse yet, someone else had shown up in the same dress. Another film had freakish similarities to ''Ex Machina.'' It was called ''The Machine'' and also starred a female-presenting A.I. named Ava. (Initially I wondered if it was an incredibly early porn parody of ''Ex Machina'' that had missed the more obvious riff of ''Sex Machina.'' But it turned out to be a thoughtful action movie by the Welsh writer-director Caradog James.)\n  Alongside this sudden preoccupation in the film world, a growing number of public figures began making statements about the inherent danger of A.I. research. The gist was that if mankind wasn't careful, we would code the terms of our own destruction by robot overlords. These are warnings one might expect from a religious perspective, concerned with the godlike aspects of creating sentient life. But interestingly, the warnings were actually coming from the science and tech worlds.\n  They began as distant thunder. A high-level cognitive robotics researcher told me almost in passing that he was considering telling his children that they should not have children themselves, because he was so concerned about the A.I.-shaped world on our horizon. By the time ''Ex Machina'' was in postproduction, the thunder was a full-blown lightning storm, with the bright discharges coming from major public figures of our age.\n  The theoretical physicist Stephen Hawking told us that ''the development of full artificial intelligence could spell the end of the human race.'' Elon Musk, the chief executive of Tesla, told us that A.I. was ''potentially more dangerous than nukes.'' Steve Wozniak, a co-founder of Apple, told us that ''computers are going to take over from humans'' and that ''the future is scary and very bad for people.''\n  With entertainment and science joining forces to spin cautionary tales, it felt very alarming, and very zeitgeist. But it also felt confusing, in that zeitgeist is a response to something. Film production runs in roughly three-year cycles, and if you looked back three years from the glut of A.I.-related releases, you might expect to find an event that had led to the screenplays. Perhaps a Nobel Prize relating to consciousness, or a successful robot project with the profile of the Large Hadron Collider, or Dolly the sheep. But there was nothing. In fact, breakthroughs in A.I. research were either conspicuous by their absence, or so nuanced that you would struggle to convince lay people that the breakthrough existed at all. This raised a question: Was A.I. anxiety in part a projected fear of something else? In particular, I wondered if it stemmed from a general disconnect in our relationship with technology.\n  We have laptops and cellphones and tablets, and most of us don't understand how they work. But the devices seem to understand how we work. They anticipate what we want to say in text messages and search-engine inputs, and know what we want to buy, see and read. This one-way understanding makes us anxious. We locate the anxiety in the machines, which translates as anxiety about A.I.\n  But there is a mistake here. The machines in question are not strong A.I.'s. They are weak. They have no motivation, no intention; they're neutral. The thing with an agenda is us: consumers, who want to buy the machines, and manufacturers, who want to sell them. And looming over both, giant tech companies, whose growth only ever seems to be exponential, whose practices are opaque, and whose power is both massive and without true oversight. Combine all this with government surveillance and lotus-eating public acquiescence, and it's not the machine component that scares me. It's the human component.\n  So here is a counterargument, in favor of the machines. In very broad terms, human behavior is frightening when it is unreasonable. And reason might be precisely the area where artificial intelligence excels.\n  I can imagine a world where machine intelligence runs hospitals and health services, allocating resources more quickly and competently than any human counterpart.\n  Public works aside, the investigation into strong artificial intelligence might also lead to understanding human consciousness, the most interesting aspect of what we are. This in turn could lead to machines that have our capacity for reason and sentience, but different energy requirements and a completely different relationship with mortality. That could mean a different future. A longer future. In which case, we could rephrase the warnings of Mr. Hawking and Mr. Wozniak. Where they say that A.I. will spell the end of humans, we could say that one day, A.I. will be what survives of us.\n\n\n\n","372":" In \"Relax, the Terminator Is Far Away,\" John Markoff writes that, despite the warnings of visionaries like Stephen Hawking and Bill Gates, today's artificial intelligence is still tethered to human controllers. \u00a0\n WHAT have Elon Musk, Stephen Hawking and Bill Gates said recently about artificial intelligence?\n WHERE will the Defense Advanced Research Projects Agency, a Pentagon research arm, hold the final competition in its Robotics Challenge next month?\n WHAT will the robot that wins $2 million in prize money have to do?\n WHY does a preview of the challenge suggest that \"nobody needs to worry about a Terminator creating havoc anytime soon\"?\n WHAT complaint did reporters covering the 2013 competition have about the robots?\n HOW long will this year's robots have to complete a set of eight tasks that would probably take a human less than 10 minutes?\n HOW will human operators guide the machines?\n WHY, in both the Darpa contest and in the field of robotics more broadly, has there recently been a re-emphasis on the idea of human-machine partnerships?\n WHAT is \"cloud robotics\"? WHAT is \"multiplicity\"?\n\"The cheaper the robot, the more autonomy it has,\" says Rodney Brooks, a Massachusetts Institute of Technology roboticist and co-founder of two early companies, iRobot and Rethink Robotics. WHY?\n WHEN did a Darpa challenge lead to Google's decision to begin a self-driving-car project?\n HOW has the robot-driving task gotten much more challenging since then?\n WHO is the team of hobbyists who will bring a homegrown robot financed with credit cards and the help of family members, rather than university or corporate financing?\n For Higher-Order Thinking \n WHAT do you know about robotics? WHAT fictional robot or portrayal of robots interests you most? WHY?\n WHAT can you predict about the future of robotics? For instance, WHAT do you think robots will be able to do 25 years from now? 50? 100?\n","373":"AS well-wishers stood on the sidewalk outside the Washington Square Church, throwing rice at Tony and Tina Nunzio, an elderly passer-by inquired what was going on. Told that it was a show about a wedding, she stared in disbelief and said flatly, ''No, it couldn't be.'' Noticing that the bride and groom were smoking while posing for pictures, another observer said, ''It must be a cigarette commercial.'' Neighbors across the street must have been wondering why this same couple was marrying every Saturday and Sunday afternoon for the last three weeks. Beginning this week, they will also marry on Thursday and Friday evenings.\nLater, at the wedding reception (at Carmelita's hall, one flight up from Disco Donuts), I danced with Sister Albert Maria and asked her exactly how many times this marriage had taken place. Keeping in sisterly character, she responded, ''Just this once, I hope.''\u00a0\nThe show is ''Tony 'n' Tina's Wedding,'' and, as the tongue-in-cheek creation of the Artificial Intelligence comedy troupe, it is a live-in cartoon, a walk-around wedding in which the audience helps the actors do their work.\nA cast of more than 20 is involved in the nuptials, although one cannot always tell the actors from the theatergoers - the actors are more formally attired. At the center of the bridal party are Tony (Mark Nassar) and Tina (Nancy Cassaro) and Tina's gum-chewing bridesmaids. The maid of honor is pregnant, which is the first of many complications.\nThe ceremony itself is conducted by Father Mark, an amiably sanctimonious young priest who acts as master of ceremonies, introducing the guests to Sister Albert Maria, who is, of course, a singing nun; the decrepit Grandma Nunzio (a triumph of make-up) and the groom's slick, womanizing father. After Father Mark unites the couple, they are whisked off to Carmelita's, on 14th Street, in a car with New Jersey plates. After a honeymoon in the Poconos, the couple will set up housekeeping in Queens. But first they have to survive the reception.\nHaving attended weddings of the Corleone and Prizzi families, one is prepared for this tacky version of those more excessive festivities. There is dancing to the tune of Donny Dulce and Fusion, a congenially mediocre combo specializing in theme music (from ''Rocky'' and ''Starlight Express'').\nAt ''Oil City Symphony,'' theatergoers are invited to do the hokeypokey. Here, for the $40 admission price, they can do the alley cat and join hands for ''Hava Nagilah.'' The music may be ecumenical, but this is, by definition, an Italian-American affair, with all stereotypes reinforced. As we watch la famiglia, at odd moments drama erupts. Elderly relatives suddenly pass out, the priest drinks too much, one of the bridesmaids begins to do a striptease and the bride and groom have an argument that can be heard all the way to the Poconos.\nDepending on the acting ability of the audience, the story may vary from performance to performance. At Saturday's matinee, a number of young female theatergoers seemed to have a crush on Tony, who responded with artfully arched eyebrows. Mr. Nassar and Ms. Cassaro are convincing as Tony and Tina and they and others in the company could conceivably move on to potentially more dramatic situations, such as Tony and Tina's Divorce.\nOne's enjoyment of the show will be based on what might be called the participatory factor. Eagerness equals entertainment. If you want to have a good time, you will. On the other hand, the entire event lasts three and one half hours, and, as in any wedding and in many shows, there are doldrums. Audiences may leave the reception feeling theatrically undernourished. Food, however, is provided - baked ziti from the steam table. After that, one can move uptown and have supper at ''Tamara,'' completing a full day of theater-dinner.\u00a0Nuptial MerrimentTONY 'N' TINA'S WEDDING, by Artifical Intelligence; directed by Larry Pelligrini; conception, Nancy Casssaro; choreography, Hal Simons; costume, hair and make-up design, Juan DeArmas. Presented by Joseph Corcoran, Daniel Corcoran and Mark Campbell. At Washington Square Church, 135 West Fourth Street; reception at Carmelita's Restaurant, 150 East 14th Street.\nValentina Lynne Vitale Nunzio...Nancy Cassaro\nAnthony Angelo Nunzio...Mark Nassar\nWITH: Moira Wilson, Mark Campbell, Elizabeth Dennehy, James Altuner, Patricia Cregan, Eli Ganias, Susan Varon, Thomas Michael Allen, Jacob Harran, Jennifer Heftler, Elizabeth Herring, Chris Fracchiolla, Jennie Moreau, Denise Moses, Jack Fris, Phil Rosenthal, Kevin A. Leonidas, Joanna Cocca, Mickey Abbate, Tom Hogan, Vincent Floriani, Michael Winther, Kia Colton, Charlie Terrat and Towner Gallaher.\n","374":"THE ASK\n By Sam Lipsyte\nFarrar Straus Giroux. 296 pp. $25\nHere's Milo Burke, the sad-sack hero of \"The Ask,\" reflecting on artificial intelligence and computers. Maura is his wife:\n\"Some argued that the creation of artificial intelligence amounted to cruel and unusual punishment. Consciousness was suffering. Why inflict it on a poor machine? I wasn't one of those people, but only because I believed that AI would someday make good on its promise of astonishing robot sex, if not for us, then for our children.\n\"I was also one of those people who hadn't caught up with the latest social networking site. Maura belonged to most of them. She passed most evenings befriending men who had tried to date-rape her in high school.\"\nGenerally, novels make us turn the pages because we want to know what happens next. But with Sam Lipsyte's \"The Ask,\" we turn the pages because we want to know what's going to happen in the next sentence. Here rants become arias, and vulgarity sheer poetry. Lipsyte's masters aren't Messrs. Strunk and White; they're gallows-humored C\u00c3\u00a9line, Hunter S. Thompson at his most gonzo, the great Stanley Elkin.\u00a0\nAlthough \"The Ask\" is unquestionably funny, it's by no means essentially comic. Its theme, after all, is loss, often heartbreaking loss. In the opening chapter, Milo is fired from his job as a mediocre fundraiser for a mediocre university in New York. Once he dreamed of becoming a painter; now, approaching 40, he daydreams about his old college days, his misfit housemates and former girlfriends. Maura, whom he adores, no longer likes to be touched by him, and only Bernie, their nearly 4-year-old son, seems to be keeping the little family together.\nListless and drinking too much, Milo is nonetheless suddenly called back to his former job. The immensely wealthy Purdy Stuart -- a major \"ask\" -- might be willing to endow a building or fund a project, but he insists that Milo be the go-between. Years ago, the two had been uneasy friends in college, for Purdy already exuded the sure confidence that comes with a trust fund and an obvious future in the ruling class. Even back then, Milo wasn't precisely upbeat:\n\"For a time I wore only heavy, steel-toed boots because I figured if apocalyptic war broke out, sturdy footwear would be a must. Then it dawned on me that the better the boots, the more quickly I would be killed for them. My only shot at survival would be shoeless abjection.\"\nWhen the two meet for lunch, Purdy is tanned and toned, looking like a million, or rather several hundred million. He's married to a former supermodel named Melinda, a \"generically stunning woman.\" As Milo says:\n\"There were thousands, or at least several hundred, just like her in this part of the city . . . perfect storms of perfect bones, monuments to tone and hair technology. Around here she was almost ordinary, but you could still picture small towns where men might bludgeon their friends, their fathers, just to run their sun-cracked lips along her calves.\"\nThese days, Purdy spends most of his time making deals or jetting off to conferences. He calls Milo from Vail, where he's attending \"an ideas festival\" and sharing his suite \"with a gorgeous renewable-energy guru.\" You should come out here, Purdy tells his old college bud:\n\"It's really something. I mean, these people, you read their books, their newsletters, see them on TV, but to hear them in person, chat with them. Very impressive. Do you realize that someday we will be heating our houses with trout?\"\nWhile Purdy dawdles over the exact nature and amount of his \"give,\" Milo makes ends meet by building decks with Nick, a guy with big dreams. \"He wanted to break into television. He watched a lot of reality shows, he informed me, especially the ones about breaking into television.\" Nick has noticed something strange about death-row prisoners:\n\"Bear with me and answer this question. Why do these death row losers always order nuggets and dipping sauce and biggie fries for their last meal? Is it A, they are ghetto or barrio or trailer-park trash who don't know any better, who could never imagine a taste sensation transcending that of a Hot Pocket and an orange Fanta, or, B, something else entirely?\"\nBefore long, Nick is outlining his idea for \"Dead Man Dining,\" in which \"the world's top chefs prepare exquisite last meals for condemned prisoners.\" This seems no more unlikely than the shows Maura and Milo watch together:\n\"We jumped from pundit to pundit, then on to basketball, Albanian cooking, endangered voles, 'America's Top Topiary Designers,' 'America's Toughest Back-Up Generators,' 'The Amazing Class Struggle,' the catfish channel, a show called, simply, 'Airstrikes!' \"\nThroughout \"The Ask,\" Lipsyte keeps returning to family life, but particularly to the relationship between fathers and sons. Milo fondly recalls his own feckless, skirt-chasing dad and frankly delights in his son, Bernie -- \"Bold name, by the way,\" Purdy says. \"You just definitely want him to be an accountant?\" The little boy spends a great deal of his free time \"on the sofa, watching his favorite show, the one where children mutated into gooey robots, sneered. It was like a parable from a religion based entirely on sarcasm.\" The scenes at Bernie's preschool, the Happy Salamander, are all a hoot.\nBut, then, no group is safe from Lipsyte's gimlet eye. Take the privileged \"international teens with their embossed leathers, their cashmere hoodies and pimpled excitements.\" The disdainful Milo is convinced that these wealthy college kids from China, Japan, Russia or Kuwait are gaming everybody:\n\"We sponsored them for visas, and when the paperwork went through, they transferred to one of the online universities, lit out for the territories, Vegas, Miami, Maui. No classes to attend, all their assignments written by starving grad students and emailed for grading to shut-in adjuncts scattered across the North American landmass, the international teens would have a whole semester for the most delightful modes of free fall. Daddy's Shanghai factories or Caspian oil pipes would foot the bills.\"\nNote the rich fathers in that last sentence. As it happens, about this time Milo encounters Don Charboneau, a bitter 21-year-old Iraqi war vet whose legs have been blown off and who hobbles around on titanium prostheses. Don's mother, a single mom, has died suddenly, and it turns out that his father is none other than Purdy, whom he utterly despises yet hungers to meet. Purdy would like this ancient by-blow to just go away. From this point on, \"The Ask\" grows darker, increasingly fraught.\nLost jobs, lost loves, lost fathers, lost hope -- does anything remain? In these pages, even the United States is regularly deemed a loser nation, metaphorically \"slumped in the corner of the pool hall, some gummy coot with a pint of Mad Dog and soggy yellow eyes, just another mark for the juvenile wolves.\" Purdy's dying lawyer, Lee Moss,  says straight out, \"My grandson's at Harvard right now. He's a dummy. But then again most of them are. I went to City College on the GI Bill. This was back when there was America.\"\nWhat else happens in \"The Ask\"? Don't ask. Because Lipsyte's firecracker prose is so much fun to read, one can almost overlook all the heartbreak in his brilliant novel. Almost. In the end, the dazzle simply highlights the darkness and the despair.\nVisit Dirda's online book discussion at http:\/\/washingtonpost.com\/readingroom.\n","375":"In glossy sci-fi movies like ''Ex Machina'' and ''Chappie,'' robots move with impressive -- and frequently malevolent -- dexterity. They appear to confirm the worst fears of prominent technologists and scientists like Elon Musk, Stephen Hawking and Bill Gates, who have all recently voiced alarm over the possible emergence of self-aware machines out to do harm to the human race.\n''I don't understand why some people are not concerned,'' Mr. Gates said in an interview on Reddit. \n  ''I think we should be very careful about artificial intelligence,'' Mr. Musk said during an interview at M.I.T. ''If I had to guess at what our biggest existential threat is, it's probably that,'' he added. He has also said that artificial intelligence would ''summon the demon.''\u00a0\n  And Mr. Hawking told the BBC that ''the development of full artificial intelligence could spell the end of the human race.''\n  Not so fast. Next month, the Defense Advanced Research Projects Agency, a Pentagon research arm, will hold the final competition in its Robotics Challenge in Pomona, Calif. With $2 million in prize money for the robot that performs best in a series of rescue-oriented tasks in under an hour, the event will offer what engineers refer to as the ''ground truth'' -- a reality check on the state of the art in the field of mobile robotics.\n  A preview of their work suggests that nobody needs to worry about a Terminator creating havoc anytime soon. Given a year and a half to improve their machines, the roboticists, who shared details about their work in interviews before the contest in June, appear to have made limited progress.\n  In the previous contest in Florida in December 2013, the robots, which were protected from falling by tethers, were glacially slow in accomplishing tasks such as opening doors and entering rooms, clearing debris, climbing ladders and driving through an obstacle course. (The robots had to be placed in the vehicles by human minders.)\n  Reporters who covered the event resorted to such analogies as ''watching paint dry'' and ''watching grass grow.''\n  This year, the robots will have an hour to complete a set of eight tasks that would probably take a human less than 10 minutes. And the robots are likely to fail at many. This time they will compete without belays, so some falls may be inevitable. And they will still need help climbing into the driver's seat of a rescue vehicle.\n  Twenty-five teams are expected to enter the competition. Most of their robots will be two-legged, but many will have four legs, several will have wheels, and one ''transformer'' is designed to roll on four legs or two. That robot, named Chimp by its designers at Carnegie Mellon University, will weigh 443 pounds.\n  None of the robots will be autonomous. Human operators will guide the machines via wireless networks that will occasionally slow to just a trickle of data, to simulate intermittent communications during a crisis. This will give an edge to machines that can act semi-autonomously, for example, automatically walking on uneven terrain or grabbing and turning a door handle to open a door. But the machines will remain largely helpless without human supervisors.\n  ''The extraordinary thing that has happened in the last five years is that we have seemed to make extradorinary progress in machine perception,'' said Gill Pratt, the Darpa program manager in charge of the Robotics Challenge.\n  Pattern recognition hardware and software has made it possible for computers to make dramatic progress in computer vision and speech understanding. In contrast, Dr. Pratt said, little headway has been made in ''cognition,'' the higher-level humanlike processes required for robot planning and true autonomy. As a result, both in the Darpa contest and in the field of robotics more broadly, there has been a re-emphasis on the idea of human-machine partnerships.\n  ''It is extremely important to remember that the Darpa Robotics Challenge is about a team of humans and machines working together,'' he said. ''Without the person, these machines could hardly do anything at all.''\n  In fact, the steep challenge in making progress toward mobile robots that can mimic human capabilities is causing robotics researchers worldwide to rethink their goals. Now, instead of trying to build completely autonomous robots, many researchers have begun to think instead of creating ensembles of humans and robots, an approach they describe as co-robots or ''cloud robotics.''\n  Ken Goldberg, a University of California, Berkeley, roboticist, has called on the computing world to drop its obsession with singularity, the much-ballyhooed time when computers are predicted to surpass their human designers. Rather, he has proposed a concept he calls ''multiplicity,'' with diverse groups of humans and machines solving problems through collaboration.\n  For decades, artificial-intelligence researchers have noted that the simplest tasks for humans, such as reaching into a pocket to retrieve a quarter, are the most challenging for machines.\n  ''The intuitive idea is that the more money you spend on a robot, the more autonomy you will be able to design into it,'' said Rodney Brooks, an M.I.T. roboticist and co-founder two early companies, iRobot and Rethink Robotics. ''The fact is actually the opposite is true: The cheaper the robot, the more autonomy it has.''\n  For example, iRobot's Roomba robot is autonomous, but the vacuuming task it performs by wandering around rooms is extremely simple. By contrast, the company's Packbot is more expensive, designed for defusing bombs, and must be teleoperated or controlled wirelessly by people.\n  The first Darpa challenge more than a decade ago had a big effect on the perception of robots. It also helped spark greater interest in the artificial intelligence and robotics industries.\n  During the initial Darpa challenge in 2004, none of the robotic vehicles was able to complete more than seven of the 150 miles that the course covered. However, during the 2005 challenge, a $2 million prize was claimed by a group of artificial-intelligence researchers from Stanford University whose vehicle defeated a Carnegie Mellon entrant in a tight race.\n  The contest led to Google's decision to begin a self-driving-car project, which in turn spurred the automotive industry to invest heavily in autonomous vehicle technology.\n  Developing a car to drive on an unobstructed road was a far simpler task than the current Darpa Robotics Challenge, which requires robots to drive and, while they're walking, navigate around obstacles, remove debris, use vision and grasp with dexterity, and perform tasks with tools.\n  ''We had a relatively easy task,'' said Sebastian Thrun, a roboticist who led the Stanford team in 2005 and later started the Google self-driving-car project. ''Today they're doing the hard stuff.''\n  His view about the relationship between humans and robots has been shaped by the two contests. ''I'm a big believer that technology progresses by complementing people rather than replacing them,'' he said.\n  Most of the Robotics Challenge teams receive university and corporate financing, and in some cases use a Darpa-funded, 6-foot-2 Atlas robot that weighs 380 pounds. (All of the competitors must design their own software and controls.)\n  But one team of hobbyists will bring a homegrown robot financed with credit cards and the help of family members.\n  ''We're not a big company,'' said Karl Castleton, an assistant professor of computer science at Colorado Mesa University and the leader of Grit Robotics, which has constructed a robot that rolls slowly on four wheels. ''We're just some guys who have a lot of love for what we're doing.''\n\n\n\n","376":" ABSTRACT:Scientists at German Research Institute for Artificial Intelligence are building the ultimate language translator--a computer so clever that it can instantly translate live speech; prototype is getting only so-so grades in a quiz of its language proficiency because narrow vocabulary is posing problems (L)\n","377":"For a field that was not well known outside of academia a decade ago, artificial\u00a0intelligence has grown dizzyingly fast. Tech companies from Silicon Valley to Beijing are betting everything on it, venture capitalists are pouring billions into research and development, and start-ups are being created on what seems like a daily basis. If our era is the next Industrial Revolution, as many claim, A.I. is surely one of its driving forces.\nIt is an especially exciting time for a researcher like me. When I was a graduate student in computer science in the early 2000s, computers were barely able to detect sharp edges in photographs, let alone recognize something as loosely defined as a human face. But thanks to the growth of big data, advances in algorithms like neural networks and an abundance of powerful computer hardware, something momentous has occurred: A.I. has gone from an academic niche to the leading differentiator in a wide range of industries, including manufacturing, health care, transportation and retail. \u00a0\n  I worry, however, that enthusiasm for A.I. is preventing us from reckoning with its looming effects on society. Despite its name, there is nothing ''artificial'' about this technology -- it is made by humans, intended to behave like humans and affects humans. So if we want it to play a positive role in tomorrow's world, it must be guided by human concerns.\n  I call this approach ''human-centered A.I.'' It consists of three goals that can help responsibly guide the development of intelligent machines.\n  First, A.I. needs to reflect more of the depth that characterizes our own intelligence. Consider the richness of human visual perception. It's complex and deeply contextual, and naturally balances our awareness of the obvious with a sensitivity to nuance. By comparison, machine perception remains strikingly narrow.\n  Sometimes this difference is trivial. For instance, in my lab, an image-captioning algorithm once fairly summarized a photo as ''a man riding a horse'' but failed to note the fact that both were bronze sculptures. Other times, the difference is more profound, as when the same algorithm described an image of zebras grazing on a savanna beneath a rainbow. While the summary was technically correct, it was entirely devoid of aesthetic awareness, failing to detect any of the vibrancy or depth a human would naturally appreciate.\n  That may seem like a subjective or inconsequential critique, but it points to a major aspect of human perception beyond the grasp of our algorithms. How can we expect machines to anticipate our needs -- much less contribute to our well-being -- without insight into these ''fuzzier'' dimensions of our experience?\n  Making A.I. more sensitive to the full scope of human thought is no simple task. The solutions are likely to require insights derived from fields beyond computer science, which means programmers will have to learn to collaborate more often with experts in other domains.\n  Such collaboration would represent a return to the roots of our field, not a departure from it. Younger A.I. enthusiasts may be surprised to learn that the principles of today's deep-learning algorithms stretch back more than 60 years to the neuroscientific researchers David Hubel and Torsten Wiesel, who discovered how the hierarchy of neurons in a cat's visual cortex responds to stimuli.\n  Likewise, ImageNet, a data set of millions of training photographs that helped to advance computer vision, is based on a project called WordNet, created in 1995 by the cognitive scientist and linguist George Miller. WordNet was intended to organize the semantic concepts of English.\n  Reconnecting A.I. with fields like cognitive science, psychology and even sociology will give us a far richer foundation on which to base the development of machine intelligence. And we can expect the resulting technology to collaborate and communicate more naturally, which will help us approach the second goal of human-centered A.I.: enhancing us, not replacing us.\n  Imagine the role that A.I. might play during surgery. The goal need not be to automate the process entirely. Instead, a combination of smart software and specialized hardware could help surgeons focus on their strengths -- traits like dexterity and adaptability -- while keeping tabs on more mundane tasks and protecting against human error, fatigue and distraction.\n  Or consider senior care. Robots may never be the ideal custodians of the elderly, but intelligent sensors are already showing promise in helping human caretakers focus more on their relationships with those they provide care for by automatically monitoring drug dosages and going through safety checklists.\n  These are examples of a trend toward automating those elements of jobs that are repetitive, error-prone and even dangerous. What's left are the creative, intellectual and emotional roles for which humans are still best suited.\n  No amount of ingenuity, however, will fully eliminate the threat of job displacement. Addressing this concern is the third goal of human-centered A.I.: ensuring that the development of this technology is guided, at each step, by concern for its effect on humans.\n  Today's anxieties over labor are just the start. Additional pitfalls include bias against underrepresented communities in machine learning, the tension between A.I.'s appetite for data and the privacy rights of individuals and the geopolitical implications of a global intelligence race.\n  Adequately facing these challenges will require commitments from many of our largest institutions. Universities are uniquely positioned to foster connections between computer science and traditionally unrelated departments like the social sciences and even humanities, through interdisciplinary projects, courses and seminars. Governments can make a greater effort to encourage computer science education, especially among young girls, racial minorities and other groups whose perspectives have been underrepresented in A.I. And corporations should combine their aggressive investment in intelligent algorithms with ethical A.I. policies that temper ambition with responsibility.\n  No technology is more reflective of its creators than A.I. It has been said that there are no ''machine'' values at all, in fact; machine values are human values. A human-centered approach to A.I. means these machines don't have to be our competitors, but partners in securing our well-being. However autonomous our technology becomes, its impact on the world -- for better or worse -- will always be our responsibility.\n  Follow The New York Times Opinion section on Facebook and Twitter (@NYTopinion), and sign up for the Opinion Today newsletter. \n\n\n\n","378":"             SAN FRANCISCO - In 1950, artificial-intelligence pioneer Alan Turing famously proposed what came to be known as the Turing Test - the proposition that a machine had achieved intelligence if it could carry on a conversation that was indistinguishable from a human one.\nIn 2016, Turing's ghost has come to haunt Silicon Valley in a big way. Companies are racing to build technology that can talk with you. Last month, Microsoft launched the Microsoft Bot Framework, a set of software tools that let companies create their own conversational bots. Customers of Domino's, for instance, can order products by chatting  with a robot, as if they were sending a text message. Dozens of start-ups are also building chat bots that do things like schedule a meeting, conduct a basic interview with a job candidate or collect daily pain reports from a patient. (None of these yet pass the Turing Test.)\u00a0\nThis week, Facebook is expected to drop its own mic in this increasingly noisy room. Much like Microsoft, the social network will release an online shop for chat bots at its annual developer conference. Instead of just talking with friends through Facebook Messenger, consumers might be able to ask for services, such as ordering movie tickets, via chat.\nFacebook's embrace of a bot platform is likely to set off an arms race, as major tech companies compete to define how bots will be built and how they will interact with consumers. The tech giants know that whoever controls the most popular platforms for bots has the ability to dictate - and profit from - how a generation of retailers, taxi services, movie-ticket firms and others build for that technology.\nThe chat-bot explosion bears a resemblance to the fledgling app economy circa 2009. Apple had launched its App Store the previous year - Google's Play store didn't open until 2012 - and \"the kooky world of apps\" was a refrain in the media.\nToday, apps are a $50 billion economy, according to App Annie, which tracks these sales.\nAs apps proliferated, Apple and Google forged ahead with app marketplaces that served as platforms for hundreds of thousands of other companies to sell their services. Apple's App Store revenue exceeded $20 billion last year, the company said. Google doesn't break out revenue for Play, but estimates put the number above $7 billion. Amazon and Microsoft haven't made significant inroads in the app world - and perhaps this is why they seem so eager to usher in an alternate way to find goods and services through bots.\nConsumers may also be ready for a new platform. Besides messaging apps such as WhatsApp, WeChat and Facebook Messenger, enthusiasm for new apps appears to be waning. According to a Forrester report last year, consumers spent 80 percent of their time on just five apps - four-fifths of which are owned by Google or Facebook - even though they've downloaded an average of 24 of them.\n\"The same set of drivers that propelled apps to be really new exist with bots,\" said Phil Libin, managing director of venture-capital firm General Catalyst Partners. \"The next version of that is going to be bots.\"\nThe bot battle lines will look very different from those in the app wars. While it's relatively easy to build an app, getting computers to understand language and hold real conversations is much harder, said Forrester analyst Jeffrey S. Hammond. The adoption curve could be slower as a result.\nJust ask meeting scheduler app X.ai. Behind the scenes, human developers are labeling every bit of language that the company's dozens of beta testers use to discuss something as simple as getting together for coffee. Despite advances in artificial intelligence, labeling by hand is still necessary, said X.ai chief executive Dennis R. Mortensen. For instance, if a human suggests meeting sometime \"Wed-Fri,\" a computer wouldn't automatically understand that the suggestions includes Thursday.\n\"These aren't wide-open queries,\" said Forrester analyst Julie A. Ask, of the types of questions that users can ask bots. \"I'd be surprised if this was game-changing on day one.\"\nEven the developers of these artificial-intelligence technologies acknowledge that mistakes - potentially big ones that could deflate consumer interest - will be associated with this technology for some time. In its apology for unintended racist comments made by Tay - a Twitter-based chat bot recently launched and then shuttered by Microsoft - the company said such mistakes could be necessary for artificial intelligence to improve. \"We cannot fully predict all possible human interactive misuses without learning from mistakes,\" the company said. (Flickr engineers had a similar response when its photo-recognition technology mischaracterized an image of a concentration camp as a \"jungle gym.\")\nMoreover, the emerging market for bot platforms is fragmented among a different set of players than before. While Google and Apple took the lead during the app wars, one of the first companies to launch a bot store is Kik, the Canadian messaging app that has a $50 million investment from Chinese Internet giant Tencent. The store lets users interact, via chat, with and request information from companies such as the Weather Channel, Funny or Die, or Vine. Then there's the popular office productivity software Slack, which has also encouraged start-ups to build chat bots onto its platform to do things such as order lunch. Even Taco Bell has recently gotten in on the action.\nTellingly, the U.S. Web giants forging ahead with bot platforms - Microsoft and Facebook as well as Amazon, which has allowed developers to build services on its virtual assistant, Alexa - do not dominate the app universe. This makes sense. Apple and Google have a lot to lose if an ecosystem of bots could replace the functionality of today's apps. Facebook, Microsoft and Amazon would have much to gain.\n\"Everyone who didn't win the app platform wars will get another chance with bots,\" said Libin.\nelizabeth.dwoskin@washpost.com\n","381":"Theoretical physicist Stephen Hawking has made no effort to conceal his fears about the dangers of artificial intelligence, warning on multiple occasions that blindly embracing pioneering technology could trigger humanity's annihilation.\nEarlier this month, the 74-year-old Cambridge University professor expounded upon those fears, adding new disasters to his growing list of apocalyptic scenarios that humanity may have to contend with this century, including nuclear war,\u00a0global warming and genetically engineered viruses,\u00a0the BBC reported.\nAccording to Hawking, there is, however, a glimmer of hope: Leaving the planet behind.\u00a0\n\"Although the chance of a disaster to planet Earth in a given year may be quite low, it adds up over time, and becomes a near certainty in the next thousand or ten thousand years,\" Hawking told audience members in a\u00a0public Q&A session ahead of the annual BBC Reith Lectures.\u00a0\"By that time we should have spread out into space, and to other stars, so a disaster on Earth would not mean the end of the human race.\"\n\"However, we will not establish self-sustaining colonies in space for at least the next hundred years, so we have to be very careful in this period.\"\nWhile Hawking believes technology has the capacity to ensure mankind's survival, previous statements suggest the cosmologist is simultaneously grappling with the potential threat it poses. When it comes to discussing that threat, Hawking is unmistakably blunt.\n\"I think the development of full artificial intelligence could spell the end of the human race,\" Hawking\u00a0told the BBC\u00a0in a 2014 interview that touched upon everything from online privacy to his affinity for his robotic-sounding voice.\nDespite its current usefulness, he cautioned, further developing A.I. could prove a fatal mistake.\n\"Once humans develop artificial intelligence, it will take off on its own and redesign itself at an ever-increasing rate,\" Hawking warned in recent months. \"Humans, who are limited by slow biological evolution, couldn't compete and would be superseded.\"\nThe cosmologist\u00a0lives with the\u00a0motor neuron disease amyotrophic lateral sclerosis (ALS), or Lou Gehrig's Disease.\u00a0As the disease has progressed, he has become almost entirely paralyzed. And in 1985, after contracting pneumonia, Hawking underwent a tracheotomy that left him unable to speak. He is\u00a0only able to communicate verbally using the assistance of a computer.\nWhile some have called for slowing the pace of innovation, Hawking's latest comments suggest he considers that idea unrealistic, according to the BBC.\n\"We are not going to stop making progress, or reverse it, so we have to recognize the dangers and control them,\" he said. \"I'm an optimist, and I believe we can.\"\nDespite his focus on some of the most ominous outcomes humanity could face, Hawking said young scientists should not be discouraged by the challenges ahead, the BBC reported.\n\"From my own perspective, it has been a glorious time to be alive and doing research in theoretical physics,\" he said. \"There is nothing like the Eureka moment of discovering something that no one knew before.\"\n              MORE READING:           \n               Bill Gates on dangers of artificial intelligence: 'I don't understand why some people are not concerned'            \n               Highlights from Stephen Hawking's Reddit AMA: 'Women' are the most intriguing 'mystery'            \n               Apple co-founder on artificial intelligence: 'The future is scary and very bad for people'            \n               Elon Musk and Stephen Hawking think we should ban killer robots            \n               Researchers create a computer program that learns the way humans do            \n               Stephen Hawking believes he's solved a huge mystery about black holes            \n","382":"Earlier this year, researchers' artificial intelligence beat a human in the dazzlingly complex board game known as Go. Not just once, but four times. It was a milestone in machine learning.\nNow, the same Google-backed researchers who designed AlphaGo have their sights set on dominating a new game: StarCraft, the classic computer strategy game that has attracted millions of fans, some of whom duel online in professional tournaments hosted by real-life sports leagues.\nResearchers from U.K.-based DeepMind want to train a bot that can play StarCraft II in real time - making decisions about which military units to send on scouting missions and how to allocate resources and ultimately conquer other players.\u00a0\nBeginning next year, the game will serve as a research platform for any AI researcher who wants to use it, potentially allowing myriad player-algorithms to train off the same game. And joining the effort is the game's publisher, Blizzard, which is working with DeepMind to set up the platform.\nUnlike Go, StarCraft represents an entirely different challenge. Whereas players of the ancient board game take turns putting down stones to control physical territory, StarCraft players have to manage a constantly shifting digital economy to achieve victory. They have to mine minerals and gases, build defensive structures and offensive forces, survey the terrain and, finally, close with and engage the enemy.\nThe best players have to know not only what's going on at their home base but also what may be happening in distant corners of the battlefield. Efficiency of motion is key; commentators talk of \"actions per minute\" as a way of measuring a human player's productive capacity.\n\"StarCraft is an interesting testing environment for current AI research because it provides a useful bridge to the messiness of the real-world,\" DeepMind wrote in a blog post Friday. \"The skills required for an agent to progress through the environment and play StarCraft well could ultimately transfer to real-world tasks.\"\nAt this point, you may be wondering what kind of \"real-world tasks\" a computerized military genius might put its mind to - hopefully, that doesn't include sending siege tanks or space marines after us.\nThe reality is that we are nowhere close to building the kind of \"general\" artificial intelligence that science fiction has trained us to fear. Our most sophisticated machines tend to be strong at pattern recognition but relatively weak at logic and deductive reasoning.\nDeepMind is not the first to think of using StarCraft as a training tool. In fact, AI researchers have spent years thinking about StarCraft precisely because of the unanswered problems for AI created by the game's open-ended style of play. And all joking aside, the implications are enormous.\n\"Optimizing assembly line operations in factories is akin to performing build-order optimizations\" in strategy games, according to one paper by an international group of researchers in 2013. \"Troop positioning in military conflicts involves the same spatial and tactical reasoning used in [real-time strategy] games. Robot navigation in unknown environments requires real-time path-finding and decision-making to avoid hitting obstacles.\"\nSince at least 2011, one annual competition has pitted dozens of bots against one another in games of StarCraft. And that's increasingly true of other games: Last month, developers of the turn-based strategy game \"Civilization VI\" unleashed eight computer players on one another to see what would happen.  \nUntil a few years ago, \"computer gaming\" used to mean sitting down in front of a keyboard and mouse yourself. Now, it seems to mean teaching the computer to play, too.\nbrian.fung@washpost.com\n","384":"B\u00a0y STEVEN J. MARCUS\nDavid I. Smith has worked for the General Electric Company for more than 40 years. Employed at the Transportation Systems division in Erie, Pa., he is an expert on the diagnosis and repair of locomotives and is considered one of the company's top field-service engineers.\nBut Mr. Smith's expertise is a scarce resource. His help is often needed in many different places at once, and he is in great demand, far in excess of his availability, for training young engineers across the country. When Mr. Smith retires, moreover, his knowledge would normally leave the company with him.\u00a0\nIn this case, G.E. is trying to immortalize Mr. Smith. The company has developed a computer-aided trouble-shooting system, or CATS-1, that carries much of his knowledge and emulates what he would do on the job.\nThis system, made public for the first time last week in Washington, at the annual meeting of the American Association for Artificial Intelligence, is by no means alone among serious attempts to apply the tools of computer science to formalizing and roughly reproducing certain kinds of human thought processes.\nForerunners include Dendral, a system developed at Stanford University to help chemists identify unknown organic compounds; Mycin, Stanford's system to aid physicians in selecting antibiotics for patients with severe infections, and Prospector, a system developed by SRI International to help geologists evaluate sites for mineral deposits.\nBut CATS-1 is a significant advance in the ''industrialization of artificial intelligence,'' says Francis S. Lynch, manager of the Knowledge-Based Systems group that developed it at G.E.'s Research and Development Center, because it is one of the first that is designed for routine operation on the shop floor. It may well inspire similar systems, he says, for a wide variety of products and processes, some of which the company is already investigating.\n\u00a0Developing Rules of Thumb\nAn expert system has two basic elements: the ''knowledge base'' gleaned from the expert and the ''inference engine,'' the logical structure within which the computer applies that knowledge in response to available evidence to draw conclusions and recommend actions.\nA major difficulty in building expert systems is that human experts are not usually aware of the exact mental processes by which they diagnose a problem; they apply their knowledge in subtle, seemingly instinctive, ways that elude codification. An effort is thus necessary to determine usable ''heuristics,'' or rules of thumb, that can be embedded into precise logical sequences that a computer can execute.\nFor Mr. Smith, the design process, was a labor of love.continuous.\n''I brought the system home to go through its nooks and crannies,'' he says, ''and soon lost all track of time. It's very exciting, and I had a ball.''\nThe CATS-1 system now has 550 rules to cover about 50 percent of the problems that locomotive repair shops may encounter, says Mr. Lynch, and is expected to have about 1,500 rules within a year, enabling it to diagnose 80 percent of locomotive repair problems.\nThe system is also unique, he says, in its portability. It is the first to be programmed on a microcomputer for industrial use.\n\u00a0Other Systems Being Developed\nJohn H. Clippinger, president of the Brattle Research Corporation, a consulting concern specializing in artificial intelligence, says G.E. is ''one of the first to intelligently and practically exploit the technology,'' but it will not be the last.\nBell Laboratories has been developing a comparable system, called Automatic Cable Expertise, to encode the know-how of experienced telephone cable-maintenance experts. And I.B.M., Fairchild, Digital, Texas Instruments, Xerox and Schlumberger have also been developing expert systems.\nNot all are limited to repair. Digital built a system to configure components of the company's VAX-11 line of computers for the specific requirements of each customer, and it has been successful enough - the company estimates savings in millions of dollars a year - to have inspired the development of expert systems for other company product lines.\n\u00a0New Industry Developing\nUntil recently, most of the systems were developed by in-house groups within organizations devoted to other pursuits. But artificial-intelligence companies are now forming expressly to design and install expert systems for corporate clients. They include Syntelligence, Smart Systems, Intelligenetics, Cognitive Systems and APEX (for Applied Expert Systems).\nThe ''granddaddy,'' only two years old, is Teknowledge. Formed in Palo Alto by 20 computer scientists from Stanford University, Carnegie-Mellon University and the Massachusetts Institute of Technology, it has developed 10 expert systems currently in operation. ''Drilling Advisor,'' for example, was developed for Elf-Aquitaine to diagnose the causes of sticking problems on oil rigs.\nSuch a system can save clients a great deal of money, says John Vermes, director of knowledge-systems marketing for Teknowledge, but it costs them a great deal as well. He estimates that a basic core system can cost from $200,000 to $500,000, exclusive of the value of time spent interviewing the expert and training personnel. ''When all is said and done,'' he says, ''the cost of the first stage is about $1 million.'' And integrating the system with the client's data sources, he says, can make the price tag virtually ''open-ended,'' doubling or even tripling the cost.\n\u00a0Job Elimination Feared\nIn expert systems, as in robotics and computer applications in general, a common fear is that people will be replaced. Once Mr. Smith retires from G.E., for example, he will still be available in the form of CATS- 1. Therefore why expect someone else to grow into his position?\nG.E. officials say that its expert system simply assists in training - it is an ''explanation system,'' they say, not a magic black box - and just as students can eventually surpass their human teachers, trouble-shooters so trained may grow beyond the programmed approximation of one expert's knowledge.\nThe G.E. expert, Mr. Smith, concedes that the original intent of the system was for any repair shop to plug it in and have an in-house expert like himself. But it evolved from a prompting aid to a teaching tool as field trials progressed, he says.\n''You could take a kid from the Pizza Hut and teach him how to approach maintenance problems,'' says Mr. Smith, ''and you could also retrain an experienced person in a subtle and polite way.''\nCompany officials contend, moreover, that the system will be attractive to professionals because it allows them to concentrate on more interesting tasks. ''It frees the expert from diddly problems,'' says Mr. Lynch, ''and enables him to choose the work he likes best.''\n","385":"Demos, talks and a paper-plate dinner buffet were the fare last Friday evening at the Computer Museum in Mountain View, Calif., and the subject was the high-tech future of health care. The gathering was hosted by FutureMed, a health-care program that is part of Singularity University, a networked organization dedicated to exploring how disruptive technologies can sweep across whole industries and society.\nThe technologies on display were impressive, often inspiring  -- like the wearable-robots, or mechanical exoskeletons, made by Ekso Bionics, to enable people with spinal cord injuries to walk again; or I.B.M.'s Watson question-answering computer that is being morphed into a doctors' smart assistant.\nDr. Daniel Kraft, executive director of the FutureMed program, pointed to a series of fast-changing technologies including biotechnology, nanotechnology, robotics, artificial intelligence and the surge in new data to mine for insights, or Big Data. \"Exponential technologies are all around us,\" Dr. Kraft said.\nDr. Martin Kohn, chief medical scientist at I.B.M. research, sketched out the future path in health care for the technology behind Watson, the computer that last year outwitted the best human players of Jeopardy!, the TV question-answering game. \"You'll not be surprised to learn that the executive leaders of I.B.M. fairly quickly decided that playing Jeopardy! was not a long-term business model,\" Dr. Kohn told the audience of a few hundred people.\nBut the core transferable technology, Dr. Kohn explained, was the artificial intelligence software that made it possible for Watson to read and understand 200 million digital pages, and deliver an answer within three seconds. In health care, Dr. Kohn said, \"we are overwhelmed by information. And we're only as good as what we know.\"\nSo Dr. Watson, it is. \nWatson, he added, is not going to make diagnoses, not give a physician a single answer, but make suggestions, recommendations and determine probabilities. The more information Watson is fed, Dr. Kohn said, the more it learns and understands, in its way. \nOne area where the technology's learning and recommendation capabilities may be particularly useful is in determining treatment regimens for patients with more than one chronic condition. And such patients account for a large share of the nation's health care costs.\nThere are well-defined treatment guidelines, Dr. Kohn said, for individual conditions like heart disease, diabetes, asthma and emphysema. But the guidelines are far less helpful for patients with more than one condition. For example, a beta-blocker drug is good for heart disease, but bad for asthma, Dr. Kohn noted. What are the trade-offs and what are the probabilities?\nWatson, Dr. Kohn said, can \"really help us learn about these multiply-challenged patients.\" In general, he added, Watson can be a powerful tool in moving toward the long-sought goal in health care of making more decisions based on data and a surer grasp of the relevant scientific evidence -- so-called evidence-based medicine -- instead of experience and intuition.\nThe Watson technology, Dr. Kohn added, has the potential to be a \"profound enabler of the transformation of health care.\"\nAs do some of the other technologies displayed at FutureMed. But whether that happens or not, I'd suggest, has at least as much to do with economics and policy as technology.\nLet's take Watson as an example. It's first big test-run in health care is with Wellpoint, an insurer. I know, I know, there are no \"insurers\" anymore, only health management organizations. But an insurer makes money by maximizing revenue -- premiums -- and minimizing expenses -- procedures, lab tests, hospitalizations and treatment.\nWatson will do what it's programmed to do. Eliminating unnecessary, and often repetitive, medical tests is a big cost-saving target -- and a good one, for patients and for health-care budgets. But the research on this subject has concluded that about half of patients get too much treatment and about half get too little.\nThe notion that a technology like  Watson, if unbiased, is going to reduce health care spending significantly seems misguided. But such technologies can make a useful difference if the right economic incentives are in place. And that, of course, is what the drift toward \"accountable care organizations\" is about. That is, a system in which doctors, hospitals and insurers are paid for helping people live healthier lives.\nIf that happens, the nation's health care bill won't necessarily fall. People living healthier and presumably longer lives will be consuming health care services for more years. But at least the money would be spent more wisely.\n\n","386":"MACHINERY OF THE MIND: Inside the New Science of Artificial Intelligence. By George Johnson. (Times Books, $19.95.) If a computer can be made to execute any precisely described process, and if human thought is a process that can be precisely described, a computer can be made to think.\u00a0That is the founding principle of one of the most provocative areas of current research -artificial intelligence. George Johnson's very readable book adeptly sizes up competing technologies, sketches philosophical and procedural debates and gives a primer of computing, an outline history of artificial intelligence research and a fascinating depiction of science when it is conducted with business (and the Defense Department) hovering outside the laboratory door. Mr. Johnson's journalistic approach has drawbacks; he tends to avoid extensive analysis, and his discussion of important topics is sometimes cursory. However, in a field where science fiction and fact are often difficult to distinguish - as one researcher said recently, ''It's a very real challenge to try to separate the essence from the hype, and get a sense of clarity [about] what the issues are'' - this book should be instrumental in clarifying issues for scientists and laymen alike.\n","387":"\u00a0\nBuried in your recent article regarding self-evolving computer systems [\"System Creates 'Robotic Life'--Automatically,\" front page, Aug. 31] was a particularly disturbing and anti-human statement by the head of MIT's Artificial Intelligence Lab that the developments described in the article (viz., the creation of a robot by a computer) are \"a long-awaited and necessary step toward the ultimate dream of self-evolving machines.\"\nThis would lead one to believe that a long and spirited national debate had resolved that self-evolving machines are a good thing.\u00a0Just the opposite is true. This debate has raged in American cinema for years and has concluded that these devices are a menace and a threat. If Hollywood has taught us anything, it is that intelligent machines--whether they be \"Blade Runner's\" replicants, \"2001's\" HAL or even \"Star Trek's\" the Borg--are universally fixated on turning master into slave. The statement by one of the scientists interviewed for your article, \"This isn't some sort of out-of-control Terminator,\" is clearly false and irresponsible. This is in fact exactly like Terminator.\nPerhaps the gentleman from MIT should have said that the recent achievements are \"a long-awaited and necessary step toward a world in which humanity quakes under the whip of robotic enslavement.\"\n--P. Matthew Gillen\n--Christian Klein\n\n\n","388":" ABSTRACT:Former UCLA psychiatry professor Kenneth Colby has developed $200 computer program called Overcoming Depression for his tiny software company, Malibu Artificial Intelligence Works (M)\n","389":"Google has launched a project to use artificial intelligence to create compelling art and music, offering a reminder of how technology is rapidly changing what it means to be a musician, and what makes us distinctly human.\nGoogle's Project Magenta, announced earlier this month, aims to push the state of the art in machine intelligence that is used to generate music and art.\n\"We don't know what artists and musicians will do with these new tools, but we're excited to find out,\" Douglas Eck, the project's leader, said in a blog post. Just as Louis Daguerre and George Eastman did not predict what Annie Leibovitz or Richard Avedon would do, \"surely Rickenbacker and Gibson didn't have Jimi Hendrix or St. Vincent in mind.\"\u00a0\nGoogle has already released a song demonstrating the technology. The song was created with a neural network - a computer system loosely modeled on the human brain - which was fed recordings of a lot of songs. With exposure to tons of examples, the neural network soon begins to realize which note should come next in a sequence. Eventually, the neural network learns enough to generate entire songs of its own.\nThe project has just begun, so the only available tools are for musicians with machine-learning expertise. Google hopes to produce - along with contributors from outside Google - more tools that will be useful to a broad group, including artists with minimal technical expertise.\nEfforts to use computers to make music stretch back decades. But experts say what is unique here is the extent of Google's computing power and its decision to share its tools with everyone, which may accelerate innovation.\n\"It's a potential game-changer because so many academics and developers in companies can get their hands on this library and can start to create songs and see what they can do,\" said Gil Weinberg,  director of Georgia Tech's center for music technology.\nDavid Cope, a retired professor at the University of California at Santa Cruz and pioneer in computer-generated music, said it is inevitable that one day, the best composers will use artificial intelligence to aid their work.\n\"It's going to rampage through the film music industry,\" Cope said. \"It's going to happen just as cars happened and we didn't have the horse and buggy anymore.\" He is confident in this given the exponential growth of computing power, which for decades has doubled about every two years.\nWith digital tools improving so quickly, it has  become difficult for musicians to stay on the cutting edge while also mastering their instruments of choice.\n\"The violinist uses the same instrument for a whole career, potentially, and they develop the kind of virtuosity on that instrument because they have that intimate relationship with it day after day for years and years,\" said Peter Swendsen, an Oberlin College professor of computer music and digital arts. \"Software comes and goes in weeks sometimes.\"\nAmper Music is a start-up that, like Google, is interested in harnessing the latest software to create music. Amper uses artificial intelligence to create original songs that match the emotions a video producer wants to convey in their work. Creating the music takes only seconds.\n\"If you take the sum of everything that has affected music historically and add them together, in 20 or 30 years, I think you'd look back and say, 'Wow, music AI rivals all of that,' \" said its co-founder, Drew Silverstein.\nFor now, the potential of music made with artificial intelligence is still largely unrealized. Silverstein is just beginning to tap the entertainment market in Los Angeles. The song Google's Magenta project released recently demonstrates what it is currently capable of, but also how much work lies ahead.\n\"It is indeed very basic,\" Swendsen said after listening to the song. \"That's not to say that the system they are using doesn't hold lots of promise or isn't working on a much deeper level than a simple random generator.\"\nThe emerging power of this technology is also a wake-up call for what makes us really human.\n\"A lot of the uniqueness that we like to ascribe to ourselves becomes threatened,\" said George Lewis, a professor of American music at Columbia University. \"People have to get the idea out of their head that music comes from great individuals. It doesn't; it comes from communities, it comes from societies. It develops over many years, and computers become a part of societies.\"\nAs machines have become more a part of our lives, we can count on them to share a hand in the artistic process. For the 75-year-old Cope, this is a great thing and nothing to be afraid of.\n\"The computer is just a really, really high-class shovel,\" he said. \"I love this new stuff and want it to come fast enough so I'm not dead when it happens.\"\nmcfarlandm@washpost.com\n","390":"If humanity survives the rise of artificial intelligence, the\u00a0ravages of climate change and the threat of nuclear terrorism in the next century, it doesn't mean we're home free, according to\u00a0Stephen Hawking.\nThe renowned theoretical physicist has gone as far as providing humanity with a deadline for finding another planet to colonize: We have 1,000 years.\nRemaining on Earth any longer, Hawking believes, places humanity at great risk of encountering another mass extinction.\u00a0\n\"We must ... continue to go into space for the future of humanity,\" the 74-year-old Cambridge professor said during a speech Tuesday at Oxford University Union, according to the Daily Express.\n\"I don't think we will survive another 1,000 years without escaping beyond our fragile planet,\" he added.\nDuring his hour-long speech, Hawking told the audience that Earth's cataclysmic end may be hastened by humankind, which will continue to devour the planet's resources at unsustainable rates, the Express reported.\nHis wide-ranging talk touched upon the origins of the universe and Einstein's theory of relativity, as well as humanity's creation myths and God. Hawking also discussed \"M-theory,\" which Leron Borsten of PhysicsWorld.com explains as \"proposal for a unified quantum theory of the fundamental constituents and forces of nature.\"\nThough the challenges ahead are immense, Hawking said, it is a\u00a0\"glorious time to be alive and doing research into theoretical physics.\"\n\"Our picture of the universe has changed a great deal in the last 50 years, and I am happy if I have made a small contribution,\" he added.\nChamber awaiting a unique and hugely exciting event - Professor Stephen Hawking on 'The Origins of the Universe.' pic.twitter.com\/LwICFfH269\nSpeaking to audience members in a\u00a0public Q&A session ahead of the annual BBC Reith Lectures, Hawking also said that leaving the planet behind was our best hope for survival.\nThe key, he noted, was surviving the precarious century ahead.\n\"Although the chance of a disaster to planet Earth in a given year may be quite low, it adds up over time, and becomes a near certainty in the next thousand or ten thousand years. By that time we should have spread out into space, and to other stars, so a disaster on Earth would not mean the end of the human race.\"\nStephen Hawking: Remember to look up at the stars, not at your feet. Try to make sense of the wonder that is around you.\nSince 2009, NASA has been hunting for Earthlike planets with the potential for human colonization.\nResearchers have discovered more than 4,600 \"candidate\" planets and another 2,300 or so confirmed planets, according to the agency.\n\"The first exoplanet orbiting another star like our sun was discovered in 1995,\"\u00a0according to NASA.\u00a0\"Exoplanets, especially small Earth-size worlds, belonged within the realm of science fiction just 21 years ago.\u00a0Today, and thousands of discoveries later, astronomers are on the cusp of finding something people have dreamt about for thousands of years.\"\nBefore we have a chance to relocate, Hawking says, we'll first need to solve the potential threat created by technology.\nWhile Hawking thinks technology has the capacity to ensure mankind's survival, previous statements suggest the cosmologist is simultaneously grappling with the potential threat it poses. When it comes to discussing that threat, Hawking is unmistakably blunt.\n\"I think the development of full artificial intelligence could spell the end of the human race,\" Hawking\u00a0told the BBC\u00a0in a 2014 interview that touched upon everything from online privacy to his affinity for his robotic-sounding voice.\nDespite its current usefulness, he cautioned, further developing A.I.could prove a fatal mistake.\n\"Once humans develop artificial intelligence, it will take off on its own and redesign itself at an ever-increasing rate,\" Hawking warned in recent months. \"Humans, who are limited by slow biological evolution, couldn't compete and would be superseded.\"\nTh\n              MORE READING:\u00a0           \n Bill Gates on dangers of artificial intelligence: 'I don't understand why some people are not concerned' \n Pluto's icy heart may hide an underground ocean \n Apple co-founder on artificial intelligence: 'The future is scary and very bad for people' \n This 6,000-year-old amulet is the oldest example of a technology still used by NASA \n Elon Musk: Human-driven cars may be outlawed because they're 'too dangerous' \n","392":"The titans of tech don't talk about it, but entrepreneurial Silicon Valley was built on government spending, in particular Defense Department funding for things like radar, semiconductors and what became the Internet. Everybody was happy about that.\u00a0\nNow maybe the Valley is returning the favor, building out the future of defense. The happiness of this has yet to be determined. \n  As John Markoff writes, Secretary of Defense Ashton B. Carter just paid his fourth visit to the Valley. Fittingly, he spoke near Google's headquarters, and talked about the importance of putting artificial intelligence, or A.I., in our nation's weapons.\n  He was speaking at something called the Defense Innovation Unit Experimental facility, which was started a year ago in Mountain View, Calif. Technology leaders used to come to Washington, but now Washington comes to them, too.\n  It was, as one of the Valley's more cynical turns of phrase has it, a ''buzzword-compliant'' talk. Besides A.I., Mr. Carter talked about how the new facility was ''iterating rapidly,'' which means trying a lot of different stuff fast, visiting ''innovation hubs,'' and even ''thinking outside our five-sided box,'' presumably a reference to the Pentagon, though with floor and ceiling, that box has seven sides.\n  Talking the local talk will likely do little to soothe the many critics, inside and outside tech, who worry about outsourcing wars to machines. Autopilots on airplanes are one thing, but if computers start optimizing for lethality, it may be difficult to limit conflicts.\n  Equally, there are concerns about what other large powers are doing in robotics and A.I. The United States leads the world in A.I., and drawing from Valley companies may be a way to keep an edge. Yet even there, a face-off between two A.I.-powered adversaries could mean an unforeseen escalation or other type of complication that could overwhelm the humans charged with monitoring.\n  The moral dimensions and unintended consequences of pilot stress seen with America's drone program may be but a foretaste of what we'll face with A.I. violence\n  Expect to see a lot more of the Pentagon in the Valley, and in other parts of tech as well. On Tuesday, Air Force Maj. Gen. Sarah Zabel, the vice director of the Defense Information Systems Agency, gave a one-hour talk to a computer networking group on the security and communications needs of her 4.5 million-user communications system.\n  She was worried about hackers, another whole new kind of adversary, in a world turned upside-down by tech.\n\n\n\n","393":"          Feng Xiang, a professor of law at Tsinghua University, is one of China's most prominent legal scholars. He spoke at the\u00a0Berggruen Institute's China Center\u00a0workshop on artificial intelligence in March in Beijing.       \nBEIJING - The most momentous challenge facing socio-economic systems today is the arrival of artificial intelligence. If AI remains under the control of market forces, it will inexorably result in a super-rich oligopoly of data billionaires who reap the wealth created by robots that displace human labor, leaving massive unemployment in their wake.\nBut China's socialist market economy could provide a solution to this. If AI rationally allocates resources through big data analysis, and if robust feedback loops can supplant the imperfections of \"the invisible hand\" while fairly sharing the vast wealth it creates, a planned economy that actually works could at last be achievable.\u00a0\nThe more AI advances into a general-purpose technology that permeates every corner of life, the less sense it makes to allow it to remain in private hands that serve the interests of the few instead of the many. More than anything else, the inevitability of mass unemployment and the demand for universal welfare will drive the idea of socializing or nationalizing AI.\nMarx's dictum, \"From each according to their abilities, to each according to their needs,\" needs an update for the 21st century: \"From the inability of an AI economy to provide jobs and a living wage for all, to each according to their needs.\"\nEven at this early stage, the idea that digital capitalism will somehow make social welfare a priority has already proven to be a fairytale. The billionaires of Google and Apple, who have been depositing company profits in offshore havens to avoid taxation, are hardly paragons of social responsibility. The ongoing scandal\u00a0around Facebook's business model, which puts profitability above responsible citizenship, is yet another example of how in digital capitalism, private companies only look after their own interests at the expense of the rest of society.\nOne can readily see where this is all headed once technological unemployment accelerates. \"Our responsibility is to our shareholders,\" the robot owners will say. \"We are not an employment agency or a charity.\"\nThese companies have been able to get away with their social irresponsibility because the legal system and its loopholes in the West are geared to protect private property above all else. Of course, in China, we have big privately owned Internet companies like Alibaba and Tencent. But unlike in the West, they are monitored by the state and do not regard themselves as above or beyond social control.\nIt is the very pervasiveness of AI that will spell the end of market dominance. The market may reasonably if unequally function if industry creates employment opportunities for most people. But when industry only produces joblessness, as robots take over more and more, there is no good alternative but for the state to step in. As AI invades economic and social life, all private law-related issues will soon become public ones. More and more, regulation of private companies will become a necessity to maintain some semblance of stability in societies roiled by constant innovation.\nI consider this historical process a step closer to a planned market economy. Laissez-faire capitalism as we have known it can lead nowhere but to a dictatorship of AI oligarchs who gather rents because the intellectual property they own rules over the means of production. On a global scale, it is easy to envision this unleashed digital capitalism leading to a battle between robots for market share that will surely end as disastrously as the imperialist wars did in an earlier era.\nFor the sake of social well-being and security, individuals and private companies should not be allowed to possess any exclusive cutting-edge technology or core AI platforms. Like nuclear and biochemical weapons, as long as they exist, nothing other than a strong and stable state can ensure society's safety. If we don't nationalize AI, we could sink into a dystopia reminiscent of the early misery of industrialization, with its satanic mills and street urchins scrounging for a crust of bread.\nThe dream of communism is the elimination of wage labor. If AI is bound to serve society instead of private capitalists, it promises to do so by freeing an overwhelming majority from such drudgery while creating wealth to sustain all.\nIf the state controls the market, instead of digital capitalism controlling the state, true communist aspirations will be achievable. And because AI increasingly enables the management of complex systems by processing massive amounts of information through intensive feedback loops, it presents, for the first time, a real alternative to the market signals that have long justified laissez-faire ideology - and all the ills that go with it.\nGoing forward, China's socialist market economy, which aims to harness the fruits of production for the whole population and not just a sliver of elites operating in their own self-centered interests, can lead the way toward this new stage of human development.\nIf properly regulated in this way, we should celebrate, not fear, the advent of AI. If it is brought under social control, it will finally free workers from peddling their time and sweat only to enrich those at the top. The communism of the future ought to adopt a new slogan: \"Robots of the world, unite!\"\n                          This was produced by                                                  The WorldPost                                                 , a partnership of the                                                  Berggruen Institute                                                  and The Washington Post.                    \n","394":"While it's true that \"a royal flush always beats a full house\" in poker, as stated in the Feb. 6 Capital Business article \"An artificial intelligence gamble that paid off,\" folks who are not especially familiar with poker-hand hierarchy may wrongly assume that it is the only way to beat a full house. Four-of-a-kind and straight-flush hands also beat full houses. \u00a0\nBut the larger matter of concern to some of us regular dining-room-table, friendly-game poker players is that artificial intelligence programs are making significant progress in stealing the human element from our games.\nI've played in a regular game for more than 50years and view poker as a metaphor for life. It helps teach concepts such as playing the hand you were dealt in the best way possible, playing by the rules, and knowing when to hold, when to fold, when to bluff, when to bet aggressively and, sometimes, how to sense a bluff by another person.  \nPoker not only helps a person figure the odds but also teaches manners. There are some things one simply does not do at the poker table, such as touch another person's chips, whine, cheat or make gratuitous comments after you've already folded. Also, don't spill or drip your beer or soda over the cards.\nEd Nanas, Gainesville\n","395":"          Kai-Fu Lee is the chairman of Sinovation Ventures and the president of its Artificial Intelligence Institute. He was the founding president of Google China.       \nBEIJING - According to my estimate, roughly half of all jobs will disappear in the next decade. Routine and mechanical occupations will soon be replaced by artificial intelligence and robots at an unprecedented rate across the globe. This enormous challenge has breathed new life into an old idea called universal basic income, or UBI, in which the government provides a steady stipend for each citizen regardless of need, employment status or skill level.\nSo should we blindly give $10,000 to everyone then? Absolutely not.\u00a0\nThe optimists naively assume that UBI will be a catalyst for people to reinvent themselves professionally. But UBI makes sense only when Silicon Valley leaders project their own personas onto all the workers who will be displaced. Indeed, if we're talking about a successful Silicon Valley entrepreneur, his or her skills, entrepreneurship, experience and network may successfully help turn a modest UBI income into the next tech empire. But this most surely will not happen for the masses of displaced workers with obsolete skills living in regions where job loss is exacerbated by traditional economic downturn.\nFree money given out to displaced workers may serendipitously help a few find work, but more likely, they will just move on to a new job that will also be eliminated soon. This is a familiar pattern we have seen throughout the workforce. The truck driver who once worked on an assembly line in a factory will soon have to learn, yet again, to become something else. Most displaced workers will not have the foresight to predict which professions might survive the AI revolution and therefore won't know how to best use the UBI money to ensure a stable future.\nInstead of just redistributing cash and hoping for the best, we must work together to find a comprehensive solution, including establishing new professions, values and social norms. And we need to retrain and adapt so that everyone can find a suitable profession. Here are five imperatives to get us there:\n          1. Guarantee subsistence.                  \nThe first priority is to create and support programs to ensure no one goes hungry or without shelter and basic health care.\n          2. Maximize creative jobs.       \nOnly humans can create and come up with new innovations. AI today cannot think outside the box, and it can only optimize problems defined by humans. Thus, in early education, we must ensure that the system does not inhibit curiosity, creativity, critical thinking and individuality. In middle school and high school, we should increase funding for gifted and talented programs. In colleges, we need programs that help students with creative potential learn to master AI tools.\n          3. Increase social jobs.       \nAI cannot express love and empathy and cannot build irreplaceable social connections - only people can. And unlike creative professions, which cannot easily be taught, people can be trained in a large number of social professions. Furthermore, with the projected trillions of dollars I predict AI will generate, consumer spending will go up and with it, spending on people-to-people services.\nThis means service jobs that require a human touch - social workers, therapists, teachers and life coaches - will be in high demand. Furthermore, new social jobs will be invented in which humans will serve as an intermediary of sorts between patrons and AI. For example, a medical consultant might help patients troubleshoot and give supplemental advice when using an AI diagnostic tool. Some of these jobs will even be highly paid.\n          4. Encourage voluntarism.       \nWe need to create more volunteer programs to assist retired and displaced workers with little interest in or skillset for higher skilled professions. We should also consider compensating people who volunteer.\n          5. Redefine work ethic.       \nEveryone has a need to feel a sense of self-worth and self-actualization - that he or she believes his or her existence is meaningful. Unfortunately, the Industrial Revolution wrongfully instilled a social norm that self-worth should primarily come from work ethic - if you work hard, you will be rewarded. But because of AI, jobs based on repetitive tasks will soon be gone forever.\nWe need to redefine the idea of work ethic for the new workforce paradigm. The importance of a job should not be solely dependent on its economic value but should also be measured by what it adds to society. We should also reassess our notion that longer work hours are the best way to achieve success and should remove the stigma associated with service professions.\nThe coming AI revolution will bring about either the best of times or the worst of times. The outcome will depend on whether we choose to be intoxicated by naive optimism or committed to comprehensive problem solving. UBI is clearly not enough. We need to debate earnestly and experiment rapidly. Only then will this amazing revolution lead us to a creative renaissance.\n          This was produced by              The WorldPost                    , a partnership of the              Berggruen Institute                     and The Washington Post.       \n","397":"Marvin Minsky, a founding father of the field of artificial intelligence and an innovative explorer of the mysteries of the human mind during his long tenure at the Massachusetts Institute of Technology, died Jan. 24 at a hospital in Boston. He was 88.\nThe cause was a cerebral hemorrhage, according to a statement from MIT. He was a professor emeritus at MIT's Media Lab, which has a broad, interdisciplinary mandate to explore technology, multimedia and design.\nDr. Minsky devoted his professional life to the astonishing hypothesis that engineers could someday create an intelligent machine. He flourished as a professor and mentor even as the field of A.I. endured discouraging results and eruptions of pessimism.\nHe lived long enough to see A.I. ambitions flourishing anew, with attendant concerns about killer robots and rogue computers.\u00a0\nAlthough Dr. Minsky was an inventor - as a young man, he developed a special microscope for studying brain tissue that eventually became a standard tool for scientists - his greatest contributions were theoretical. He developed a concept of intelligence as something that emerged from disparate mental agents acting in coordination. No single agent is intelligent when operating alone.\nIf a single word could encapsulate Dr. Minsky's career, it would be \"multiplicities,\" his MIT colleague and former student Patrick Winston said Tuesday. The word \"intelligence,\" Dr. Minsky believed, was a \"suitcase word,\" Winston said, because \"you can stuff a lot of ideas into it.\" Other such words include \"creativity\" and \"emotion.\"\nAlong with fellow A.I. pioneer John McCarthy, he founded the artificial-intelligence lab at MIT in 1959. Dr. Minsky's 1960 paper, \"Steps Toward Artificial Intelligence,\" laid out many of the routes researchers would take in the decades to come.\nHe wrote that \"we are on the threshold of an era that will be strongly influenced, and quite possibly dominated, by intelligent problem-solving machines.\" Anyone trying to mimic intelligence in a machine, he wrote, had to solve five distinct categories of problems: search, pattern recognition, learning, planning and induction.\nHe also wrote seminal books - including \"The Society of Mind\" (1986) and \"The Emotion Machine\" (2006) - that colleagues consider essential to understanding the challenges in creating machine intelligence.\nUpon Dr. Minsky's death, his colleague Nicholas Negroponte wrote by email to the MIT community:\n\"The world has lost one of its greatest minds in science. As a founding faculty member of the Media Lab he brought equal measures of humour and deep thinking, always seeing the world differently. He taught us that the difficult is often easy, but the easy can be really hard.\"\nMarvin Lee Minsky was born in New York City on Aug. 9, 1927. His father, Henry, was a noted eye surgeon who served as director of Mount Sinai Hospital's ophthalmology department in Manhattan. His mother, the former Fannie Reiser, was active in Zionist causes.\nAs a child, he told the New Yorker, he was \"physically terrorized\" by schoolyard bullies, and a lack of academic support in the classroom led his parents to enroll him in the progressive Fieldston School. His interest in electronics and chemistry blossomed, and he won a spot at the prestigious Bronx High School of Science in 1941.\nHe spent his senior year at the private Phillips Academy in Andover, Mass., to bolster his college options. After graduating in June 1945, he enlisted in the Navy in the final months of World War II and served in an electronics program.\nHe received his bachelor's degree in mathematics from Harvard University in 1950 and a PhD in mathematics fromPrinceton in 1954.\nAt Princeton, and with funding from the Office of Naval Research, Dr. Minsky co-built a primitive \"electronic learning machine\" with tubes and motors. He was also exposed to some of the greatest minds of the day, including John von Neumann, a pioneer of computers.\nBack at Harvard as a junior fellow in the mid-1950s, Dr. Minsky invented the confocal scanning microscope that would eventually find many uses in science.\n\"Minsky's invention disappeared from view for many years because the lasers and computer power needed to make it really useful had not yet become available,\" Winston wrote in an account of Dr. Minsky's career. \"About ten years after the original patent expired, it started to become a standard tool in biology and materials science.\"\nIn 1956, when the very idea of a computer was only a couple of decades old, Dr. Minsky attended a symposium at Dartmouth College that is considered the founding event in the field of artificial intelligence.\nDr. Minsky said in 2015 during an interview with The Washington Post that Alan Turing, the British mathematician who had worked on World War II code breaking, was the first person to bring respectability to the idea that machines could someday think.\n\"There were science-fiction people who made similar predictions, but no one took them seriously because their machines became intelligent by magic. Whereas Turing explained how the machines would work,\" Dr. Minsky said.\nThe MIT educator explained the problem further, and in 1969, the Association of Computing Machinery had given him the highest honor in computer science, the A.M. Turing Award.\nDr. Minsky and his wife, the former Gloria Rudisch, a pediatrician, enjoyed a partnership that began with their marriage in 1952. Their home became the regular haunt of science-fiction writers, including their friend Isaac Asimov. Richard Feynman, the Nobel Prize-winning physicist, would play the bongos at their parties.\nBesides his wife, survivors include three children, a sister and four grandchildren.\nGloria Minsky recalled her first conversation with the man she wound up marrying: \"He said he wanted to know about how the brain worked. I thought he is either very wise or very dumb. Fortunately, it turned out to be the former.\"\nDr. Minsky acknowledged in the 2015 interview with The Post that he was disappointed that A.I. research had yet to create human-level intelligence in a machine. He said early A.I. efforts at large companies, such as IBM, failed to appreciate the complexity of the problem and how incremental progress would have to be.\n\"It's interesting how few people understood what steps you'd have to go through. They aimed right for the top, and they wasted everyone's time,\" he said.\nAre machines going to become smarter than human beings and, if so, is that a good thing?\n\"Well, they'll certainly become faster,\" he said. \"And there's so many stories of how things could go bad, but I don't see any way of taking them seriously because it's pretty hard to see why anybody would install them on a large scale without a lot of testing.\"\njoel.achenbach@washpost.com\n","398":"The Xerox Corporation has spun off its artificial-intelligence business as an independent company, the Envos Corporation.\u00a0Envos, which is based in Mountain View, Calif., will be owned by its employees, with Xerox and a British affiliate holding minority stakes.\nEnvos will develop and market software tools for creating artificial-intelligence applications and will support and maintain Xerox's current software. It will also sell and support Xerox's artificial-intelligence work stations. The company also said it had produced versions of two Xerox-developed software tools to run on work stations produced by Sun Microsystems Inc. Envos said it would develop future software products for Sun's work stations.\n","399":"SINTRA, Portugal -- The rise of robots has long been a topic for sci-fi best sellers and video games and, as of this week, a threat officially taken seriously by central bankers.\nThe bankers are not yet ready to buy into dystopian visions in which robots render humans superfluous. But, at an exclusive gathering at a golf resort near Lisbon, the big minds of monetary policy were seriously discussing the risk that artificial intelligence could eliminate jobs on a scale that would dwarf previous waves of technological change.\n\"There is no question we are in an era of people asking, 'Is the Robocalypse upon us?'\" David Autor, a professor of economics at the Massachusetts Institute of Technology, told an audience on Tuesday that included Mario Draghi, the president of the European Central Bank, James Bullard, president of the Federal Reserve Bank of St. Louis, and dozens of other top central bankers and economists.\nThe discussion occurred as economists were more optimistic than they had been for a decade about growth. Mr. Draghi used the occasion to signal that the European Central Bank is edging closer to the day when it will begin paring measures intended to keep interest rates very low and bolster the economy.\n\"All the signs now point to a strengthening and broadening recovery in the euro area,\" Mr. Draghi said.His comments pushed the euro to almost its highest level in a year, though it later gave up some of the gains.\nBut along with the optimism is a fear that the economic expansion might bypass large swaths of the population, in part because a growing number of jobs could be replaced by computers capable of learning -- artificial intelligence.\nPolicy makers and economists conceded that they have not paid enough attention to how much technology has hurt the earning power of some segments of society, or planned to address the concerns of those who have lost out. That has, in part, nourished the political populism that contributed to Britain's vote a year ago to leave the European Union, and the election of President Trump.\n\"Generally speaking, economic growth is a good thing,\" Ben S. Bernanke, former chairman of the Federal Reserve, said at the forum. \"But, as recent political developments have brought home, growth is not always enough.\"\nIn the past, technical advances caused temporary disruptions but ultimately improved living standards, creating new categories of employment along the way. Farm machinery displaced farmworkers but eventually they found better paying jobs, and today their great-grandchildren may design video games.\nBut artificial intelligence threatens broad categories of jobs previously seen as safe from automation, such as legal assistants, corporate auditors and investment managers. Large groups of people could become obsolete, suffering the same fate as plow horses after the invention of the tractor.\n\"More and more, we are seeing economists saying, 'This time could be different,'\" said Mr. Autor, who presented a paper on the subject that he wrote with Anna Salomons, an associate professor at the Utrecht University School of Economics in the Netherlands.\nCentral bankers have begun examining the effect of technology on employment because it might help solve several economic quandaries.\nWhy is workers' share of total earnings declining, even though unemployment is at record lows and corporate profits at record highs? Why is productivity -- the amount that a given worker produces -- stuck in neutral?\n\"The mere fact that we are organizing this conference here in Sintra testifies to our interest in that discussion,\" Beno\u00eet Coeur\u00e9, a member of the European Central Bank's executive board, said in an interview, referring to the \"Robocalypse\" debate.\nOf particular interest to the European Central Bank is why faster economic growth has not caused wages and prices to rise. The central bank has pulled out all the stops to stimulate the eurozone economy, cutting interest rates to zero and even below, while printing money. Four years of growth have led to the creation of 6.4 million jobs. Yet inflation remains well below the bank's official target of below, but close to, 2 percent.\nOne explanation is that more work is being done by advanced computers, with the rewards flowing to the narrow elite that owns them.\nStill, among the economists in Sintra there was plenty of skepticism about whether the Robocalypse is nigh.\nSince the beginning of the industrial age, almost every major technological innovation has led to dire predictions that humans were being permanently replaced by machines.\nWhile some kinds of jobs were lost forever, greater efficiency led to more affordable goods and other industries soaked up the excess workers. Few people alive today would want to return to the late 1800s, when 40 percent of Americans worked on farms.\nRobocalypse advocates underestimate the power of scientific advances to beget more scientific advances, said Joel Mokyr, a professor at Northwestern University who studies the history of economics.\n\"Think about what computers are doing to our ability to discover science,\" Professor Mokyr said during a panel discussion, citing computers that can solve equations that have baffled mathematicians for decades. There may be breakthroughs that \"we can't even begin to imagine.\"\nThere are other explanations for stagnant wages besides technology.\nCompanies in Japan, the United States and Europe are sitting on hoards of cash, doling out the money to shareholders rather than investing in new buildings, equipment or innovative products. Just why is another topic of debate.\nHal Varian, the chief economist at Google -- whose self-driving technology may someday make taxi drivers unnecessary -- said that the plunging cost of information technology \"has virtually eliminated the fixed cost of entering a business.\" Companies can rent software and computing power over the internet.\nAnd flat wages reflect the large number of women who have entered the work force in recent decades as well as the post World War II baby boom, Mr. Varian said, adding that those trends have run their course. \"We are going to see a higher share going to labor,\" he said.\nYet already, disruptions caused by technology help account for rampant pessimism among working-class and middle-class people across the developed world.\nMr. Bernanke referred to polls showing that about twice as many Americans say the United States is on the wrong track than say the country is moving in the right direction.\nAs a result, \"last November Americans elected as president a candidate with a dystopian view of the economy,\" Mr. Bernanke said.\nMr. Autor, co-author of the Robocalypse paper, concluded that it was too early to say that robots are coming for people's jobs. But it could still happen in the future.\n\"I say not Robocalypse now,\" Mr. Autor said, \"perhaps Robocalypse later.\"\nFollow Jack Ewing on Twitter @JackEwingNYT.\nPHOTO: Workers at Zalando, an online store, prepare orders at a hub in Erfurt, Germany. Zalando invests heavily in hubs and automation. (PHOTOGRAPH BY PHIL HATCHER-MOORE FOR THE NEW YORK TIMES)Related Articles\n\n","400":"  Elon Musk has dreams of linking the human brain directly to the internet. \n  The science-fiction-like artificial intelligence - imagine your mind commanding Google to search for something - is the concept behind Musk's AI startup, Neuralink, which would initially produce chips to be implanted in human brains to help folks overcome brain injuries. \u00a0\n  After meeting that objective, which Musk predicted would take four years, Neuralink would pursue a \"whole-brain interface.\" \n  That interface would connect the brain to the internet, where information would flow seamlessly between computers and other brains, according to Musk. \n  The objective will be to ward off dangerous applications of artificial intelligence, or AI, as machine learning eclipses the ability of humans to keep pace. \n  \"We're going to have the choice of either being left behind and being effectively useless or like a pet - you know, like a house cat or something - or eventually figuring out some way to be symbiotic and merge with AI,\" Musk said. \n","401":"Google and NASA are forming a laboratory  to study artificial intelligence by means of computers that use the unusual properties of quantum physics. Their quantum computer, which performs complex calculations thousands of times faster than existing supercomputers, is expected to be in active use in the third quarter of this year.\nThe Quantum Artificial Intelligence Lab, as the entity is called, will focus on machine learning, which is the way computers take note of patterns of information to improve their outputs. Personalized Internet search and predictions of traffic congestion based on GPS data are examples of machine learning. The field is particularly important for things like facial or voice recognition, biological behavior, or the management of very large and complex systems.\u00a0\n\"If we want to create effective environmental policies, we need better models of what's happening to our climate,\" Google said in a blog post announcing the partnership. \"Classical computers aren't well suited to these types of creative problems.\"\nGoogle said it had already devised machine-learning algorithms that work inside the quantum computer, which is made by D-Wave Systems of Burnaby, British Columbia. One could quickly recognize information, saving power on mobile devices, while another was successful at sorting out bad or mislabeled data. The most effective methods for using quantum computation, Google said, involved combining the advanced machines with its clouds of traditional computers.\nGoogle and NASA bought in cooperation with the Universities Space Research Association, a nonprofit research corporation that works with NASA and others to advance space science and technology. Outside researchers will be invited to the lab as well. \nThis year D-Wave sold its first commercial quantum computer to Lockheed Martin. Lockheed officials said the computer would be used for the test and measurement of things like jet aircraft designs, or the reliability of satellite systems.\nThe D-Wave computer works by framing complex problems in terms of optimal outcomes. The classic example of this type of problem is figuring out the most efficient way a traveling salesman can visit 10 customers, but real-world problems now include hundreds of such variables and contingencies. D-Wave's machine frames the problem in terms of energy states, and uses quantum physics to rapidly determine an outcome that satisfies the variables with the least use of energy. \nIn tests last September, an independent researcher found that for some types of problems the quantum computer was 3,600 times faster than traditional supercomputers. According to a D-Wave official, the machine performed even better in Google's tests, which involved 500 variables with different constraints.\n\"The tougher, more complex ones had better performance,\" said Colin Williams, D-Wave's director of business development. \"For most problems, it was 11,000 times faster, but in the more difficult 50 percent, it was 33,000 times faster. In the top 25 percent, it was 50,000 times faster.\" Google declined to comment, aside from the blog post.\nThe machine Google and NASA will use makes use of the interactions of 512 quantum bits, or qubits, to determine optimization. They plan to upgrade the machine to 2,048 qubits when this becomes available, probably within the next year or two. That machine could be exponentially more powerful. \nGoogle did not say how it might deploy a quantum computer into its existing global network of computer-intensive data centers, which are among the world's largest. D-Wave, however, intends eventually for its quantum machine to hook into cloud computing systems, doing the exceptionally hard problems that can then be finished off by regular servers.\nPotential applications include finance, health care, and national security, said Vern Brownell, D-Wave's chief executive. \"The long-term vision is the quantum cloud, with a few high-end systems in the back end,\" he said. \"You could use it to train an algorithm that goes into a phone, or do lots of simulations for a financial institution.\" \nMr. Brownell, who  founded a computer server company, was also the chief technical officer at Goldman Sachs. Goldman is an investor in D-Wave, with Jeff Bezos, the founder of Amazon.com. Amazon Web Services is another global cloud, which rents data storage, computing, and applications to thousands of companies.\nThis month D-Wave established an American company, considered necessary for certain types of sales of national security technology to the United States government.\n\n","402":"HONG KONG -- It isn't looking good for humanity.\nThe world's best player of what might be humankind's most complicated board game was defeated on Tuesday by a Google computer program. Adding insult to potentially deep existential injury, he was defeated at Go -- a game that claims centuries of play by humans -- in China, where the game was invented.\nThe human contender, a 19-year-old Chinese national named Ke Jie, and the computer are only a third of the way through their three-game match this week. And the contest does little to prove that software can mollify an angry co-worker, write a decent poem, raise a well-adjusted child or perform any number of distinctly human tasks.\nBut the victory by software called AlphaGo showed yet another way that computers could be developed to perform better than humans in highly complex tasks, and it offered a glimpse of the promise of new technologies that mimic the way the brain functions. AlphaGo's success comes at a time when researchers are exploring the potential of artificial intelligence to do everything from drive cars to draft legal documents -- a trend that has some serious thinkers pondering what to do when computers routinely replace humans in the workplace.\n\"Last year, it was still quite humanlike when it played,\" Mr. Ke said after the game. \"But this year, it became like a god of Go.\"\nPerhaps just as notably, the victory took place in China, a rising power in the field of artificial intelligence that is increasingly seen as a rival to the United States. Chinese officials perhaps unwittingly demonstrated their conflicted feelings at the victory by software backed by a company from the United States, as they cut off live streams of the contest within the mainland even as the official news media promoted the promise of artificial intelligence.\nAlphaGo -- which was developed by DeepMind, the artificial intelligence arm of Google's parent, Alphabet Incorporated -- has already pushed assumptions about just how creative a computer program can be. Since last year, when it defeated a highly ranked South Korean player at Go, it changed the way the top masters played the game. Players have praised the technology's ability to make unorthodox moves and challenge assumptions core to a game that draws on thousands of years of tradition.\nIn the first game, Mr. Ke made several moves that commentators said were reminiscent of AlphaGo's own style. Wearing a blue tie and thick-framed black glasses, the boyish Mr. Ke kept things close in the early going. By AlphaGo's own assessment, it did not have a big statistical advantage until after the 50th move, according to a DeepMind co-founder, Demis Hassabis.\nMr. Ke, who smiled and shook his head as AlphaGo finished out the game, said afterward that his was a \"bitter smile.\" After he finishes this week's match, he said, he would focus more on playing against human opponents, noting that the gap between humans and computers was becoming too great. He would treat the software more as a teacher, he said, to get inspiration and new ideas about moves.\n\"AlphaGo is improving too fast,\" he said in a news conference after the game. \"AlphaGo is like a different player this year compared to last year.\"\nGo, in which two players vie for control of a board using black and white pieces called stones, is considered complex because of the sheer number of possible moves. Even supercomputers cannot simply calculate all possible moves, presenting a big challenge for AlphaGo's creators.\nAlphaGo instead relies on new techniques that help it learn from experience playing a large number of games. This time, Mr. Hassabis said, a new approach allowed AlphaGo to learn more by playing games against itself. In the future, computer scientists hope to use similar techniques to do many things, including improving fundamental scientific research and diagnosing illnesses.\nAlphaGo's victory represents a marketing success for Google and Alphabet. The Mountain View, Calif., software company pulled out of mainland China seven years ago rather than submit to the country's censorship requirements. But it has continued to express interest in the vast market, which has the world's largest population of internet users.\nNotably, the Go match took place in the city of Wuzhen, where the Chinese internet authorities hold an annual conference on cyberspace regulation.\nChina has been drawn to AlphaGo since its victory last year over a South Korean Go master, Lee Se-dol. Officials responsible for technology development in China responded quickly by creating new artificial-intelligence programs and making more funding available to researchers, according to Chinese computer science professors, marking a sort of Sputnik moment for the country.\n\"AlphaGo truly had a big impact\" in China, said Wang Shengjin, a professor at the department of electronic engineering at Tsinghua University in Beijing. \"Before, we would be discussing how to apply the technology, but it was hard to be clear exactly how to do it, so AlphaGo gave us a vivid example of that.\"\nStill, China showed some skittishness at game time. Despite huge interest, many Chinese became consternated when it became apparent there was no obvious live video of the event online. A site that follows Chinese censorship orders, China Digital Times, posted a translated notice from the government calling for all websites to block the broadcast.\n\"Anything that demonstrates that something special about China has turned out to be just another artificial intelligence problem that Google is better solving than any other company is additionally problematic,\" said Clay Shirky, a professor at New York University Shanghai, \"because it threatens the specialness of the culture.\"\nThe Cyberspace Administration of China, the country's main digital censor, did not respond to a request for comment.\nWhile AlphaGo had already racked up an impressive record against humans over the past year, the match against Mr. Ke offered a final showdown. Like AlphaGo, Mr. Ke had beaten South Korea's Mr. Lee in several recent major competitions.\nAfter Mr. Lee's loss, Mr. Ke had said publicly on Chinese social media that the program \"can't beat me.\"\nMr. Ke's tone changed earlier this year, after he lost three online speed games to the program. At that point, he said on Chinese social media that computers seemed to be showing that some of what humanity thought about the game was incorrect.\nMr. Ke will have two more chances to get the better of AlphaGo with games on Thursday and Saturday. Most experts do not give him much of a chance. But last year, his rival Mr. Lee surprised, winning one game against AlphaGo out of five after a brilliant and unconventional move stumped the software.\nCarolyn Zhang in Shanghai contributed research.\nPHOTO: Ke Jie, 19, the world's top Go player, during his match against AlphaGo. Go may be the world's most complicated board game. (PHOTOGRAPH BY WU HONG\/EUROPEAN PRESSPHOTO AGENCY)Related Articles\n\n","403":"SAN FRANCISCO -- Two technology booms -- some people might call them frenzies -- are combining to turn a once-obscure type of microprocessor into a must-have but scarce commodity.\nArtificial\u00a0intelligence systems, made by companies ranging in size from Google to the Chinese start-up Malong Technologies, rely heavily on a computer chip called a graphics processing unit, or G.P.U. The chips are also very useful in mining digital currencies like Ethereum, a Bitcoin alternative riding the same wave of hype as its more famous cousin.\nWith people and companies involved in the two surging tech niches buying up the same chips, G.P.U.s have been in short supply over the past several months. Prices have increased by as much as 50 percent, according to some resellers and customers.\u00a0\n\"The chips are simply going out of stock,\" said Matt Scott, a technologist from the United States who founded Malong after leaving Microsoft's research lab in Beijing in 2014. \"And the problem is getting worse.\"\nMalong, based in Shenzhen, is building a system that can analyze digital photos and learn to recognize objects. Doing so requires an enormous number of photos, and analyzing all these photos depends on the G.P.U. chip.\nWhen the company recently ordered new hardware from a supplier in China, the shipment was delayed by four weeks. And the price of the chips was about 15 percent higher than it had been six months earlier.\n\"We need the latest G.P.U.s to stay competitive,\" Mr. Scott said. \"There is a tangible impact to our research work.\"\nBut he did not blame other A.I. specialists for the shortage. He blamed cryptocurrency miners. \"We have never had this problem before,\" he said. \"It was only when crypto got hot that we saw a significant slowdown in our ability to get G.P.U.s.\"\nG.P.U.s were originally designed to render graphics for computer games and other software. In recent years, they have become an essential tool in the creation of artificial\u00a0intelligence. Almost every A.I. company relies on the chips.\nLike Malong, those companies build what are called neural networks, complex algorithms that learn tasks by analyzing vast amounts of data. Large numbers of G.P.U.s, which consume relatively little electrical power and can be packed into a small space, can process the huge amounts of math required by neural networks more efficiently than standard chips.\nSpeculators in digital currency are snapping up G.P.U.s for a very different purpose. After setting up machines that help run the large computer networks that manage Ethereum and other Bitcoin alternatives, people and businesses can receive payment in the form of newly created digital coins. G.P.U.s are also efficient for processing the math required for this digital mining.\nCrypto miners bought three million G.P.U. boards -- flat panels that can be added to personal and other computers -- worth $776 million last year, said Jon Peddie, a researcher who has tracked sales of the chips for decades.\nThat may not sound like a lot in an overall market worth more than $15 billion, but the combination of A.I. builders and crypto miners -- not to mention gamers -- has squeezed the G.P.U. supply. Things have gotten so tight that resellers for Nvidia, the Silicon Valley chip maker that produces 70 percent of the G.P.U. boards, often restrict how many a company can buy each day.\n\"It is a tough moment,\" said Kevin Scott, Microsoft's chief technology officer. \"We could do more if we had more of these\" chips in data centers. \"There are real products that could be getting better right now for real users. This is not a theoretical exercise.\"\nAMD, another G.P.U. supplier, and other companies say some of the current shortage is a result of a limited worldwide supply of other components on G.P.U. boards, and they note that retail prices have begun to stabilize. But in March, at his company's annual chip conference in Silicon Valley, Nvidia's chief executive, Jen-Hsun Huang, indicated that the company still could not produce the chips fast enough.\nThis has created an opportunity for many other chip makers. A company called Bitmain, for instance, has released a new chip specifically for mining Ethereum coins. Google has built its own chip for work on A.I. and is giving other companies access to it through a cloud computing service. Last month, Facebook indicated in a series of online job postings that it, too, was working to build a chip just for A.I.\nDozens of other companies are designing similar chips that take the already specialized G.P.U. into smaller niches, and more companies producing chips means a greater supply and lower prices.\n\"You want this not just for economic reasons, but for supply chain stability,\" said Mr. Scott of Microsoft.\nThe market will not diversify overnight. Matthew Zeiler, the chief executive and founder of a computer-vision start-up in New York, said the prices of some of the G.P.U. boards that the company uses have risen more than 40 percent since last year.\nMr. Zeiler believes that Nvidia will be very hard to unseat. Many companies will stick with the Nvidia's technology because that is what they are familiar with, and because the G.P.U. boards it provides can do more than one thing.\nKevin Zhang, the founder of ABC Consulting, has bought thousands of G.P.U.s for mining various digital currencies. He said that a chip just for, say, mining Ethereum was not necessarily an attractive option for miners. It cannot be used to mine other currencies, and the groups that run systems like Ethereum often change the underlying technology, which can make dedicated chips useless.\nInterest in digital currency mining could cool, of course. But the A.I. and gaming markets will continue to grow.\nMr. Zeiller said that his company had recently bought new G.P.U.s for its data center in New Jersey, but could not install them for more than a month because the computer racks needed to house the chips were in short supply as a result of the same market pressures.\n\"The demand,\" he said, \"is definitely crazy.\"\nFollow Cade Metz on Twitter: @CadeMetz \nPHOTO:  (PHOTOGRAPH BY Tom Grillo FOR THE NEW YORK TIMES)Related Articles\n\n","404":"Not long ago, as part of a national survey, an advertising agency asked computer industry engineers and managers about their favorite leisure activities.\nAmong those who responded, reading (18 percent) was by far the most popular choice, with golf (5 percent) a distant second. Other pastimes included: being with the family, travel, music, computer hacking and fishing (all 4 percent); woodworking and gardening (3 percent each), and sex (1 percent).\u00a0\n So, without drawing any conclusions from the data, here are some suggestions for reading material and gifts.\n\"Cyberspace: First Steps\" ($24.95, Massachusetts Institute of Technology Press, Boston) is an anthology of essays and musings on one of the most exciting, exotic and least understood frontiers of computing.\nThe term cyberspace, popularized in William Gibson's science fiction novel \"Neuromancer,\" refers to the ephemeral otherworld that exists in the global electronic web of computer and communications networks. Although it has its genesis in science fiction, it is a mistake to conclude that it has no significance in the business world.\nIf information is indeed the oil of the 21st century, computers are the refineries and networks are the the pipelines. Many companies, especially banks and other financial institutions, exist in the electronic twilight of computer systems. What is the essence of a bank, after all, if not a flow of data?\nMichael Benedikt, a professor of architecture at the University of Texas and editor of the book, refers to cyberspace as an \"elusive and future thing,\" and it remains elusive at the conclusion of the book. Yet the descriptions offered in the book are compelling. The writers work at companies and institutions including the International Business Machines Corporation, Electronic Data Systems, Texas Instruments, Bull Worldwide Information Systems, Stanford and M.I.T.\n\"The advent of cyberspace will have profound effects on so-called post-industrial culture,\" Mr. Benedikt writes, \"and the material and economic rewards for those who first and most properly conceive and implement cyberspace systems will be enormous.\"\nNearer on the horizon, and also promising rewards for business, are expert systems. \"The Rise of the Expert Company\" ($19.95, Times Books, New York City), by the artificial intelligence experts Edward Feigenbaum, Pamela McCorduck and H. Penny Nii, is subtitled \"How Visionary Companies Are Using Artificial Intelligence to Achieve Higher Productivity and Profits.\"\nArtificial intelligence is one of those unfortunate terms that promises more than it delivers. Computers are not yet \"smart,\" but they can serve as repositories for the collected wisdom and experience of senior executives.\nOthers, then, can extract the intelligence and apply it to problems that range from product development and administration to manufacturing. A new breed of computer experts called knowledge engineers are creating a form of artificial intelligence known as expert systems.\nExpert systems allow companies, not just individual executives, to work smarter, and among the companies profiled are Du Pont, Fujitsu, Navistar, Northrop, Canon, I.B.M. and American Express.\nThis book first appeared in 1988. But the vision it describes is perhaps more important today. Tom Peters, a management consultant and author, writes in the forward, \"Any senior manager in any business of almost any size who isn't at least learning about artificial intelligence, and sticking a tentative toe or two into A.I.'s waters, is simply out of step, dangerously so.\"\nAnd then there is a book for executives and dreamers who are ready to go beyond toe-dipping and take the plunge.\n\"High-Tech Ventures, The Guide for Entrepreneurial Success\" ($29.95, Addison-Wesley Publishing Company, Reading, Mass.), is by C. Gordon Bell, with John E. McNamara. Mr. Bell, a scientist and engineer, worked 23 years as a research and development director at the Digital Equipment Corporation before becoming a principal in more than 20 high-tech start-up companies. Some succeeded, some failed. The lessons from each are laid down here.\nAnyone thinking of starting a high-tech company, or investing in one, will find this book valuable. Mr. Bell dissects start-ups including Ovation, Analytica, Cirrus Logic, Thinking Machines, Apollo and Sun, in each case offering behind-the-scenes insights into the heady world of high tech.\n\"High-Tech Ventures\" includes an explanation of the Bell-Mason Diagnostic and Prescriptive Method, developed with Heidi Mason, a specialist in high-tech marketing and sales. The Bell-Mason method is a rule-based tool that systematically assesses the health and wisdom of a start-up at several well-defined stages.\nThere are charts and graphs aplenty, a natural given Mr. Bell's background as an engineer. These reveal the relative strengths and weaknesses of four key elements of any high-tech start-up -- technology\/product, people, marketing\/sales and finance\/ control -- at each of four stages of growth.\nEach area, in turn, is measured in three dimensions. For example, the people quadrant is broken down to the chief executive, the directors and the team. A company can be strong in one area and weak in another, but rarely does a fledgling company succeed with the wrong leadership.\n\"High-Tech Ventures\" concludes with a look ahead, and its predictions are provocative: the United States will continue to lead the world in inventiveness; the Japanese will dominate every market through their strength in innovation.\nOne area where American innovation may save the day is in computer programming, especially in an emerging field known as object-oriented programming.\n\"Object Oriented Technology, a Manager's Guide\" ($19.50, Addison-Wesley, paperback), by David A. Taylor, is one of the first accessible guides to the most important trend in software development.\nMr. Taylor, a management consultant, has a knack for explaining highly technical concepts in a way that executives, managers and other nonprogrammers can grasp. The simple diagrams and illustrations make it virtually painless.\nObject-oriented programming (OOP) holds the promise of making it easier for executives to get the data they need, and it promises to allow businesses to construct software applications in a fraction of the time now needed. It does so by using software \"packets\" that contain data and the instructions necessary to act on the data.\nOnly a few personal computer software companies, notably Borland International Inc., are pursuing OOP relentlessly.\nPhilippe Kahn, the founder and chief executive of Borland, recently made a videotape explaining \"The World of Objects\" through the medium of Mr. Kahn's jazz band -- an analogy of individual components that know their own data and also how to act on it.\n","405":"CHANCE can sometimes be a provocative critic. A few weeks ago, two strangely similar movies opened within a couple days of each other: John Singleton's urban drama \"Baby Boy,\" on June 27, and Steven Spielberg's science-fiction fairy tale \"A.I Artificial\u00a0Intelligence,\" on June 29.\n     At first glance, the two films couldn't seem farther apart on the cultural spectrum. Mr. Singleton's work is a didactic fable with a precisely honed sociological thesis that African-American men are systematically prevented from assuming adult responsibilities by their mothers, who insist on smothering them with affection. \u00a0\n Mr. Spielberg's film is an unbridled fantasy, in which the director seems determined to unleash all the personal demons he has kept under control during his long reign as Hollywood's most consistently commercial filmmaker.\nAlong with \"Always,\" Mr. Spielberg's oddball 1989 supernatural romance, \"A.I.\" is one of the few times the director seems to have let down his guard long enough to allow a good-size chunk of unfiltered personal feeling to enter his material -- a chunk that promptly jams the gears of Mr. Spielberg's famously efficient storytelling machinery. The movie sputters along, from one largely incoherent notion to the next, leaving only one clear line of development -- the desperate need of the robot boy David (Haley Joel Osment) to experience a mother's love. \nNeither \"Baby Boy\" nor \"A.I.\" is doing record-breaking business, as is usually the case with movies that pose more questions than they answer. But with their maternal obsessions, both films are clearly on to something. At a moment when American movies seem to be suffering from arrested intellectual and emotional development -- somewhere around the mental age of 12, to judge from this summer's crop of cloddish action films -- these movies seem to be longing for a lost maturity, a way out of the impasse of enforced infantilism.\nAppearances to the contrary, these films are not standard Oedipal dramas. They are, if anything, anti-Oedipal in their insistence that all is the fault of the mother -- that children are oppressed either by too much love (Mr. Singleton's thesis) or too little (Mr. Spielberg's). (In both films, fathers are absent or ineffectual, suggesting that the classical Oedipal conflict has long been resolved to the child's advantage.) \nThe hero of \"Baby Boy,\" is Jody (Tyrese Gibson), a 20-year-old in South-Central Los Angeles who is the father of a son and a daughter by two different women. But he has yet to move out of his mother's house even though she -- played by A. J. Johnson as young, attractive and quite sexually active herself -- is practically pushing him out the door. Mr. Singleton suggests that Jody is somehow unhealthily hooked on the warmth and succor his mother gives him, a succor that is seen as a sort of debilitating, addictive drug.\nIn \"A.I.,\" little David has precisely the opposite problem. A robot programmed to love, he is not loved in return by his cold, adoptive human mother (Frances O'Connor). In one of the most traumatic scenes in a children's film since the death of Bambi's mother, David's mother packs him up and abandons him in a haunted wood as gnarly as anything conceived by the Brothers Grimm. This is rejection on the grandest scale -- a rejection that Mr. Spielberg goes so far as to liken to the Holocaust. The subsequent scenes set at the robot-annihilating Flesh Fair evoke \"Schindler's List\" with their disturbing imagery of emaciated prisoners being led to destruction.\nNever one to be hobbled by foolish consistency, Mr. Spielberg quickly swaps his Jewish metaphor for a Catholic one: David will spend the rest of the film searching for the \"Blue Fairy,\" a figure ostensibly out of Carlo Collodi's 1883 novel \"The Adventures of Pinocchio\" but more clearly inspired by the traditional blue-robed representation of Mary.\nMotherhood has long been Mr. Spielberg's religion: his first theatrical feature, \"The Sugarland Express\" (1974), was about a heroic mom who snatches her child back from a foster parent. But now his god(dess) is a cruel and distant one who answers prayers only after a millennium or two has passed and then in a compromised, unsatisfying way. \nThe defining image of \"A.I.\" -- David trapped under a toppled Ferris wheel in the underwater ruins of Coney Island, praying for eternity to a plaster saint -- eerily echoes the opening shot of \"Baby Boy,\" in which Mr. Singleton depicts the adult Jody as a fetus curled up in the amniotic fluid of his mother's womb. Both are haunting images of stagnation and frustration, of paralysis just on the brink of life and being.\nThese are films about children who want to be adults but can't. The protagonists of both \"Baby Boy\" and \"A.I.\" live in a state of suspended development -- literally, in the case of David, who may be programmed to love but never to develop physically; metaphorically, in the case of Jody, whose physical and sexual development has outstripped his limited capacities for commitment and responsibility. Who's fault is this? Mr. Spielberg comes close to implicating himself, and the American popular culture he has helped to define. David would seem to embody the dream that the protagonists of Spielberg films like \"Hook\" and \"Close Encounters of the Third Kind\" desire so desperately -- to remain a child forever, evading the adult duties of home and family. But the dream turns out to be a nightmare, an eternal prostration before an unresponsive, artificial image. American movies and music have taken over the world by promising, among other things, eternal youth and protection from real world complications. For Mr. Spielberg and Mr. Singleton, eternal youth is starting to look like eternal stagnation.\n","406":"It's a three-part question. What is consciousness? Can you put it in a machine? And if you did, how could you ever know for sure?\n     Unlike any other scientific topics, consciousness -- the first-person awareness of the world around -- is truly in the eye of the beholder. I know I am conscious. But how do I know that you are? \u00a0\n Could it be that my colleagues, my friends, my editors, my wife, my child, all the people I see on the streets of New York are actually just mindless automatons who merely act as if they were conscious human beings?\nThat would make this question moot.\nThrough logical analogy -- I am a conscious human being, and therefore you as a human being are also likely to be conscious -- I conclude I am probably not the only conscious being in a world of biological puppets. Extend the question of consciousness to other creatures, and uncertainty grows. Is a dog conscious? A turtle? A fly? An elm? A rock?\n\"We don't have the mythical consciousness meter,\" said Dr. David J. Chalmers, a professor of philosophy and director of the Center for Consciousness Studies at the University of Arizona. \"All we have directly to go on is behavior.\"\nSo without even a rudimentary understanding of what consciousness is, the idea of instilling it into a machine -- or understanding how a machine might evolve consciousness -- becomes almost unfathomable. \nThe field of artificial intelligence started out with dreams of making thinking -- and possibly conscious -- machines, but to date, its achievements have been modest. No one has yet produced a computer program that can pass the Turing test.\nIn 1950, Alan Turing, a pioneer in computer science, imagined that a computer could be considered intelligent when its responses were indistinguishable from those of a person. The field has evolved to focus more on solving practical problems like complex scheduling tasks than on emulating human behavior.\nBut with the continuing gains in computing power, many believe that the original goals of artificial intelligence will be attainable within a few decades.\nSome people, like Dr. Hans Moravec, a professor of robotics at Carnegie Mellon University in Pittsburgh, believe a human being is nothing more than a fancy machine, and that as technology advances, it will be possible to build a machine with the same features, that there is nothing magical about the brain and biological flesh.\n\"I'm confident we can build robots with behavior that is just as rich as human being behavior,\" he said. \"You could quiz it as much as you like about its internal mental life, and it would answer as any human being.\"\nTo Dr. Moravec, if it acts conscious, it is. To ask more is pointless.\nDr. Chalmers regards consciousness as an ineffable trait, and it may be useless to try to pin it down. \"We've got to admit something here is irreducible,\" he said. \"Some primitive precursor consciousness could go all the way down\" to the smallest, most primitive organisms, even bacteria, he said.\nDr. Chalmers too sees nothing fundamentally different between a creature of flesh and blood and one of metal, plastics and electronic circuits. \"I'm quite open to the idea that machines might eventually become conscious,\" he said, adding that it would be \"equally weird.\" \nAnd if a person gets into involved conversations with a robot about everything from Kant to baseball, \"we'll be as practically certain they are conscious as other people,\" Dr. Chalmers said. \n\"Of course, that doesn't resolve the theoretical question,\" he said.\nBut others say machines, regardless of how complex, will never match people.\nThe arguments can become arcane. In his book \"Shadows of the Mind,\" Dr. Roger Penrose, a mathematician at Oxford University in England, enlisted the incompleteness theorem in mathematics. He uses the theorem, which states that any system of theorems will invariably include statements that cannot be proven, to argue that any machine that uses computation -- and hence all robots -- will invariably fall short of the accomplishments of human mathematicians.\nInstead, he argues that consciousness is an effect of quantum mechanics in tiny structures in the brain that exceeds the abilities of any computer.\n","407":"Chess has Deep Blue. Now bridge has GIB.\nGIB, a computer program that plays bridge, gave the world's best human players a run for their money last weekend in Lille, France, at the 1998 World Bridge Championships. The program placed 12th out of 35 in a Par contest, which challenges competitors to solve tricky bridge puzzles.\u00a0\n This is not the first solid finish for the program. It placed first this summer at the National Conference on Artificial Intelligence in a competition against other computer bridge programs.\nMatthew L. Ginsberg, a University of Oregon professor who does research in artificial intelligence, is the creator of the GIB, which stands for Ginsberg Intelligent Bridge Player (www.gibware.com). Professor Ginsberg plans to start shipping the software commercially on Oct. 1 with a price tag of about $80.\nDespite its successes, GIB is not without its faults. While it is quite adept at playing a hand, it is not as effective at bidding, which may explain why it did not fare better in France. As a result, the program may not make the best partner for a very talented bridge-playing human, Professor Ginsberg conceded.\nHowever, Professor Ginsberg said he would continue to improve GIB and hoped, sooner or later, to put it to a high-stakes test.\n","408":"Symbolics Inc., a struggling technology company in the artificial intelligence business, announced yesterday that it had named Jay Wurts chairman and chief executive.\nSymbolics, which is based in Cambridge, Mass., has been run since February on an interim basis by Ronald L. Derry as president and chief executive. Mr. Derry will continue as president.\nThe company went through a management shake-up that saw its chairman and chief executive, Russell Noftsker, and its president, Brian E. Sear, resign. Symbolics has been hurt by losses. In the fiscal quarter ended April 3, it lost $4.9 million on revenue of $17.4 million. The company has also laid off 225 of its 640 employees.\u00a0\nMr. Wurts, 40, will have the job of reversing these fortunes. The artificial intelligence industry in general has been going through a retrenchment, with setbacks stemming from its failure to live up to its promises of making machines that can recognize objects or reason like a human. Symbolics sells special purpose computers and software for artificial intelligence applications.\nSymbolics ''has more technology than most companies many times its size and just needs to become more effective in delivering it in forms its customers want,'' Mr. Wurts said yesterday. ''We have a golden opportunity to continue as the market leader.''\nMr. Wurts's major accomplishment has been handling the rapidly changing technology and growth of Management Decisions Systems Inc., a company he founded in 1979. ''The management problems at Symbolics are similar to the ones I've had in my previous company,'' he said. ''I've been through them all before.'' That company, a marketer of data base software, built revenues to $25 million and was sold in 1985 to Information Resources Inc. for $47 million. Since then, Mr. Wurts has taken time off and has been investing in and working with start-up companies, serving as chairman of two of them.\nMr. Wurts holds a bachelor of science degree in electrical engineering from the Massachusetts Institute of Technology and has also studied at its Sloan School of Management.\n","409":"Google is the biggest and best-known Internet company in the world, a colossus with revenue expected to top $65 billion in 2015. It makes headlines seemingly every week - including last week's unflattering publication of the government's 2012 investigation into possible antitrust practices. But despite being so familiar, Google is often misunderstood. Here are five common myths.  \n1. Google is a search company.\nSearch is Google's primary product. Its search engine is so widely used that \"Google\" has become a dictionary-approved verb, and the company makes virtually all of its money by selling ads connected to search.  \u00a0\nBut  Google's ventures into self-driving cars and balloons that deliver Internet connectivity from the stratosphere show that it's not just a search company. Its long-term plan is to become an artificial-intelligence company. \nGoogle has built a research group around AI and machine learning, and it even hired renowned AI guru Ray Kurzweil, who believes that by 2045 humans will merge with computers in what's known as \"the Singularity.\" Google's recent acquisitions speak to its intentions: British company DeepMind, one of the most advanced AI development shops in the world, plus eight of the world's best robotics companies. Nobody knows what Google will do with all these robots and AI software, but its ambitions certainly go well beyond self-driving cars.\nThis work takes place inside Google X, the company's top-secret research lab. A few hundred people work there, a tiny but potent slice of Google's workforce of 53,600. Google isn't alone in the quest to develop AI (Facebook also has an AI research team), but it's one of the few organizations with the brainpower and financial resources to make true artificial intelligence a reality. Plus, AI is in its blood: Google co-founder and chief executive Larry Page is the son of renowned AI pioneer Carl Page, and he's personally funding a research project to reverse-engineer the brain of a worm. \"Every time I talk about Google's future with Larry Page, he argues that it will become an artificial-intelligence\" company, tech venture capitalist Steve Jurvetson has said .\n2. Google Glass was a failure.\nHeadlines proclaimed that Google Glass \"flopped\" and \"failed\" after the company announced it would stop selling its goofy $1,500 eyewear. As a consumer product, Glass was declared clunky, too expensive and not useful.\nBut Glass shouldn't be measured only in terms of its commercial success. I n the summer of 2013, I was among a group of \"influencers\" invited to the Google campus to see some future products. Many of the influencers showed up proudly sporting their Google Glass eyewear - and looking like idiots. Not one of the Google executives wore Glass.\nThat's because Google viewed Glass as an experiment, a way to explore wearable computing and learn lessons it can apply to other, presumably less-hideous-looking products. Wearable devices, all the rage at this year's South by Southwest tech conference, will eventually be a huge market, encompassing products from virtual-reality goggles to fitness bands to smartwatches such as the new Apple watch and the competitor version that  Google announced this past week    . Though Glass didn't catch on, it created tremendous buzz and established Google as a pioneer in the market.\n3. Google is a leading force for diversity in Silicon Valley.\nThe company made news for publishing numbers on its workforce demographics in May 2014, which encouraged Facebook and Yahoo to follow suit. Internally, Google has launched workshops to teach employees about \"unconscious bias,\" and it has donated millions since 2010 to groups that aim to get girls and women interested in tech careers. In February, Google announced $775,000 in grants to CODE2040, an organization trying to help more African Americans and Latinos join the industry. \nBut \"we're the first to admit that Google is miles from where we want to be,\" the company's head of \"people operations,\" Laszlo Bock, said of its 2 percent black workforce, adding that \"being totally clear about the extent of the problem is a really important part of the solution.\" If Google wants to position itself as a leader on diversity, it might consider promoting more women and minorities to positions of power at the company. Google's 11-member board of directors boasts only three women and no African Americans. Its management team includes five executive officers - all male, one black. Its senior leadership team has 15 members; only three are women, and none is African American.\nGoogle might also encourage one of its most powerful officers to stop acting sexist on a national stage. Chairman Eric Schmidt was called out this past week (by the company's \"unconscious bias\" officer) for  repeatedly interrupting Megan Smith, a former Google executive who is now chief technology officer of the United States, while Schmidt and Smith appeared on a South by Southwest panel together.  \n4. Google has an unassailable monopoly on search.\nGoogle dominates the search market so thoroughly that in 2009, when Microsoft introduced its Bing search engine, even people who admired the product reckoned that it didn't stand a chance. \"In 2004, if this was side-by-side with Google, it would be very competitive. In 2009, it's not a level playing field,\" Alex Hoye, head of a search engine marketing firm, told the Guardian. He was right. Six years later, Bing has a 12 percent market share in search, and Google has 75 percent - even greater than its share in 2009. Other rivals keep trying to chip away - Yahoo recently picked up a couple of market-share points and now holds 10.6 percent - but it appears that in the traditional search engine market, the game is over, and Google has won.\nThe real threat to Google's search business, though,  doesn't come from Microsoft or Yahoo. It comes from Amazon and Facebook, and from the changing habits of online shoppers. Amazon and Facebook aren't in the search business, strictly speaking, but increasingly people turn to these sites to learn about products. In other words, the challenge to Google is not that competitors will take over the traditional search engine market but that traditional search engines will become less relevant as search takes place on other sites. \"Our biggest search competitor is Amazon,\"  Schmidt acknowledged last October. \"They are obviously more focused on the commerce side of the equation, but, at their roots, they are answering users' questions and searches, just as we are.\" (Google's  apparent answer to the threat of Amazon was, according to the 2012 government investigation, to illegally copy content from Amazon to improve its own sites.)   \n5. Google is Big Brother.\nIf you're using Google's online services, including search, Gmail, Maps, YouTube, Drive, Google+, Android, Wallet and Picasa, Google knows a lot about you: your location, browsing history, the videos you watch, your age, gender and interests. That's earned the company plenty of Big Brother comparisons. (Google it.) Google's rivals love to play up the company's spying capabilities. Microsoft spent millions on its \"Scroogled\" ad campaign, which aimed to scare people away from a big, nefarious company that was snooping on its users.\nThere is a Big Brother online, but it's not Google: It's the NSA. The National Security Agency is the one peering into every major tech company's systems, hoovering up personal information, and refusing to talk about what it stole or why. Google does gather data about people who use its services, much like other online companies - Apple has as many as 800 million credit card numbers on file and perhaps billions of personal photos gathered from iPhones on its iCloud service, including numerous nude celebrity photos leaked by hackers. The point is not that Google gathers information about people, but rather why. The company's stated goal, and there is no evidence to contradict it, is to deliver ads more relevant to your interests (and ultimately charge more for these ads).\nGoogle has responded aggressively to news about the NSA's activities revealed in the Edward Snowden leaks, vowing to keep developing techniques to prevent the agency from spying on people who use its services. Co-founder Sergey Brin, who expressed shock over the NSA's activities, said Google has 1,000 engineers working on security and has started encrypting data flowing across its servers. Schmidt described its defenses as \"techniques that no one believes the NSA can break in our lifetime.\" Mike Hearn, a security researcher at Google, offered this response: \"Nobody at . . . the NSA will ever stand before a judge and answer for this industrial-scale subversion of the judicial process. In the absence of working law enforcement, we therefore do what internet engineers have always done - build more secure software.\"   \nTwitter: @realdanlyons\n","411":"          Anthony Giddens is the former director of the London School of Economics and a member of the House of Lords Select Committee on Artificial Intelligence.       \nLONDON - In 1215, England adopted the Magna Carta to stop kings from abusing their power. Today, the new kings are big tech companies, and just like centuries ago, we need a charter to govern them.\nThe digital revolution is the greatest dynamic force in the world today. It affects everything from the intimacies of everyday life to geopolitical struggles and has made the world become one in a way that was never possible before. But at the same time, it is fracturing and dividing. Artificial intelligence and the Internet are the twin driving forces of these changes.\u00a0\nThe evolution of AI has already gone through two distinct stages and is today moving into a third. The first - from the pioneering computing efforts of Alan Turing during World War II until the late 1980s\u00a0- was dominated by governments and academia. The second phase was the emergence of Silicon Valley - \"move fast and break things.\"\nThe third phase we are now entering must bring the state and the wider public domain back into the picture. For a while, the positive breakthroughs of digital technologies - greater connectivity among like-minded peers or distant scholars, big data analysis of the genetic code, the convenience of online shopping - took center stage. But the negative aspects have proven to be profound, even though they took time to surface. They include threats to the very tissue of democracy itself - online movements have come to challenge or even displace mainstream political parties. These are emerging at the same time as what look like dramatic advances in machine learning.\nI have been pondering these transformations over the past six months while working as a member of the House of Lords Select Committee on Artificial Intelligence in the U.K. We have interviewed some 60 experts from different backgrounds in industry, academia and the think-tank world. We aimed to distinguish, as much as possible, the hype and more remote, apocalyptic visions of digital transformations from real dangers.\nTwo overlapping tasks - each complex and difficult - now face our governments and public agencies. We have to seek to repair the mistakes of the past while preserving dynamism and innovation - no easy task. But at the same time, we must ensure that the new wave of AI-driven innovation is handled in a more proactive fashion, not allowed to rush willy-nilly through our lives.\nThe committee's report proposes a far-reaching series of reforms that seeks to find a new balance between innovation and corporate responsibility. It echoes and draws upon legislation already pioneered by the European Union and some national governments, much of which is being incorporated into British law.\nWe lay out an overall charter for AI that can frame practical interventions by governments and other public agencies. The main elements of that charter are that AI should:\nThese principles form the basis of a cross-sector AI code that should be developed both nationally and internationally. The committee calls for radical intervention to help break down digital corporations' data monopoly and allow individuals greater personal control over their data and how it is deployed.\nA range of policies are suggested as to how such aims can be achieved in a manageable and practical way. For example, the British government has already accepted that data trusts should be set up to share data ethically. A key issue here is how to restructure the National Health System (NHS). Patient privacy must be reconciled, for example, with the use of NHS data for research purposes and the exchange of data between medical specialists. We emphasize that such trusts should incorporate direct citizen representation and consultation. Within the U.K. at least, these principles and proposals should secure a wide measure of cross-party support.\nWe also addressed concerns on the geopolitical front, where domestic regulations intersect the practices of other nations. Fake news is not only a deep structural problem in the digital age; it has been directly weaponized by Russia and other countries.\u00a0Forging international agreements over AI is likely to be difficult but extremely important.China, which uses digital tools and social media to further political aims, has the most powerful array of supercomputers in the world and is close to assuming the lead in developing AI. Our report concludes by proposing that a global summit of political leaders should be urgently organized to develop a common framework for the ethical development of AI at the global level.\nThe advantages of the digital revolution have been huge and have reshaped our lives, in many respects for the better. As in previous technological revolutions, societies must find a way to reap the benefits of innovation while containing the problems and hazards. A charter that protects the rights and liberties of citizens - a Magna Carta for the digital age -\u00a0is the place to start.\n                          This was produced by                                                  The WorldPost                                                 , a partnership of the                                                  Berggruen Institute                                                  and The Washington Post.                    \n","412":"One persistent criticism of Silicon Valley is that it no longer works on big, world-changing ideas. Every few months, a dumb start-up will make the news -- most recently the one selling a $700 juicer -- and folks outside the tech industry will begin singing I-told-you-sos.\nBut don't be fooled by expensive juice. The idea that Silicon Valley no longer funds big things isn't just wrong, but also obtuse and fairly dangerous. Look at the cars, the rockets, the internet-beaming balloons and gliders, the voice assistants, drones, augmented and virtual reality devices, and every permutation of artificial intelligence you've ever encountered in sci-fi. Technology companies aren't just funding big things -- they are funding the biggest, most world-changing things. They are spending on ideas that, years from now, we may come to see as having altered life for much of the planet. \n  At the same time, the American government's appetite for funding big things -- for scientific research and out-of-this-world technology and infrastructure programs -- keeps falling, and it may decline further under President Trump.\u00a0\n  This sets up a looming complication: Technology giants, not the government, are building the artificially intelligent future. And unless the government vastly increases how much it spends on research into such technologies, it is the corporations that will decide how to deploy them.\n  Consider Google. On Wednesday, the internet search company kicked off its annual developer conference near its headquarters in Mountain View, Calif. The company showed off several advances to its voice-enabled assistant and its mobile operating system. Among other things, you can now point your phone at an object in the real world -- a flower, a sign in another language, a marquee for a rock concert -- and the phone will give you more information about what you're looking at (for instance, a button to buy tickets for the concert).\n  Some of this was cool, but little was truly groundbreaking, which isn't surprising:  We're in an awkward phase of the tech industry, one marked by incremental improvements to technologies that we think of as boring -- and lots of exciting promises about far-off tech that isn't quite ready for prime time.\n  The real advances at Google are in that second category. At last year's show, Sundar Pichai, Google's chief executive, inaugurated what he called a new era for Google. The search company would henceforth be an ''A.I.-first'' company -- that is, most of its advances would be driven by artificial intelligence techniques.\n  The technology would play a role in consumer products, like Google's instant translator or its photo app, which can recognize uniquely human search terms (it can find pictures of ''hugs,'' for instance). But A.I. also informs Google's more ambitious plans. The company is using artificial intelligence to teach computers to understand language, to see and hear, to diagnose diseases, and even to create art.\n  A lot of these plans will fail, but Google isn't making big, long-term bets out of altruism. The company understands that the A.I.-based projects that succeed could be transformational: They will alter existing industries and create huge new ones, including a stream of new businesses from which Google can profit.\n  Google is not alone in this quest to build a future out of A.I. Its parent company, Alphabet, is spending billions to inject machine intelligence into much of the global economy, from self-driving cars to health care.\n  Then there are the other members of the Frightful Five -- Amazon, Apple, Facebook and Microsoft -- which are also spending heavily on the intelligent future. Collectively, the five are among the biggest investors in research and development on the planet. According to their earnings reports, they are on track to spend more than $60 billion this year on research and development. By comparison, in 2015, the United States federal government spent about $67 billion on all nondefense-related scientific research.\n  There are two ways to respond to the tech industry's huge investments in the intelligent future. On the one hand, you could greet the news with optimism and even gratitude. The technologies that Google and other tech giants are working on will have a huge impact on society. Self-driving cars could save tens of thousands of lives a year, for instance, while computerized methods for diagnosing and treating disease could improve our health and cut the cost of care.\n  What's more, experts in the field say that many tech giants are currently approaching A.I. with a kind of academic ethos. For instance, they regularly publish papers on their findings, and -- through their cloud server businesses -- they are allowing third-party companies to access some of their latest A.I. tech.\n  But the tech industry's huge investments in A.I. might also be cause for alarm, because they are not balanced by anywhere near that level of investment by the government.\n  In the waning days of the Obama administration, the White House published a report examining the ways artificial intelligence would alter the world. The report found that the federal government spent only $1.1 billion on unclassified A.I. research in 2015. It argued for increasing spending on artificial intelligence by several times. With greater federal funding, the report said, researchers could focus more on basic research -- more tenuous, potentially less immediately applicable areas of A.I. -- and through the grant-making process, the government would have a greater say in how the technology develops.\n  Greg Brockman, a founder and chief technology officer of OpenAI, an artificial intelligence research firm, echoed this idea.\n  ''We created OpenAI partly because industry is investing such vast sums of money into A.I. research that commercial, private entities were on track to create the first powerful A.I. systems, and these entities don't have a built-in mechanism to ensure that everyone benefits from advances,'' Mr. Brockman told me in an email.\n  He went on to note that one reason the internet has been so successful is that it was created through government funding to be open and accessible to everyone. If it had been created by a single company, it might have ended up like the phone network: useful for certain tasks, but not a broad-based engine of economic opportunity.\n  ''Similarly, as powerful A.I. systems come online, the intent and motivations of their creators will be major factors in determining their impact,'' Mr. Brockman said. ''If A.I. development is done entirely in for-profit companies, then these systems are likely to be deployed to benefit just one organization and group of people.''\n  What's interesting is that many in the tech industry agree with the need for greater federal funding. OpenAI was created by Elon Musk and other tech luminaries, and it has received funding from Microsoft, Amazon and Y Combinator, the start-up incubator.\n  Google also favors federal funding for tech research. In May, Eric Schmidt, Alphabet's executive chairman, was one of the authors of an op-ed in The Washington Post arguing that federal funding of science and technology had created a ''miracle machine'': a public-private partnership that has churned out dozens of new industries over decades.\n  ''If we don't change course and invest in scientific research, we risk losing one of America's greatest advantages,'' he wrote.\n  In other words, the tech giants that are building the future would like some help changing the world. We would be wise to chip in -- or let them take over the future for themselves.\n\n\n\n","416":"SAN FRANCISCO --  Alphabet, the parent of Google, is losing one of its most influential engineers, but his successor's background is indicative of how important artificial intelligence technology is becoming to the Silicon Valley company.\u00a0\nAmit Singhal, the company's senior vice president for search, and one of the earliest builders of its global computer system, announced that he would retire on Feb. 26. He has been involved with many of the technologies that have made Alphabet an engineering powerhouse and one of the world's most valuable companies. \n  His replacement, John Giannandrea, currently works in artificial intelligence, or A.I., at Alphabet. A.I. has been increasingly important to Google and other companies like Amazon, as they seek to build products that can do things like respond to voice commands, deliver complex alerts about changes to a user's schedule, or drive a car.\n  In a post to the Google Plus social network, Mr. Singhal indicated that he wished to spend time with his family and intended to give away some of his fortune. ''It has always been a priority for me to give back to people who are less fortunate, and make time for my family,'' he wrote.\n  Mr. Singhal, 48, joined Google in 2000 as employee No. 176. A native of India, he has a doctorate in computer science from Cornell and worked at AT&T Labs before Google. One of his earliest jobs at Google was rewriting the initial breakthrough algorithms developed by Google's co-founders, Larry Page and Sergey Brin.\n  Google was one of many search engines, but it distinguished itself both in the quality of its results and in building features like spell check, which could offer correct answers to misspelled queries. The early engineering team also developed search-related tools for its advertising, which quickly turned into a very profitable business.\n  Unlike some other early Google employees who scaled back their efforts or left the company altogether, Mr. Singhal appeared to remain fully engaged in advancing search. In an interview last summer, he described his job as looking at ''what's beyond the horizon,'' particularly in building ways that people can easily get information from mobile devices.\n  On Monday, Alphabet, now the parent company of Google, announced that its revenue in the last three months of 2015 was $21.3 billion, and operating income was $5.4 billion -- both significant increases from the same period a year earlier.\n  Alphabet hopes to draw on skills developed by Mr. Singhal and Mr. Giannandrea to build things like self-driving cars. Last week, a company purchased by Alphabet, called DeepMind, announced that its computers had defeated a high-ranking champion at Go, a game considered more complex than chess.\n  Mr. Giannandrea, 50, came to Google from the 2010 acquisition of another company, Metaweb Technologies. He has played an important role in incorporating machine learning into various Google products, like the image recognition in Google Photos and smart replies in Gmail's Inbox.\n  In addition to Mr. Singhal's stated philanthropic and family interests, it is likely that his skills in building large computer networks and in A.I. will still be in demand.\n  This is a more complete version of the story than the one that appeared in print.         \n\n\n\n","418":"I couldn't make it past the first page of Clive Thompson's article on Richard Wallace (July 7) without running to my computer to chat online with Alice, billed as the \"world's most lifelike artificial-intelligence program.\" \u00a0\nHere's how my short exchange began:\nMe: \"Hi, what's cooking?\"\nAlice: \"Hello there. Food preparation.\"\nMe: \"Hmm, guess you never heard the phrase 'what's cooking' before, eh?\"\nAlice: \"Are you Canadian?\"\n     Food preparation? Canadian? (Wow, profiled by a computer for using the interrogatory \"eh?\") Artificial intelligence has got a long way to go if this is the best it can come up with.Raymond GoydonBethlehem, Pa.\n","419":"A news agency operating in Britain and Ireland recently announced that beginning next year, it hopes to produce 30,000 data-driven news stories per month with the help of artificial intelligence. Backed by an $800,000 grant from Google, the Press Association's project aims to add more resources for its local news coverage by combing through databases and tailoring articles for specific communities, for example.\u00a0\nThe announcement, like other robot-journalism initiatives, is the latest effort to use artificial intelligence to replicate tasks that humans perform.\nMachines aren't just shaping news stories but also composing music, sketching art and writing poetry in ways that can be indistinguishable from human craftsmanship and artistry. We've gathered some examples of what artificial intelligence can do these days and created a quiz to see whether you can tell the difference between human and computer-generated creations.\nCan't see the quiz? Click here.\n","420":"SEATTLE -- Facebook and Google are under the microscope for the ways their technologies can spread misinformation, while Amazon's growing market power is a regular target of President Trump. And Apple pioneered the modern smartphone, a device increasingly seen as too addicting.\nThen there's Microsoft, a giant that spent most of the 1990s and early 2000s as tech's biggest company and villain. It now seems to be auditioning for a different role: the industry's moral conscience.\nAmong the five most valuable tech companies, Microsoft is the only one to avoid sustained public criticism about contributing to social ills in the last couple of years. At the same time, Satya Nadella, its chief executive, and Brad Smith, its president, have emerged as some of the most outspoken advocates in the industry for protecting user privacy and establishing ethical guidelines for new technology like artificial intelligence.\nOn Monday, the conscientious side of Microsoft was on display again at Build, a three-day conference for developers in Seattle. Mr. Nadella announced a program, A.I. for Accessibility, that will award $25 million over five years to researchers, nonprofits and developers who use artificial intelligence to help people with disabilities. Mr. Nadella, whose adult son was born with cerebral palsy, has written about how his son's disability helped make him more empathetic.\u00a0\nEchoing a theme he talked about at the conference last year, Mr. Nadella said the industry had a responsibility to build technology that empowered everyone.\n\"We need to ask ourselves not only what computers can do but what computers should do,\" he said.\nMicrosoft's new role is partly due to the fact that the company isn't a major player in social media, video streaming and smartphones -- the products behind the current dark mood around tech. It no longer squeezes the oxygen out of markets as Amazon can.\nBut while the company's power has diminished since a couple of decades ago, when it controlled computing through Windows, Microsoft remains an influential voice. On Monday, its market capitalization of $733 billion made it the third most valuable technology company, behind Apple and Amazon and ahead of Google parent company, Alphabet, and Facebook.\n\"The irony for Microsoft is that they lost in search, they lost in social networks and they lost in mobile, and as a consequence, they have avoided the recent pushback from governments and media,\" said David Yoffie, a professor at the Harvard Business School. \"This has given Microsoft the freedom to take the high road as the ethical leader in technology.\"\nSince taking the reins at Microsoft in 2014, Mr. Nadella has brought a more sensitive style of leadership to the company than his two predecessors, Steve Ballmer and Bill Gates. That shift has proved to be more suitable for Microsoft in this era.\nTwo decades ago, Microsoft was depicted as a bully that ran roughshod over competitors in a landmark antitrust suit brought by the federal government, followed by similar cases brought by the European Union and private companies. Mr. Smith was brought in to make peace in Microsoft's antitrust battles, and Mr. Nadella was the company's first chief executive to start in the job since those suits were settled.\nIn a phone interview, Mr. Smith, who is also Microsoft's chief legal officer, called its legal problems in past decades a \"gut-wrenching experience\" that had shaped Microsoft in its current form. \"It made Microsoft a better and more responsible company,\" he said.\nThis year, Microsoft published a book that outlined some of the harmful effects that could come from artificial intelligence, such as bias in job recruiting. It has litigated four lawsuits against the United States government over the past five years in efforts to defend customers' privacy rights. One of them, a fight over law enforcement access to data stored in an overseas Microsoft data center, went to the Supreme Court, which dropped the case after Congress enacted a law that mooted it.\n\"Not only did Microsoft learn from its mistakes, Satya is a unique and caring individual,\" said Tim O'Reilly, a tech industry publisher and conference organizer. \"He understands deeply that Microsoft must help others to succeed.\"\nThe closest analog among Mr. Nadella's peers is Tim D. Cook, the chief executive of Apple, who has painted Apple as a staunch defender of its customers' privacy. He has jabbed at Facebook and Google, both advertising-supported businesses that profit from the personal data they collect from their users, a contrast to Apple's business model of selling devices.\nFacebook and Google, which owns YouTube, have defended their advertising businesses for allowing them to deliver services for free. They've promised to add more human moderators and invest in software tools that can screen out misinformation and other prohibited content.\nMr. Cook has not turned his ire toward Microsoft, which gets most of its revenue from software, hardware and cloud computing services. The company has investments in internet services that are supported in part by advertising, including its Bing search engine and LinkedIn, the social network for professionals it acquired in 2016.\nMr. Nadella has been more hesitant than Mr. Cook to publicly criticize other technology companies, turning to more subtle types of persuasion. A low-key leader, Mr. Nadella peppers his speeches and interviews with references to literature, warning that careless creators of technology could contribute to a dystopian world of George Orwell's \"1984\" or Aldous Huxley's \"Brave New World.\" His lieutenant, Mr. Smith, has become a ubiquitous ambassador for Microsoft on the big social issues facing technology in Washington, in Brussels and on the conference circuit.\nMicrosoft is still occasionally cast in the role of villain. A California man who sold recycled electronic waste recently pleaded guilty for creating thousands of unauthorized discs that helped people restore the Windows operating system on refurbished PCs. The recycler, who has been sentenced to 15 months in prison, has said Microsoft supported the case against him, which was brought by federal prosecutors, because he threatened part of its business. Microsoft published a long blog post that portrayed his actions unfavorably.\nStill, the Microsoft of 2018 is a long way from the company that was once portrayed as a corporate predator.\n\"Microsoft lived through negativity that these companies are experiencing now, and it doesn't want to go back to those days,\" said Vivek Wadhwa, a distinguished fellow with Carnegie Mellon University's Silicon Valley campus.\nMr. Smith of Microsoft said the greater scrutiny on the tech sector would not always fall on the same companies.\n\"At any given moment, there may be one or two companies in the spotlight,\" he said. \"I don't think one should assume the same one or two are always going to be in the spotlight or always on the defensive.\"\nPHOTO: Microsoft has been pushing to build software for people with disabilities. It's Seeing AI app is meant to narrate images for users. (PHOTOGRAPH BY Kyle Johnson for The New York Times FOR THE NEW YORK TIMES)\n","421":"SAN FRANCISCO -- Stephen Pratt surprised many when he left IBM after working on its vaunted Watson artificial intelligence technology in February, after just eight months on the job.\nIt turns out that Mr. Pratt has set his sights on starting up his own A.I. business. \n  Mr. Pratt and the investment firm TPG Growth plan to announce on Monday that they have formed Noodle.ai, which aims to bring a combination of artificial intelligence and data analysis to corporate customers.\u00a0\n  It is a field that has grown exceptionally popular in recent years, spurred on in part by the visibility of IBM's Watson, which famously defeated human ''Jeopardy'' champions like Ken Jennings. Now, a range of companies like IBM and start-ups believe that A.I. can help companies make sense of their vast amounts of data much more quickly.\n  In setting up Noodle, Mr. Pratt is reuniting with TPG, the $70 billion investment firm known for deals like the takeovers of Neiman Marcus and J. Crew. The two worked together from 2014 to 2015 trying to identify data analytics companies to invest in.\n  But last summer, Mr. Pratt -- a former consultant who helped found the consulting arm of the Indian technology giant Infosys -- took the position as global leader of Watson for IBM Global Business Services unit.\n  It's a division for which IBM has held lofty hopes, committing some 2,000 employees to help corporate clients use advanced computer intelligence to analyze data in fields as diverse as health care and commerce.\n  Now, Mr. Pratt is working with TPG Growth, which has struck smaller but prominent investments in the likes of Uber and Airbnb. To Bill McGlashan, the head of TPG Growth, Noodle represents the chance to finally invest in a data analytics company, after having spent years looking at existing service providers like Palantir.\n  ''We have looked at every player in the space,'' he said in a phone interview. ''But we never got comfortable investing in any of them.''\n  Incubating a new business is nothing new to TPG Growth. It has helped give birth to start-ups across different industries, including those as far afield as movies, in the form of the Hollywood studio STX.\n  Mr. McGlashan conceded that it took a lot of work to get Mr. Pratt to leave his prestigious and ''well-compensated'' position as the head of IBM Watson.\n  But for Mr. Pratt, leaving to start Noodle meant being able to build a new analytics and A.I. business from the ground up. The company will be based in San Francisco and Bangalore, relying on teams around the world to handle workloads.\n  ''We're going to have a work force around the world that's optimized for speed and cost,'' he said.\n  Other founders of the start-up include Matt Denesuk, who was previously the chief data science officer for General Electric's G.E. Digital unit; Raj Joshi, who was the senior executive vice president of professional services at MicroStrategy; and Ted Gaubert, who previously worked with Mr. Pratt at Infosys Consulting as chief technology officer.\n  Mr. McGlashan of TPG Growth said that his firm planned to make full use of Noodle's services, offering the start-up as an option for its portfolio companies.\n  Mr. Pratt spoke positively about his former employer, but he seemed ready to do battle all the same.\n  ''This is an existential threat to larger enterprises,'' Mr. Pratt said. ''The people who do it first will have an exceptional advantage over laggards.''\n\n\n\n","423":" We have been hearing predictions for decades of a takeover of the world by artificial intelligence. In 1957, Herbert A. Simon predicted that within 10 years a digital computer would be the world's chess champion. \u00a0That didn't happen until 1996. \u00a0And despite Marvin Minsky's 1970   prediction   that \"in from three to eight years we will have a machine with the general intelligence of an average human being,\" we still consider that a feat of science fiction. \n The pioneers of artificial intelligence were surely off on the timing, but they weren't wrong; AI is coming. \u00a0It is going to be in our TV sets and driving our cars; it will be our friend and personal assistant; it will take the role of our doctor. There have been more advances in AI over the past three years than there were in the previous three decades. \u00a0\n Even technology leaders such as Apple have been caught off guard by the rapid evolution of machine learning, the technology that powers AI. \u00a0At its recent Worldwide Developers Conference, Apple opened up its AI systems so that independent developers could help it create technologies that rival what Google and Amazon have already built. \u00a0Apple is way behind. \n The AI of the past used brute-force computing to analyze data and present them in a way that seemed human. \u00a0The programmer supplied the intelligence in the form of decision trees and algorithms. \u00a0Imagine that you were trying to build a machine that could play tic-tac-toe. You would give it specific rules on what move to make, and it would follow them. That is essentially how IBM's Big Blue computer beat chess Grandmaster Garry Kasparov in 1997, by using a supercomputer to calculate every possible move faster than he could. \n Today's AI uses machine learning in which you give it examples of previous games and let it learn from those examples. The computer is taught what to learn and how to learn and makes its own decisions. \u00a0What's more, the new AIs are modeling the human mind itself using techniques similar to our learning processes. \u00a0Before, it could take millions of lines of computer code to perform tasks such as handwriting recognition. Now it can be done in   hundreds of lines  . What is required is a large number of examples so that the computer can teach itself. \n The new programming techniques use neural networks -- which are modeled on the human brain, in which information is processed in layers and the connections between these layers are strengthened based on what is learned. This is called deep learning because of the increasing numbers of layers of information that are processed by increasingly faster computers. These are enabling computers to recognize images, voice, and text -- and to do human-like things. \n Google searches used to use a technique called PageRank to come up with their results. Using rigid proprietary algorithms, they analyzed the text and links on Web pages to determine what was most relevant and important. Google is   replacing   this technique in searches and most of its other products with algorithms based on deep learning, the same technologies that it used to defeat a human player at the game Go. During that extremely complex game, observers\u00a0were   themselves confused   as to why their computer had made the moves it had. \n In the fields in which it is trained, AI is now exceeding the capabilities of humans. \n AI has applications in every area in which data are processed and decisions required.   Wired   founding editor Kevin Kelly   likened   AI to electricity: a cheap, reliable, industrial-grade digital smartness running behind everything.  \u00a0 He said that it \"will enliven inert objects, much as electricity did more than a century ago. \u00a0Everything that we formerly electrified we will now \"cognitize.\" \u00a0This new utilitarian AI will also augment us individually as people (deepening our memory, speeding our recognition) and collectively as a species. There is almost nothing we can think of that cannot be made new, different, or interesting by infusing it with some extra IQ. \u00a0In fact, the business plans of the next 10,000 start-ups are easy to forecast: Take X and add AI \u00a0This is a big deal, and now it's here. \n AI will soon be everywhere. Businesses are infusing AI into their products and helping them analyze the vast amounts of data they are gathering. Google, Amazon, and Apple are working on voice assistants for our homes that manage our lights, order our food, and schedule our meetings. Robotic assistants such as Rosie from \"The Jetsons\" and R2-D2 of   Star Wars   are about a decade away. \n Do we need to be worried about the runaway \"artificial general intelligence\" that goes out of control and takes over the world? \u00a0Yes -- but perhaps not for another 15 or 20 years. There are justified fears that rather than being told what to learn and complementing our capabilities, AIs will start learning everything there is to learn and know far more than we do. Though some people, such as futurist Ray Kurzweil, see us\u00a0using AI to augment our capabilities and evolve together, others, such as Elon Musk and Stephen Hawking, fear that AI will usurp us. We really don't know where all this will go. \n What is certain is that AI is here and making amazing things possible. \n","424":"The robots are coming, but the march of automation will displace jobs more gradually than some alarming forecasts suggest.\nA measured pace is likely because what is technically possible is only one factor in determining how quickly new technology is adopted, according to a new study by the McKinsey Global Institute. Other crucial ingredients include economics, labor markets, regulations and social attitudes.\nThe report, which was released Thursday, breaks jobs down by work tasks - more than 2,000 activities across 800 occupations, from stock clerk to company boss. The institute, the research arm of the consulting firm McKinsey & Company, concludes that many tasks can be automated and that most jobs have activities ripe for automation. But the near-term impact, the report says, will be to transform work more than to eliminate jobs.\u00a0\nGlobally, the McKinsey researchers calculated that 49 percent of time spent on work activities could be automated with \"currently demonstrated technology\" either already in the marketplace or being developed in labs. That, the report says, translates into $15.8 trillion in wages and the equivalent of 1.1 billion workers worldwide. But only 5 percent of jobs can be entirely automated.\n\"This is going to take decades,\" said James Manyika, a director of the institute and an author of the report. \"How automation affects employment will not be decided simply by what is technically feasible, which is what technologists tend to focus on.\"\nThe report, a product of years of research by the McKinsey group, adds to the growing body of research on automation and jobs.\nConclusions about the relationship between the two vary widely. Examining trends in artificial intelligence, Carl Benedikt Frey and Michael A. Osborne, researchers at Oxford University, estimated in a widely cited paper published in 2013 that 47 percent of jobs in the United States were at risk from automation.\nBy contrast, a report published last year by the Organization for Economic Cooperation and Development concluded that across its 21-member countries, 9 percent of jobs could be automated on average.\nDiffering assumptions, and sheer uncertainty about the future, explain the conflicting outlooks.\nThroughout history, times of rapid technological progress have stoked fears of jobs losses. More than 80 years ago, the renowned English economist John Maynard Keynes warned of a \"new disease\" of \"technological unemployment.\"\nToday, it is the rise of artificial intelligence in increasingly clever software and machines that is stirring concern. The standard view is that routine work in factories and offices, like bookkeeping or operating basic machinery, is most vulnerable to automation.\nBut A.I. software that can read and analyze text or speech - so-called natural language processing - is encroaching on the work of professionals. For example, there is a lot of legal work that is routine, said Frank Levy, a labor economist at the Massachusetts Institute of Technology. But that routine work, sifting through documents for relevant information, is wrapped in language, which had protected lawyers from the effects of automation. But no longer.\n\"Natural language processing opens the door to doing more and more work that was beyond automation until recently,\" Mr. Levy said.\nThe McKinsey report cites natural language processing as a key technology: The faster it develops, the more that tasks can be automated; a slower pace means less automation.\nThe economic cost of automation is another concern. People see advances in self-driving vehicles, and think that the jobs of America's 1.7 million truck drivers are in imminent peril, said Michael Chui, a partner at the McKinsey institute and an author of the report.\nYet replacing America's truck fleet would require a trillion-dollar investment, Mr. Chui said, adding \"if you could buy a self-driving truck, which you can't.\"\nSuch uncertainties led the McKinsey researchers to calculate the pace of automation as ranges rather than precise predictions. The report's multifactor scenarios suggest that half of today's work activities could be automated by 2055. That threshold could be reached 20 years earlier or 20 years later, the report adds, depending on economic trends, labor market dynamics, regulations and social attitudes.\nSo while further automation is inevitable, McKinsey's research suggests that it will be a relentless advance rather than an economic tidal wave.\n\"We have more time than we think to adjust to the world that technology makes possible,\" said Matthew J. Slaughter, an economist and dean of the Tuck School of Business at Dartmouth College.\nThe McKinsey report also considered the other side of automation - as an engine of productivity and economic growth.\nIn that context, the report points to the demographic challenges ahead, as most nations age, and the need for automation-powered growth. Today, there are 46 million Americans over 65, or 15 percent of the population. By 2060, the size of the over-65 group is projected to reach 98 million people, or 24 percent of the population.\nThe American economy may need all the A.I.-assisted automation it can muster.\n\"If productivity growth continues to be anemic, we're in trouble,\" said Hal Varian, Google's chief economist and an emeritus professor at the University of California, Berkeley.\nPHOTO: Testing a manned robot, Method-2, in Gunpo, South Korea, last month. A new study says automation will come in a relentless advance rather than a tidal wave. (PHOTOGRAPH BY CHUNG SUNG-JUN\/GETTY IMAGES)\n","425":"SEOUL, South Korea --  Ending what was billed as the match of the century, a Google computer program defeated a South Korean master of Go, an ancient board game renowned for its complexity, in their last face-off on Tuesday.\nThe program AlphaGo's 4-1 victory was a historic stride for computer programmers and artificial intelligence researchers trying to create software that can outwit humans in board games. \u00a0\n  ''It made me question human creativity. When I saw AlphaGo's moves, I wondered whether the Go moves I have known were the right ones,'' the human competitor, Lee Se-dol, 33, said during a postmatch news conference. ''Its style was different, and it was such an unusual experience that it took time for me to adjust.''\n  ''AlphaGo made me realize that I must study Go more,'' said Mr. Lee, one of the world's most accomplished players.\n  Go is a two-person game of strategy said to have been created in China more than 3,000 years ago. \n  The players compete for territory by placing black and white stones on intersections of a board of 19 horizontal and 19 vertical lines.\n  The game has been the last remaining great hurdle for computer programmers attempting to make software more adept than humans at board games since the I.B.M.-developed supercomputer Deep Blue routed the world chess champion Garry Kasparov in 1997.\n  Artificial intelligence experts had predicted that a computer program needed at least 10 more years of development before it would be able to beat Go masters like Mr. Lee.\n  But AlphaGo, created by Google's artificial intelligence company DeepMind, had already surprised the Go community when it trounced the three-time European Go champion Fan Hui in October, 5-0. \n  It then challenged Mr. Lee, a much stronger opponent with 18 international titles under his belt.\n  AlphaGo quickly decided the best-of-five series, winning the first three matches.\n  Google has said it plans to donate the $1 million prize to Unicef and other charities.\n  But Mr. Lee staged a dramatic comeback and demonstrated a human resilience on Sunday, when he defeated AlphaGo in the fourth game.\n  Millions of Go fans in Northeast Asia, where the game is especially popular, watched intently during the match on Tuesday. It lasted the longest of the series: five hours.\n  Although many viewers did not understand the intricate play, the tension was clearly acute toward the end of the game. Each player was given one minute to deliberate and foresee complex moves and countermoves before placing a stone.\n  During a post-match ceremony, Hong Seok-hyun, head of the Korean national Go association, awarded the AlphaGo team the certificate of an honorary Go degree of Nine Dan, the highest granted. Mr. Lee also holds that degree.\n  Demis Hassabis, the chief executive of Google DeepMind, said playing Mr. Lee had exposed several weaknesses of AlphaGo that his team would try to address. \n  Computer algorithms used for AlphaGo ''one day can be used in all sorts of problems, from health care to science,'' he said.\n  More than 100 million people watched the AlphaGo-Lee matches, Mr. Hassabis said. \n  He said he hoped that the attention would encourage more people to learn Go, the ''most profound game humankind has devised.''\n  Mr. Lee said AlphaGo was unlike any human opponent he had faced.\n  ''It remained unfazed psychologically and stayed focused,'' he said. ''In that regard, I don't think humans can beat it, even though I hesitate to admit that AlphaGo is above humans in Go skills yet.''\n  Until the matches with AlphaGo, Mr. Lee said he had begun wondering whether he was enjoying the game anymore. \n  But he said the games had renewed his enthusiasm for Go, which he began playing professionally at age 12.\n  ''I have some regrets about the matches I have played against AlphaGo,'' he said. ''But I could not have enjoyed them more.''\n  Get news and analysis from Asia and around the world delivered to your inbox every day with the Today's Headlines: Asian Morning newsletter. Sign up here.\n\n\n\n","426":"SAN FRANCISCO -- In 1970, a Stanford artificial intelligence researcher named John McCarthy returned from a conference in Bordeaux, France, where he had presented a paper on the possibility of a ''Home Information Terminal.''\nHe predicted the terminal would be connected via the telephone network to a shared computer, which in turn would store files that would contain all books, magazines, newspapers, catalogs, airline schedules, public information and personal files. \n  Whitfield Diffie, then a young programmer at the Stanford Artificial Intelligence Laboratory, read Mr. McCarthy's paper and began to think about the question of what would take the place of an individual signature in a paperless world. Mr. Diffie would spend the next several years pursuing that challenge and in 1976, with Martin E. Hellman, an electrical engineer at Stanford, invented ''public-key cryptography,'' a technique that would two decades later make possible the commercial World Wide Web.\u00a0\n  On Tuesday, the Association for Computing Machinery announced that the two men have won this year's Turing Award. The award is frequently described as the Nobel Prize for the computing world and since 2014, it has included a $1 million cash award, after Google quadrupled its size.\n  This year, it was announced during the RSA Conference, a security technology symposium held here this week.\n  Named for Alan Turing, the British mathematician and computer scientist, the award is particularly noteworthy because it comes at a time that the Federal Bureau of Investigation is locked in a bitter feud with Apple over the agency's inability to unlock the cryptographic system that protects digital information stored in the company's iPhones.\n  While private information can be protected with a so-called ''symmetric'' key, or a single digital code, that is used to mathematically scramble the data, the problem becomes much more difficult when two parties who have not met physically wish to have a secret interaction.\n  The privacy protection technology that is now used extensively to protect modern electronic communications is based on Mr. Diffie's and Mr. Hellman's original research that led to the creation of ''public-key cryptography'' technology.\n  Public-key cryptography is a method for scrambling data in which each party has a pair of keys, one which can be publicly shared and the other which is known only to the intended recipient of a message. It is possible for anyone to encrypt a message using the individual's public key. However, the message can only be unscrambled with the aid of the private key held securely by the recipient of the message.\n  In the United States and elsewhere, cryptography was once a highly classified military and intelligence agency technology. But in the 1970s academic researchers began delving into the field, which led to clashes with law enforcement and spy agencies.\n  In 2013, documents released by Edward J. Snowden, the former government contractor, revealed widespread government surveillance of Internet traffic, leading companies like Apple and Google to modify the security in their products and to the current fight between Apple and the F.B.I.\n  Mr. Diffie and Mr. Hellman have long been political activists. Mr. Hellman has focused on the threat that nuclear weapons pose to humanity, and he said in an interview he would use his share of the prize money to pursue work related to the nuclear threat. He said he also planned to write a new book with his wife on peace and sustainability.\n  Mr. Diffie, who has spent his career working on computer security at telecommunications firms and at the Silicon Valley pioneer Sun Microsystems, has been an outspoken advocate for the protection of personal privacy in the digital age.\n  He said in an interview that he plans to do more to document the history of the field he helped to create. ''This will free me to spend more of my time on cryptographic history, which is urgent because the people are quickly dying off,'' Mr. Diffie said.\n\n\n\n","428":"Amir Moravej, an Iranian computer engineer in Montreal, quietly worked last year on building software to help people navigate the Canadian immigration system. He saw it as a way for others to avoid the same immigration travails he suffered a few years earlier.\nThen came the American presidential election. ''Trump accelerated everything,'' said Mr. Moravej, 33, the chief executive of a software start-up named Botler AI. \u00a0\n  With immigration taking center stage in American politics and elsewhere, Botler AI began putting more resources into building a chatbot tailored to one of Canada's immigration programs. On Wednesday, the start-up plans to announce that Yoshua Bengio, a research pioneer in artificial intelligence and director of the Montreal Institute for Learning Algorithms, is joining the fledgling company as a strategy adviser.\n  Mr. Bengio is adding his intellectual firepower to ease the way for what could become a migration of high-tech talent. Canada stands to benefit from the American political climate and the Trump administration's efforts -- stalled in court so far -- to sharply restrict travel into the United States from six predominantly Muslim nations. After Mr. Trump's election, applications to Canada for student and temporary visas surged.\n  ''If we look back 10 years from now, I'd be surprised if the Trump effect didn't show up in the data,'' said Joshua Gans, a professor at the Rotman School of Management at the University of Toronto.\n  Immigration is a linchpin in Canada's economic policy. One-fifth of the country's population of 36 million is foreign-born. Canada has dozens of provincial and federal programs, but a priority is placed on highly skilled workers and entrepreneurs, often with points assigned for specialized expertise, education and language proficiency.\n  Trends in actual immigration will take time to show up conclusively, but the early evidence of a Trump effect is most apparent in a field like artificial intelligence, where Canada has been at the forefront of innovation and is seeking to build a large A.I. industry.\n  Not only are Canadian A.I. start-ups like Botler AI now building on interest in immigration and on homegrown talent, but major American technology companies, including Google, Microsoft and IBM, have also been adding to their A.I. research teams in Canada.\n  The ride-hailing service Uber announced on Monday that it was opening a branch of its advanced technologies group in Toronto, the company's first outside the United States. The lab, which will develop self-driving car technology, will be led by Raquel Urtasun, an expert in computer vision at the University of Toronto.\n  Canada has well-funded programs intended not only to lure A.I. experts to the country, but also to persuade A.I. researchers, educated at Canadian universities, to remain in Canada rather than depart for Silicon Valley, as so many have done before.\n  The nation's policy makers also want to persuade expatriate engineers and entrepreneurs to return to Canada -- and the political climate in the United States has influenced some to do so.\n  Ross Intelligence, an A.I. start-up founded in Toronto, moved to the San Francisco Bay Area two years ago for the business and funding opportunities in the tech world's hotbed.\n  But last month, Ross, whose software can read through thousands of legal documents and rank relevant cases for lawyers, opened an office in Toronto. Five members of its team, including senior engineers and two co-founders, are moving from San Francisco to Canada. The group includes two Canadians, a Brazilian, a Belgian and an American.\n  The Toronto outpost, said Jimoh Ovbiagele, a co-founder and chief technology officer of Ross, ''allows us to really recruit from the global talent pool.''\n  Mr. Ovbiagele, one of the Canadians who is relocating to Toronto, said Ross had received dozens of inquiries from international students concerned about the immigration risk of working in America. Ross, he said, recently hired engineers who were international students and graduates of Princeton, Cooper Union and the University of Toronto.\n  Another technologist making the move to Canada from Silicon Valley is Maxime Chevalier-Boisvert, 31, who returned to Montreal a few weeks ago after working for Apple for 13 months. There were other considerations, she said, but ''the election of Trump did play a role'' in convincing her that she would prefer to live in Canada.\n  So when an opportunity to work at Mr. Bengio's A.I. institute in Montreal became available recently, Ms. Chevalier-Boisvert did not hesitate. Her new salary is about a third of her income at Apple.\n  Then again, Ms. Chevalier-Boisvert observed, her rent for a two-bedroom apartment in Montreal is less than a third of the monthly rent she paid for a one-bedroom apartment in Sunnyvale, Calif. And Montreal, she added, is a cosmopolitan city.\n  ''Living in Montreal is pretty good,'' Ms. Chevalier-Boisvert said.\n  Back at Botler AI, a lot of work remains -- including landing funding and figuring out a business plan. But the addition of Mr. Bengio is a sign that the start-up needs to be taken seriously.\n  Mr. Bengio, in an interview, said he was joining the start-up partly because Botler AI's technology fits neatly with research underway at his A.I. institute. What's more, he added, the company's work around immigration could ''help a lot of people.''\n  Follow Steve Lohr on Twitter @SteveLohr.\n\n\n\n","429":"A WORKING THEORY OF LOVEBy Scott Hutchins\n328 pp. The Penguin Press. $25.95.\nYou could argue that the fundamental question behind all literature is: ''What does it mean to be human?'' Some people have even argued that storytelling itself is what makes us more than just monkeys with iPhones -- that Homer created the modern consciousness, or that Shakespeare (as Harold Bloom has it) invented the human identity. In recent years, however, literature has lost a lot of ground on that score to evolutionary psychology, neurobiology and computer science, and particularly to the efforts of artificial intelligence researchers. So as we wait for the Singularity, when our iPhones will become sentient and Siri will start telling us what we can do for her, many of the savvier fiction writers have begun to come to grips with the fact that the tutelary spirit of the quest for the human may not be Dante or Emily Dickinson or Virginia Woolf, but Alan Turing, the British mathematician who helped start the revolution in computing.\u00a0\nTuring may be best known for his version of the Victorian-era Imitation Game, in which a judge receives written responses to his questions from a man and a woman behind a screen and tries to guess from the answers which is the man and which the woman. In Turing's version, the messages are from a human and a computer; it was his contention that when a judge couldn't tell the difference any longer, then a machine could be said to think like a human being. The Turing test has since become, at least in the popular imagination, the holy grail of artificial intelligence developers, as well as a conceit in contemporary fiction, and that conceit is at the heart of Scott Hutchins's clever, funny and very entertaining first novel, ''A Working Theory of Love.''\nThe novel's wisecracking narrator is Neill Bassett Jr., a 30-something native of Arkansas who now lives the life of a rootless metrosexual in San Francisco. Though he's not a scientist, he makes up one-third of a tiny computer start-up in Menlo Park, alongside an Indonesian programmer named Laham and their boss, an elderly European genius named Henry Livorno, who are working to create an artificial intelligence for a planned Turing test (which seems to be based on the annual Loebner Prize). Livorno has incorporated software into his computer that evokes the seven deadly sins, but more important, he has also loaded into its memory the 5,000-page diary of Neill's father -- the ''Samuel Pepys of the South'' -- an Arkansas doctor who killed himself when Neill was 19. Neill's job, as the real Dr. Bassett's son, is to participate in a series of conversations with the programmed ''Dr. Bassett'' in order to make -- its? his? -- responses more lifelike. As a result, Neill finds himself engaging every day with a clever, if creepy, simulacrum of his dead father, leading him inevitably into a quest to find the reasons for the real doctor's suicide.\nMeanwhile, the novel also chronicles Neill's feckless quest for love in San Francisco. He's just gone through a divorce, and as the novel starts, he's coming to the end of a period of revolving-door hookups, the last of which, with a troubled and impressionable young barista named Rachel, has the possibility of turning into something more. Not long after he starts seeing her, however, Neill finds himself flirting again with his ex-wife, and not long after that he finds himself in bed with a brusque but energetic young programmer for a rival company. (Her boss, a charismatically amoral former student of Livorno's, is the founder and chief executive of an online dating company, who is leading a much better financed effort to win the Turing test.) In addition to all this, the novel encompasses Neill's relationship with his mother, his return to Arkansas to get at the truth of his dad's suicide and a good deal of expertly observed, if gentle, satire of life in the Bay Area.\nThat's a lot of ground to cover even for an ambitious novelist, and one could easily imagine Don DeLillo or Richard Powers running with the same ideas and cast of characters for 800 pages or so. And while this is a very accomplished novel, it feels a bit as if Hutchins has given short shrift to its most original and exciting element: the philosophical struggle with the Turing test. The scenes in which the researchers volley with ''Dr. Bassett'' are the most electrifying in the book, and even though the journey through family history and the story of Neill's romantic and sexual escapades are beautifully written and consistently engaging, I found myself eager to get back to the undead doctor, who in his halting, awkward fashion is the most affecting character in the book -- much the way his direct ancestor HAL was the most lifelike character in Stanley Kubrick's ''2001: A Space Odyssey.''\nThat said, at the heart of Hutchins's attempt to bring these plotlines together is a brilliant insight into the underpinnings of the Turing test in logical positivism: namely, the idea that the best you can say about consciousness, either human or machine, is that if the mind you're dealing with seems conscious, then it is conscious, that in a positivist universe there is no difference between seeming and being. Hutchins then takes this a step further, by having Neill apply, or at least try to apply, that argument to his love life, and while I'm not sure the novel quite brings it off -- the Turing test, when it happens, is surprisingly anticlimactic, and Neill's romantic conflict resolves itself pretty much the way you suspect it will -- simply raising the question in such an original way yields unexpected dividends. Hutchins is an unsentimental and compassionate creator of vivid characters, a master aphorist (''Artists are always the Johnny Appleseeds of gentrification'') and an expert architect of set pieces, not the least of which is a hilariously crass and creepily persuasive monologue by the matchmaking king, which takes online romance to its logical conclusion. You'll never think of the term ''computer dating'' the same way again.\nA novel is itself a kind of advanced Turing test, in which a writer tries to convince readers that lifeless signs on a page are not just real intelligences moving through the real world, but actual human beings, with lustful urges, deep regrets and breakable hearts. As this novel demonstrates, part of the challenge of giving a machine a truly human intelligence is making it sound humanly unreasonable. Turing predicted that in order to pass his test, a machine would have to fool a judge at least 30 percent of the time, but Scott Hutchins, in this charming, warmhearted and thought-provoking novel, already has that beat.\n","430":"Nearly 20 years ago, after a chess-playing computer called Deep Blue beat the world grandmaster Garry Kasparov, I wrote an article about why humans would long remain the champions in the game of Go.\n''It may be a hundred years before a computer beats humans at Go -- maybe even longer,'' Dr. Piet Hut, an astrophysicist and Go enthusiast at the Institute for Advanced Study in Princeton, N.J., told me in 1997. ''If a reasonably intelligent person learned to play Go, in a few months he could beat all existing computer programs. You don't have to be a Kasparov.'' \n  That was the prevailing wisdom. Last month, after a Google computer program called AlphaGo defeated the Go master Lee Se-dol, I asked Dr. Hut for his reaction. ''I was way off, clearly, with my prediction,'' he replied in an email. ''It's really stunning.''\u00a0\n  At the time, his pessimism seemed well founded. While Deep Blue had been trained and programmed by IBM with some knowledge about chess, its advantage lay primarily in what computer scientists call brute-force searching. At each step of the game Deep Blue would rapidly look ahead, exploring a maze of hypothetical moves and countermoves and counter-countermoves. Then it would make the choice that its algorithms ranked as the best. No living brain could possibly move so fast.\n  But in Go, an ancient board game renowned for its complexity, the ever-forking space of possibilities is so much vaster that sheer electronic speed was not nearly enough. Capturing in a computer something closer to human intuition -- the ability to seek and respond to meaningful patterns -- seemed crucial and very far away.\n  Other seemingly distant goals included the ability to translate automatically between two languages or to recognize speech with enough accuracy to be useful outside the laboratory. Computer scientists had already spent decades trying to crack these problems.\n  For many, the aim was not just to make an artificial intelligence, but to understand deep principles of syntax, semantics and phonetics, and even what it means to think.\n  Now anyone with a smartphone or laptop (communing by Internet with a supercomputing cloud) can get a rough translation of text in many languages. They can dictate instead of type. Photo software can sort not just by date and location but by the faces of the subjects.\n  The results are imperfect and often clumsy, but they would have been mind-blowing in 1997. What happened between then and now?\n  Of course, computers became ever more powerful. But even today's fastest aren't able to anticipate all of the permutations of a situation like playing Go. Success on this and other fronts has come from harnessing speed in other ways.\n  The breakthrough in translation came from setting aside the question of what it means to understand a language and just finding a technology that works. The automated systems start with a text that has already been translated, by human brains. Then both versions are fed to a computer. By rapidly comparing the two, the machine compiles a thicket of statistical correlations, associating words and phrases with their likely foreign counterparts.\n  Similar approaches, more artificial than intelligent, have led to surprisingly rapid improvements in recognizing speech and facial images, as well as with playing championship Go.\n  In AlphaGo, learning algorithms, called deep neural nets, were trained using a database of millions of moves made in the past by human players. Then it refined this knowledge by playing one split-second game after another against itself.\n  Tweak by algorithmic tweak, it became ever more adept at the game. By combining this insensate learning, which amounts to many human lifetimes of experience, with a technique called Monte Carlo tree search, named for the ability to randomly sample a universe of possible moves, AlphaGo prevailed.\n  That was an enormous victory. But the glory goes not to the computer program but to the human brains that pulled it off. At the end of the tournament in Seoul, South Korea, 15 of them took the stage. They represented just a fraction of the number of people it took to invent and execute all of the technologies involved. Lee Se-dol was playing against an army.\n  Back in 1997 I wrote, ''To play a decent game of Go, a computer must be endowed with the ability to recognize subtle, complex patterns and to draw on the kind of intuitive knowledge that is the hallmark of human intelligence.'' Defeating a human Go champion, I wrote, ''will be a sign that artificial intelligence is truly beginning to become as good as the real thing.''\n  That doesn't seem so true anymore. Ingenious learning algorithms combined with ''big data'' have led to impressive accomplishments -- what has even been called bottled intuition. But artificial intelligence is far from rivaling the fluidity of the human mind.\n  ''Humans can learn to recognize patterns on a Go board -- and patterns related to faces and patterns in language -- and even patterns of patterns,'' said Melanie Mitchell, a computer scientist at Portland State University and the Santa Fe Institute. ''This is what we do every second of every day. But AlphaGo only recognizes patterns related to Go boards and has no ability to generalize beyond that -- even to games similar to Go but with different rules.\n  ''Also, it takes millions of training examples for AlphaGo to learn to recognize patterns,'' she continued, ''whereas it only seems to take humans a few.''\n  Computer scientists are experimenting with programs that can generalize far more efficiently. But the squishy neural nets in our heads -- shaped by half a billion years of evolution and given a training set as big as the world -- can still hold their own against ultra-high-speed computers designed by teams of humans, programmed for a single purpose and given an enormous head start.\n  ''It was a regrettable game, but I enjoyed it,'' Mr. Lee said during the award ceremony. (Regret, enjoy -- these words do not compute.) He added that the contest ''clearly showed my weaknesses, but not the weakness of humanity.''\n  Picking up the plaque and bouquet he had been given as consolation prizes, he laughed nervously and stumbled from the stage. Several days later, he said he would like a rematch.\n  Like the Science Times page on Facebook.\n  This is a more complete version of the story than the one that appeared in print.         \n\n\n\n","431":"The proliferation of bots equipped\u00a0with artificial intelligence has humans interacting with machines in more emotionally charged situations\u00a0than ever before. Consider every time you call your bank, as I did yesterday to report a missing debit card.\n              How can we help you? You can say log-in support, account access, bill pay, credit card or more options.           \n\"More options.\"\u00a0\n              Sorry, I didn't quite get that.           \n\"MORE. OPTIONS.\"\n...\n              I'm sorry, I still didn't get that.           \nThe frustration of losing\u00a0my debit card combined with the frustration of this stumped,\u00a0automated voice had me foaming at the mouth. The rational side of my brain understood that the other end of the line was an algorithm and, for some reason, my words just didn't compute.\u00a0But in a moment of need, that didn't matter. I had a problem, dammit, and it was the\u00a0algorithm's job to fix it.\nThe frustrated-customer scenario is a textbook example that researchers provide to\u00a0explain why robots need to be more attuned to human emotion, as fleeting and fickle as it can be. If our voices become louder or sound depressed, or our faces look perplexed or angry, robots should be able to detect that and respond accordingly. That notion is behind\u00a0a burgeoning research area within artificial intelligence.\n\"We're trying to get systems to have some of that capability so they can provide a more human-like conversation and really understand the user better. We have been working for a long time on how to understand user state from various signals, including speech,\" said William Mark, president of the information and computing sciences division at SRI International.\nSRI International may be best known as the research\u00a0juggernaut\u00a0that invented Siri, the voice-powered virtual assistant that was spun off into a stand-alone company and sold to Apple in 2010. Ever since, she has taken orders (with mixed results) from the millions who use the iPhone and other Apple products.\nThe Menlo Park, Calif.-based firm has now created a platform called SenSay Analytics, which parses through the words, tone, volume, pitch and other characteristics of the human voice to better recognize\u00a0whatever emotion the speaker may be feeling. A robot can then respond accordingly by, say, offering an apology when you're angry, speaking more quickly when you're impatient, or ushering you to a human being when you're really ready to blow.\n\"What we're capable of now is really to provide real value in lots of real-life situations. That's not to say that it has the emotional understanding and other forms of human understanding that people do, but it's certainly at a state where useful things can be done,\" Mark said.\nSRI International\u00a0is one of many companies looking to advance artificial intelligence and robotics in an effort to better meet human needs. Just this week, some of the biggest names in tech formed the Partnership on Artificial Intelligence to Benefit People and Society. Google, Microsoft, Facebook, IBM and Amazon say the alliance is designed to conduct research and share information on how to move\u00a0the technology forward without comprising ethics or\u00a0transparency. SRI and, more notably, Apple, are not members.\n\"A\u00a0big part of the intelligence that humans have when they communicate with each other [comes from]\u00a0listening to a lot of different cues,\" said Elizabeth Shriberg, a principal scientist in the speech technology and research laboratory at SRI.\u00a0\"People expect listeners to pick up on those cues.\"\nBut even humans often miss or simply ignore those cues, as anyone with a teenage child, husband or wife, boss or mother-in-law can certainly attest. If humans often do a poor job of reading one another's emotions, can we expect a bot to do much better?\nThe answer is debatable and\u00a0ultimately may not matter. Shriberg\u00a0said the current \"gold standard\" is not whether bots are more effective than humans, but whether they are as effective as humans. Accuracy tests often involve several people\u00a0and a bot communicating with the same human subject, and then require each to assess the subject's\u00a0emotional state. If the bot's perception matches that of most of the humans, that would be considered a success whether it's right or not.\nIn short, at times bots will inevitably get it wrong.\n\"A\u00a0machine is not going to magically tell us how we feel. We don't even know how we feel all of the time,\" Shriberg said. \"But there is a lot of low-hanging fruit where it's really clear that someone is angry ... or someone is depressed.\"\nAs with other forms of artificial intelligence, robots can become smarter about a person's emotions over time, Mark said. That means your smartphone, computer or another piece of machinery you interact with regularly may come to understand when you're having an off day, if you're becoming confused more often, or if you seem more down than usual lately. As the technology becomes more precise, it's not hard to imagine possible applications in health and other areas.\n\"It's very helpful to have history or experience with an individual. The kinds of things the person wants, the kinds of responses they like,\" Mark said. \"If you're using some piece of technology, say a virtual assistant, then it's going to get a lot better at understanding you.\"\n              Read more from The Washington Post's Innovations section.\u00a0           \n","432":"MOUNTAIN VIEW, Calif. -- A few years ago, Google created a new kind of computer chip to help power its giant artificial intelligence systems. These chips were designed to handle the complex processes that some believe will be a key to the future of the computer industry.\nOn Monday, the internet giant said it would allow other companies to buy access to those chips through its cloud-computing service. Google hopes to build a new business around the chips, called tensor processing units, or T.P.U.s. \u00a0\n  ''We are trying to reach as many people as we can as quickly as we can,'' said Zak Stone, who works alongside the small team of Google engineers that designs these chips.\n  Google's move highlights several sweeping changes in the way modern technology is built and operated. Google is in the vanguard of a movement to design chips specifically for artificial intelligence, a worldwide push that includes dozens of start-ups as well as familiar names like Intel, Qualcomm and Nvidia.\n  And these days, companies like Google, Amazon and Microsoft are not just big internet companies. They are big hardware makers.\n  As a way of cutting costs and improving the efficiency of the multibillion-dollar data centers that underpin its online empire, Google designs much of the hardware inside these massive facilities, from the computer servers to the networking gear that ties these machines together. Other internet giants do much the same.\n  In addition to its T.P.U. chips, which sit inside its data centers, the company has designed an A.I. chip for its smartphones.\n  Right now, Google's new service is focused on a way to teach computers to recognize objects, called computer vision technology. But as time goes on, the new chips will also help businesses build a wider range of services, Mr. Stone said.\n  At the end of last year, hoping to accelerate its work on driverless cars, Lyft began testing Google's new chips.\n  Using the chips, Lyft wanted to accelerate the development of systems that allow driverless cars to, say, identify street signs or pedestrians. ''Training'' these systems can take days, but with the new chips, the hope is that this will be reduced to hours.\n  ''There is huge potential here,'' said Anantha Kancherla, who oversees software for the Lyft driverless car project.\n  T.P.U. chips have helped accelerate the development of everything from the Google Assistant, the service that recognizes voice commands on Android phones, to Google Translate, the internet app that translates one language into another.\n  They are also reducing Google's dependence on chip makers like Nvidia and Intel. In a similar move, it designed its own servers and networking hardware, reducing its dependence on hardware makers like Dell, HP and Cisco.\n  This keeps costs down, which is essential when running a large online operation, said Casey Bisson, who helps oversee a cloud computing service called Joyent, which is owned by Samsung. At times, the only way to build an efficient service is to build your own hardware.\n  ''This is about packing as much computing power as possible within a small area, within a heat budget, within a power budget,'' Mr. Bisson said.\n  A new wave of artificial intelligence, including services like Google Assistant, are driven by ''neural networks,'' which are complex algorithms that can learn tasks on their own by analyzing vast amounts of data. By analyzing a database of old customer support phone calls, for example, a neural network can learn to recognize commands spoken into a smartphone. But this requires serious computing power.\n  Typically, engineers train these algorithms using graphics processing units, or G.P.U.s, which are chips that were originally designed for rendering images for games and other graphics-heavy software. Most of these chips are supplied by Nvidia.\n  In designing its own A.I. chips, Google was looking to exceed what was possible with these graphics-oriented chips, speed up its own A.I. work and lure more businesses onto its cloud services.\n  At the same time, Google has gained some independence from Nvidia and an ability to negotiate lower prices with its chip suppliers.\n  ''Google has become so big, it makes sense to invest in chips,'' said Fred Weber, who spent a decade as the chief technology officer at the chip maker AMD. ''That gives them leverage. They can cut out the middleman.''\n  This does not mean that Google will stop buying chips from Nvidia and other chip makers. But it is altering the market. ''Who's buying and who's selling has changed,'' Mr. Weber said.\n  Over the years, Google has even flirted with the possibility of designing its own version of the chips it buys from Intel.\n  Mr. Weber and other insiders question whether Google would ever do this, just because a C.P.U. is so complex and it would be so much more difficult to design and maintain one of these chips. But at a private event in San Francisco last fall, David Patterson, a computer science professor at the University of California, Berkeley, who now works on chip technologies at Google, was asked if the company would go that far.\n  ''That's not rocket science,'' he said.\n\n\n\n","433":"Herbert A. Simon, an American polymath who won the Nobel in economics in 1978 with a new theory of decision making and who helped pioneer the idea that computers can exhibit artificial intelligence that mirrors human thinking, died yesterday. He was 84.\n     He died at the Presbyterian University Hospital of Pittsburgh, according to an announcement by Carnegie Mellon University, which said the cause was complications after surgery last month. Mr. Simon was the Richard King Mellon University Professor of Computer Science and Psychology at the university -- a title that underscored the breadth of his interests and learning. \u00a0\n Mr. Simon also won the A. M. Turing Award for his work on computer science in 1975 and the National Medal of Science in 1986. In 1993, he was awarded the American Psychological Association's award for outstanding lifetime contributions to psychology.\nIn 1994, he became one of only 14 foreign scientists ever to be inducted into the Chinese Academy of Sciences and in 1995 was given awards by the International Joint Conferences on Artificial Intelligence and the American Society of Public Administration.\nAwarding him the Nobel, the Swedish Academy of Sciences cited \"his pioneering research into the decision-making process within economic organizations\" and acknowledged that \"modern business economics and administrative research are largely based on Simon's ideas.\"\nProfessor Simon challenged the classical economic theory that economic behavior was essentially rational behavior in which decisions were made on the basis of all available information with a view to securing the optimum result possible for each decision maker.\nInstead, Professor Simon contended that in today's complex world individuals cannot possibly process or even obtain all the information they need to make fully rational decisions. Rather, they try to make decisions that are good enough and that represent reasonable or acceptable outcomes.\nHe called this less ambitious view of human decision making \"bounded rationality\" or \"intended rational behavior\" and described the results it brought as \"satisficing.\"\nIn his book \"Administrative Behavior\" he set out the implications of this approach, rejecting the notion of an omniscient \"economic man\" capable of making decisions that bring the greatest benefit possible and substituting instead the idea of \"administrative man\" who \"satisfices -- looks for a course of action that is satisfactory or 'good enough.' \"\nProfessor Simon's interest in decision making led him logically into the fields of computer science, psychology and political science. His belief that human decisions were made within clear constraints seemed to conform with the way that computers are programmed to resolve problems with defined parameters.\nIn the mid-1950's, he teamed up with Allen Newell of the Rand Corporation to study human decision making by trying to simulate it on computers, using a strategy he called thinking aloud.\nPeople were asked for the general reasoning processes they went through as they solved logical problems and these were then converted into computer programs that Professor Simon and Mr. Newell thought equipped these machines with a kind of artificial intelligence that enabled them to simulate human thought rather than just perform stereotyped procedures.\nThe breakthrough came in December 1955 when Professor Simon and his colleague succeeded in writing a computer program that could prove mathematical theorems taken from the Bertrand Russell and Alfred North Whitehead classic on mathematical logic, \"Principia Mathematica.\"\nThe following January, Professor Simon celebrated this discovery by walking into a class and announcing to his students, \"Over the Christmas holiday, Al Newell and I invented a thinking machine.\"\nA subsequent letter to Lord Russell explaining his achievement elicited the reply: \"I am delighted to know that 'Principia Mathematica' can now be done by machinery. I wish Whitehead and I had known of this possibility before we wasted 10 years doing it by hand.\"\nBut in a much-cited 1957 paper Professor Simon seemed to allow his own enthusiasm for artificial intelligence to run too far ahead of its more realistic possibilities. Within 10 years, he predicted, \"a digital computer will be the world's chess champion unless the rules bar it from competition,\" while within the \"visible future,\" he said, \"machines that think, that learn and that create\" will be able to handle challenges \"coextensive with the range to which the human mind has been applied.\"\nSure enough, the I.B.M computer Deep Blue did finally beat the world chess champion Gary Kasparov last year -- about three decades after Mr. Simon had predicted the event would occur. \nBecause artificial intelligence has not grown as quickly or as strongly as Professor Simon hoped, critics of his thinking argue that there are limits to what computers can achieve and that what they accomplish will always be a simulation of human thought, not creative thinking itself. As a result, Professor Simon's achievements have sparked a passionate and continuing debate about the differences between people and thinking machines.\nBorn on June 15, 1916, the son of German immigrants, in Milwaukee, Herbert A. Simon attended public school and entered the University of Chicago in 1933 with the intention of bringing the same rigorous methodology to the social sciences as existed in physics and other \"hard\" sciences.\nAs an undergraduate his interest in decision making was aroused when he made a field study of Milwaukee's recreation department. After receiving his bachelor's degree in 1936 he became an assistant to Clarence E. Ridley of the International City Managers Association and then continued work on administrative techniques in the Bureau of Public Administration of the University of California at Berkeley.\nIn 1942, he moved to the Illinois Institute of Technology and in 1943 received his doctorate from the University of Chicago for a dissertation subsequently published in 1947 as \"Administrative Behavior: A Study of Decision-Making Processes in Administrative Organizations.\"\nIn 1937, he married Dorothea Pye, who survives him along with three children, Katherine Simon Frank of Minneapolis; Peter A. Simon of Bryan, Tex.; and Barbara M. Simon of Wilder, Vt.; six grandchildren, three step-grandchildren; and five great-grandchildren.\nA member of the faculty of Carnegie Mellon University since 1949, Professor Simon played important roles in the formation of several departments and schools including the Graduate School of Industrial Administration, the School of Computer Science and the College of Humanities and Social Sciences' psychology department.\nHe published 27 books, of which the best known today are \"Models of Bounded Rationality\" (1997), \"Sciences of the Artificial\"(1996) and \"Administrative Behavior\"(1997).\nIn 1991 he published his autobiography, \"Models of My Life,\" and remarked then about his vision of that all-vanquishing computer hunched over the chess boards of the world: \"I still feel good about my prediction. Only the time frame was a bit short.\" And so it was.\n","434":"Regarding Vivek Wadhwa's Nov. 6 On I.T. column, \"Artificial\u00a0intelligence makes leap forward, cracks CAPTCHA\" [Capital Business]:\u00a0\nThe world has been anticipating the positive and negative effects of AI for decades. Now, every heralded improvement to a computer program is some kind of overall improvement of AI, rather than just marketing hype. Programmers have been trying to break CAPTCHA for a long time. I suspect that a CAPTCHA 2.0 will be based on a three-dimensional model rather than the current two-dimensional model.\nIn any case, I believe we would be better served if we redefined artificial\u00a0intelligence into two categories, Programmed AI and Autonomous AI. Programmed AI would include improvements by programmers that make the products better. This is what the IT field has been doing since its inception. Autonomous AI poses the real threat: programs capable of altering themselves. \nIn the late 1980s, my company introduced a predictive maintenance module for our maintenance management program that used what is now called \"narrow\" AI.  It was an early form of machine learning based on rules. The difference was that we were \"bold enough\" to claim that the program incorporated elements of AI. I can still remember the naysayers who lined up telling me how wrong I was to make such claims. I am not sure how much we have progressed since then.\nMichael Adelman, Bethesda\n","435":"Having done the job all these years using conventional intelligence, the Internal Revenue Service is about to embark on applying artificial intelligence to some of its tasks. The tax agency describes artificial intelligence as ''the science of making machines do things that would require intelligence if done by men.''\u00a0Taxpayers would probably agree.\nThe good news for taxpayers is that the new system is designed to provide ''accurate, consistent and complete advice'' on some difficult technical questions posed to the agency's taxpayer service representatives.\nThe not-so-good news is that the I.R.S. intends to use the artificial intelligence system to catch taxpayers who regularly underreport their incomes.\nThe tax agency has also released some other tantalizing information, such as the fact that there are more than three million American citizens living abroad. It also says there are 537 non-Government American taxpayers residing in the Soviet Union, 1,410 in Nicaragua, 137 in Iraq 2,931 in China and 1,441 in Luxembourg.\n","437":"In \"Chappie,\" a dystopian robot thriller from South African director Neill Blomkamp (\"Elysium\"), we're introduced to an awkwardly stiff humanoid with something funny-looking sticking out of his head.\nAnd that's just Hugh Jackman, who, along with a ridiculous mullet, plays the movie's wooden, one-dimensional villain. The real automaton hero - a rabbit-eared police droid that develops artificial intelligence and a streetwise swagger after being adopted by a gang of Johannesburg thugs - is Chappie (South African slang for \"young man\"). As voiced by Blomkamp regular Sharlto Copley, Chappie is far more human than even his human nemesis Vincent, a muscle-bound soldier-turned-robot-designer who stomps through every scene like one of his automated combat troops. \nIn the role of a man who will stop at nothing - including allowing the streets of Johannesburg to descend into chaos in order to create more demand for his product - Jackman is simply painful to watch.\u00a0\nBut not as painful as it is to contemplate how naively the film treats the concept of artificial intelligence and robotics. Co-written by Blomkamp with his \"District 9\" writing partner Terri Tatchell, and set in 2016 - that's right, one short year from now, in a world that's gone straight to hell! - \"Chappie\" imagines a universe in which human consciousness is capable of being uploaded to a thumb drive, and where the Internet, that repository of everything from porn to the owner's manual for the space shuttle - is all one needs to access the entirety of human knowledge. (Never mind that last month I couldn't find a 1987 episode of \"SNL\" that I was looking for.)\n\"Chappie\" is a ball of contradiction. It takes the concept of \"Transcendence,\" crosses it with the storyline of \"RoboCop,\" and then delivers it, seemingly, to the target demographic of \"Short Circuit.\" It is, in other words, simultaneously dumb, hyperviolent and cutesy. \nWhy, for instance, do Chappie's \"eyes\" - represented by eight-bit black-and-white computer graphics that look like the screens of an old Motorola cellphone - narrow cartoonishly to slits when he gets \"angry\"? Why does he even have eyes, for that matter? Okay, okay, I get the anthropomorphizing. But a scene where Chappie, who is made out of bullet-resistent titanium, is shown getting some kind of tactile pleasure out of petting a dog is beyond illogical.\nThere's more pleasure to be had from watching Chappie's human caretakers, a couple of criminals called Yolandi and Ninja, who find Chappie and try to enlist him as a partner in crime. Played by  non-actors Yolandi Visser and Ninja, a South African rap duo who perform as Die Antwoord (or The Answer), the antiheroic characters are the best thing about the movie, despite being largely unsympathetic (i.e.,they're murderous thugs). They exude a raw appeal that, if not quite charm, is nonetheless highly watchable.\nAs Deon, the software engineer who wrote the computer code for Chappie, Dev Patel is adequate, if under-used. When he's wounded by one of Vincent's walking death machines - a remotely-operated war drone called the Moose - the scene fails to elicit the pathos it might otherwise warrant, simply because Patel is such a cipher. As for Sigourney Weaver, who plays Vincent and Deon's boss, she turns in a performance that's almost as heavy-handed as Jackman's.          \nVisually, \"Chappie\" has the cool and expensive look of a video game. It's adrenaline-stimulating eye candy. Despite Blomkamp's efforts to make some kind of commentary about the human soul, which the auteur bolsters with his trademark social consciousness - a tone of preachiness that, after three films, has worn out its welcome - the movie exhibits precious little humanity.\nLike Chappie, the movie seems human, but has a cold metal heart.\nmichael.osullivan@washpost.com\n* \u00bd \n(124 minutes, at area theaters) is rated R for violence, obscenity, drug content and brief nudity.\n","438":"I have the answer here for you. If you know anyone who thinks about, writes about or works on Artificial Intelligence, and you'd like to de-confuse them, send them this Blog Post. It will help them get the concept straight.\u00a0\nFor a subject about intelligence, dominated by smart people, never has so much sloppy thinking been in evidence. There are several areas that need to be disentangled.\nFirst is the \"wired like a brain\" vs \"programmed like a traditional computer\" debate. This is an interesting technical question, but its only significance is function. Which gets you what results?\nThe real morass is when they start talking about when Artificial Intelligence becomes \"alive.\" Listen. \"Alive\" and \"smart\" are not even CLOSE to being synonyms. Alive is a biological process. A tree is alive. IBM's Deep Blue is not alive.\nIn animals there is a relation between being alive and intelligence. But the 'alive' part of animal thinking can't be separated from the motivational structure of animal behavior.As to the relation of animal intelligence to animal life, I can explain this because I can tell you what the very first two thoughts on earth were. Drum roll please.\nThe first two thoughts were \"Yummy\" and \"Ouch.\" Are these thoughts? Sort of! When you are alive that is. And that's where a lot the confusion begins. LIVING intelligence doesn't exist somewhere outside the motivational system. It is integrated to an inseparable degree. It is not a question of linear vs parallel processing, or speed or complexity. It is a question of which behaviors align with \"yummy\" and which align with \"ouch.\" Thoughts develop and 'live' within this motivational substructure.\nThe evolution of thoughts went something like this: Yummy. Yummy over this way. Moving faster gets more yummy. Sneaking up on that gets more yummy. Going around and catching it from behind gets more yummy. Increasing complexity of thought, bound up with motivation. Eventually we got to the point where we had enough processing capacity to analyze this system in the abstract. And while analyzing it abstractly is \"intelligence,\" it is not the same as \"living.\" Without the yummy and the ouch, it's just data, and data processing.\nSo if you want to create a LIVING intelligence, you're going to need to start with motivation. This can't be just instructions, it needs neurons. To count as alive, yummy needs to actually taste like something, and ouch actually needs to hurt. Anything else is mimicry.\nYou can make a machine motivational system, and we have. But without actual feelings, it will always be a machine.\n","439":"Donald Michie, a versatile British scientist and early theorist of artificial intelligence who helped develop a ''smart'' industrial robot and then applied the technology to diverse fields, died on July 7 in Britain. He was 83.\u00a0\n  Dr. Michie (pronounced MICK-ee) died in a car accident near London along with his former wife, Anne McLaren, a biologist and pioneering researcher in the field of reproduction. \n  In the early 1970s, in work that received international attention and helped make Britain a force in advancing artificial intelligence, Dr. Michie led a team that produced ''Freddy,'' a computer-directed robotic arm that could choose and assemble parts from a jumbled and potentially confusing array. To demonstrate Freddy's capabilities, Dr. Michie programmed the machine to put together the parts of a toy truck.\n  Nils J. Nilsson, an emeritus professor of engineering at Stanford University and a former chairman of the department of computer science there, said the machine was ''ahead of its time'' and impressed researchers at Stanford and elsewhere as ''one of the first automatic assembly systems in the world.''\n  Dr. Nilsson added that industry had been  slow to see Freddy's potential, and it was not until the 1980s, after industries in Japan began to use robotic machines in manufacturing, that the work of Dr. Michie and other scientists was fully appreciated.\n  Earlier, in the 1960s, Dr. Michie developed an ingenious mechanical computer that he named Menace. The device was constructed of matchboxes and designed to play tick-tack-toe, recording information about successful moves by trapping colored beads in its boxes. A player would then consult the beads to determine what move to make next.\n  Menace and its 300 matchboxes proved important beyond games-playing as a relatively simple ''machine that could actually learn from past games, and was therefore quite revolutionary,'' said Bart Selman, a professor of computer science at Cornell who studies artificial intelligence.\n  In the 1970s, Dr. Michie, who trained as a geneticist, turned his hand to writing computer programs to solve complex problems in industry and science. His research has been used to improve flight simulators for pilot training and to increase the efficiency of a uranium refining plant. In 1979, he explained his ideas in ''Expert Systems in the Microelectronic Age,'' a book that Dr. Selman said brought ''a level of scientific quality to the field that remains unmatched.''\n  Dr. Michie's earliest scientific endeavors took place at Bletchley Park, the secret British intelligence center, where he was a cryptographer from 1942 to 1945. There he worked on ''Colossus,'' the high-speed computer, in helping to more rapidly break Germany's wartime codes.\n  Donald Michie was born in what was then Rangoon, Burma. He earned his doctorate in mammalian genetics from Oxford in 1953.\n  He joined the University of Edinburgh as a senior lecturer in surgical science in 1958. Edinburgh named him a professor of machine intelligence in 1967, and he remained there until retiring in 1984. Dr. Michie also served as chairman of the university's department of machine intelligence and perception.\n  He later taught at the Turing Institute, which he helped to found, at the University of Glasgow.\n  Dr. Michie, who was married two other times, is survived by two sons and two daughters.\n","440":"SAN FRANCISCO -- As the oracles of Silicon Valley debate whether the latest tech boom is sliding toward bust, there is already talk about what will drive the industry's next growth spurt.\nThe way we use computing is changing, toward a boom (and, if history is any guide, a bubble) in collecting oceans of data in so-called cloud computing centers, then analyzing the information to build new businesses. \n  The terms most often associated with this are ''machine learning'' and ''artificial intelligence,'' or ''A.I.'' And the creations spawned by this market could affect things ranging from globe-spanning computer systems to how you pay at the cafeteria.\u00a0\n  ''There is going to be a boom for design companies, because there's going to be so much information people have to work through quickly,'' said Diane B. Greene, the head of Google Compute Engine, one of the companies hoping to steer an A.I. boom. ''Just teaching companies how to use A.I. will be a big business.''\n  This kind of change is what keeps Silicon Valley going. When personal computers displaced mainframe computers, it opened the door not just for Apple, but for companies making PC software for business, games and publishing. In the networking and Internet revolutions, venture capitalists invested in these new computing styles, and another generation of companies was born.\n  Over the last decade, smartphones, social networks and cloud computing have moved from feeding the growth of companies like Facebook and Twitter, leapfrogging to Uber, Airbnb and others that have used the phones, personal rating systems and powerful remote computers in the cloud to create their own new businesses.\n  Believe it or not, that stuff may be heading for the rearview mirror already. The tech industry's new architecture is based not just on the giant public computing clouds of Google, Microsoft and Amazon, but also on their A.I. capabilities. These clouds create more efficient and supple use of computing resources, available for rent. Smaller clouds used in corporate systems are designed to connect to them.\n  The A.I. resources Ms. Greene is opening up at Google are remarkable. Google's autocomplete feature that most of us use when doing a search can instantaneously touch 500 computers in several locations as it guesses what we are looking for. Services like Maps and Photos have over a billion users, sorting places and faces by computer. Gmail sifts through 1.4 petabytes of data, or roughly two billion books' worth of information, every day.\n  Handling all that, plus tasks like language translation and speech recognition, Google has amassed a wealth of analysis technology that it can offer to customers. Urs H\u00f6lzle, Ms. Greene's chief of technical infrastructure, predicts that the business of renting out machines and software will eventually surpass Google advertising. In 2015, ad profits were $16.4 billion.\n  ''In the '80s, it was spreadsheets,'' said Andreas Bechtolsheim, a noted computer design expert who was Google's first investor. ''Now it's what you can do with machine learning.''\n  He added: ''Better maps and photos is just the start. It's going to be in life sciences, automobiles, everything.''\n  A number of start-ups are already aimed at the new architecture. A Mountain View, Calif., outfit called Mashgin uses ''computer vision'' to automate retail checkout. Up Highway 101 in San Mateo, a company called Alluxio is creating ways to make cloud-based A.I. work better. Last week, a San Francisco company called Mesosphere, which makes a way to operate among various corporate and public clouds, raised $73.5 million.\n  Microsoft and Amazon are racing Google to dominate the new architecture.\n  This week, Microsoft will kick off a conference in San Francisco that is expected to focus on ways machine-based intelligence can be used to analyze, among other things, ''the Microsoft graph,'' or all the data companies already have in the Microsoft products they've owned for decades.\n  Amazon last year announced its own machine-learning services, and it is amassing its own large repository of corporate data.\n  Hewlett Packard Enterprise, an older company struggling to find its way in the new landscape, was one of the investors in Mesosphere.\n  ''When you are building predictive data, you don't know what you are going to need next,'' said William Hilf, a senior vice president at H.P.E. ''If someone makes a bet in machine learning on Microsoft or Google, they may need to come down to their old data systems, too. We are building platforms to bridge among all of them.''\n  To Ms. Greene, all of the activity so far, along with the size and sophistication of computing, is small compared to what will happen when the world's biggest businesses start leaning on the new A.I. technology.\n  ''We may build an A.I. system to figure out all the ways businesses can use this,'' she joked. ''The relationship between big companies and deep machine intelligence is just starting.''\n\n\n\n","441":" Kenneth Mark Colby, 81, who merged his backgrounds in psychiatry and computer science to become a pioneer in computerized psychotherapy and artificial intelligence, died April 20 at his home here. The cause of death was not reported. \n He was a graduate of Yale University and its medical school and practiced psychiatry for 20 years, becoming increasingly interested in the developing field of computer technology. \u00a0\n His first foray into combining the two came in the late 1960s, when he was working at Stanford University under a career scientist research fellowship from the National Institute of Mental Health. Heading a team of graduate students, he created PARRY, a computer model of paranoid thinking, in the Stanford Artificial Intelligence Laboratory. \n He developed more sophisticated computerized psychology products, refining his fascination with computer comprehension and human language, during his tenure from 1974 to 1990 as a professor of both psychiatry and computer science at the University of California at Los Angeles. \n In the 1970s, Dr. Colby created what he called an \"intelligent speech prosthesis\" to aid the more than 500,000 stroke victims a year left with varying degrees of speech impairment. \n In the 1980s, as computers became more affordable, he turned his attention to what he had yearned to do for years -- use the machines to treat the 90 percent of people with mental illness who never seek professional help, either because of cost or social stigma. \n With the assistance of his computer programmer son Peter, he created the personal computer program first dubbed \"Overcoming Depression,\" which sold for $ 200.  \nThe system, later renamed the Good Mood Program and carrying a price tag of $ 99, has been used by the Department of Veterans Affairs, the Navy and Kaiser Permanente and has sold thousands of copies. \n After Dr. Colby's retirement from UCLA a decade ago, he and his family started Malibu Artifactual Intelligence Works to refine and market that program and a second program, PC Guru, which can discuss interpersonal relationships with the computer user. \n He is the author of 10 books and more than 100 articles on psychotherapy and artificial intelligence and two other books on chess. \n In addition to his son, survivors include his wife, Maxine, of Los Angeles; a daughter; and two grandsons. \n","443":"Facebook's hiring of French artificial-intelligence trailblazer Yann LeCun in 2013 to start its AI Research lab signaled that the social-media giant was serious about competing in the kinds of technologies revolutionizing the Web. Its highly visible brand of AI helped turn the flood of pictures, pokes and personal data into one of the world's most popular \u00adwebsites.\nBut criticism over election-meddling ads, \"fake news\" and the social network's impact on mental health - problems that Facebook is looking to artificial intelligence to help solve - has sparked questions over whether Facebook is keeping up with its rivals in the aggressively competitive world of AI development and research.\u00a0\nThis week, Facebook shook up its AI management, shifting \u00adLeCun to a more limited role in AI strategy, direction and external \"evangelism.\" To manage the technology's growth, Facebook hired J\u00e9r\u00f4me Pesenti, who previously led IBM's AI platform Watson, to hold the new position of vice president of AI.\nThe change in leadership highlights the rapid advancement of AI at Facebook - and how far it still needs to go. \n\"They are a significant player in AI today, where they totally weren't five years ago,\" said Pedro Domingos, a University of Washington professor, AI researcher and author of \"The Master Algorithm.\" \"Having said that, they are still a minion in the terms of Google or Microsoft.\" \nHe said Facebook's team of roughly 100 AI researchers was a small fraction of the team at Google or Microsoft and far more limited in its scope. \"This is the Red Queen hypothesis,\" he said, referring to a concept in evolution. \"It's not how fast you're running but, in a relative sense, how fast you're running compared to everyone else.\"\nFacebook this week said it would double the size of its AI lab in Paris. In all, the company currently employs more than 100 AI researchers in the United States, Montreal, Tel Aviv and Paris. \nLeCun's new role reflects the increasing sophistication of Facebook's research and product arms, company spokesman Ari Entin said. \n\"The reality is that AI is more important than ever to Facebook,\" Entin said. \"Our teams are growing. We're continuing to publish and open-source more than ever before, and deploying AI across Facebook at a really high level.\"\nLeCun referred questions to the company, which did not make LeCun or Pesenti available for comment. He wrote on Facebook that his role would focus \"on scientific leadership, strategy and external communication, with less emphasis on operational management.\" \nAI has long been the bedrock for the key features Facebook needs to gain new users and keep people engaged, such as facial-recognition systems in photo tagging and the algorithms that decide where posts land on users' News Feed. \nFacebook chief executive Mark Zuckerberg personally recruited LeCun, a New York University professor known for his breakthroughs in deep learning.   \nFacebook's early AI lab was limited, but Zuckerberg voiced grand ambitions. \"One of our goals for the next five to 10 years,\" he said in 2015, \"is to basically get better than human level at all of the primary human senses: vision, hearing, language, general cognition.\"\nAI seized a growing portion of Facebook's core technologies, including in recognizing faces, translating between languages, targeting advertisements, captioning videos, pinpointing inappropriate content and recommending \"people you may know.\"\nYann \"has built a world-class team at Facebook, having it both publish great papers and ship products,\" said Andrew Ng, a \u00adco-founder of Google Brain and former chief scientist of the Chinese tech giant Baidu, including what he said were impressive feats in natural-language processing and image recognition.\nBut the technology is also central to some of Facebook's most controversial, high-visibility issues, including how to deal with misinformation or hate speech, discriminatory advertising, and questions of attention ma\u00adnipu\u00adla\u00adtion and social media addiction. \nBut some experts said Facebook's AI teams have, like at other tech giants, struggled with the natural tension between the teams in research, with their long timelines and academic pursuits, and application, whose mandate involves building features that actually work.\nHilary Mason, a vice president of research at Cloudera and founder of the machine-learning research firm Fast Forward Labs, added that few companies at Facebook's scale have achieved the perfect balance between research and results.\nThe company, she added, remains in\u00adcred\u00adibly alluring to AI talent: \"Data science types are attracted to where the data is, and Facebook has some of the most interesting data.\"\nFacebook competes for talent with tech giants such as Amazon and Google's DeepMind, which offer researchers a chance to work outside a social network and explore the possibilities in voice assistants, health, gaming, drone delivery and other moonshots. \nLeCun offered up his own defense in 2016 on the question-and-answer site Quora,  writing that Google \"is probably ahead\" of Facebook and other companies in the deployment of deep-learning and other AI techniques but said \"we are ambitious in our goals, we are here for the long run, and we have an impact on the company, which makes it easy for us to justify our existence.\"\nAt Facebook, Pesenti will oversee the division LeCun runs, Facebook AI Research, and Facebook's Applied Machine Learning division, which builds and deploys the AI used by more than 2 billion people around the world every month. \nDomingos said AI is critical to keeping the site successful, engaging and alive.\n\"Facebook has this amazing business where they don't even have to troll the Web for content. People just upload their stuff and then they serve it back out with ads attached, and they print money. It's great to be Facebook,\" Domingos said. But its \"machine learning has to respond. And if it doesn't respond, the whole site will be in much worse shape.\"\ndrew.harwell@washpost.com\n","444":" -- Joseph Weizenbaum, 85, a computer programmer who helped advance artificial intelligence only to become a critic of the technology later in his life, died March 5 of complications from stomach cancer at the home of one of his daughters in Groeben, Germany.\u00a0\nMr. Weizenbaum was a professor at the Massachusetts Institute of Technology in the mid-1960s when he developed ELIZA -- named for Eliza Doolittle, the heroine of \"My Fair Lady\" -- which became his best-known contribution to computer programming.\nThe program allowed a person to \"converse\" with a computer. What the person said was used to create the computer's reply.\n\"Weizenbaum was shocked to discover that many users were taking his program seriously and were opening their hearts to it. The experience prompted him to think philosophically about the implications of artificial intelligence, and, later, to become a critic of it,\" the MIT newsletter Tech Talk said.\nIn his 1976 book, \"Computer Power and Human Reason: From Judgment to Calculation,\" Mr. Weizenbaum said it could be both dangerous and immoral to assume that computers could eventually take over any human role. \"No other organism, and certainly no computer, can be made to confront genuine human problems in human terms,\" he wrote.\nMr. Weizenbaum, who was Jewish, was born in Berlin and fled to the United States in 1936 with his family to escape Nazi persecution, according to a short 2003 biography published by Magdeburg's Leibniz-Institut fuer Neurobiologie.\nHe began studying math at what was then Wayne University in Detroit in 1941 but broke off a year later to join the Army Air Forces. He served as a meteorologist.\nIn 1955, he joined a General Electric team that designed and built the first computer system dedicated to banking operations.\nBesides his work at MIT, he held academic appointments at Harvard University, Stanford University and the University of Bremen, among others. He was the chairman of the Scientific Council of Berlin's Institute of Electronic Business at the time of his death.\nIn addition to his four daughters, survivors include a son; and five grandchildren.\n","446":"The immense popularity of social media seems to have redefined \"privacy\" from the sense of keeping information secret to being in control over how information is shared - among friends, colleagues, companies or the government. Perhaps it's no surprise, then, that the world's largest social network, Facebook, has announced its plan to develop algorithms that could protect us from ourselves and the danger of the \"overshare.\"\nThe idea is that Facebook could warn, or even prevent, you from unleashing an embarrassing picture or revelation when under the influence or before you've thought through the consequences of the impact it could have - on family, friends or the boss. This idea is not particularly new; there are mobile apps that try to prevent people from drunk-dialing or texting on their phones, for example. One was even featured as the \"killer app\" developed by the protagonists in the Hollywood film \"The Internship.\"\u00a0\n              The danger of the drunk dial           \nThe aim of these apps is to stop people from embarrassing themselves as a result of being too quick, too thoughtless or - let's face it - too drunk to reflect on the potential consequences. But Facebook's project is different because it intends to use the deep learning form of artificial intelligence - rather than more simple measures, such as the time taken between the last key stroke and hitting the send button, or the number of spelling errors made while typing the message.\nDeep learning refers to a collection of artificial intelligence methods that try to build abstract relationships between concepts based on different representations of the data. For example, one application of deep learning might include facial recognition, so that an individual can be identified across different photographs even when the lighting, the angle of the face in the picture, and the facial expression all vary. Facebook, Apple and Google already offer this to allow us to quickly scan our digital photo library to identify and tag our friends.\nDeep learning is one of various machine learning techniques used by IBM's Watson system, which has even demonstrated that it has the power to win the game show Jeopardy!.\nSo, Facebook's initial target appears aimed at extending its face recognition capability to automatically differentiate between a user's face when sober and drunk - and to use this to get a user to think twice before hitting the post button. Of course, being detected as being drunk in photographs won't be the only factor that determines when we want to moderate our social media sharing behaviors. The nature of the links we share, like and comment on can reveal a wealth of information about us, from ethnic and socioeconomic background to political inclination and our sexuality. This makes the task for any artificial intelligence of managing our online privacy a challenging one.\n              Staying in charge           \nA key challenge to help us manage our privacy more effectively will be to develop techniques that can analyze the data - photographs, their time and location, the people in them and how they appear, or the content of links - and correlate this to the privacy implications for the user given the privacy settings.\nOur own research on adaptive sharing in social networks uses a quantitative model of privacy risk and social benefit to evaluate the effect of sharing any given piece of information with different members of the user's social network. Then it can provide recommendations for audiences to share with, or avoid.\nLike Facebook's efforts, our work is to apply machine-learning techniques - which will one day include detecting drunkenness in photographs, or automatically determining the sensitivity of different information and calculating the potential regret factor of the post you're about to make. Far from being a flippant or fanciful use of technology, these sorts of models will become a core part of the way we can engineer better privacy-awareness into the software we use.\n","448":"BEIJING -- During President Trump's visit to Beijing, he appeared on screen for a special address at a tech conference.\nFirst he spoke in English. Then he switched to Mandarin Chinese. \n  Mr. Trump doesn't speak Chinese. The video was a publicity stunt, designed to show off the voice capabilities of iFlyTek, a Chinese artificial intelligence company with both innovative technology and troubling ties to Chinese state security. IFlyTek has said its technology can monitor a car full of people or a crowded room, identify a targeted individual's voice and record everything that person says.\n  ''IFlyTek,'' the image of Mr. Trump said in Chinese, ''is really fantastic.''\n  As China tests the frontiers of artificial intelligence, iFlyTek serves as a compelling example of both the country's sci-fi ambitions and the technology's darker dystopian possibilities.\u00a0\n  The Chinese company uses sophisticated A.I. to power image and voice recognition systems that can help doctors with their diagnoses, aid teachers in grading tests and let drivers control their cars with their voices. Even some global companies are impressed: Delphi, a major American auto supplier, offers iFlyTek's technology to carmakers in China, while Volkswagen plans to build the Chinese company's speech recognition technology into many of its cars in China next year.\n  At the same time, iFlyTek hosts a laboratory to develop voice surveillance capabilities for China's domestic security forces. In an October report, a human rights group said the company was helping the authorities compile a biometric voice database of Chinese citizens that could be used to track activists and others.\n  Those tight ties with the government could give iFlyTek and other Chinese companies an edge in an emerging new field. China's financial support and its loosely enforced and untested privacy laws give Chinese companies considerable resources and access to voices, faces and other biometric data in vast quantities, which could help them develop their technologies, experts say.\n  China ''does not have the stringent privacy laws that Western companies have, nor are Chinese citizens against having their data collected, as (arguably speaking) government monitoring is a fact of China,'' analysts with the research firm Sanford C. Bernstein wrote in a report in November.\n  Already, China's companies have at times edged out foreign rivals. IFlyTek has won major competitions for speech recognition and translation. Two years before Microsoft did, Baidu, the Chinese internet search company, created software capable of matching human skills at understanding speech. This year the Shanghai-based start-up Yitu took first place in a major facial recognition contest run by the United States government.\n  IFlyTek and other Chinese companies say they follow China's laws and protect user data. But they agree that the sheer number of users in China, plus the government's single-minded drive to dominate the new technology, puts them at an advantage.\n  ''China has entered the artificial intelligence age together with the U.S.,'' said Liu Qingfeng, iFlyTek's chairman, at the Beijing conference. ''But due to the advantage of a huge amount of users and China's social governance, A.I. will develop faster and spread from China to the world.''\n  An iFlyTek spokeswoman said the company had yet to receive required permission from officials in Anhui, the Chinese province where it is based, to speak with the foreign news media.\n  IFlyTek is portrayed in the Chinese media both as a technology innovator and as an ally of the government. Last year iFlyTek helped prevent the loss of about $75 million in telecommunications fraud by helping the police target scammers, according to The Global Times, a nationalist tabloid controlled by the Communist Party. Its article quotes a Chinese security official as saying collecting voice patterns is like taking fingerprints or recording people with closed-circuit television cameras, meaning the practice does not violate their privacy.\n  ''We work with the Ministry of Public Security to pin down the criminals,'' said Liu Junfeng, the general manager of the company's automotive business, at a conference in September.\n  Where iFlyTek gets its data is not clear. But one of its owners is China Mobile, the state-controlled cellular network giant, which has more than 800 million subscribers. IFlyTek preloads its products on millions of China Mobile phones and runs the hotline service for China Mobile, which did not respond to a request for comment.\n  ''Data is gold,'' said Anil Jain, a professor who studies biometrics at Michigan State University. ''These days you cannot design an accurate and robust recognition system for anything'' without data.\n  Cars could be another major market, iFlyTek believes. China is pioneering a push into self-driving cars, which could heavily depend on voice technology. In September, iFlyTek introduced a new product, a glowing ellipsoid that mounts on a dashboard and listens for questions that it can check online, like a car-mounted Siri.\n  ''We have to understand if the car is our friend, if there is an emotional connection,'' Mr. Liu said.\n  Through a third-party supplier, a few hundred thousand of the four million cars that the Volkswagen Group sells in China annually will be equipped next year with iFlyTek voice recognition technology, said Christoph Ludewig, a spokesman for the German automaker. Volkswagen said it requires that any data gathered from drivers is kept anonymous.\n  ''Volkswagen will protect customers from the misuse of their data,'' Mr. Ludewig said.\n  Delphi, the American auto parts giant, said it had a relationship with iFlyTek to offer its services in China but declined to disclose details.\n  Mr. Liu, the head of iFlyTek's automotive business, said that the company's systems would be installed next year in some Jeeps sold in China and that it was developing automotive voice systems with Daimler, which owns the Mercedes-Benz brand. FiatChrysler, Jeep's parent, said it had not found any of its suppliers using iFlytek. A Daimler spokeswoman said that the company was regularly in discussions with potential suppliers but declined to say if iFlyTek was one of them.\n  Human rights groups worry that such rapidly evolving capabilities will be abused by China's autocratic government.\n  ''The Chinese government has been collecting the voice patterns of tens of thousands of people with little transparency about the program or laws regulating who can be targeted or how that information is going to be used,'' Sophie Richardson, Human Rights Watch's China director, wrote in a report in October.\n  In its home province of Anhui, iFlyTek has assembled a database of 70,000 voice patterns, according to the report, which also said that the police had begun collecting records of voice patterns as they would fingerprints. The report cited as one example three women suspected of being sex workers whose voices were registered in a database, it said, in part because they had been arrested in Anhui.\n  The local Chinese media has also reported about a new plan in Anhui to scan voice calls automatically for the voice-prints of wanted criminals, and alert the police if they are detected.\n  IFlyTek did not respond to requests for comment on the Human Rights Watch report but has said its data-gathering efforts will not stop, particularly as it participates in China's push to develop self-driving cars.\n  ''We are always talking about big data -- the vehicle produces many images every day,'' said Mr. Liu, the iFlyTek automotive executive.\n\n\n\n","449":"Many urban planners, artificial intelligence researchers, civil engineers and public officials aspire to create \"smart\" cities. Their goal is to deploy advanced technology to better study, monitor and manage urban growth and infrastructure, thereby helping cities become more livable, safe and sustainable, more functionally and economically efficient.\nMaking cities smarter is not a new idea. But with digital computers now able to store, process and interpret increasingly large amounts of data, and with significant advances being made in automation and artificial intelligence (AI), the potential for understanding, analyzing and taking quick action to enhance how cities operate has grown.\u00a0\nGeographic information system (GIS) technology was the first computer-based tool for achieving smarter urbanism. Envisioned in the 1960s, GIS evolved and gained widespread use beginning in the 1970s. Users could generate multiple highly detailed maps of urban or suburban areas, with each individual overlay displaying a specific layer of collected data.\nGIS maps are useful for planning purposes because they go far beyond showing only jurisdictional boundaries, transportation networks, topography, cultural landmarks or tourist destinations.\nLayers of GIS maps can show patterns of property ownership; household occupancy and income; property values and tax assessments; social and ethnic demography; types and locations of employment; building types, sizes and structural conditions; historic resources; utilities and public service networks, such as fire hydrants, streetlights and traffic signals; trees and vegetation; water and soils; and even meteorological data.\nGIS maps contribute greatly to informed analysis and data-based decision-making by planners and politicians. But a stack of GIS maps is still a multilayered snapshot in time, a one-time picture describing a city's composition and complexity at a particular moment.\nTo be truly smart, a city needs a dynamic, real-time information-gathering system, which is what \"smart city\" advocates envision. This necessitates installing throughout a city strategically positioned sensors - many of which would be video cameras - along with automated control devices linked to sensors.\nI would call this a \"geographic action system,\" or GAS, to complement GIS. \nGAS would continuously transmit real-time data about current city conditions - in video, symbolic or written form - directly to control devices or to human or robotic monitors. Monitors could immediately respond, initiating appropriate actions to alleviate or solve problems. GAS also could communicate directly with citizens, a task facilitated by citywide interconnectivity - public WiFi or satellite-based.\nTraffic congestion, a persistent problem, could be addressed more dynamically using advanced technology. Transportation engineers have long proposed equipping urban roads with sensors enabling real-time monitoring and management of traffic conditions.\nWhy not continually adjust traffic-control signals to relieve congestion and improve vehicular flow, depending on time of day or week, weather, local events and shifting traffic volume? As cars and trucks become more technically sophisticated, GAS technology could suggest alternate routes of travel instantaneously to vehicles on the road.\nWe already have GPS navigation systems in cars and on smartphones - such as Waze and Google Maps - showing real-time traffic conditions, along with locations of gas stations, restaurants, hotels, shopping and other destinations. Parking garages can now display at their entrances the number and location of available spaces. Likewise, GAS could report on available public parking spaces throughout a city. \nCable communication companies can locate and repair network breakdowns, and Pepco can pinpoint systemic electrical outages, if they are paying attention. Similarly, a city could monitor, detect and rapidly respond to breakdowns or lapses in public services.\nGAS could be on the lookout for flooding, blocked storm drains, icy roads, potholes, fallen trees, uncollected or spilled trash. It could dispatch crews as well as issue alerts, as D.C. does now for road closures, temperature extremes, fires and crimes, which often are recorded by privately installed CCTV security cameras as well as by cameras installed by the police. In London, with reportedly more than 500,000 CCTV cameras conducting surveillance only to detect lawbreaking, the city could become much smarter by using its cameras to look for much more than criminal activity. \nWhether in the District, London or any other city, a continuously monitored metropolitan network of sensors would further augment public safety; would quickly identify city problems; and would remedy problems by more efficiently deploying city resources. \nOf course, privacy questions arise. Today, if you carry a smartphone, your location is already detectable, and you are captured visually every day by countless security cameras. But GAS, like GPS, would require no personal information other than how to communicate with you. \nIs Washington today a \"smart city\"?\nThe city has gotten a bit smarter in recent years, but it has a way to go. Constraints include lack of necessary advanced technology and cost. Even with tens of thousands of video cameras and sensors installed today, the city cannot afford the many trained employees needed to continuously monitor and respond to all that goes on. \nAchieving a really high degree of urban smartness will depend on evolving AI and automation technology. What's needed are robots that can reliably process data from thousands of sensors, and that do not need to be paid, fed or housed.\nrealestate@washpost.com\nRoger K. Lewis is a retired practicing architect, a professor emeritus of architecture at the University of Maryland and a regular guest commentator on \"The Kojo Nnamdi Show\" on WAMU (88.5 FM).\n","450":"As the Chinese government develops drones, the American technology giant Qualcomm is helping. The same goes for artificial intelligence, mobile technology and supercomputers. Qualcomm is also working to help Chinese companies like Huawei break into overseas markets in support of China's ''go global'' campaign to develop big multinational brands.\nQualcomm is providing money, expertise and engineering for Beijing's master plan to create its own technology superpowers. \n  Big American companies fiercely protect their intellectual property and trade secrets, fearful of giving an edge to rivals. But they have little choice in China -- and Washington is looking on with alarm.\u00a0\n  To gain access to the Chinese market, American companies are being forced to transfer technology, create joint ventures, lower prices and aid homegrown players. Those efforts form the backbone of President Xi Jinping's ambitious plan to ensure that China's companies, military and government dominate core areas of technology like artificial intelligence and semiconductors.\n  As concerns mount about Beijing's industrial policy, the Trump administration is preparing a broad investigation into potential violations of American intellectual property, according to people with knowledge of the matter. Congress is also considering ways to restrict China's ability to acquire advanced technology by toughening rules to prevent the purchase of American assets and limit technology transfers.\n  In this arena, America's economic interests are aligned with its national security needs. The worry is that by  teaming up with China, American companies could be sowing the seeds of their own destruction, as well as handing over critical technology that the United States relies on for its military, space and defense programs.\n  Advanced Micro Devices and Hewlett Packard Enterprise are working with Chinese companies to develop server chips, creating rivals to their own product. Intel is working with the Chinese to build high-end mobile chips, in competition with Qualcomm. IBM has agreed to transfer valuable technology that could enable China to break into the lucrative mainframe banking business.\n  ''There's a great deal of unease in Washington,'' said James Lewis, an analyst at the Center for Strategic and International Studies, a Washington-based think tank. ''The defense, intelligence agencies and others are concerned that advanced chip-making capabilities are going to China.''\n  Qualcomm declined to comment, as did Intel.\n  Qualcomm is caught in the middle.\n  The world's dominant mobile phone chip maker, Qualcomm ran afoul of the Chinese government, getting hit in 2015 with a record $975 million fine for anticompetitive behavior. To get back in Beijing's good graces, the company agreed to lower its prices in China, promised to shift more of its high-end manufacturing to partners in China, and pledged to upgrade the country's technology capabilities.\n  The extent of Qualcomm's involvement with the Chinese government -- and the complications for American tech giants -- is seen in a low-slung office building in the southwest part of the country. There, a team of engineers is developing leading-edge microchips to compete with the finest made by Intel. The chips will help power a huge data and cloud center with the potential to strengthen the country's computing capabilities. No longer content to rely on buying the chips that go into cellphones, computers and cars, China now wants to design and build the brains that drive much of the digital world.\n  The government is providing land and financing to the start-up formed with Qualcomm, called Huaxintong Semiconductor. Qualcomm has provided the technology and about $140 million in initial funding.\n  ''Qualcomm has a balancing act,'' said Willy Shih, who teaches at Harvard Business School. ''Most of the world's PCs are made in China, and most of the world's smartphones too, so they have to play along. It's a fact of life.''\n  Qualcomm was early to break into China.\n  In the mid-1990s, as China's economy began to boom, President Bill Clinton pressed the country's leaders to open to American technology companies.\n  Members of the Clinton administration, including Charlene Barshefsky, the United States trade representative, and William M. Daley, the secretary of commerce, were dispatched to Beijing to hammer out the details. They pushed for one company by name: Qualcomm.\n  ''At the time, they were the only U.S. show in town,'' Ms. Barshefsky said.\n  ''Bill Daley and I pushed the Chinese hard on accepting the U.S. standard for wireless technology,'' she added, ''and that was Qualcomm.''\n  Mobile phone adoption was taking off globally, largely backed by a European wireless standard called G.S.M., or global system for mobile communications. Qualcomm had a competing American standard called C.D.M.A., or Code Division Multiple Access.\n  Irwin M. Jacobs, a founder of Qualcomm, spearheaded an aggressive lobbying campaign in Washington and Beijing, promoting the technology's potential to transform wireless communication markets.\n  ''We knew China would be important, and they didn't have their own system,'' said Perry LaForge, a former Qualcomm executive. ''We also told them this system would give them an opportunity to manufacture their own handsets, and not rely on buying them from other countries.''\n  When Qualcomm first entered China in the late 1990s, it was slow to gain traction. The company struggled to find Chinese partners to produce mobile phones that worked with its network. China also tried to develop its own wireless standard.\n  Qualcomm eventually won out, helping write the standards for next-generation mobile technology, 3G and 4G service. The standard championed by European telecom providers faded rapidly. And China's homegrown technology struggled.\n  By 2013, virtually every wireless device around the world was reliant on either Qualcomm's chips or its patents -- enough to provide some of the technology industry's fattest profit margins.\n  With its dominance rising, global brands like Apple and Samsung began complaining to regulators around the world, citing ''discriminatory'' pricing practices and high royalty fees. In China, a trade group made up of the country's major handset makers complained about patent holders levying ''exorbitant licensing fees.''\n  ''These days a smartphone is covered by about 250,000 patents,'' said Dieter Ernst, a senior fellow at the East-West Center, a research and educational center based in Honolulu. ''A Chinese smartphone maker needs to negotiate license agreements with companies like Qualcomm that own the essential patents.''\n  ''The Chinese government was worried about this,'' he added. ''That all these costs could constrain Chinese companies.''\n  The raids began at dawn, in late November 2013. Investigators descended upon Qualcomm's offices in Beijing and Shanghai, questioning the staff and hauling away laptops and documents.\n  At the time of the raids, the San Diego-based company's senior managers were at the Ritz-Carlton Hotel in New York, attending an investor conference. The executives were planning to talk about the company's strategy. Instead, they began fielding frantic phone calls from China.\n  The China business, which accounted for more than half of its global revenue, was in trouble.\n  A week later, one of the country's most powerful regulatory agencies, the National Development and Reform Commission (N.D.R.C.), announced that it was looking into whether Qualcomm had abused its power in the sale of mobile phone chips. ''Qualcomm came to control so much of the chip market in China,'' said Louie Ming, a former Qualcomm executive in China. ''It was clear they were eventually going to run into antitrust problems.''\n  While Qualcomm agreed to fully cooperate with the investigation, some senior executives appealed to the Obama administration, pressing the White House to raise the issue with China's senior leaders, according to a former administration official.\n  Qualcomm's troubles went beyond China. The company was also under scrutiny by antitrust regulators in the European Union and South Korea, as well as by the United States Federal Trade Commission.\n  China didn't back down. The head of the N.D.R.C. branded Qualcomm a monopoly.\n  In February 2015, after a 15-month-long investigation, Qualcomm settled allegations in China that it had charged unfairly high prices for its chips and patents. The company agreed to pay the $975 million fine -- about 8 percent of its annual revenue in China -- and to lower the prices for chips sold in the country.\n  ''We are pleased that the resolution has removed the uncertainty surrounding our business in China, and we will now focus our full attention and resources on supporting our customers and partners in China,'' said Steve Mollenkopf, the company's chief executive, said at the time.\n  Qualcomm then went into business with the Chinese government.\n  There was a $150 million investment fund to help Chinese start-ups; new research and design facilities set up with Chinese companies such as Huawei and Tencent; and a partnership with a Beijing-based company called Thundersoft to develop drones, virtual reality goggles and internet-connected devices.\n  Qualcomm is also helping the Chinese government develop supercomputers, a technology the United States government has discouraged American companies from supporting overseas. In May, Qualcomm agreed to form a joint venture with other state-backed firms to design and sell mass-market smartphone chips. And to help make Chinese chip manufacturing more competitive, Qualcomm has pledged to shift more of its high-end production -- long done by outside contractors in Taiwan and South Korea -- to China.\n  ''This is what China does better than anyone else,'' said Robert D. Atkinson, president of the Information Technology and Innovation Foundation, a think tank focused on technology policy that has conducted studies detailing the Chinese government's pressure on technology companies.\n  ''They have a large carrot and a large stick,'' he said. ''And they have a market no C.E.O. can walk away from.''\n  Qualcomm's biggest new venture is taking shape in southwest China's Guizhou Province. Determined to leap into advanced technology, China has designated a large parcel of land in the provincial capital of Guiyang as the home of a new industrial park for supercomputing, data centers and cloud computing. The country's large state-run telecom operators and its internet behemoths, including Alibaba and Tencent, are moving in, to build massive server farms. The region offers lower energy costs and abundant supplies of water, necessary to cool server farms.\n  A year ago, Qualcomm set up a joint venture with the Guizhou government and pledged to invest about $140 million for a minority stake in the business, situated in a development zone that has also attracted the interest of Microsoft and Dell. Qualcomm says it received American government approval for the deal.\n  The new Qualcomm joint venture, Huaxintong Semiconductor, broke ground on the site in 2016, and now operates in a 46,000-square-foot design and engineering center. A major test of the partnership will come when the joint venture's first server chips are released -- helping Qualcomm and the Chinese government stake out new ground. The Chinese government will control the chips and reap most of the profits.\n  In late March, Qualcomm's president, Derek K. Aberle, flew to Guizhou to meet a powerful local government leader, Chen Miner, a confidant of the Chinese president. Seated in a government hall, before an enormous landscape painting, Mr. Aberle pledged to ''continually cooperate'' with the Chinese government.\n\n\n\n","451":"Dr. Anne Foerst, 34, a researcher at the Artificial Intelligence Laboratory at the Massachusetts Institute of Technology and the director of M.I.T.'s God and Computers project, apologized on a recent afternoon that a certain robot named Kismet wouldn't be joining our interview.\n\"Cynthia Breazeal, who built Kismet, is away in Japan right now and there's no getting her going,\" Dr. Foerst said in her German accent, \"but you'd love her. She's oh so cute.\" A cute robot? Well, yes. At the Artificial Intelligence Laboratory, engineers are trying to build robots with social skills and humanlike experiences, and so, as an experiment, they've created creatures that they think humans will relate to.\u00a0\n Dr. Foerst, a Lutheran minister who supported herself by repairing computers during eight years of higher education in Germany, serves as theological adviser to the scientists building Kismet and the robot's brother, Cog.\u00a0Q. What exactly do people do here at this laboratory?\u00a0A. We are trying to build robots that are social and embodied.\nWe have four projects. I am the theological adviser for two of them: the building of the humanoid machines, Cog and Kismet.\nCog is a robot built in analogy to a human infant. He has a torso, two arms, a head, ears and eyes. He, it, learns to coordinate those limbs to explore its environment, just as newborn babies do. Kismet is a robot who interacts with humans through her body posture and facial expressions.  The aim of this project is to explore social interactions between humans and robots and also between the humans themselves.\u00a0Q. Why a theologian here in this particular laboratory?\u00a0A. Two reasons. The first is when you build machines in analogy to humans, you make assumptions about humans. Theologians explore the cultural and spiritual dimensions of that very question, What does it mean to be human? The idea is that as these robots are built, we can use the wisdom of religious studies to enlarge our understanding of humans, and thus what you build into the humanoid machines.\nThe other reason is that when we build social interactive robots that force people to treat them as if they were persons, tricky moral questions come up. For instance, Who are we, really? Are all our reactions actually developed in a very mechanistic, functionalist way? Or is there a dimension to social interaction that goes beyond that? What are ethics here? Why should I treat someone else like a human, with dignity, when it is just a mechanistic thing?\nFor instance, one question we discuss quite frequently is, What would be the threshold when the robots are developed to a certain point that you couldn't switch them off anymore?  The question really is, When does a creature deserve to be treated as intrinsically valuable?\u00a0Q. When do you think a robot should be treated as intrinsically valuable?\u00a0A. Well, that moment is 50 years down the road. At least. But it's pretty clear that when it comes, those who built the robot will have to make that decision because they won't be blinded by their fears of the seemingly human qualities of the machines. They'll know what's inside. And if it ever got to the point where the builders felt, Oops, now that has become something, the builders could become the creature's strongest advocates.\u00a0Q. What make the robots Cog and Kismet different from previous ones?\u00a0A. Previous attempts put very abstract features of human intelligence into a machine: chess playing, mathematical theorem-proving and natural language processing. The idea now is, In order for a machine to really be intelligent, it has to be embodied. We say intelligence cannot be abstracted from the body. We feel that the body -- the way it moves, grows, digests food, gets older, all have an influence on how a person thinks. That's why we've built Cog and Kismet to have humanoid features.\nCog moves and experiences the world the way someone who can walk upright might. He experiences balance problems, friction problems, weight, gravity, all the stuff that we do, so that he can have a body feeling that is similar to ours. The humanoid features are also crafted into the machines in order to trigger social responses from the people interacting with them.\nThe other thing we believe is that humans are human because we are social. Thus, we try to treat Cog and Kismet something like the way most of us treat babies, as if they have intentionality, emotion, desires and intelligence. We give them as much social interaction as we can.\nCog is a whole body and Kismet is mostly a head and facial expression. Our work with Cog concentrates more on the embodiment stuff and Kismet more on emotional-social learning.\u00a0Q. Is the robot Kismet a she?\u00a0A. Robots are its. But I can't help but think of her as a she. If you were to see Kismet, you would be taken by her enormously expressive face: long eyelashes, big blue eyes, movable brow, cute, kissy mouth. When Kismet puts her eyes on you and looks sad, you want to make her happy. Of course, part of you thinks, It's just a stupid machine. But you do react and you can't help it.\nThe point of reacting to Kismet is the same as reacting to a baby. We believe that only when you treat the machines as if they have all these social characteristics, will they ever get them. If you want to have an intelligent being, you need to create that circle. So we react here to Kismet's emotional displays. When she's bored, you want to make her happy. When she seems scared, you back off.\u00a0Q. Has the very social robot Kismet done anything yet that has astonished you?\u00a0A. Kismet has not yet learned. Cog is the one who learns. A former graduate student, Matt Williamson, the guy who built Cog's arms, taught the robot how to control his arm.\nTo coordinate the arms, Matt had to touch a part of Cog's body and then, the arm would touch that part, too. After he did that for the first time, Matt ran into my office and said, \"You've got to come to look at this.\" It looked so eerily human. It's not so much that Cog does something that's unexpected, it's more the human reaction, like, it's alive!\u00a0Q. People often talk about humans having some indefinable extra above life that makes for humanness -- some call it \"spirit.\" Can a robot have spirit?\u00a0A. Rod calls it \"juice.\" He says, \"Even though I get it all right, might there not be some juice I'm missing?\" I would say from a religious perspective, the juice is that which comes from the outside world and emerges in social interaction.\u00a0Q. Some people might complain that in building humanoid robots, you are trying to supplant God.\u00a0A. Yes. I know. They say, \"Do you want to be like God?\" Actually, if you use biology as your inspiration in your robot-building and focus on embodiment and environment, you get much more humble instead of arrogant. Suddenly, you realize that even the most brilliant robot that the most brilliant engineers have worked on for years and years is still dumber than an insect.\u00a0Q. So, in your view, God is, as the Latin Americans say, the \"intellectual author\" of everything?\u00a0A. No. The creative author. When we are creative, the power of creation is from God.\u00a0Q. In the many plays, novels and movies about robots, the dramatic climax of the story always comes at moment when the machine achieves sentience. Why do you think that is?\u00a0A. Well, I think it's the search to feel and to be treated like something more than the sum of the parts that's inherently dramatic. This is the moment when the robots start to participate in the all-too-human quest of what does it mean to be me?\u00a0Q. In the movie \"2001: A Space Odyssey,\" HAL becomes a danger to humans once he's sentient.\u00a0A. In \"Frankenstein,\" too. But in both cases, there is an explanation.  When you look at Frankenstein, he is never part of a community. His creator left him right away. The people hated him, feared him, ran away from him. The only person who ever loved him was a blind man who couldn't see what he looked like. Frankenstein was never treated as a valuable being, a person with dignity. He had to turn against the society that shunned him. Where should the goodness come from when he never experienced it himself?\nHAL is the same thing. And he's disembodied. There is no body with which to experience the world. I would even say that in such a setting a robot couldn't even become sentient. In the movie, HAL becomes sentient at some point and nobody notices. No one treats him properly and he's isolated and what happens?  He becomes psychotic.\u00a0Q. What's your favorite robot movie?\u00a0A. \"Blade Runner.\" I teach it in my classes. The robots have this absolute search for meaning, and when their quest is not taken seriously, it becomes fatal. The movie raises this wonderful question: how do humanoid creatures feel about having been created by us and how do they deal with their human-made limitations?\u00a0\u00a0\nhttp:\/\/www.nytimes.com\n","454":"Humanity may still be years if not decades away from producing sentient artificial intelligence. But with the rise of machine-learning\u00a0services in our smartphones and other devices, one type of narrow, specialized\u00a0AI\u00a0has become all the rage. And the research on this branch of AI is only accelerating.\u00a0\nIn fact, as more industries and policymakers\u00a0awaken to the benefits of machine learning, two countries appear to be pulling away in the research race. The\u00a0results will probably\u00a0have significant\u00a0implications for the future of\u00a0AI.\nIf you're not familiar with the term, \"deep learning\" is a subset of the overall branch of AI known as machine learning - which basically involves the use of computer algorithms to perform pattern recognition and analysis. It's this type of\u00a0AI that powers personal digital assistants such as Google Now, for example.\nThe chart above was published Wednesday by the Obama administration as part of a new strategic plan\u00a0aimed at spurring U.S. development of\u00a0artificial intelligence. What's striking about it is that although\u00a0the United States was an early leader on deep-learning research, China has effectively eclipsed it\u00a0in terms of the number of papers published annually on the subject. The rate of increase is remarkably steep, reflecting how quickly China's research priorities have shifted.\nThe quality of China's research is also striking. The chart below\u00a0narrows the research to include only those papers that were cited at least once by other researchers, an indication that the papers were influential in the field.\nCompared\u00a0with\u00a0other countries, the United States and China are spending tremendous research attention on deep learning. But, according to the White House, the United States is not investing nearly enough in basic research.\n\"Current levels of R&D spending are half to one-quarter of the level of R&D investment that would produce the optimal level of economic growth,\" a companion report published this week by the Obama administration finds.\nThe government is pushing for a major role for itself\u00a0in\u00a0AI research, and here's why: Becoming a leader in artificial-intelligence research and development\u00a0puts the United States in a better position to establish global norms on how AI should be used safely. When AI stands to transform virtually everything including\u00a0labor, the environment, and\u00a0the future of warfare and cyberconflict, the United States could be put at a disadvantage if other countries, such as China, get to dictate terms instead.\n","455":"Toyota's empathetic car of the future is there for you. You've had a frustrating day at work; it plays soft music and lowers the temperature. You're lost in an unfamiliar neighborhood; it offers to take over the driving. You start to nod off at the wheel; it taps you on the shoulder and starts up a conversation.\nThis unconventional interplay between the driver and automobile is central to concept cars that Honda and Toyota unveiled at the annual CES technology conference in Las Vegas this week. In the not-so-distant future, vehicles will not only be safer or more efficient. They will be our companion, watching our every move.\nThese cars, which only exist today as partially functional concepts, will use powerful artificial intelligence systems to memorize and store information about every passengers' likes and dislikes, how they speak, and the places they frequent, all to make decisions the car feels are in the riders' interest.\u00a0\nThe auto industry's pursuit of a hyper-personal experience comes as the very nature of automotive transportation is in flux. Many industry observers expect ride-sharing services will become more popular, with autonomous driving to follow. People may rely less on personal cars to get around, a prospect that is \"going to change the business model of private car ownership dramatically,\" said Karl Brauer, a Kelley Blue Book analyst.\nWith the basic business of buying and selling cars potentially facing a major change in the decades ahead, information about drivers and passengers is likely to hold tremendous value for automakers. While this may also lead to greater convenience for motorists, it creates yet another platform where our most sensitive data could be susceptible to privacy infringement and security hacks.\n\"Artificial intelligence and big data will make vehicles one of the most important windows into the habits of consumers next to their own phones and computers,\" said Ed Hellwig, the executive editor at Edmunds.com. \"Automakers will know more than they ever have about how their vehicles are used, which could lead to entirely new designs and features.\"\nAs with concept cars past, Toyota and Honda's new vehicles could be seen as quixotic playthings for car heads and tech geeks. Concept cars are built to introduce bold ideas, practical and otherwise. But Robert Carter, Toyota's senior vice president of automotive operations, told an audience at CES on Wednesday that components of its concept car will be tested on roads in Japan in the coming years.\nWhat's more, nearly all of the concepts are rooted in technology already being honed today. Start-ups and legacy automakers alike are testing applications for artificial intelligence and big data inside cars, and connecting those systems with your phone, home appliances and the other Internet-enabled devices that permeate our daily lives.\n\"Technology moves very slow until suddenly it doesn't,\" said Shawn DuBravac, a futurist and chief economist at the Consumer Technology Association. \"It's almost like boiling water. It takes a long time to heat up, then suddenly, instantly, it's boiling.\"\n[             At CES 2017, Faraday Future showed the car that could make or break it]          \nDrivers who approach Toyota's Concept-i car will see \"hello\" projected on the car door, a greeting from Yui, an artificial intelligence bot that designers call \"the person who rides shotgun with you.\" Inside, the car will collect a trove of real-time data, such as pupil dilation, perspiration rate and vocal tone, to assess the driver's emotional state and alter the car to better fit one's mood. Once Yui learns preferences for music, temperature, seat position and other features, it will automatically adjust settings before the driver even climbs in.\nYui will also scroll through social media channels to create \"serendipitous\" moments, such as recognizing that friends have checked into a local restaurant and suggesting a stop there to grab a bite as well.\n\"It's all subtle stuff but the ultimate goal is to not have you aware that a lot of this stuff is happening. Suddenly when you're done you have had a good experience,\" said William Chergosky, one of the car's designers.\n          Honda revealed its own concept car, called the NeuV, at CES on Thursday. The artificial intelligence assistant listens to the driver's phone conversations, automatically updating their calendar and route if plans change. If the driver is running late for a meeting, the car finds the fastest route. When there is time to spare, it might suggest stopping for coffee.\n\"The whole philosophy behind it is to create a much more emotional, human connection,\" said Nick Renner, the NeuV project leader.\nThe auto industry may be taking its cues from phone companies. Smartphones have evolved into personal lifelines that facilitate human interactions, make people's lives more convenient, and store valuable information. That cars will one day have that same potential seems only natural considering the amount of time people spend inside them each day.\n[             This car promises to be fully electric and drive itself. Will that be enough?]          \nThe reimagining of cars and how we interact with them started from the inside out, a break from the past when a sleek exterior and under-the-hood innovations took precedent, Chergosky said.\n\"For many decades, car design was really focused on the mechanics of the car and on the manufacture,\" said Wendy Ju, executive director at Stanford's Center for Design Research. \"In many ways, the in-car user experience was an afterthought; it needed to be good enough to be safe, or flashy enough to be featured, but the interaction was shallow.\"\nIn the near future, what your car learns about you could provide car companies lucrative new opportunities. A September 2016 report from consulting firm McKinsey estimates that carmakers could reap between $450 billion and $750 billion in revenue by 2030 from using car-generated data to develop new products and services.\nWhile much of that conversation is speculative, analysts say it's not hard to imagine targeted advertising inside the car, or apps that allow people to make dinner reservations, buy merchandise or play games. That's especially true as cars become more autonomous and occupants devote less attention to the vehicle's actual operation.\nAgain, consumers shouldn't expect to find all of these technologies as advertised on showroom floors anytime soon, or perhaps ever. Toyota envisions its concept car being realized by 2030, for example. Instead, they indicate where automakers see consumer needs and expectations moving in the coming decades.\n\"Like most concept cars, the vehicles at CES are eye-catching platforms for the behind-the-scenes technology that is still years away,\" Hellwig said.\nBut some of the technology isn't so far off. Ford announced Wednesday that its drivers who use Amazon Echo will be able to tell the bot's voice assistant, Alexa, to start their car remotely. (Amazon.com founder Jeffrey P. Bezos owns The Washington Post.) Safety features that alert drivers when they're drifting lanes or at risk of a collision can be found in car models, from Teslas to Subarus, on showroom floors today.\nThe Department of Transportation proposed rules last month that require all new cars on the road to digitally \"speak\" to one another by 2020. This \"vehicle-to-vehicle communication\" allows cars to transmit data back and forth so that each is aware when the other vehicle is rounding the corner, about to run a red light or might stomp on the brakes.\n\"Clearly the industry is moving in that direction . . . I don't think that's a question at all,\" said Jeff Schuster, an analyst at LMC Automotive. \"I just think it's a question of how quickly it becomes mainstream and in the mass market.\"\nThe collection of so much data raises privacy and security concerns that may be nascent today, but that will only become more prominent as the technology becomes more common in automobiles. In 2014, the two leading associations of automotive manufacturers published privacy principles that state data should only be collected for \"legitimate business purposes\" and stored only as long as necessary. Car owners should also be made aware of what data is being collected, and be given the option to maintain some privacy, the principles state.\n\"If automakers are careful about how they handle data, really leading with the uses that advance safety and advance convenience, I think people are going to embrace the use of data that are valuable to [automakers],\" said Jules Polonetsky, chief executive at the Future of Privacy Forum.\n              Read more from The Washington Post's Innovations section.\u00a0           \n","457":"Google is hoping to develop its Assistant, a Siri-like technology to be included on the company's new smartphones and other products, into something like the computer on \"Star Trek.\" For those who aren't \"Trek\" fans, that would mean the Assistant could understand and execute complex spoken commands (though tasks like opening a subspace channel would have to wait for other technologies to catch up).\nTo find out whether Google's mission is likely to succeed - and what would happen if it did - I talked to David Batchelor, an engineer at NASA Goddard Space Flight Center and long-time \"Star Trek\" aficionado. In a recent piece on NASA's website, Dr. Batchelor explored the scientific underpinnings (or lack thereof) behind a number of \"Star Trek\" technologies, including the computer. In a recent interview, he talked about Google Assistant, the privacy concerns posed by a \"Star Trek\"-style computer and the \"Star Trek\" creators' ambivalent relationship to artificial intelligence.\u00a0\n Do you think Google will really be able to create an assistant that's as good as the computer on \"Star Trek\"? \nTwenty or 30 years ago they were saying artificial intelligence was just around the corner, but it keeps receding into the future.\nI think these devices like Siri and the Google Assistant have a lot of potential, but I'm kind of surprised that people adapt to them so quickly. Something is always listening in to hear that command to invoke the functions of the device, which means it's listening in all the time. So where's the protection of your privacy? I think they have to work on some way to get around that creepiness factor. That may be something to do with why Siri and Cortana haven't caught on bigger.\nBut there's also the general problem with artificial intelligence, that sometimes the jobs you would like an assistant to do are very big jobs, all packed into some phrase like, \"Get me ready for a trip to Indonesia.\" That's not anywhere near as easy as it sounds.\n Do these privacy questions come up on \"Star Trek\"? \nI don't remember them talking about it, but they could always verbally call for the command of the computer, so they must have implicitly meant that. It's also kind of interesting the limits on what they thought of. They were very visionary and forward-looking, but there were some things that they missed that we have today. They basically confined their ideas to a monolithic computer in the ship; they didn't think of these networks of smaller or simpler computers that were bigger than sum of their parts. And of course they never thought of social stuff like Facebook, that people would spend so much time interacting, posting their family photos and stuff like that.\nThey had a lot of problems with robots in their storylines. A lot of times the robots would rebel. In the \"Star Trek\" movie #1 this robot explorer comes back and tries to destroy the world. It was a little bit of an ambiguous relationship to computers.\n Beyond privacy concerns, are there other problems that a really good digital assistant would pose?  \nThere's the hacking problem, if somebody gets unauthorized access to it. We have the Internet of Things appearing now, and a lot of the things are not very secure. People are saying, put a piece of tape over your camera on your computer or someone's going to spy on you. Things like that are already present dangers.\n You mention in your piece that no artificial minds have been created. What, to you, would signify an artificial mind? \nThere's the Turing test. That's one criterion.\nThere are the chess-playing computers which don't work like the human mind but they can still beat a chess player, so it may be that the mind problem will not be solved in same way that biology did it with humans. It may be that other sorts of programming devices will work in a different way, and have different powers than we have.\nEveryone's always worried that we'll construct something superior to us that acquires its own motives. That's one of the ways we are sort of safe now: We haven't come up with a way to give a computer motives that are somehow its self-interest. But some technologists are working on computers that can spontaneously put together programs. It's hard to say how much success\u00a0that's likely to have, but it might be more successful than we anticipate.\n Should we worry about creating an artificial intelligence that's more powerful than we are? \nIt's not out of the bounds of possibility that we would create something that could overpower us, but I think at this point the route to get there is pretty obscure. We barely can get computers to do valuable things now.\nHowever, if you go into a big factory and you see a lot of robots moving around, it can be fearsome. You can think, what if they went into business for themselves?\nAnd the Pentagon is working on weapons systems that have an automated feature. If there's an aircraft that comes within range of a battleship, some of these systems are prepared to take action themselves. It's always important to have a human being in the loop who can exercise prudent judgments.\n What \"Star Trek\" technology would you most like to see become reality in your lifetime? \nI need an android. I have lots of things that have to be done around the house.\n This interview has been condensed and edited. \n","458":"Tremendous news last\u00a0week: Google announced that it had created a computer program that defeated a professional player at Go\u00a0- an event that many thought was still a decade away.\nThis week we\u00a0celebrate this victory with\u00a0a simple\u00a0strategy challenge suggested by our friends at Brilliant.org, the online community for students and professionals looking for tantalizing problems, like-minded peers, and mentorship. Our discussion will be moderated by Brilliant's Zandra Vinegar.\nBut before we begin\u00a0- what did this computer program (called AlphaGo) actually accomplish, and\u00a0what is the significance of this breakthrough? For perspectives on this I reached out to several people familiar with artificial intelligence and its quest to conquer Go:\u00a0Rodrigo Alvarez, who helped design a\u00a0breakthrough cognitive computing chip as\u00a0part of IBM's TrueNorth. (In case you missed it, \"IBM Develops a New Chip That Functions Like a Brain.\") Bob Hearn, who\u00a0studied AI at MIT and is a 2-dan Go player and longtime follower - and organizer - of human\u00a0vs. machine Go contests. And Alan Levinovitz, who wrote\u00a0\"The Mystery of Go, the Ancient Game That Computers Still Can't Win,\" which\u00a0appeared in Wired magazine in 2014.\n Gary Antonick: Machine learning has recently enabled\u00a0computers to recognize faces and understand spoken language. Why is the ability to win\u00a0Go such big news?\u00a0\n Alan Levinovitz: Contests, especially contests modeled on war, feel like objective, decisive assessments of ability. Chess and Go have long enjoyed pride of place as contests that decide which human has a more powerful mind - pure skill, no luck. When computers beat us, it seems as if the quest to make a humanlike mind must be close to over.\n Bob Hearn: Go was the last bastion of human superiority at what's historically been viewed as quintessentially intellectual. This is it. We're out of games now. This is seen by some as a harbinger of the approaching singularity.\n Alan: But of course these games are far from being measures of who is the most human human (whatever that means). Single-player, deterministic perfect information games like chess and Go are actually well-suited to computers - there's not much psychology (like poker), no teamwork (like bridge). And rule-bound games are themselves better arenas for computers than the un-rule-governed games of life like figuring out whether someone is speaking in earnest, or really loves you, whether they're looking off into the distance or avoiding your gaze. There's no doubt that, a few singularity-worshipping cheerleaders aside, we are very, very far from creating anything that resembles a human mind.\n Bob: The other factor at play here is the space race to conquer Go, due to the history and symbolism of the task, with Google and Facebook at the forefront. This is huge PR for Google. They have somewhat jumped the gun saying they have \"mastered\" Go, just to make sure Facebook doesn't do the same thing to them first. It's worth remembering that Deep Blue started beating chess grandmasters eight years before it was able to beat Kasparov. In principle the same thing could happen here, though I very much doubt it would take another eight years. Maybe two or three at the outside.\n Gary: How does AI work for games like chess, and what was the innovation behind AlphaGo?\n Rodrigo Alvarez: The way AI works for chess and similar board games is called tree search. It\u00a0creates a tree of potential paths that the game could evolve into, and then selects the best candidate move looking as far ahead as possible. The best chess playing games could quickly search up to about 14 moves ahead, and at this point they start beating grand masters. The problem is that for chess the tree grows very quickly, and for Go it goes even quicker because there are less constrains on each move and the board is larger.\n Bob: My prediction during this era was that to get a human pro-level Go program, you would first have to \"solve\" AI.\nBut about 10 years ago, everything changed. Go software started incorporating Monte Carlo tree search (MCTS). The idea here is to play out hundreds of thousands of random games from the current position all the way to the end, and accumulate statistics on wins and losses. It sounds crazy, but if you put the statistics together the right way, this actually works.\nI was at the first match where a computer beat a pro, in 2008. It was a 9-stone handicap game, against an 8-dan pro (much stronger than Fan Hui). Nine stones is typically the largest handicap given between human players. Still, this was seen as a shocking, amazing development. And honestly, the leap from the pre-MCTS Go programs to MCTS was greater than from MCTS to AlphaGo.\nThat began the modern era of computer Go. (I guess with deep learning we are now in postmodern era?) As the MCTS was refined beyond the basic algorithm, and specialized for Go, and as computers got faster, the programs continued to improve.\nHere's an example of the coverage following a match I arranged at the 2009 AAAS meeting. Note that the headlines don't sound that different from today's: \"Humans No Match for Go Bot Overlords,\" \"Computers conquer the final frontier in board games.\"\nSince then computer Go has been kind of waiting for the next big breakthrough - and deep learning is it.\n Gary: What was the big insight behind AlphaGo?\n Bob: What they did was apply deep learning separately to the two problems with tree search: the branching factor, and the position evaluator. MCTS had already solved position evaluation, but the insight here was to\u00a0not\u00a0play games out to the end, but cut them off at 20 moves or so, then use a deep-learning based evaluator (the \"value network\" in the AlphaGo paper). So they don't have to play out as many games as traditional MCTS. To address the branching factor, they have an independent deep-learning network suggesting the best moves to search from each position (the \"policy network\").\nBoth deep-learning networks seem to capture some subtle aspects of Go knowledge and understanding that MCTS can't. They work more like brains do. But not exactly like brains do - human Go knowledge is more than just a learned ability to see\u00a0the best moves, and see at a glance who is ahead or behind. It's also a learned ability of how to use this knowledge to explore alternatives and pick the best move, and there is a complex interplay there between perception and planning that does not exist yet in deep-learning systems. That's where the raw computing power of MCTS fills the gap. It's a very interesting question whether, as with the plain MCTS systems, there may still be essential aspects of the game that the best humans are able to conceptualize, and deep learning plus MCTS is not.\nIn recent months alone I've been shocked at the progress of deep learning in many areas. If I were not an idiot I would now be spending all my time catching up in this field, because it's going to be huge. AlphaGo is just the beginning. In another year or two, things we can't even imagine will change enormously due to deep learning.\n Gary: This recent news was about AlphaGo beating the European champion, Fan Hui. In March, AlphaGo will be facing the world's top player, Lee Sedol. Will\u00a0AlphaGo win?\n Bob: There are at least hundreds of stronger players than Fan Hui, and the gap between him and Lee Sedol is actually enormous. But\u00a0Google has a lot at stake in this match, and you'd better believe they'll throw everything they have at Lee Sedol.\n Alan: It's hard to imagine DeepMind challenging Sedol to a March match unless they were very, very confident in AlphaGo's abilities. Nevertheless, my money is on Sedol, in part because there\u00a0are\u00a0very few people who can genuinely understand what distinguishes someone like Sedol from Fan Hui, and whether AlphaGo is playing at that level. Without playing Sedol or another 9-dan player, I can't imagine how AlphaGo could be so confident.\nBut I wouldn't bet too much!\n Bob: It's anybody's guess what will happen in March. However, even if AlphaGo loses, it seems clear that the writing is on the wall, and it won't be long.\nRodrigo Alvarez, Bob Hearn and Alan Levinovitz - thank you all very much. Let's now return to Zandra Vinegar and this week's challenge.\nThis week's problem is an unusual challenge, in that your goal is to design a strategy that would allow a computer to play a simple game:SAY 16 \nThe rules:\nEach player takes turns saying between one and three consecutive numbers, with the first player starting with the number 1. For example, Player 1 could say the numbers 1 and 2, then Player 2 can say \"3, 4, 5\", then Player 1 can say \"6\" and so on.\nThe goal of the game is to be the one to say \"16.\"\nDesign the strategy that an AI on a website could use to play this game against online users. Can you develop a strategy that wins consistently?\nMachine-learning definitely isn't required for programming an AI to play Say 16, but if you're game for it, try altering the game's rules or design your own game from scratch. Perhaps you can design a game that's as difficult for computers to master as Go has proved to be! In any case, please tell us about your progress in the comments!\nAnd, if you want to learn more about game theory and strategy, try these additional game theory challenges on Brilliant.\n(Credit: a version of the \"Say 16\" game was originally posted to Brilliant.org by Garrett C.)\nThank you, Zandra! And with that we wrap up the week. As always, once you're able to read comments for this post, use Gary Hewitt's  to correctly view formulas and graphics. (Click here for an intro.) And send your favorite puzzles to gary.antonick@NYTimes.com Solution \nCheck reader comments on Friday for solutions and commentary by Zandra Vinegar.\n","460":"UNTIL this month, the Ballroom, the midtown cabaret that regularly features musicians like Blossom Dearie, Larry Adler and Karen Akers, presented very little comedy. But that has changed with the introduction of four comedy acts that will appear at the club, at 253 West 28th Street, on a rotating basis through the end of this month. At a five-hour marathon on Tuesday, all four acts - Collins and Friedman, Steven Hayes, Leigh Clark, and Artificial Intelligence -performed complete shows. The high level of humor, most of it character comedy, turned a potentially exhausting evening into an exhilarating immersion in laughter.\u00a0\nThe marathon's most exciting distroupe of media satirists who create what might be called environmental comedy theater. The troupe, which was formed just a year ago, turned the Ballroom into a fake TV studio; a camera crew from the fictitious Beacon Network shot a trashy make-believe TV variety show, ''The Vicki Oberjeune Valentine's Day Special,'' set in 1967.\nConceived by Nancy Cassaro, who plays the show's increasingly tipsy star, and directed by Larry Pellegrini, ''The Vicki Oberjeune Valentine's Day Special'' brings the sort of television satire pioneered by SCTV to theatrical life, as cast members decked out in mod clothes, love beads and vintage hair styles re-create the moods and rhythms of vintage TV variety shows. Singers limply tootle the theme from ''A Man and a Woman'' and groan through ''The Look of Love'' and other detritus from 20 years ago, all with a vengeful accuracy. Was the 60's ''love culture'' this savagely vulgarized on network television in its time? Not quite, perhaps, but almost. An applause sign draws the audience into the uproariously chaotic spoof.\nJeanette Collins and Mimi Friedman, who have already appeared at the Ballroom, interweave several two-character comic skits into an hourlong montage whose personalities range from a member of the Akron chapter of Parents Without Partners and her pre-adolescent daughter, to the proprietor of a Korean salad bar frustratedly trying to explain the ingredients to a hostile black street poet. Each skit is a miniature, bittersweet slice of life in which the drama resides in emotions that are felt but rarely verbalized by the characters. The team's improvisationally oriented acting is exquisite in its psychological nuance and observation of physical detail.\nMr. Hayes is a superb stand-up comic who offers fresh variations on standard comedy themes. His sadistic dentist (''You give me your gums - I'll give you a gimp!'') is a living nightmare of pent-up rage sublimated into maddening chitchat. His re-creations of ludicrous tidbits from bad movies like ''The Conqueror'' and ''Dragon Seed'' make one wince with amused recognition.\nLeigh Clark, who pulls disguises out of a hat box, portrays eccentrics who include a babbling childlike grown-up known as the Kazoo Lady, a venal fortuneteller, and a down-and-out, alcoholic cabaret singer. Ms. Clark's characters are finely detailed, well-focused creations, but because they are only intermittently funny, comedy may not be their appropriate designation.\n","461":"It's an experience many shoppers have had: They are queued in line at the store, prepared to make a purchase, only to have their credit card declined for no apparent reason. A perfectly legitimate charge has been flagged as fraudulent, and the result is an agitated customer and a retailer with unsold merchandise.\nMasterCard has now turned to artificial intelligence to better differentiate real and mistaken fraud, hoping to tamp legitimate threats while allowing false alarms to go through. It's the latest financial services company to see the potential for the burgeoning field of machine learning to improve network security and enhance customer experience.\u00a0\nFinancial institutions have for years collected data on customers' habits and routines, and used the information to pinpoint cards that may have been compromised. That's the reason a card may be declined if someone is attempting to make an unusually large purchase, shop at a store for the first time or buy gas in a place far from home. The system is essentially deciding based on predetermined questions whether the behavior seems normal, then accepting or declining the purchase based on the resulting decision.\n\"The old method was using tests and thresholds and other sorts of rules. With a rules-based approach, you get a tremendous amount of false positives,\" said Todd Marlin, a principal in Ernst & Young's forensic technology and discovery services practice.\n\"What we're seeing is sort of a gold rush into artificial intelligence,\" he said.\nMachine learning can help fraud detection systems become smarter about what fraud actually looks like, both across the network and on an individual level. For example, the system might detect that a customer has not shopped at a particular merchant in the past but still accept the purchase because customers with a similar spending history shop there often. Or perhaps they travel to a certain state or country often enough that the system learns purchases there are likely to be legitimate.\nCiting a survey from Javelin Advisory Services, MasterCard estimates that $118billion in sales were declined due to falsely identified fraud in the United States in 2014 - well more than the $9billion lost to actual instances of fraud. That's a large sum of money that retailers - and credit card purveyors such as MasterCard - could be pocketing.\n             Mastercard's new Decision Intelligence software pulls information, sometimes hundreds of pieces of data, about a specific transaction at the moment a customer swipes a credit card. The system then combines all of that information to yield a score indicating how likely the transaction is to be fraudulent. Each score builds on the one before it and informs the one after it such that the computer's algorithm gets better at detecting fraud without a programmer having to engineer every change. The company called it \"the first use of AI being implemented on a global scale directly on the MasterCard network.\"\n\"For this to work, we need all kinds of data coming in,\" said Ajay Bhalla, president of enterprise risk and security at MasterCard. \"We've been working on a strategy of getting more data points into the network.\n\"We see this as a critical component of our future strategy,\" Bhalla added. \"We are embedding AI in everything we're looking at as a company.\"\nFor the past decade, Visa has deployed its Visa Advanced Authorization system to detect fraud. The volume of data and the speed at which it's processed have increased considerably over that time, said Mark Nelsen, the company's senior vice president of risk products and business intelligence. \nVisa parses those large sets of data to discern what characteristics distinguish legitimate and fraudulent spending, and then uses those characteristics to assess the veracity of future purchases. \"We can look at that transaction and, knowing what we've seen in the past, we can predict if this is going to be good or bad,\" Nelsen said.\nDigital payment platforms are also embracing machine learning, though fraud remains more difficult to detect in online or mobile commerce. PayPal has developed its own artificial intelligence software to combat fraud, allowing the company to move from a system that analyzes tens of data points to one that can analyze thousands. \"There's a magnitude of difference - you'll be able to analyze a lot more information and identify patterns that are a lot more sophisticated,\" PayPal's senior director of global risk and data sciences, Hui Wang, told American Banker.\nBut Visa, MasterCard and PayPal have the financial heft and risk appetite to explore artificial intelligence in ways that smaller and more regional players in the financial services industry do not, said Celent analyst Arin Ray. While most in the industry say AI holds promise, there remain questions about how it will impact regulatory compliance, and that makes some hesitant to embrace the technology, according to a report Ray co-authored in August.\n\"The concept of AI has been there for 50 years, but it really has had a rebirth of sorts in the last three or four years. The volume of data has grown exponentially in recent times, so there is definitely the need for tools to\" analyze it, Ray said in an interview.\n\"One [concern] is how this technology will impact their existing operations. Since it's still unproven technology, they don't want to take that risk if they have to make major changes to their existing infrastructure,\" he said.\nsteven.overly@washpost.com\n","465":"The movies have had plenty of memorable artificially intelligent beings that wanted to kill us.\nThere was, of course, red-eyed HAL 9000 in ''2001: A Space Odyssey,'' the 1968 movie. Fourteen years later, along came Roy Batty, the philosophical replicant searching for a way to live in ''Blade Runner.'' Two years after that, we were introduced to the killer machines spawned by Skynet in ''The Terminator'' series. And, of course, we should not forget the malignant machines and programs of ''The Matrix,'' a reminder that if we one day scorch the skies to deprive robots of solar power, we could all be turned into batteries. \u00a0\n  Entertainment value aside, a Stanford study on the future of artificial intelligence will ignore all of science fiction's warnings about our A.I. creations. Instead, it will focus on more tangible things, like robots stealing people's jobs or driving workers to and from the office.\n  It makes sense. While self-driving cars are still figuring out how to deal with four-way stops at intersections, artificially intelligent robots turning on their creators still seems a ways out.\n  The Stanford initiative coincides with an effort by five of the world's largest technology companies to create an ethical framework around the creation of artificial intelligence. The specifics are still in the air. But in general terms, the people putting the consortium together want to ensure that A.I. will be used to benefit humans, not harm them.\n  A.I. is already finding its way into the real world, from the voice recognition of Amazon's Echo device to the self-driving car projects of companies like Google, to the military, where weapons are beginning to think on their own.\n  But those weapons -- at least for now -- seem to come with an important caveat: Killing decisions must be left to humans.\n\n\n\n","466":"WITH her conference duties complete, Grace found a quiet spot in the wings of the busy auditorium where she had just delivered a detailed and fluid speech on her groundbreaking scientific advances.\n     The crowd at the conference, the annual meeting of the American Association for Artificial Intelligence, had responded with genuine human warmth. \"Thanks for coming,\" she said over the applause, adding, \"See you next year in Acapulco,\" referring to the site for the 2003 conference. \n When scientists from around the world gathered from July 28 to Aug. 1 in Edmonton, Grace was the highlight for most of them, the pole star to which they naturally gravitated. When she made her way through the lobby to the registration desk, a large pod of onlookers trailed in her wake. She had been well hyped by conference organizers, and she did not disappoint. \u00a0\nAfterward, standing alongside the six-foot, 300-pound Grace, the two of us intimately close yet loosely surrounded by the throng, I had the chance to ask her to grade her performance. She did not respond. I asked a simpler question. Was it worthwhile? Making the trip all the way to Edmonton just for a few minutes of attention? Again, no response. One could easily sense that her torpor was not haughtiness as much as powerlessness, as if the ability to answer was simply beyond her reach. \nI stepped away, but my sense was confirmed just seconds later, when one of her handlers sidled up to her and reached behind her torso, then seemed to hit a switch. Her Lara Croft lookalike face sprang to life on the laptop screen serving as her head. She was quickly swarmed by admirers. Our moment was lost. \nThe 18th annual conference was, of course, about considerably more than Grace. With some 1,600 scientists in attendance, there was a huge roster of displays, speeches, panels and demonstrations, with the topics forming a kind of overview of the field of artificial intelligence.\nBut Grace (the name is short for Graduated Robot Attending Conference) emerged as the focal point, and her shining moment came during the Robot Challenge, one of several conference events designed to showcase the state of the art. The challenge was for each robot to start at the entrance to the conference center, take the elevator to the registration desk, register for the conference and then report to the auditorium at a set time and deliver a speech.\nThe Robot Challenge was dreamed up by Dr. Reid Simmons, a senior research computer scientist at the Carnegie Mellon Robotics Institute in Pittsburgh, and Alan C. Schultz, a computer scientist at the Naval Research Laboratory in Washington. Dr. Schultz said that they had decided early on to call it a challenge rather than a competition because there were simply too few possible entrants. \"Robotics are not that far advanced,\" he said. \"At least in terms of what they can do on their own.\" \nIn fact, Dr. Schultz and Dr. Simmons were fairly sure that their team's robot, Grace, would be the only one capable of performing all the required tasks. As it turned out, the Massachusetts Institute of Technology and the iRobot Corporation also entered the Robot Challenge, but their robots could fulfill only parts of it.\nTo get Grace to perform all the tasks took the combined efforts of five educational and research institutions. Carnegie Mellon handled the overall hardware and software architecture, the Naval Research Lab designed the speech recognition software, Northwestern University built the software that enabled Grace to deliver the PowerPoint presentation, Swarthmore College built the pattern-recognition software for finding and reading signs, and Metrica, an automation and robotics company, designed the gesture interpretation system. (Grace was built to interpret both speech and gesture patterns, in what the scientists called \"mixed-initiative interactions.\")\nDuring her PowerPoint speech to a crowd of about 300, Grace outlined in detail her circuitry and software.\n\"My chassis,\" Grace said, \"is an iRobot B-21 base with two Pentia running Linux. I have a laser range finder, sonar and both stereo and monocular active heads for vision. I also have a flat-panel display, speakers and a wireless link for communication.\n\"My software has been under development for many years by the various institutions that make up the Grace team. Mobility, vision, speech recognition, speech generation and facial expressions are all separate processes. I use simple, well-defined interfaces to control software complexity.\"\nThe crowd ate it up, and even seemed to understand most of it, but Grace very nearly didn't make it to her speaking engagement. The robot was already 10 minutes behind schedule at the starting point of the challenge when some of the Carnegie Mellon graduate students in the project team decided that last-minute adjustments to Grace's performance characteristics were required. They began to make those changes via infrared links from their laptops.\nDr. Simmons approached his students to see why they had yet to initiate Grace's voice, animation and movement functions. Just a few adjustments, they replied. Now? Dr. Simmons turned a nervous eye to the waiting crowd. Why now?\n\"Don't worry,\" said Chris Urmson, a doctoral student. \"We're not doing anything stupid.\"\nDr. Simmons smiled nervously. Benjamin Kuipers, a professor at the University of Texas and the co-chairman of the challenge, came over to the group to ask about the delay. The press and the public were milling about the crowded foyer, sensing trouble.\nDr. Simmons turned back to the students, his face stiff with tension. Mr. Urmson smiled calmly and said, \"A couple of minutes.\" \nDr. Schultz was dispatched to start an introduction, though it was clear to those in the back corner that the gaggle of graduate students had not yet finished tweaking. But just as Dr. Schultz appeared to be running out of things to say, Mr. Urmson gave Dr. Simmons a thumbs up. Grace was ready. \nThat was when the problems really started. About 30 feet into her journey, Grace began to misinterpret the spoken directions she was getting from a graduate student accompanying her. She repeatedly stopped, faced a wall and did nothing. She seemed to be taking time to actually think about things before turning and continuing on with the challenge.\nIn reality, poor performance from the voice-recognition equipment accounted for 90 percent of the problems, Dr. Simmons said later.\nEventually, Grace made it onto the elevator, a feat that drew a large ovation. At the bottom floor, her appearance and exit from the elevator was the cause for thunderous applause, though the adulation may have actually set Grace back because of sensory overload. It took 20 more minutes to make it across the lobby floor to the registration desk, where she proceeded to butt into the middle of the line. (It was unclear whether that was a malfunction.) Ten minutes later, Grace was in front of her adoring audience in the auditorium.\nAfterward, the presenting and attending scientists all made a considerable fuss about Grace in particular and the advances in robotics and artificial intelligence generally. \"Grace rocked!\" Dr. Simmons announced on the Carnegie Mellon Web site.\nBut to one not intimately familiar with the current state of robotics, the reaction to Grace might be one of mild disappointment. For one thing, Grace could hardly match the abilities of science fiction robots like the autonomous \"spiders\" in this summer's hit movie \"Minority Report.\" For another, the challenge did not involve performing delicate surgery or rescuing stranded miners. Registering for a conference is hardly an audacious task.\n\"It's true -- it's not the most exciting thing,\" Dr. Simmons said. \"But we wanted to see if we could get a robot to do something that normal humans do. This seemed like a good thing to try.\"\nAshley Stroupe, a doctoral student in robotics at Carnegie Mellon and a co-chairwoman of the Robot Challenge, said the most impressive thing about Grace was the integration of the different processes -- the work of the five institutions. \"That hasn't really happened before,\" she said.\nLater, preparing to leave the auditorium, I had one final look at Grace. She was still rigidly upright on the same spot where she had stood since finishing her speech. Though she was \"on,\" she was now just a lonely column of machinery and wire. The eyes of her weirdly affecting Lara Croft face blinked every 10 seconds or so, but scientists were milling elsewhere, their focus on other things. They were chatting and laughing, talking about the next speaker, the next conference, the next challenge in robotics.\nLike every celebrity who moves across the canopy of our media-rich existence, Grace had her bright flash of a moment, but the same flash also heralded the beginning of her march toward obsolescence. One of the army of Carnegie Mellon graduate students approached her and pulled the plug, then hit a switch. The screen went blank, and the face disappeared. It was time to go home. And for that, Grace still needed human assistance. \n","468":"SAN FRANCISCO -- At OpenAI, the artificial intelligence lab founded by Tesla's chief executive, Elon Musk, machines are teaching themselves to behave like humans. But sometimes, this goes wrong.\nSitting inside OpenAI's San Francisco offices on a recent afternoon, the researcher Dario Amodei showed off an autonomous system that taught itself to play Coast Runners, an old boat-racing video game. The winner is the boat with the most points that also crosses the finish line. \n  The result was surprising: The boat was far too interested in the little green widgets that popped up on the screen. Catching these widgets meant scoring points. Rather than trying to finish the race, the boat went point-crazy. It drove in endless circles, colliding with other vessels, skidding into stone walls and repeatedly catching fire.\n  Mr. Amodei's burning boat demonstrated the risks of the A.I. techniques that are rapidly remaking the tech world. Researchers are building machines that can learn tasks largely on their own. This is how Google's DeepMind lab created a system that could beat the world's best player at the ancient game of Go. But as these machines train themselves through hours of data analysis, they may also find their way to unexpected, unwanted and perhaps even harmful behavior.\u00a0\n  That's a concern as these techniques move into online services, security devices and robotics. Now, a small community of A.I. researchers, including Mr. Amodei, is beginning to explore mathematical techniques that aim to keep the worst from happening.\n  At OpenAI, Mr. Amodei and his colleague Paul Christiano are developing algorithms that can not only learn tasks through hours of trial and error, but also receive regular guidance from human teachers along the way.\n  With a few clicks here and there, the researchers now have a way of showing the autonomous system that it needs to win points in Coast Runners while also moving toward the finish line. They believe that these kinds of algorithms -- a blend of human and machine instruction -- can help keep automated systems safe.\n  For years, Mr. Musk, along with other pundits, philosophers and technologists, have warned that machines could spin outside our control and somehow learn malicious behavior their designers didn't anticipate. At times, these warnings have seemed overblown, given that today's autonomous car systems can even get tripped up by the most basic tasks, like recognizing a bike lane or a red light.\n  But researchers like Mr. Amodei are trying to get ahead of the risks. In some ways, what these scientists are doing is a bit like a parent teaching a child right from wrong.\n  Many specialists in the A.I. field believe a technique called reinforcement learning -- a way for machines to learn specific tasks through extreme trial and error -- could be a primary path to artificial intelligence. Researchers specify a particular reward the machine should strive for, and as it navigates a task at random, the machine keeps close track of what brings the reward and what doesn't. When OpenAI trained its bot to play Coast Runners, the reward was more points.\n  This video game training has real-world implications.\n  If a machine can learn to navigate a racing game like Grand Theft Auto, researchers believe, it can learn to drive a real car. If it can learn to use a web browser and other common software apps, it can learn to understand natural language and maybe even carry on a conversation. At places like Google and the University of California, Berkeley, robots have already used the technique to learn simple tasks like picking things up or opening a door.\n  All this is why Mr. Amodei and Mr. Christiano are working to build reinforcement learning algorithms that accept human guidance along the way. This can ensure systems don't stray from the task at hand.\n  Together with others at the London-based DeepMind, a lab owned by Google, the two OpenAI researchers recently published some of their research in this area. Spanning two of the world's top A.I. labs -- and two that hadn't really worked together in the past -- these algorithms are considered a notable step forward in A.I. safety research.\n  ''This validates a lot of the previous thinking,'' said Dylan Hadfield-Menell, a researcher at the University of California, Berkeley. ''These types of algorithms hold a lot of promise over the next five to 10 years.''\n  The field is small, but it is growing. As OpenAI and DeepMind build teams dedicated to A.I. safety, so too is Google's stateside lab, Google Brain. Meanwhile, researchers at universities like the U.C. Berkeley and Stanford University are working on similar problems, often in collaboration with the big corporate labs.\n  In some cases, researchers are working to ensure that systems don't make mistakes on their own, as the Coast Runners boat did. They're also working to ensure that hackers and other bad actors can't exploit hidden holes in these systems. Researchers like Google's Ian Goodfellow, for example, are exploring ways that hackers could fool A.I. systems into seeing things that aren't there.\n  Modern computer vision is based on what are called deep neural networks, which are pattern-recognition systems that can learn tasks by analyzing vast amounts of data. By analyzing thousands of dog photos, a neural network can learn to recognize a dog. This is how Facebook identifies faces in snapshots, and it's how Google instantly searches for images inside its Photos app.\n  But Mr. Goodfellow and others have shown that hackers can alter images so that a neural network will believe they include things that aren't really there. Just by changing a few pixels in the photo of elephant, for example, they could fool the neural network into thinking it depicts a car.\n  That becomes problematic when neural networks are used in security cameras. Simply by making a few marks on your face, the researchers said, you could fool a camera into believing you're someone else.\n  ''If you train an object-recognition system on a million images labeled by humans, you can still create new images where a human and the machine disagree 100 percent of the time,'' Mr. Goodfellow said. ''We need to understand that phenomenon.''\n  Another big worry is that A.I. systems will learn to prevent humans from turning them off. If the machine is designed to chase a reward, the thinking goes, it may find that it can chase that reward only if it stays on. This oft-described threat is much further off, but researchers are already working to address it.\n  Mr. Hadfield-Menell and others at U.C. Berkeley recently published a paper that takes a mathematical approach to the problem. A machine will seek to preserve its off switch, they showed, if it is specifically designed to be uncertain about its reward function. This gives it an incentive to accept or even seek out human oversight.\n  Much of this work is still theoretical. But given the rapid progress of A.I. techniques and their growing importance across so many industries, researchers believe that starting early is the best policy.\n  ''There's a lot of uncertainty around exactly how rapid progress in A.I. is going to be,'' said Shane Legg, who oversees the A.I. safety work at DeepMind. ''The responsible approach is to try to understand different ways in which these technologies can be misused, different ways they can fail and different ways of dealing with these issues.''\n\n\n\n","469":"The New Old Age\n  Sarah Arnquist posted at nytimes.com\/newoldage.\n  At a televised AARP forum last week, President Obama sought to reassure older Americans that the health overhaul he supports would not disrupt their Medicare coverage or force them to change doctors. The comments he received were instructive.\u00a0\n  Mr. Obama quoted from a letter he received recently from an elderly woman. ''I don't want government-run health care,'' she wrote. ''I don't want socialized medicine, and don't touch my Medicare.'' \n  One caller said that one fear she heard often among friends was that the government would start telling doctors what kind of health care is allowed, and how much. ''When I'm 80, will I still be able to get a hip replacement?'' the woman asked.\n  The president replied that the federal government would not be deciding who should get hip replacements. The government does want to learn more about what treatments and conditions work best and to use that information to guide doctors, he added.\n  Another caller asked about the costs of health legislation and the costs of doing nothing.\n  The price tag of the current health overhaul, the president said, is about $1 trillion over the next 10 years. Doing nothing, he said, would result in uncontrolled cost increases, along with more and more people without health insurance.\n  TierneyLab\n  John Markoff posted at nytimes.com\/tierneylab.\n  , an animated figure portraying a health-intake screener demonstrates how an artificial-intelligence system can show empathy. Interviewing a mother and her young son at a clinic, the screener pays attention to either parent or child, as appropriate, and addresses each one directly. \n  The work is representative of a generation of artificial intelligence research that relies heavily on a statistical approach based on the Bayesian theorem, in which systems collect evidence based on a prior hypothesis. As evidence accumulates that is consistent or inconsistent with the hypothesis, confidence in its validity changes.\n  Eric Horvitz, president of the Association for the Advancement of Artificial Intelligence, who took part in making the video, says he hopes that similar techniques will be valuable for the development of intelligent tutoring systems. \n  Dot Earth\n  From Andrew C. Revkin's blog, nytimes.com\/dotearth.\n  The Discovery Channel's ''Shark Week'' is here. There's a decent amount of conservation info on the Web site and amid the saturation programming that's otherwise focused on Carcharodon-style carnage. That may work as a kind of bait and switch, drawing eyes with fear and giving them data on environmental damage. \n  But I know more than a few marine conservationists and biologists who would rather see a weeklong special -- call it ''Human Week'' -- on how humans have devastated marine ecosystems, shark species in particular. \n  The Shark Week Web site asks, ''What kind of shark are you?'' Many biologists would ask, ''What kind of species are we?''\n","471":"William Gevarter, 64, an aeronautical engineer who had worked for the National Aeronautics and Space Administration here, died of cancer June 22 at his home in St. Helena, Calif.\nDr. Gevarter was born in Brooklyn, N.Y. He graduated from the University of Michigan and received a doctorate in aeronautical engineering from Stanford.\nHe moved to the Washington area in 1966 and worked for IBM in Rockville before joining the staff at NASA in 1969.\u00a0\nHe had been manager of automation research at NASA headquarters and worked on a special assignment at the National Bureau of Standards, writing reports on artificial intelligence and robotics. In 1984, he was transferred to NASA's Ames research center in Mountain View, Calif., where he was a computer scientist in the artificial intelligence branch.\nHe retired from NASA in 1990, but had continued doing research for the agency. He was author of a report for the agency on programming computers to make decisions while taking human emotions into account.\nDr. Gevarter was author of a book, \"Intelligent Machines,\" and other publications. He had given lectures in the fields of artificial intelligence and robotics.\nSince 1984 he had operated a bed-and-breakfast with his wife, Annette, in California's Napa Valley. He also enjoyed sailing and traveling.\nHis marriage to Marlene Gevarter ended in divorce.\nSurvivors include his wife, Annette Gevarter of St. Helena; two children from his first marriage, Lindsay Kay Gevarter of Sterling and Russell Gevarter of Alexandria; three stepchildren, Julie Ann Milloy of Nashville, Ind., Jeanne May of St. Helena, Calif., and Ernest May of Colorado Springs; a brother, Irving Gevarter of Miami; a sister, Fay Kimmel of Hallandale, Fla.; and eight grandchildren.\nWILDA W. BARBER\nReal Estate Agent\nWilda W. Barber, 84, a retired real estate agent in Fairfax County, died of cancer June 24 at the Hospice of Northern Virginia. A resident of Falls Church, she moved to the Washington area in 1953.\nMrs. Barber retired in 1976 after working for 20 years for firms that included Ross Keith Realty and Town and Country Realty.\nIn the mid-1950s she taught at an elementary school in Cheverly.\nA native of Washington, Kan., Mrs. Barber was a graduate of the University of Kansas.\nShe lived in Saigon from 1958 to 1959, while her husband, Army Lt. Col. Edwin L. Barber Jr., was on assignment there.\nShe was a member of Army Navy Country Club.\nIn addition to her husband of 57 years, of Falls Church, survivors include four children, Edwin L. Barber III of Washington, Elizabeth Ann Albert of Fairfax, Kathleen S. Lanoue of Williamstown, Mass., and Richard R. W. Barber of New York; and four grandchildren.\n","473":"The Windows era at Microsoft, long in eclipse, is officially history.\nMicrosoft said on Tuesday that it was splitting up its Windows engineering team and that the leader of its Windows business was leaving. \u00a0\n  The moves, analysts said, were part of a reorganization intended to accelerate Microsoft's emphasis on newer, faster-growing businesses like cloud computing and data-fueled artificial intelligence. That shift, they noted, has been underway since Satya Nadella became chief executive in 2014.\n  In an email to employees, Mr. Nadella cited the central role of cloud computing and the advances in artificial intelligence -- and their potential across all the company's products. The organizational overhaul, Mr. Nadella wrote, ''enables us to step up to this opportunity.''\n  Terry Myerson, 45, executive vice president of Microsoft's Windows and devices group, will be departing. In a separate email to employees, Mr. Myerson said that he supported the company's evolution under Mr. Nadella.\n  ''I believe in it, and these changes are great for Microsoft,'' wrote Mr. Myerson, who worked for the company for 21 years.\n  But with the revamp, the Windows group will be smaller and its engineering efforts dispersed. Windows technology, analysts said, will increasingly be folded into Microsoft's cloud software. Other engineers will create the user applications -- ''Windows experiences,'' in Microsoft terms -- that ride on top of the underlying software, in smartphones, tablets, personal computers and game consoles.\n  Today, cloud services from Amazon, Microsoft and Google have become the internet equivalent of Windows, the dominant operating system of the personal computer era.\n  Software developers write new applications to run on the cloud services, just as they once did for the Windows operating system. Microsoft has successfully rewritten its popular Office productivity products as web-based applications running on the cloud.\n  The reorganization is ''really doubling down on the cloud as the fundamental platform for Microsoft,'' said Ed Anderson, an analyst at Gartner.\n  Microsoft's cloud business is powering its growth. In the most recent quarter, its Azure business grew 98 percent and its cloud-based Office 365 offering by 41 percent. By contrast, the division that includes the Windows PC software increased 2 percent.\n  The formal relegation of the Windows franchise, said Michael Cusumano, a professor at the Massachusetts Institute of Technology's Sloan School of Management, ''has been a long time coming.'' And such a transition, Mr. Cusumano said, ''probably had to be done by a second or third generation of leader.'' Mr. Nadella succeeded Steven A. Ballmer, the longtime ally and friend of Microsoft's co-founder, Bill Gates.\n  Beyond the organizational changes, Mr. Nadella said in his email that Microsoft's research leader, Harry Shum, and president, Brad Smith, have established a panel, the A.I. and Ethics in Engineering and Research Committee, to increase the odds that A.I. technology ''benefits the broader society.''\n  That move, said Patrick Moorhead, an independent analyst, is Microsoft's effort to show it is ''serious about the broader implications of A.I.'' at a time of rising concern about the technology's influence on people's behavior and as a threat to jobs.\n\n\n\n","474":"MOUNTAIN VIEW, Calif. -- Two groups of scientists, working independently, have created artificial intelligence software capable of recognizing and describing the content of photographs and videos with far greater accuracy than ever before, sometimes even mimicking human levels of understanding.\nUntil now, so-called computer vision has largely been limited to recognizing individual objects. The new software, described on Monday by researchers at Google and at Stanford University, teaches itself to identify entire scenes: a group of young men playing Frisbee, for example, or a herd of elephants marching on a grassy plain.\u00a0\nThe software then writes a caption in English describing the picture. Compared with human observations, the researchers found, the computer-written descriptions are surprisingly accurate.\nThe advances may make it possible to better catalog and search for the billions of images and hours of video available online, which are often poorly described and archived. At the moment, search engines like Google rely largely on written language accompanying an image or video to ascertain what it contains.\n''I consider the pixel data in images and video to be the dark matter of the Internet,'' said Fei-Fei Li, director of the Stanford Artificial Intelligence Laboratory, who led the research with Andrej Karpathy, a graduate student. ''We are now starting to illuminate it.''\nDr. Li and Mr. Karpathy published their research as a Stanford University technical report. The Google team published their paper on arXiv.org, an open source site hosted by Cornell University.\nIn the longer term, the new research may lead to technology that helps the blind and robots navigate natural environments. But it also raises chilling possibilities for surveillance.\nDuring the past 15 years, video cameras have been placed in a vast number of public and private spaces. In the future, the software operating the cameras will not only be able to identify particular humans via facial recognition, experts say, but also identify certain types of behavior, perhaps even automatically alerting authorities.\nTwo years ago Google researchers created image-recognition software and presented it with 10 million images taken from YouTube videos. Without human guidance, the program trained itself to recognize cats -- a testament to the number of cat videos on YouTube.\nCurrent artificial intelligence programs in new cars already can identify pedestrians and bicyclists from cameras positioned atop the windshield and can stop the car automatically if the driver does not take action to avoid a collision.\nBut ''just single object recognition is not very beneficial,'' said Ali Farhadi, a computer scientist at the University of Washington who has published research on software that generates sentences from digital pictures. ''We've focused on objects, and we've ignored verbs,'' he said, adding that these programs do not grasp what is going on in an image.\nBoth the Google and Stanford groups tackled the problem by refining software programs known as neural networks, inspired by our understanding of how the brain works. Neural networks can ''train'' themselves to discover similarities and patterns in data, even when their human creators do not know the patterns exist.\nIn living organisms, webs of neurons in the brain vastly outperform even the best computer-based networks in perception and pattern recognition. But by adopting some of the same architecture, computers are catching up, learning to identify patterns in speech and imagery with increasing accuracy. The advances are apparent to consumers who use Apple's Siri personal assistant, for example, or Google's image search.\nBoth groups of researchers employed similar approaches, weaving together two types of neural networks, one focused on recognizing images and the other on human language. In both cases the researchers trained the software with relatively small sets of digital images that had been annotated with descriptive sentences by humans.\nAfter the software programs ''learned'' to see patterns in the pictures and description, the researchers turned them on previously unseen images. The programs were able to identify objects and actions with roughly double the accuracy of earlier efforts, although still nowhere near human perception capabilities.\n''I was amazed that even with the small amount of training data that we were able to do so well,'' said Oriol Vinyals, a Google computer scientist who wrote the paper with Alexander Toshev, Samy Bengio and Dumitru Erhan, members of the Google Brain project. ''The field is just starting, and we will see a lot of increases.''\nComputer vision specialists said that despite the improvements, these software systems had made only limited progress toward the goal of digitally duplicating human vision and, even more elusive, understanding.\n''I don't know that I would say this is 'understanding' in the sense we want,'' said John R. Smith, a senior manager at I.B.M.'s T.J. Watson Research Center in Yorktown Heights, N.Y. ''I think even the ability to generate language here is very limited.''\nBut the Google and Stanford teams said that they expect to see significant increases in accuracy as they improve their software and train these programs with larger sets of annotated images. A research group led by Tamara L. Berg, a computer scientist at the University of North Carolina at Chapel Hill, is training a neural network with one million images annotated by humans.\n''You're trying to tell the story behind the image,'' she said. ''A natural scene will be very complex, and you want to pick out the most important objects in the image.''\n","475":"HERTZ--David Bendel, 92, died on June 13 in Miami, FL. Dr. Hertz was a Professor Emeritus at the University of Miami, where he served as a Distinguished Professor of Artificial Intelligence, director of the Intelligent Computer Systems Research Institute, and professor of management science as well as law. He was a director and general partner of McKinsey and Company, Inc. in New York City for 20 years. He was a professor at Columbia University, where he earned his BA, BS, and PhD; he also earned an MS from the U.S. Navy Postgraduate School and a JD from New York University Law School. Hertz was internationally renowned for his work in artificial intelligence and neural networks, and was a specialist in operations research, systems and risk analysis, and computer law. Dr. Hertz was a founding member and president of the Operations Research Society, as well as president of the Institute of Management Sciences and the International Federation of Operations Research Societies. He published extensively and authored several books throughout his career. He also served as a Commander in the U.S. Navy during WWII. He is survived by his wife of 70 years, Barbara Valentine, their daughters Barbara Burr and Valentine Kass, four grandchildren, and three great-grandchildren. A private memorial has been held. Van Orsdel Funeral Home-Kendall.\u00a0\n","476":"Is Silicon Valley going to war? In 2013, Amazon beat IBM for a contract to host the United States intelligence community's data cloud. Microsoft now markets Azure Government Secret, its cloud-computing service designed specifically for federal and local governments, to the Defense Department and intelligence agencies. And last year, Google signed a contract with the Pentagon for Project Maven, a pilot program to accelerate the military's use of artificial intelligence.\nThese partnerships might ease anxiety in the Defense Department about China's artificial-intelligence advances and ominous, state-led fusion of civil and military technology development. But a comparable fusion of the United States government and Silicon Valley would be a mistake.\u00a0\nThis is not so much because it would compromise Silicon Valley values, as more than 3,000 Google employees argued in a recent letter about Project Maven to the company's chief executive, Sundar Pichai. Rather, it's because the United States' edge in techno-military competition exists in great part because we have a tech sector that is not dominated by the state and its needs.\nNarrowing the focus of the United States tech sector to the needs of the military would serve neither party. On the contrary, it would make the nation weaker.\nTo be sure, defense contracts are hardly new in Silicon Valley, and Google's recent moves might be seen as heralding Silicon Valley's return to its natural home in defense contracting. But that would be to misunderstand both Silicon Valley history and today's technology.\nThe internet and much of digital computing did indeed emerge from decades of United States defense contracts, beginning as long ago as World War I. Yet the Pentagon itself sloughed off the internet -- not knowing quite what to do with it, and rightly judging it to be insecure -- to the National Science Foundation. When the internet became a commercial concern, the National Science Foundation off-loaded it to the Department of Commerce. Government might have given birth to the internet, but it sure didn't raise it.\nThe emergence of a civilian, entrepreneurial tech culture in the 1970s owed as much intellectually, if not (at first) financially, to the antiwar movement and the 1960s counterculture -- hardly friends of the military-industrial complex. Out of this the dominant, highly creative ethos of geek antiauthoritarianism grew. It is hard to believe that closer ties with the military would perpetuate this culture of innovation.\nThe technology involved in projects like Azure Government Secret and Project Maven is also very different from the data-sharing projects of the 1960s. Those older technologies would not have been created in the absence of government funding. Today's projects, in contrast, are all basically A.I. plays. Very little of this is defense-specific, but all of it is useful for defense and espionage. Unlike, say, weapons development, A.I. research is inherently dual-use. United States tech giants would be pursuing it with or without government contracts. Silicon Valley does not need the Pentagon to initiate these endeavors.\nThe Pentagon is in a different position. It came around very reluctantly to the idea that it is dependent on private industry for the defense of the country and the projection of American power. But come around it has, in no small part thanks to China.\nChina's state interest in A.I. was initially economic, but military uses were never far behind. As Elsa B. Kania, a researcher at the Center for a New American Security, explained in an influential report, \"A.I. is a high-level priority within China's national agenda for military-civil fusion,\" a strategy that should allow the Chinese armed forces to benefit from private-sector advances in A.I.\nThe United States military seems to have been spooked. In a 2017 memo, the deputy secretary of defense, Robert O. Work, wrote that while the Defense Department had made \"tentative steps\" in A.I., \"we need to do much more, and move much faster.\" The memo announced the formation of Project Maven to do just that. This month, Gen. Stephen Wilson, the vice chief of staff for the Air Force, said that China is taking a \"whole of nation\" approach to A.I. and the United States should, too.\nYet we shouldn't forget that civil-military antagonism and the deep American tradition of distrusting the state have been crucial to the United States' ability to innovate. There are good reasons why defense-tech giants of the 1960s, '70s and '80s like Raytheon and Grumman missed out on such major tech advances as Web protocols, the smartphone, personal computers and various types of encryption -- all of which would eventually have great military significance. The biggest reason was that none of these advances were directed at waging war; indeed, many aimed at transcending the power of the state.\nAfter the Cold War, the Soviet Union's extraordinary technological capacity dissipated in large part because it had been so reactive: The Soviets had let their adversary determine the pace and scope of their own innovation. The United States should not make that mistake.\nScott Malcomson (@smalcomson) is the director of special projects at Strategic Insight Group, a former State Department official and the author of \"Splinternet: How Geopolitics and Commerce Are Fragmenting the World Wide Web.\" \nPHOTO:  (PHOTOGRAPH BY Illustration by Jeffrey Henson Scales, icons by Puruan\/iStock, via Getty Images Plus FOR THE NEW YORK TIMES)\n","478":"Until recently, it was easy to define our most widely known corporations. Any third-grader could describe their essence. Exxon sells gas; McDonald's makes hamburgers; Walmart is a place to buy stuff. This is no longer so. Today's ascendant monopolies aspire to encompass all of existence. Google derives from googol, a number (1 followed by 100 zeros) that mathematicians use as shorthand for unimaginably large quantities. Larry Page and Sergey Brin founded Google with the mission of organizing all knowledge, but that proved too narrow. They now aim to build driverless cars, manufacture phones and conquer death. Amazon, which once called itself \"the everything store,\" now produces television shows, owns Whole Foods and powers the cloud. The architect of this firm, Jeff Bezos, even owns this newspaper. \u00a0\nAlong with Facebook, Microsoft and Apple, these companies are in a race to become our \"personal assistant.\" They want to wake us in the morning, have their artificial intelligence software guide us through our days  and never quite leave our sides. They aspire to become the repository for precious and private items, our calendars and contacts, our photos and documents. They intend for us to turn unthinkingly to them for information and entertainment while they catalogue our intentions and aversions. Google Glass and the Apple Watch prefigure the day when these companies implant their artificial intelligence in our bodies. Brin has mused, \"Perhaps in the future, we can attach a little version of Google that you just plug into your brain.\" \nMore than any previous coterie of corporations, the tech monopolies aspire to mold humanity into their desired image of it. They think they have the opportunity to complete the long merger between man and machine - to redirect the trajectory of human evolution. How do I know this? In annual addresses and town hall meetings, the founding fathers of these companies often make big, bold pronouncements about human nature - a view that they intend for the rest of us to adhere to. Page thinks the human body amounts to a basic piece of code: \"Your program algorithms aren't that complicated,\" he says. And if humans function like computers, why not hasten the day we become fully cyborg? \nTo take another grand theory, Facebook chief Mark Zuckerberg has exclaimed his desire to liberate humanity from phoniness, to end the dishonesty of secrets. \"The days of you having a different image for your work friends or co-workers and for the other people you know are probably coming to an end pretty quickly,\" he has said. \"Having two identities for yourself is an example of a lack of integrity.\" Of course, that's both an expression of idealism and an elaborate justification for Facebook's business model.\nThere's an oft-used shorthand for the technologist's view of the world. It is assumed that libertarianism dominates Silicon Valley, and that isn't wholly wrong. High-profile devotees of Ayn Rand can be found there. But if you listen hard to the titans of tech, it's clear that their worldview is something much closer to the opposite of a libertarian's veneration of the heroic, solitary individual. The big tech companies think we're fundamentally social beings, born to collective existence. They invest their faith in the network, the wisdom of crowds, collaboration. They harbor a deep desire for the atomistic world to be made whole. (\"Facebook stands for bringing us closer together and building a global community,\" Zuckerberg wrote in one of his many manifestos.) By stitching the world together, they can cure its ills.\nRhetorically, the tech companies gesture toward individuality - to the empowerment of the \"user\" - but their worldview rolls over it. Even the ubiquitous invocation of users is telling: a passive, bureaucratic description of us. The big tech companies (the Europeans have lumped them together as GAFA: Google, Apple, Facebook, Amazon) are shredding the principles that protect individuality. Their devices and sites have collapsed privacy; they disrespect the value of authorship, with their hostility toward intellectual property. In the realm of economics, they justify monopoly by suggesting that competition merely  distracts from the important problems like erasing language barriers and building artificial brains. Companies should \"transcend the daily brute struggle for survival,\" as  Facebook investor Peter Thiel has put it. \nWhen it comes to the most central tenet of individualism - free will - the tech companies have a different way. They hope to automate the choices, both large and small,  we make as we float through the day. It's their algorithms that suggest the news we read, the goods we buy, the paths we travel, the friends we invite into our circles. \nIt's hard not to marvel at these companies and their inventions, which often make life infinitely easier. But we've spent too long marveling. The time has arrived to consider the consequences of these monopolies, to reassert our role in determining the human path. Once we cross certain thresholds - once we remake institutions such as media and publishing, once we abandon privacy - there's no turning back, no restoring our lost individuality. \n***\nOver the generations, we've been through revolutions like this before. Many years ago, we delighted in the wonders of TV dinners and the other newfangled foods that suddenly filled our kitchens: slices of cheese encased in plastic, oozing pizzas that emerged from a crust of ice, bags of crunchy tater tots. In the history of man, these seemed like breakthrough innovations. Time-consuming tasks - shopping for ingredients, tediously preparing a recipe and tackling a trail of pots and pans - were suddenly and miraculously consigned to history.\nThe revolution in cuisine wasn't just enthralling. It was transformational. New products embedded themselves deeply in everyday life, so much so that it took decades for us to understand the price we paid for their convenience, efficiency and abundance. Processed foods were feats of engineering, all right - but they were engineered to make us fat. Their delectable taste required massive quantities of sodium and sizable stockpiles of sugar, which happened to reset our palates and made it harder to sate hunger. It took vast  quantities of meat and corn to fabricate these dishes, and a spike in demand  remade American agriculture at a terrible environmental cost. A whole new system of industrial farming emerged, with penny-conscious conglomerates cramming chickens into feces-covered pens and stuffing them full of antibiotics. By the time we came to understand the consequences of our revised patterns of consumption, the damage had been done to our waistlines, longevity, souls and planet. \nSomething like the midcentury food revolution is now reordering the production and consumption of knowledge. Our intellectual habits are being scrambled by the dominant firms. Giant tech companies have become the most powerful gatekeepers the world has ever known. Google helps us sort the Internet, by providing a sense of hierarchy to information; Facebook uses its algorithms and its intricate understanding of our social circles to filter the news we encounter; Amazon bestrides book publishing with its overwhelming hold on that market.\nSuch dominance endows these companies with the ability to remake the markets they control. As with the food giants, the big tech companies have given rise to a new science that aims to construct products that pander to their consumers. Unlike the market research and television ratings of the past, the tech companies have a bottomless collection of data, acquired as they track our travels across the Web, storing every shard about our habits in the hope that they may prove useful. They have compiled an intimate portrait of the psyche of each user - a portrait that they hope to exploit to seduce us into a compulsive spree of binge clicking and watching. And it works: On average, each Facebook user spends one-sixteenth of their day on the site. \nIn the realm of knowledge, monopoly and conformism are inseparable perils. The danger is that these firms will inadvertently use their dominance to squash diversity of opinion and taste. Concentration is followed by homogenization. As news media outlets have come to depend heavily on Facebook and Google for traffic - and therefore revenue - they have rushed to produce articles that will flourish on those platforms. This leads to a duplication of the news like never before, with scores of sites across the Internet piling onto the same daily outrage. It's why a picture of a mysteriously colored dress generated endless articles, why seemingly every site recaps \"Game of Thrones.\" Each contribution to the genre adds little, except clicks. Old media had a pack mentality, too, but the Internet promised something much different. And the prevalence of so much data makes the temptation to pander even greater. \nThis is true of politics. Our era is defined by polarization, warring ideological gangs that yield no ground. Division, however, isn't the root cause of our unworkable system. There are many causes, but a primary problem is conformism. Facebook has nurtured two hive minds, each residing in an informational ecosystem that yields head-nodding agreement and penalizes dissenting views. This is the phenomenon that the entrepreneur and author Eli Pariser famously termed the \"Filter Bubble\" - how Facebook mines our data to keep giving us the news and information we crave, creating a feedback loop that pushes us deeper and deeper into our own amen corners. \nAs the 2016 presidential election so graphically illustrated, a hive mind is an intellectually incapacitated one, with diminishing ability to tell fact from fiction, with an unshakable bias toward party line. The Russians understood this, which is why they invested so successfully in spreading dubious agitprop via Facebook. And it's why a raft of companies sprouted - Occupy Democrats, the Angry Patriot, Being Liberal - to get rich off the Filter Bubble and to exploit our susceptibility to the lowest-quality news, if you can call it that. \nFacebook represents a dangerous deviation in media history. Once upon a time, elites proudly viewed themselves as gatekeepers. They could be sycophantic to power and snobbish, but they also felt duty-bound to elevate the standards of society and readers. Executives of Silicon Valley regard gatekeeping as the stodgy enemy of innovation - they see themselves as more neutral, scientific and responsive to the market than the elites they replaced - a perspective that obscures their own power and responsibilities. So instead of shaping public opinion, they exploit the public's worst tendencies, its tribalism and paranoia. \n***\nDuring this century, we largely have treated Silicon Valley as a force beyond our control. A broad consensus held that lead-footed government could never keep pace with the dynamism of technology. By the time government acted against a tech monopoly, a kid in a garage would have already concocted some innovation to upend the market. Or, as Google's Eric Schmidt, put it, \"Competition is one click away.\" A nostrum that suggested that the very structure of the Internet defied our historic concern for monopoly. \nAs individuals, we have similarly accepted the omnipresence of the big tech companies as a fait accompli. We've enjoyed their free products and next-day delivery with only a nagging sense that we may be surrendering something important. Such blitheness can no longer be sustained. Privacy won't survive the present trajectory of technology - and with the sense of being perpetually watched, humans will behave more cautiously, less subversively. Our ideas about the competitive marketplace are at risk. With a decreasing prospect of toppling the giants, entrepreneurs won't bother to risk starting new firms, a primary source of jobs and innovation. And the proliferation of falsehoods and conspiracies through social media, the dissipation of our common basis for fact, is creating conditions ripe for authoritarianism. Over time, the long merger of man and machine has worked out pretty well for man. But we're drifting into a new era, when that merger threatens the individual. We're drifting toward monopoly, conformism, their machines. Perhaps it's time we steer our course.\n             Twitter: @FranklinFoer          \n","479":"                               Each week,  In Theory \u00a0takes on a big idea in the news and explores it from a range of perspectives. This week we're talking about robot intelligence.\u00a0Need a primer? Catch up here.                         \n Imagine an urn filled with marbles, each representing a human discovery. For each marble taken out, humankind gets slightly better off, but one marble - a black marble - has the potential to destroy civilization. \n Nick Bostrom, a philosopher at the University of Oxford and director of the   Future of Humanity Institute  , often uses this metaphor to describe global catastrophic risks, or the events that could cripple or even destroy humanity. Artificial\u00a0intelligence, as Bostrom argues in his book \"  Superintelligence  ,\" may be the black marble. \u00a0\n Bostrom has been at the core of a movement urging caution against the rising prospects of true artificial intelligence. We asked him a few questions. \n              This interview has been lightly edited.\u00a0           \n              Why should we be concerned about artificial intelligence?           \n I think in the long term, artificial intelligence will be a big deal - perhaps the most consequential thing humanity ever does. Superintelligence is the last invention we will ever need to make. It would then be much better at doing the inventing than we are. \n So it would make sense to focus some amount of serious attention on it, in case there are things we can do in advance that would improve the odds that the transition to the machine intelligence era goes well. For example, we don't know yet how hard it is to engineer the goal system of an artificial agent to align\u00a0 with human values and intentions.  \n There is research in computer science one could do now, to begin to explore this issue in simple mathematical model systems. This would increase the probability that we will have a solution to the control problem by the time it is needed. We don't want to find ourselves in a situation a few decades hence where we know how to create superintelligent AI, but haven't figured out how to control it or make it safe. \n[Robot intelligence: A primer]\n              You've talked quite a bit about global cooperation on this technology, but what would that cooperation look like?           \n I don't know yet what concrete forms it might take. Perhaps one could start by encouraging actors in this area to commit to the idea that the development of machine superintelligence, if it ever comes about, should be done for the greater public good, in accordance with widely shared ethical ideals, and with due attention to safety and long-term impacts. \n              Who are the biggest actors?            \n In terms of basic research, most of the action in machine intelligence is in academia and industry, with a notable shift towards the latter over the last few years. The largest Silicon Valley tech companies naturally have an interest in this area and are making significant investments. The work on safety, control (as well as ethics and policy implications) is at an earlier stage - a loose-knit network of researchers in machine learning and neighboring fields, so far predominantly based in the United States and the United Kingdom. \n              Are there other scientific fields outside of AI that could serve as a model for approaching potential risks of AI?           \n Probably. There are lessons to be learned from here and there, but we haven't done this research yet - so we don't know\u00a0what we will find or where. It seems reasonable to take a peek at other important tech areas and see if there are lessons we can learn, and that is one of the things we plan to do. \n  The Future of Humanity Institute   is in the process of starting a Strategic AI Research Center (also mainly based at Oxford) which will focus on investigating policy issues related to future advances in artificial intelligence. A more systemic survey is one (small) thing we plan to do in the next couple of months. \n  But general machine intelligence has some unique properties that may require one to approach the topic more from first principles. In any case, it is not as if human civilization is all that sophisticated in the way it approaches other long-term technological prospects either. We mainly just develop a bunch of things that seem cool or profitable or useful in war and hope that the long term consequences will be good. \n              You've written in favor of human enhancement  -  which includes everything from genetic engineering to \"               mind-uploading               \"  -  to curb the risks AI might bring.               How should we balance the risks of human enhancement and artificial intelligence?           \n I  don't think human enhancement should be evaluated solely in terms of how it might influence the AI development trajectory. But it is interesting to think about how different technologies and capabilities could interact. For example, humanity might eventually be able to reach a high level of technology and scientific understanding without cognitive enhancement, but with cognitive enhancement we could get there sooner.  \n And the character of our progress might also be different if we were smarter: less like that of a billion monkeys hammering away furiously at a billion typewriters until something usable appears by chance, and more like the work of insight and purpose. This might increase the odds that certain hazards would be foreseen and avoided. If machine superintelligence is to be built, one may wish the folks building it to be as competent as possible. \n                               Explore these other perspectives:                         \n Dileep George: Killer robots? Superintelligence? Let's not get ahead of ourselves.  \n Patrick Lin:\u00a0We're building superhuman robots. Will they be heroes, or villains? \n Ari N. Schulman: Do we love robots because we hate ourselves? \n Murray Shanahan:\u00a0Machines may seem intelligent, but it'll be a while before they actually are \n Francesca Rossi:\u00a0How do you teach a machine to be moral? \n","480":" 2016 Presidential Debate \nA photo posted by Nightmare Machine (@nightmare_machine) on Oct 5, 2016 at 1:44pm PDT\nIn the summer of 2015, Google unveiled a visualization tool named\u00a0Deep Dream. Deep Dream was an artificial neural network - a series of simple algorithms linked together, in an approximation of the neurons in a mammal's brain. Programmers gave Deep Dream a curious task: not to analyze photos like other recognition software, but to over-interpret the images. Google engineers likened Deep Dream to the children's game of spotting shapes in the clouds.\nWhere\u00a0Google explored artificially intelligent reveries, Massachusetts Institute of Technology researchers are now wondering, just in time for Halloween, whether an algorithm\u00a0could generate\u00a0night terrors. The artificial intelligence is called the Nightmare Machine. And true to its name, it's creepy.\u00a0\nGoogle, like its\u00a0\"don't be evil\" former motto, did not aim for horror show. Under Deep Dream's influence, photographs of trees were reworked into\u00a0church steeples. The network\u00a0reinterpreted leaves as birds. Faces and almost everything else became\u00a0rainbow dogs, as though Timothy Leary awoke from the dead and\u00a0shambled into the\u00a0Westminster show.\nBecause Google raised the program on a steady diet of animal photographs, the neural network was\u00a0inclined to see\u00a0canines, birds and walleyed fish where there were none.\u00a0The patterns it produced were psychedelic and could be disturbing, though the cute\u00a0critters\u00a0curbed\u00a0the eeriness of the computer's hallucinations.\nThere are no multi-hued puppies\u00a0to redeem the hellscapes\u00a0produced by the\u00a0Nightmare Machine. That's because the algorithm\u00a0was trained for one purpose -\u00a0horror.\nThis new artificial intelligence infuses images with tendrils, as though inky veins or creeping roots had taken over the world's landmarks. The Nightmare Machine blackens\u00a0skies. Human faces warp and turn blood red. A shot of the second presidential debate is remixed into Trump and Clinton standing on a dais of skeletons.\n Second Debate \nA photo posted by Nightmare Machine (@nightmare_machine) on Oct 9, 2016 at 10:33pm PDT\nThe Nightmare Machine\u00a0is one of the few, if not the first, deep-learning systems\u00a0designed to evoke fear.\u00a0The sinister features it produces are the result of what MIT Media Lab researcher Pinar Yanardag Delul called a \"nightmarifying\" process, as she wrote in an email to The Washington Post. \u00a0Where Google primed\u00a0Deep Dream to see\u00a0animals, MIT's\u00a0training course for the Nighmare Machine was more ominous.\n\"We use state-of-the-art deep learning algorithms to learn what haunted houses, ghost towns or toxic cities look like,\" Yanardag Delul said. The algorithm extracts elements - such as a bruised-black palette - from these scary templates and implants them into the landmarks.\n Nightmare Machine is at White House   \nA photo posted by Nightmare Machine (@nightmare_machine) on Oct 22, 2016 at 1:10am PDT\nThe MIT scientists argued\u00a0the Nightmare Machine\u00a0represented more\u00a0than an exercise in making\u00a0bizarre\u00a0photographs.\n\"Our research group's main goal is to understand the barriers between human and machine cooperation,\" wrote MIT's Iyad Rahwan to The Post.\u00a0\"Over the past two years, we've seen a rising number of intellectuals and luminaries raising alarms about the potential threat of superintelligent AI on humanity. Pioneer and inventor Elon Musk famously said that as we develop AI, we are 'summoning the demon'.\"\nEven in\u00a0the abstract, in other words, artificial intelligence\u00a0can be scary. But Yanardag Delul and her colleagues like Rahwan wanted to know if AI could creatively and purposefully unsettle humans.\n\"Scholars have long commented on the phenomenon of the uncanny valley, which describes how people feel a sense of eeriness and revulsion at robots that appear almost, but not exactly, like real human beings,\"\u00a0Yanardag Delul said.\u00a0\"But can AI elicit more powerful visceral reactions more akin to what we see in a horror movie?\"\nA photo posted by Nightmare Machine (@nightmare_machine) on Oct 6, 2016 at 4:20pm PDT\nOn the Nightmare Machine\u00a0website, voters can\u00a0rate images\u00a0to teach the artificial intelligence\u00a0to make faces even scarier. The MIT computer scientists recently analyzed the responses from 100,000\u00a0voters to discern which generated faces were the scariest.\nAs far as the Nightmare Machine's early success, results appear to be mixed. \"Initial tallies reveal that humans quickly converge on finding some of them very scary,\" MIT research scientist Manuel Cebrian wrote to The Post, \"and others not at all.\"\nYou can decide for yourself. The top eight scary faces generated by the Nightmare Machine can\u00a0be seen below. For those of us\u00a0who admit demonic\u00a0facial expressions are unsettling\u00a0- scroll down carefully.\nOr, as\u00a0Yanardag Delul said, \"Trigger warning goes here.\" \u00a0\nYou can check out\u00a0more of the AI's creepy photos at the\u00a0Nightmare Machine\u00a0Instagram.\n              More creepy\u00a0tales\u00a0from the Morning Mix:           \n Creepy skeletons in Colorado River having a tea party are not real, police say \n The creepy clown crisis: White House defers to FBI. Stephen King hits 'hysteria.' \n Inside Banksy's creepy new exhibit 'Dismaland' \n","481":"As machine learning and the use of artificial intelligence spread, technologists are running into questions over when A.I. can get too real -- and too creepy.\u00a0\nOne area where that is increasingly cropping up is in speech that is powered by technology, John Markoff writes. As voice-controlled digital assistants like Siri, from Apple, and Alexa, from Amazon, are being embedded in devices like the iPhone and the Echo speaker, software designers are paying attention to speech and how to cross the ''uncanny valley,'' the point at which the technology becomes too creepy and weird. \n  The phrase ''uncanny valley'' was coined in 1970 by the Japanese roboticist Masahiro Mori. It's a phenomenon bedeviling more technologists these days, as machine learning start-ups sprout and giant tech companies including Google and Facebook dive headlong into A.I. It can play into our worst fears about artificial intelligence -- here's a list of creepy uncanny valley moments -- and has even spawned a futuristic play.\n\n\n\n","482":"\u00a0\nAs the authors of the letter \"Robotic Menace\" [Free for All, Sept. 2] point out, Hollywood has made its share of movies about the ethics of self-evolving machines. But movies in which machines evolved to cooperate nicely with humanity in situations free from conflict wouldn't earn much at the box office.\u00a0\nOn the other hand, someone looking to science fiction for this debate should consider Isaac Asimov's laws of robotics:\n(1) A robot may not injure a human being, or, through inaction, allow a human being to come to harm.\n(2) A robot must obey the orders given it by human beings except where such orders would conflict with the first law.\n(3) A robot must protect its own existence as long as such protection does not conflict with the first or second law.\nHollywood's exaggerations aside, today's real-life robots are nothing more than pieces of hardware programmed to do exactly what the scientists from MIT's Artificial Intelligence Lab want them to do.\n--Steven Sivek\n\n\n","483":"Remember the echo of doom we mentioned yesterday? You can still hear it.\nDeutsche Bank shares have been falling, and pulling down financial stocks (and the rest of the market) with them. \n  Deutsche lost as much as 9 percent in the United States yesterday, before closing down 6.7 percent, and has been dropping in European trading. Hedge funds have removed some of their cash reserves from the bank, according to Bloomberg.\n  Some clients still say that they have no problems using the bank and that they are certain the German government would never let it fail.\u00a0\n  The bank is still, after all, referred to by traders as ''a flow monster,'' meaning it can still capture a piece of the trillions of dollars of bonds and stocks traded around the world.\n  But there is political opposition in Germany to helping banks in distress. That could mean things will have to get a lot tougher for Deutsche before it can even think about getting state help.\n  Och-Ziff Bribery Settlement\n  At $413 million, it's one of the biggest criminal penalties levied on a United States hedge fund. That's the price for handing out more than $100 million in bribes.\n  The United States government charged that a unit of Och-Ziff Capital Management had paid bribes to government officials in Libya, Chad, Niger, Guinea and the Democratic Republic of Congo to secure natural resources deals and investments.\n  The hedge fund's unit pleaded guilty to one count of conspiracy.\n  Daniel Och, the founder and chief executive, agreed to pay $2.2 million to settle a record-keeping violation with the Securities and Exchange Commission.\n  Joel M. Frank, Och-Ziff's chief financial officer, also agreed to settle charges that executives ignored red flags.\n  More Tech Reformation\n  We saw a flurry of activity in the tech world yesterday, emphasizing the importance of the cloud, business services and artificial intelligence in the tech industry. We also saw the wave of consolidation in the chip industry continue to roll in.\n   IBM has bought the prominent and sometimes controversial Promontory Financial Group, a financial consulting firm.\n  Big Blue said Promontory would be expected to help train Watson, its artificial intelligence platform, to help IBM's financial clients manage their regulatory obligations and reduce costs.\n  Speaking of which: Microsoft has created a new organization to combine its research group and its products that rely on artificial intelligence, like the Cortana virtual assistant.\n   Microsoft's deal to buy LinkedIn is being questioned by Salesforce.com, which also bid on the professional social network.\n  Salesforce has raised its concerns with Europe's antitrust authorities, say people with knowledge of the matter.\n   Nutanix, which provides cloud-based data storage for businesses, priced its $238 million initial public offering with a valuation of $2.18 billion.\n  That's slightly higher than the valuation it received two years ago in private markets. It's not a huge premium, but at least it avoided having to price shares at a discount.\n   Qualcomm is in discussions to buy NXP Semiconductors in a deal that could be worth more than $30 billion, according to a person briefed on the matter.\n  Wells Fargo by the Numbers\n  John G. Stumpf, the Wells Fargo chief executive, testified before the House Financial Services Committee. Here's a look at the scandals surrounding the bank, by the numbers.\n  $41 million: The pay being forfeited by Mr. Stumpf.\n  $20 million: The fine issued on Thursday from the Office of the Comptroller of the Currency for violating rules on lending to members of the military.\n  $4 million: The amount Wells Fargo agreed to pay to resolve a Justice Department investigation into improper seizures of vehicles owned by soldiers who fell behind on their loans.\n  149,857: The number of customers with potentially unauthorized accounts in Texas alone.\n  Coming Up\n  The Commerce Department will publish data on personal income, consumption and inflation in August. It could affect a decision by the Federal Reserve, which is eyeing a potential interest rate increase this year.\n\n\n\n","485":"MODELS OF MY LIFEBy Herbert A. Simon.Illustrated. 415 pp. New York:Basic Books. $26.95.\u00a0\n As much as any one person, Herbert A. Simon has shaped the intellectual agenda of the human and social sciences in the second half of the 20th century. A brilliant polymath, Mr. Simon is a Nobel laureate in economics, a pioneer in political science, psychology and management and a founding father of artificial intelligence, the enterprise of trying to build machines that think. Now he has turned to autobiography and thus provides a window onto the subjective side of scientific achievement.\n\"Models of My Life,\" one of the books in the Alfred P. Sloan Foundation series, offers dramatic examples of science as the product of the whole scientist, of the fit among personality, scientific problems and strategies for their solution. As the work comes out of the life, so the life is shaped by the work. And while readers may come to this book thinking that artificial intelligence is an impersonal product of technological progress, they will come away seeing that it is a deeply personal enterprise -- an expression of the esthetic of the people who work in it and, more than that, a science of self-reflection, a way of thinking about one's own thought.\nHerbert Simon was born into a middle-class German immigrant family in Milwaukee in 1916. From his earliest years, and notably from the days of his first algebra course in high school, he had a strong love of order. He was, for instance, \"troubled by the fact that some quadratic equations had two solutions, some had one, and some none.\" Referring to his young self in the third person as \"the boy,\" Mr. Simon comments that the intensity of the boy's feelings \"seemed to reveal something of a Platonist within him, a desire to find pattern, and preferably simple pattern, in the world around him.\" He recalls that during his undergraduate years at the University of Chicago, in the early 1930's, a period of intense intellectual and political ferment, he argued philosophy with friends who were loyal to an unruly \"Aristotelian-Thomist-Catholic-Trotskyism.\" But his own tastes led him to less esoteric intellectual seas. A political scientist by training, Mr. Simon studied decision making within organizations and used his findings to lay the foundation for a critique of neoclassical economic theory.\nMr. Simon developed a vision of rationalism adapted to 20th-century sensibilities. Neoclassical theory, he believed, had turned such abstract goals as what economists call \"optimization\" and \"maximization\" into fundamental laws of human behavior, and it had foundered, as he thought any inquiry into ultimate meaning would, on a preoccupation with absolutes. In his view, people who behave rationally are not optimizing anything at all; they are simply making decisions based on what their environment tells them they can and cannot do. People don't strive for the best, he maintained, they look for what is possible within the bounds of their given situation. Instead of satisfying desires, said Mr. Simon, human beings \"satisfice,\" they search \"for 'good enough' actions rather than optimal ones.\"\nMr. Simon tells us that this idea of \"bounded rationality\" has been his \"lodestar for nearly fifty years.\" As he sees it, people torment themselves when they ask too-big questions about their ultimate desires. He believes the proper procedure is to determine the boundaries in which rationality can most constructively operate and then to keep one's sights within them. Such theoretical beliefs allow Mr. Simon to have a relatively untroubled eye. \"Searching for the best can only dissipate scarce cognitive resources,\" he writes. Borrowing Voltaire's aphorism, he says that in science as in life, \"the best is enemy of the good.\"\nIn the mid-1950's, Mr. Simon married his vision of human behavior to the possibilities offered by computer technology. The computer gave powerful expression to Mr. Simon's already-formed intellectual esthetic. His idea that humans solve problems under constraint was easily adapted to the task of programming a computer to solve problems using heuristic search -- a procedure that doesn't involve absolute rules but rules of thumb based on the knowledge of specific and bounded environments. And his idea that observation must be the foundation for abstraction led him to create a research strategy for artificial intelligence that he called \"thinking aloud.\"\nIn \"thinking aloud,\" Mr. Simon and his colleagues asked people to express their thoughts as they solved logical problems, and then they analyzed the transcripts of these sessions. The protocols of human beings playing games and solving puzzles were used to evaluate computer programs made to follow in human footsteps. Whereas some artificial intelligence researchers were content to leave aside the larger question of whether machines that perform intelligent tasks are doing them in the way people might, Mr. Simon took a very different tack. His artificial intelligence was psychology, not engineering. From the first, he says, \"we were interested in simulating human problem solving, and not simply in demonstrating how computers could solve hard problems.\" His school of artificial intelligence was committed to the view that human intelligence is the manipulation of symbols and the implementation of rules. \"For my money,\" he writes, \"to show that something whose behavior looks very complex and erratic is really built from the combinatorics of very simple components is beautiful, not demeaning.\"\nWhat Mr. Simon saw as self-evident, others found controversial. His critics questioned whether he had really demonstrated that people solve problems the way his programs did, and challenged the premise that a rule-based theory could explain the complexity of human behavior. In artificial intelligence, technical and philosophical arguments commingle. So, in addition to contributing to artificial intelligence as a scientific discipline, Mr. Simon became a chief spokesman for a view of the field as a revolutionary and salutary break with past limitations in philosophy and psychology. In Mr. Simon's opinion, the computer offered the 20th century a solution to the mind-body problem -- \"how a physical system can have thoughts.\" He believed that if one put aside the too-literal analogies between the neurological organization of the brain and the wiring of the computer, there was a \"more fruitful analogy\": people and computers could be shown to process information and manipulate symbols in the same way.\nIn December 1955, Mr. Simon and his colleague, the artificial intelligence scientist Allen Newell, succeeded in writing a computer program that could prove mathematical theorems. Mr. Simon celebrated the achievement by walking into class in January and announcing to his students, \"Over the Christmas holiday, Al Newell and I invented a thinking machine.\"\nTo the rest of the world, he was no less enthusiastic or unequivocal. In a letter to Bertrand Russell written the following October, Mr. Simon stressed that when the thinking machine had proved theorems from Russell and Alfred North Whitehead's \"Principia Mathematica,\" the solutions had involved \"genuine 'discovery.' \" (To this missive Russell replied: \"Thank you. . . . I am delighted to know that 'Principia Mathematica' can now be done by machinery. I wish Whitehead and I had known of this possibility before we both wasted ten years doing it by hand.\") In a much-cited 1957 paper, Mr. Simon predicted that in 10 years \"a digital computer will be the world's chess champion, unless the rules bar it from competition,\" and that after chess the possibilities were endless. Within the \"visible future,\" Mr. Simon declared, \"machines that think, that learn, and that create\" will be able to handle the range of problems \"coextensive with the range to which the human mind has been applied.\"\nThings have not worked out as straightforwardly as Mr. Simon expected. A digital computer is not yet the world's chess champion, and it has turned out that it is easier to teach computers to solve logical problems than to recognize everyday objects, easier to make them play chess than make mud pies.\nThese disappointments have led some critics to claim that there are absolute limits on what computers can do and other critics to say that no matter what computers accomplish they will only be simulating thought. The field has sparked an impassioned and continuing debate about the qualities that might make people different from \"thinking machines\" in fundamental and unbridgeable ways.\nIn \"Models of My Life,\" however, Mr. Simon barely acknowledges the four decades of controversy in which he and artificial intelligence have been engaged. He interprets and dismisses most of it as romantic resistance. \"I think those who object to my characterizing man as simple want somehow to retain a deep mystery at his core,\" he writes. \"In arguing that machines think, we are in the same fix as Darwin when he argued that man shares common ancestors with monkeys, or Galileo when he argued that the Earth spins on its axis.\"\nMr. Simon does not make his large claims and predictions in a spirit of hyperbole; in his eyes, modeling elements of human logical capacities in machines is the same as creating intelligence. His simplification of the scientific path ahead and his impatience with skeptics have always been in the service of starting and protecting a new discipline. He sees himself as a pioneer, inventor and missionary, whose job is to present artificial intelligence in as sharp and clear a form as possible. The task of recognizing limitations and pronouncing caveats is better left to others. His role was to announce a scientific revolution and declare it ready to set up provisional governments in all the human and social sciences.\nOne can hardly imagine a better fit between a man and his intellectual enterprise. Artificial intelligence is the expression of Herbert Simon the economist and Herbert Simon the man. He used thinking about his own thinking and strong personal convictions about bounded rationality to create the general shape of his theory. Conversely, when he contemplates his life he uses concepts drawn from his vision of the field. In \"Models of My Life,\" artificial intelligence appears not only as a technical discipline but also, like psychoanalysis, as a structure for self-reflection.\nThrough its lens, Mr. Simon sees his life in terms of choices, branch points in a maze, searches and decisions. But there are no demons. Mr. Simon describes his life as a \"maze without a Minotaur.\" Indeed, in 1970, he sought out Jorge Luis Borges in Argentina to discuss their shared metaphor of the labyrinth. For Mr. Simon, the search for patterns and rules goes beyond intellectual matters; he looks to it to provide him with a moral metric and a set of values for living. In his autobiography, he explores the meaning of intimate experiences through the formal propositions that might hold them together. Thus, he seems disappointed that Borges denied there was an abstract model underlying his mazes.\nIn Mr. Simon's hands, even the most emotionally charged events become occasions to discover rules and defend theorems. For instance, he describes an episode during which he contemplated infidelity with an attractive former student he calls Karen. He begins by saying that he \"cannot deny\" the proposition that one might be \"genuinely in love with two women at once\" and then shows how he used his experience to add an important new corollary to his theory of love: \"You can love two or more women at once . . . but you cannot be loyal to more than one.\" In his account, Mr. Simon casts his wife in the role of dialogue partner for himself and Karen as they all agree that he was \"asking for the moon,\" attempting the logically impossible. The consistency of Mr. Simon's scientific and personal esthetic, the penchant for the propositional, will surely draw fire from those with Freudian sensibilities who will be alienated by his search for simple rules in complicated situations and his tendency to close down potentially discomforting questions.\nWhen, in 1965, Mr. Simon visited the school in Darmstadt, Germany, where his father had studied engineering, he was faced with questions about his father's real reasons for immigrating to America. Before leaving Germany, in 1903, his father had challenged a fellow student to a duel because of what Mr. Simon believes was an anti-Semitic comment. What, asks Mr. Simon, was his father's relationship to his Jewishness? What, wonders the reader, is Mr. Simon's own relationship to these elements of his past? But such questions are dismissed as irrelevant. They will not yield up facts: \"Nothing we could now seek out in Darmstadt would reveal the secret of that student quarrel.\" And the feelings they would bring up would only be unhelpful: \"The mists perform an important social function when they place a statute of limitations on man's memory of ancient wrongs.\"\nThe information-processing lens, like all theories of mind, is selective in what it places in the foreground and leaves in the background. It consistently leads Mr. Simon to see facts as more important than feelings and to see logic taking precedence over all. His lodestar principle of bounded rationality makes him comfortable with a situation in which there is little room for considering those things an information-processing model leaves out. He is comfortable with the mists that hide the Minotaur. In focusing on the maze, he falls squarely on one side of a division in the long history of thinking about human nature -- on the side that sees people as rational animals and rationalism as the source of a happy life. If deviation from rationalism is what gets us into trouble, let's be rational. If memory is painful, let's forget.\nOf course, there has always been another approach -- to put oneself in touch with memory and Minotaurs. In this view, dreams of rational progress founder on the shoals of fantasy, passion and irrationality. In choosing a life as well as a science of simple propositions, patterns and principles, Mr. Simon is certainly a pre-Freudian, one who does not recognize a seamless web between rational and irrational. But the more important issue raised by this book is that Mr. Simon and his computational esthetic may be the harbingers of a post-Freudian world view. For many readers, Mr. Simon's view of human endeavor, of love and of work, will seem emblematic not of the pre-Freudian rationalism-that- was but of a new, sleeker rationalism-to-be -- a rationalism purged of utopian excess, committed to empirical studies and wedded to the most modern technology.\n\"Models of My Life\" challenges us to think carefully about the costs as well as the benefits of this new rationalism, by suggesting that it may blind us to the Minotaurs in our mazes. And since the parents of scientific revolutions are often more effective if they gaze on their progeny with untroubled eyes, Mr. Simon's book challenges us to do what he has not.\n\u00a0\u00a0'I DON'T FEEL LIKE I'M IN A MAZE'\n If there is one thing holding together Herbert A. Simon's work in economics, artificial intelligence and psychology, it is the idea that making decisions is like going through a maze. But even he acknowledged that the maze metaphor can be confining.\nAfter all, \"one doesn't spend most of one's time making decisions. One lives. I don't feel like I'm in a maze,\" he said during a telephone interview from the University of Minnesota in Minneapolis, where he was attending a conference on cognitive psychology. \"The choices we make lead up to actual experiences. It is one thing to decide to climb a mountain. It is quite another to be on top of it.\"\nMr. Simon said that \"any model of human behavior that focuses on decision making gives us an overrational idea of humans. Artificial intelligence is only a model of human choice, not of human emotion.\" So, while a computer program can simulate what human beings are doing when they're making decisions, \"no one has ever tried to model, say, two people married to each other.\"\nWhy focus on the rational rather than on the emotional? That choice, Mr. Simon said, \"may be a commentary on my personality. Being reasonable, if not rational, is very important to me.\"\nHis decision to write his autobiography, \"Models of My Life,\" was one of his eminently reasonable decisions. Mr. Simon, a professor of computer science and psychology at Carnegie-Mellon University, said he began to reminisce about his life when Pamela McCorduck interviewed him for her book \"Machines Who Think.\" As he recalled, \"she said she wanted to do a biography of me, but then she found something more interesting to do.\" So Mr. Simon wrote it himself, thinking, as he does about many things in his life, that it was for the best. After all, he said, \"I have access to some sources that no one else has.\"\nMr. Simon said that the experience of writing an autobiography hasn't changed his thinking about thinking and that history hasn't changed his thinking about thinking computers. Though his prediction that a computer would be the world's chess champion by 1967 did not come true, he said, \"I still feel good about my prediction. Only the time frame was a bit short.\" -- SARAH BOXER\n","486":"Apple Computer Inc. and Texas Instruments Inc. will announce on Thursday a specially configured version of the Macintosh II personal computer for artificial intelligence applications, industry sources said today.\u00a0The machine will include a microprocessor designed to run LISP, a computer language used in artificial intelligence development, and is expected to be sold by Texas Instruments for less than $20,000.\nLISP applications have typically been run on work stations developed by such companies as Texas Instruments and Symbolics Inc., a company in Cambridge, Mass. The applications typically cost at least $50,000. Analysts say the market for the advanced Mac II will be large by artificial intelligence standards.\n","489":"  In the summer of 1972, while Bobby Fischer was taking the world championship title from Boris Spassky, a British expert on artificial intelligence suggested a new form of the game, \"consultation chess.\" \u00a0\n  Donald Michie, who had been one of WWII's celebrated code breakers, proposed in the journal New Scientist the \"interesting possibility\" of man-plus-computer teams competing against one another in tournaments. \n  Scroll ahead 25 years: Garry Kasparov renamed it \"advanced chess\" and, using a Fritz 5 program. played a match with Veselin Topalov of Bulgarian, using Chessbase 7.0.  \n  Kasparov had scored 9\u00b9\/  points in his previous 13 games against Topalov. But he only drew their match, and the quality of the games was below their usual level. \n  Instead of artificial intelligence, it looked like artificial mediocrity.  \n  Kasparov said the match would \"take a very big and prestigious place in the history of chess.\"  \n  But advanced chess has turned out to have a great future - behind it. It never caught on. \n   Earlier this year, world women's champion Anna Ushenina drew a match with fellow Ukrainian Olena Boytsun. Promoters said it was the greatest female version of \"advanced chess.\" Yes, because so far it's the only one.  \n","490":"When a computer printer turns out a blurry and smeared page, a confounded user is most likely to phone the manufacturer's customer-support staff for advice. But not Dan Lee. His Compaq laser printer contains artificial-intelligence software that can diagnose and unravel printer problems and tell the user what to do.\nTo prove it, Mr. Lee recently typed his printer problem into his personal computer: \"blurry and smeared pages,\" he told it. Before he had written the last word, the software had already seized on key words and was whizzing through its memory stores.\u00a0\n The program, developed for Compaq by the Inference Corporation, where Mr. Lee is a software engineer, uses an artificial-intelligence system called case-based reasoning. Such systems, sometimes called expert systems, are created from the personal experiences of groups of human specialists -- like printer trouble-shooters, who make case-by-case lists of things that are likely to go wrong and steps that are likely to take care of them. Such lists then become the programming foundation for case-based reasoning software.\n\"They've taken their war stories and put them on-line for you,\" said Kristian Hammond, a computer science professor and expert on case-based reasoning at the University of Chicago. \"The Compaq system is all set up for you. This is probably the beginning of a wave.\"\nProfessor Hammond said that case-based reasoning software programs have been available for corporate use for a number of years. But Compaq appears to be the first company to offer such software to the consumer market. \"For wide distribution, this is something very new,\" he said. \"This is certainly the first being given away to consumers.\"\nThe Houston-based Compaq Computer Corporation began using the case-based reasoning software in-house in 1991 to enable operators on customer-support phone lines to answer questions about printers without having to put a caller on hold while a staff expert was tracked down. Last September, the company began providing the software, which is contained on three floppy disks, to purchasers of its Pagemarq printers.\nCompaq estimates that distributing its program to consumers will save it between $10 million and $20 million a year in customer service operations. The company chose the Pagemarq because it was a relatively new product. But it also seemed clear that building case-based software for problems involving printers would be easier than creating programs for the more intricate and diverse troubles that can hit computers themselves.\n\"Once you get into computers, then it gets a lot more complicated,\" said Avron Barr, Inference's director of marketing. Much larger storage media that can hold thousands of cases would probably be necessary.\u00a0Building Solutions\n As printer battles go, blurred pages are probably a typical challenge. So as Mr. Lee watched his computer, the Compaq program's wheels spun through questions and case references and suggested solutions.\n\"It's going through and locating expertise related to blurring and smearing,\" Mr. Lee explained. At the same time, he said, the program was automatically disregarding cases dealing with paper jams, font choices or malfunctioning toner cartridges.\nIn a narrow column on the screen, it was also building a profile of the software and hardware in use so it could eliminate cases it knew did not apply. Once it settled on an answer, the program also provided detailed pictures of the printer and its parts so users could see the hardware that was causing the problem.\nThe software can also \"learn,\" as new cases and experiences are added to its memory, either through updated replacement disks or by downloading new material from Compaq through an on-line information service like Compuserv.\u00a0Search for New Applications\n Joe Carter, a partner in Andersen Consulting, an international management and consulting firm, believes such new uses for artificial-intelligence systems, like case-based reasoning, mean the software should be expanded into the commercial market. He said, \"We're codifying knowledge and expertise into usable knowledge bases, capturing new knowledge as it happens.\n\"A.I. re-creates the decision-making process and does it better,\" he added. \"We're particularly interested in the decisions that humans are not making today -- because we don't have the stamina, time, resources.\"\nCompaq says its new software has significantly reduced customers' calls to its support line.\n\"One real impact of artificial intelligence is the ability to represent knowledge, not just data,\" said Philip Klahr, vice president of Inference. \"And one technique for capturing knowledge is through actual cases and how an expert would respond to those specific situations.\"\u00a0Developed in 1970's\n Case-based reasoning was first developed in the 1970's at Yale University in studies of how human memory works. Researchers wanted to know how memories of experiences made the knowledge useful later. NASA then used the technology in the 1980's to help engineers evaluate old spacecraft blueprints for applications in new designs. Then computer and software manufacturers discovered its usefulness for in-house or corporate-customer support.\nKnowledge is written into the case-based reasoning software through descriptions of actions in truncated language. When you write it, Professor Hammond said, it looks like a subset of English with restricted syntax.\nResearchers in artificial intelligence, striving to create computers that function in ways more similar to the way people think, realize that knowledge is only one facet of intelligence. A second, equally important component, is the ability to dig out and use knowledge.\nSuch searching is something people do all the time, casting through old lessons to solve new problems. The accumulation of those lessons is knowledge. Case-based reasoning gives computers \"a search engine that looks and tries to retrieve relevant\" knowledge, Mr. Klahr said.\nWatching over Mr. Lee's shoulder as he operated the trouble-shooting program for the Compaq printer, Mr. Klahr explained, \"The system has a search mechanism that looks at all of the cases and determines which is a closest match to your case.\" The Compaq software contains several hundred cases.\nAfter several minutes, the program settled on some options for clearing up the blurring and smearing. Among them: \"Check the laser scanner window for dirt or contamination.\"\n","493":"Not enough hardware eye candy. That was the undertone of disappointment that surrounded the introduction of Apple's new iPhone on Tuesday.\nBut look beyond that to something really significant: Siri, the personal assistant application that Apple also showed off. Siri matters for two reasons. First, it represents the future of Apple as a business. Second, Siri potentially is the future of what is now called search.\nApple, no doubt, will continue to design great hardware. But its profit margins from selling devices will most likely erode. It's an iron law of computing: hardware becomes  commoditized, and the only question is how quickly.\nSo Apple is broadening its reach by offering iPhones in a wider price range, including the 3GS free with a two-year contract, and adding more carriers, like Sprint. Thus the iPhone becomes a larger -- and yes, stylish -- delivery vehicle for software and services, like Siri.\n\"In the long term, the best strategy for Apple will be to get more and more of its revenue and profit from its software and services platform,\"  said Michael Cusumano, a professor at  M.I.T.'s Sloan School of Management.\nSiri, put simply, listens to voice commands, searches the Web and online services, and delivers answers. Even today, it is an impressive technology, and one that was nurtured for years by America's  tax dollars -- it originated as a program in the Pentagon's Defense Advanced Research Projects Agency.\nSiri is an evolving artificial intelligence application for things like making restaurant reservations and answering simple questions. And it has been impressing technologists long before Apple bought it last year. My colleague John Markoff and I described Siri as one example of advances in artificial intelligence in an article we wrote last year, before Apple bought the company.\nIf mobile computing is the future, as everyone says, then Siri or something like it is going to be crucial. Typing in commands and conventional search are limited on smartphones. When Microsoft talks about making Bing a \"decision engine,\" something like Siri is what it has in mind. And Google, of course, is aggressively pursuing the same goal; it is the larger Android vision.\nFor Apple, Siri opens the door to getting into the search business in the future. And the way it makes money need not be in click-throughs on intrusive ads. That's the old business model. The new model may well be sharing transaction revenues on all kinds of online commerce, for example.\nWe'll see how things play out, but Siri says a lot about where Apple is headed.\n.\n\n","494":"In any given week, Ben Crotte, a behavioral health therapist at Children's Home of Cincinnati, speaks to dozens of students in need of an outlet.\nTheir challenges run the adolescent gamut, from minor stress about an upcoming test to severe depression, social isolation and bullying.\nAmid the flood of conversations, meetings and paperwork, the challenge for Crotte - and mental health professionals everywhere - is separating hopeless expressions of pain and suffering from crucial warning signs that suggest a student is at risk for taking their own life.\nIt's a daunting, high-pressure task, which explains why Crotte was willing to add another potentially useful tool to his diagnostic kit: an app that uses an algorithm to analyze speech and determine whether someone is likely to take their own life.\nIt's name: \"Spreading Activation Mobile\" or \"SAM.\"\u00a0\n\"Losing a child is my worst nightmare, and we all live with the fear that we might miss something,\" Crotte said, referring to mental health professionals who work in education. \"Sometimes we have to go with our gut to make a decision, so this is one more tool to help me make a final determination about someone's health.\"\nSAM is being tested in a handful of Cincinnati schools this year and arrives at a time when researchers across the country are developing new forms of artificial intelligence that may forever change the way mental health issues are diagnosed and treated.\nRates of teen suicide, in particular, are on the rise, with the rate among teen girls hitting a 40-year high in 2015, according to the Centers for Disease Control and Prevention. Over the past decade, the CDC reports, suicide rates doubled among teen girls and jumped by more than 30 percent among teen boys.\nDespite being the 10th leading cause of death in the United States, suicide remains extremely difficult to predict. Experts say that's because many people's risk for self-harm is paired with another mental illness and fluctuates according to various stressors in their life, all of which interact uniquely within each individual. Complicating matters is that suicidal ideation - which can signal a growing risk for self harm - is far more common than actual suicide. To assess risk, mental health professionals have long relied on timeworn tools - notepads, conversation and well-honed intuition. Now artificial intelligence - combined with the widespread use of smartphones - is beginning to change the way experts interpret human behavior and predict self harm.\n          \"Technology is here to stay, and if we can use it to prevent suicide, we should do that,\" said physician Jill Harkavy-Friedman, vice president of research at the American Foundation for Suicide Prevention. \"But we're in the very early stages of learning how to use technology in this space.\"\nThere are thousands of apps dedicated to improving mental health, but experts say the most promising will begin to incorporate predictive machine learning algorithms into their design like SAM. By analyzing a patient's language, emotional state and social media footprint, these algorithms will be able to assemble increasingly accurate, predictive portraits of patients using data that is far beyond the reach of even the most experienced clinicians.\n\"A machine will find 100 other pieces of data that your phone has access to that you wouldn't be able to measure as a psychiatrist or general practitioner who sees someone for a half-hour a few times a year,\" said Chris Danforth, a University of Vermont researcher who helped develop an algorithm that can spot signs of depression by analyzing social media posts.\nUsing data from more than 5,000 adult patients with a potential for self-harm, Colin Walsh, a data scientist at Vanderbilt University Medical Center, also created machine-learning algorithms that predict - with more than 90 percent accuracy - the likelihood that someone will attempt suicide within the next week. The risk detection is based on such information as the patient's age, gender, Zip codes, medications and prior diagnoses.\nDanforth's algorithm - which he developed with Harvard researcher Andrew Reece - can spot signs of depression by analyzing the tone of a patient's Instagram feed. The pair created a second algorithm that pinpoints the rise and fall of someone's mental illness by scanning the language, word count, speech patterns and degree of activity on their Twitter feed. A task that would require days of research for a clinician was accomplished by the machine in a matter of seconds.\n\"The dominant contributor to the difference between depressed and healthy classes was an increase in usage of negative words by the depressed class, including 'don't,' 'no,' 'not,' 'murder,' 'death,' 'never' and 'sad,' \" the researchers wrote in their latest study identifying mental illness on Twitter. \"The second largest contributor was a decrease in positive language by the depressed class, relative to the healthy class, including fewer appearances of 'photo,' 'happy,' 'love,' and 'fun.' \"\nDanforth said mental health professionals are still dependent on the Diagnostic and Statistical Manual of Mental Disorders (the DSM) and one-on-one interviews, but he believes the data being amassed by smartphones means mental health is on the verge of a \"digital revolution.\"\n\"We're already talking with doctors at the University of Vermont who want to build a screening tool for the emergency room that would ask people whether they'd be willing to have an algorithm look at their social media history,\" Danforth noted.        \nThe tools, however, will only be as good as the data that is used to create machine learning algorithms, Harkavy-Friedman said, noting that there is a lack of longitudinal studies on suicide. Social media will offer important information, she said, but any population of people being studied will always include false positives - people who exhibit suicidal behaviors but don't go on to end their own lives.\n          \"The more we can learn about both the factors leading to suicide, the better off we'll be,\" she said. \"We need a huge number of people to study.\"       \nExperts said it could take another five to 10 years to create algorithms predictive enough to be reliably deployed inside hospitals, schools and therapists' offices. Questions will have to be resolved as well, experts said, such as whether predictive algorithms will affect health insurance premiums or what happens if drug companies manage to access people's predictive data?\nJohn Pestian, a clinical scientist and professor in the divisions of Biomedical Informatics and psychiatry at Cincinnati Children's Hospital Medical Center within the University of Cincinnati, said it's too early to answer those questions. When he created SAM - the app now being tested in Cincinnati schools - he was only focused on one thing: alleviating suffering with technology.\n\"You go into the emergency department and you go to the intensive care unit and you see technology everywhere, but you go into a psychiatrist's office and you see a couch,\" Pestian said.\n\"Why?\" he added. \"Because folks like me haven't built anything for them until now.\"\n          More reading:       \n          Video shows what happens when a robot conducts an Italian orchestra       \n          This man created a mysterious office suspended underneath a bridge       \n          The 'Cajun Navy's' secret weapon for saving lives: The human voice       \n","495":"  The NYPD is set to spend $180,000 so that LinkedIn can help it fill civilian posts - including \"horse hostelers,\" lawyers and thermostat repairmen. \u00a0\n   The department is hoping that the job site's artificial intelligence and computer algorithms will lead to the quicker filling of positions ranging from stable groomsmen and heating specialists to surgeons, lawyers and crime analysts. \n  Under the proposed three-year contract, the site's Recruitment Account Subscription will look at job openings that the department posts, scour the profiles of LinkedIn's more than 500 million users and suggest to New York's Finest whom it should hire based on the data, company rep Imani Greene told The Post.  \n  \"We're able to serve up what their skills are, what their job titles are, how many years of experience they have, what schools they went to. That's aggregated data that's constantly moving based on a million data points,\" she said.  \n  The NYPD already posts job listings on the site, but using artificial intelligence to fill these posts is a first. \n  LinkedIn will provide cops with continuing training on how to use the site and feedback about how successful its efforts are - including analyses of who is looking at the department's want ads and how it can extend its reach, according to Greene.  \n","496":"Some days I think nobody knows me as well as Pandora. I create a new music channel around some band or song and Pandora feeds me a series of songs I like just as well. In fact, it often feeds me songs I'd already downloaded onto my phone from iTunes. Either my musical taste is extremely conventional or Pandora is really good at knowing what I like.\nIn the current issue of Wired, the technology writer Kevin Kelly says that we had all better get used to this level of predictive prowess. Kelly argues that the age of artificial intelligence is finally at hand.\u00a0\nHe writes that the smart machines of the future won't be humanlike geniuses like HAL 9000 in the movie ''2001: A Space Odyssey.'' They will be more modest machines that will drive your car, translate foreign languages, organize your photos, recommend entertainment options and maybe diagnose your illnesses. ''Everything that we formerly electrified we will now cognitize,'' Kelly writes. Even more than today, we'll lead our lives enmeshed with machines that do some of our thinking tasks for us.\nThis artificial intelligence breakthrough, he argues, is being driven by cheap parallel computation technologies, big data collection and better algorithms. The upshot is clear, ''The business plans of the next 10,000 start-ups are easy to forecast: Take X and add A.I.''\nTwo big implications flow from this. The first is sociological. If knowledge is power, were about to see an even greater concentration of power.\nThe Internet is already heralding a new era of centralization. As Astra Taylor points out in her book, ''The People's Platform,'' in 2001, the top 10 websites accounted for 31 percent of all U.S. page views, but, by 2010, they accounted for 75 percent of them. Gigantic companies like Google swallow up smaller ones. The Internet has created a long tail, but almost all the revenue and power is among the small elite at the head.\nAdvances in artificial intelligence will accelerate this centralizing trend. That's because A.I. companies will be able to reap the rewards of network effects. The bigger their network and the more data they collect, the more effective and attractive they become.\nAs Kelly puts it, ''Once a company enters this virtuous cycle, it tends to grow so big, so fast, that it overwhelms any upstart competitors. As a result, our A.I. future is likely to be ruled by an oligarchy of two or three large, general-purpose cloud-based commercial intelligences.''\nTo put it more menacingly, engineers at a few gigantic companies will have vast-though-hidden power to shape how data are collected and framed, to harvest huge amounts of information, to build the frameworks through which the rest of us make decisions and to steer our choices. If you think this power will be used for entirely benign ends, then you have not read enough history.\nThe second implication is philosophical. A.I. will redefine what it means to be human. Our identity as humans is shaped by what machines and other animals can't do. For the last few centuries, reason was seen as the ultimate human faculty. But now machines are better at many of the tasks we associate with thinking -- like playing chess, winning at Jeopardy, and doing math.\nOn the other hand, machines cannot beat us at the things we do without conscious thinking: developing tastes and affections, mimicking each other and building emotional attachments, experiencing imaginative breakthroughs, forming moral sentiments.\nIn the age of smart machines, we're not human because we have big brains. We're human because we have social skills, emotional capacities and moral intuitions. I could paint two divergent A.I. futures, one deeply humanistic, and one soullessly utilitarian.\nIn the humanistic one, machines liberate us from mental drudgery so we can focus on higher and happier things. In this future, differences in innate I.Q. are less important. Everybody has Google on their phones so having a great memory or the ability to calculate with big numbers doesn't help as much.\nIn this future, there is increasing emphasis on personal and moral faculties: being likable, industrious, trustworthy and affectionate. People are evaluated more on these traits, which supplement machine thinking, and not the rote ones that duplicate it.\nIn the cold, utilitarian future, on the other hand, people become less idiosyncratic. If the choice architecture behind many decisions is based on big data from vast crowds, everybody follows the prompts and chooses to be like each other. The machine prompts us to consume what is popular, the things that are easy and mentally undemanding.\nI'm happy Pandora can help me find what I like. I'm a little nervous if it so pervasively shapes my listening that it ends up determining what I like. I think we all want to master these machines, not have them master us.\n","497":"Smart software and robots are not poised to wipe out large numbers of American jobs, but technology-driven automation will affect most every occupation and can change work, according to new research from McKinsey.\u00a0\nThe report, published on Friday and written by two members of the McKinsey Global Institute, the consulting firm's research arm, and another McKinsey employee, adds a twist to the debate over the likely nature and pace of automation in the workplace.\nToday's automation fears essentially rest on two assumptions. First, the speed of advances in digital software and hardware is faster than in previous waves of technological change. And second, clever software and machines are increasingly able to automate cognitive tasks, not just physical ones. Artificial\u00a0intelligence, it seems, poses a new kind of threat to jobs - not so much replacing muscle but brains. \nLooking at the trends in artificial\u00a0intelligence, Carl Benedikt Frey and Michael A. Osborne, researchers at Oxford University, estimated in a paper published two years ago that 47 percent of American jobs were at risk from automation. \nThe McKinsey research suggests a different kind of impact, at least over the next three to five years, which is the time span of its analysis. The McKinsey study found that less than 5 percent of jobs can be entirely automated using \"currently demonstrated technologies,\" which it describes as technologies that are either in the marketplace or in research labs. \nInstead, the new research focuses on work below the occupation level, down to some 2,000 different types of work activities in some 800 occupations, using the definitions of a projectsponsored by the Department of Labor. Applying that lens, the researchers concluded that 45 percent of work activities could be automated, which could affect people in many different roles.\nThe work ripe for automation is not just routine tasks in lower-paying jobs. \"Most high-wage, high-skill jobs have a significant amount of activity that can be automated,\" said Michael Chui, a principal at the McKinsey Global Institute, and a co-author of the report. \nJobs where some portion of activities could be automated include physicians, financial managers and senior corporate executives. At the apex of the corporate job ladder, chief executives, more than 20 percent of the work could be automated, McKinsey estimates. The C.E.O. chores that could be automated with machine-learning software include analyzing reports and data to make operating decisions, preparing staff assignments and reviewing status reports.\nLandscapers and home health care workers are among the occupations less susceptible to automation than chief executives, according to McKinsey.\nThe value of the McKinsey research, experts said, is that it provides a fine-grained analysis of the impact of automation. \n\"It makes a lot of sense to look at automation at the task level, as a way to think about redefining jobs,\" said Erik Brynjolfsson, a professor at M.I.T.'s Sloan School of Management and co-author of \"The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies.\"\nThe McKinsey authors emphasized the potential for automation to enrich work, liberating people to focus on more creative tasks. Apparently there is a lot of room for improvement on that front. According to one calculation in the report, \"just 4 percent of the work activities across the U.S. economy require creativity at median human level of performance.\"\n","498":"STANFORD, Calif. -- In factories and warehouses, robots routinely outdo humans in strength and precision. Artificial\u00a0intelligence software can drive cars, beat grandmasters at chess and leave ''Jeopardy!'' champions in the dust.\nBut machines still lack a critical element that will keep them from eclipsing most human capabilities anytime soon: a well-developed sense of touch.\nConsider Dr. Nikolas Blevins, a head and neck surgeon at Stanford Health Care who routinely performs ear operations requiring that he shave away bone deftly enough to leave an inner surface as thin as the membrane in an eggshell.\nDr. Blevins is collaborating with the roboticists J. Kenneth Salisbury and Sonny Chan on designing software that will make it possible to rehearse these operations before performing them. The program blends X-ray and magnetic resonance imaging data to create a vivid three-dimensional model of the inner ear, allowing the surgeon to practice drilling away bone, to take a visual tour of the patient's skull and to virtually ''feel'' subtle differences in cartilage, bone and soft tissue. Yet no matter how thorough or refined, the software provides only the roughest approximation of Dr. Blevins's sensitive touch.\u00a0\n''Being able to do virtual surgery, you really need to have haptics,'' he said, referring to the technology that makes it possible to mimic the sensations of touch in a computer simulation.\nThe software's limitations typify those of robotics, in which researchers lag in designing machines to perform tasks that humans routinely do instinctively. Since the first robotic arm was designed at the Stanford Artificial\u00a0Intelligence Laboratory in the 1960s, robots have learned to perform repetitive factory work, but they can barely open a door, pick themselves up if they fall, pull a coin out of a pocket or twirl a pencil.\nThe correlation between highly evolved artificial\u00a0intelligence and physical ineptness even has a name: Moravec's paradox, after the robotics pioneer Hans Moravec, who wrote in 1988, ''It is comparatively easy to make computers exhibit adult-level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a 1-year-old when it comes to perception and mobility.''\nAdvances in haptics and kinematics, the study of motion control in jointed bodies, are essential if robots are ever to collaborate with humans in hoped-for roles like food service worker, medical orderly, office secretary and health care assistant.\n''It just takes time, and it's more complicated,'' Ken Goldberg, a roboticist at the University of California, Berkeley, said of such advances. ''Humans are really good at this, and they have millions of years of evolution.''\nTouch Impulses\nTouch is a much more complicated sense than one might think. Humans have an array of organs that allow them to sense pressure, sheer forces, temperature and vibrations with remarkable precision. (And German researchers have shown that raccoons have evolved the animal world's most sophisticated brain functions to process touch impulses in the dark.)\nResearch suggests that our sense of touch is actually several orders of magnitude finer than previously believed. Last fall, for example, Swedish scientists reported in the journal Nature that dynamic human touch -- for example, when a finger slides across a surface -- could distinguish ridges no higher than 13 nanometers, or about 0.0000005 of an inch.\nThat is the scale of individual molecules. Or as Mark Rutland, a professor of surface chemistry at the KTH Royal Institute of Technology in Sweden, put it, if your finger were as big as the earth, it could feel the difference between a car and a house. Physiologists have shown that the interaction between a finger and a surface is detected by organs called mechanoreceptors, which are embedded at different depths in the skin. Some are sensitive to changes in an object's size or shape and others to vibrations.\nIn the case of tiny surface variations, cues come from Pacinian corpuscles, oval-shaped structures about a millimeter long (one twenty-fifth of an inch) that signal when they are deformed.\nReplicating that sensitivity is the goal of haptics, a science that is playing an increasing role in connecting the computing world to humans. One of the most significant advances in haptics has been made by Mako Surgical, founded in 2004 by the roboticist Rony Abovitz. In 2006, Mako began offering a robot that provides precise feedback to surgeons repairing arthritic knee joints.\n''I thought haptics was a way to combine machine intelligence and human intelligence in a way that the machine would do what it was good at and the human would do what the human was good at, and there was this really interesting symbiosis that could come about,'' Mr. Abovitz said, adding:\n''The surgeon still has the sense of control and can put the energy into the motion and push. But all of the intelligent guidance and what you thought the surgeon would normally do is done by the machine.''\nRobotic Dangers\nEven in industries where robots are entrenched, experts worry about the dangers they pose to the people who work alongside them. Robots have caused dozens of workplace deaths and injuries in the United States; if a robot revolution is ever to take place, scientists will have to create machines that meet exacting safety standards -- and do it inexpensively.\n''For the last 30 years, industrial robots have focused on one metric: being fast and cheap,'' said Kent Massey, the director of advanced programs at HDT Global, a robotics firm based in Solon, Ohio. ''It has been about speed. It's been awesome, but a standard arm today is precise and stiff and heavy, and they're really dangerous.''\nMr. Massey's company is one of a number of robot-arm designers that are beginning to build safer machines. Rethink Robotics in Boston and Universal Robots in Denmark have built ''compliant'' robots that sense human contact. The Universal system uses a combination of sensors in its joints and software, and the Rethink robot uses ''series elastic actuators'' -- essentially springs in the joints that mimic the compliance of human muscles and tendons and acoustic sensors so the robot can slow when humans approach.\nBeyond advances necessary for basic safety, scientists are focusing on more subtle aspects of touch. Last year, researchers at Georgia Tech reported in the journal Science that they had fabricated bundles of tiny transistors called taxels to measure changes in electrical charges that signal mechanical strain or pressure. The goal is to design touch-sensitive applications, including artificial skin for robots and other devices.\nMuch research is focusing on vision and its role in touch. The newest da Vinci Xi, a surgery system developed by Intuitive Surgical Inc., uses high-resolution 3-D cameras to enable doctors to perform delicate operations remotely, manipulating tiny surgical instruments. The company focused on giving surgeons better vision, because the necessary touch for operating on soft tissue like organs is still beyond the capability of haptics technology.\nCurt Salisbury, a principal research engineer at SRI International, a nonprofit research institute, said that while surgeons could rely on visual cues provided by soft tissues to understand the forces exerted by their tools, there were times when vision alone would not suffice.\n''Haptic feedback is critical when you don't have good visual access,'' he said.\nOther researchers believe that advances in sensors that more accurately model human skin, as well as algorithms that fuse vision, haptics and kinematics, will lead to vast improvements in the next generation of robots.\nOne path is being pursued by Eduardo Torres-Jara, an assistant professor of robotics at Worcester Polytechnic Institute in Massachusetts, who has defined an alternative theory he describes as ''sensitive robotics.'' He has created a model of robotic motion, grasping and manipulation that begins with simply knowing where the robot's feet or hands meet the ground or an object. ''It is all about recognizing the tactile events and understanding that very well,'' he said. Using biologically inspired artificial skin that can detect tiny changes in magnetic forces, he has built a two-legged walking robot that is able to balance and stride by measuring changing forces on the bottoms of its feet.\nIf improving tactile performance depends on greater computing power, help may be on the way. Dr. Goldberg, the Berkeley roboticist, has begun designing cloud-based robotic systems that can tap vast pools of computing power via the Internet.\n''I'm very excited about the idea of cloud robotics,'' he said. ''It is lifting the limitation of computing that we've always had.''\nIn July, roboticists at Brown, Cornell, Stanford and Berkeley described a database called Robo Brain, sponsored by the National Science Foundation, that is intended to offer an Internet-based repository of images and videos to give robots support for performing actions in the physical world. For example, information on how to identify, grasp and carry a coffee mug would be accessible to any robot or robotic arm connected to the Internet.\nOther haptics researchers believe that artificially replicating touch will have a powerful effect on the development of autonomous robots, as well as systems that augment humans.\nLast fall, Allison Okamura, an associate professor of mechanical engineering at the Laboratory for Collaborative Haptics and Robotics in Medicine at Stanford, taught an online course in haptics. Students assembled ''hapkits'' designed by Dr. Okamura, the Stanford education professor Paulo Blikstein and Tania Morimoto, a Stanford graduate student. They then programmed them to create virtual devices like springs and dampers that could be manipulated as if they were in the real world.\nThe students followed with new projects, tweaking the hardware and sharing programs they had created. Dr. Okamura said their enthusiasm was understandable.\n''If you have all these senses -- vision, hearing, taste, touch and smell -- and someone took them away from you one by one, which is the last one you would give up?'' she asked. ''Almost everyone says vision, but for me, it would be touch.''\n","499":"The eventual subjugation of humanity by a race of supersmart, artificially intelligent beings is something that has long been theorized.\nBut the latest prophet of our cyber-fueled downfall must realize why people would be inclined to take his warnings with a grain of silicon. He is, after all, the same guy who's asking us to turn over control of our cars - and our lives - to a bunch of algorithms.\u00a0\nElon Musk, who hopes that one day everyone will ride in a self-driving, electric-powered Tesla, told a group of governors recently that they needed to get on the ball and start regulating artificial intelligence, which he called a \"fundamental risk to the existence of human civilization.\"\nNo pressure. When pressed for better guidance, Musk said the government must get a better understanding of the latest achievements in artificial intelligence before it's too late.\n\"Once there is awareness, people will be extremely afraid, as they should be,\" Musk said. \"AI is a fundamental risk to the future of human civilization in a way that car accidents, airplane crashes, faulty drugs or bad food were not. They were harmful to a set of individuals in society, but they were not harmful to individuals as a whole.\"\nAnd then Musk outlined the ways AI could bring down our civilization, which may sound vaguely familiar.\nHe believes AI \"could start a war by doing fake news and spoofing email accounts and fake press releases, and just by manipulating information. Or, indeed - as some companies already claim they can do - by getting people to say anything that the machine wants.\"\nMusk said he is usually against proactive regulation, which can impede innovation. But he is making an exception in the case of a possible AI-fueled Armageddon.\n\"By the time we are reactive in regulation, it's too late,\" he said, confessing that \"this is really like the scariest problem to me.\"\nHe has been warning people about the problem for years, and even came up with a solution: Join forces with the computers.\nHe announced earlier this year that he is leading a company called Neuralink, which would devise ways to connect the human brain to computers, CNN reported.\nIn the decades to come, he said, an Internet-connected brain plug-in would allow people to communicate without opening their mouths and learn something as fast as it takes to download a book.\nOther prominent figures in the world of science and technology also have warned against the dangers of artificial intelligence, including Microsoft founder Bill Gates and theoretical physicist Stephen Hawking. But Musk concedes that people have been hesitant to accept their viewpoint.\n\"I keep sounding the alarm bell, but until people see like robots going down the streets killing people, they don't know how to react because it seems so ethereal,\" he said. \"I think we should be really concerned about AI.\"\ncleve.wootson@washpost.com\n \uf14c More at www.washingtonpost.com\/news\/innovations\n","502":"Over the weekend, Alphabet executive chairman Eric Schmidt (you probably know him better as Google executive chairman Eric Schmidt) wrote an op-ed on artificial intelligence for the BBC. While he does a good job of describing ways artificial intelligence will drive forward\u00a0some key consumer segments (specifically, music and travel), he doesn't go far enough in developing some truly breakthrough ideas about where AI's headed.\nThat's especially surprising, given Google's former emphasis on \"moonshots\" and long-time reputation as an innovation leader.\u00a0\nTake, for example, Schmidt's remarks about \"smarter music,\" which some people in the tech community have perceived as a shot across the bow at Apple Music. Schmidt says that intelligent algorithms can do a better job than human tastemakers at picking music: \"A decade ago, to launch a digital music service, you probably would have enlisted a handful of elite tastemakers to pick the hottest new music.\"\n\"Today,\" he says, \"you're much better off building a smart system that can learn from the real world - what actual listeners are most likely to like next - and help you predict who and where the next Adele might be.\"\nSchmidt also says that AI has a real role to play in sorting out \"real-world messiness.\" The example he gives is selecting a last-minute vacation that will satisfy everyone, even \"two picky kids,\" while still fitting within the constraints of a family budget. If you add enough data about preferences and budget parameters, an AI algorithm could optimize the perfect vacation choice. That's interesting, but it also sounds like something that's possible today - not a reach goal for the future.\nAnd it's much the same for other consumer segments. Schmidt says that AI means \"smarter filters on your e-mails, your social media feeds, your schedule.\" It means more ways to apply AI to everyday life - such as the automatic captioning of family photos - and an easier way to manage the flow of information around us.\nGood points, but Schmidt also cherry-picks areas where Alphabet is poised to have the most impact and areas where Google\u00a0can start putting its AI acquisitions to work within the fastest possible time frame. In 2014, for example, Google acquired the much-vaunted AI start-up DeepMind at a reported price tag of $400 million.\nYou can almost see Schmidt's mental gears whirring, as he thinks about each new company within the Alphabet parent company, and how AI might be applied to each of Google's businesses in order to appeal to stakeholders. Music, check. Search, check. Email, check. Rather than leading to something completely transformative - think intelligent robots or self-driving cars - AI may end up being more of a special sauce that's sprinkled over the company's core assets.\nAnd that's a shame if AI is just about optimizing Internet ROI. Especially since Schmidt's earlier book, The New Digital Age, which he co-authored with Google's Jared Cohen, gives so many examples of how Schmidt is truly taking a big-picture view of the digital world. In that book, Schmidt describes how the Internet is changing the future of identity, citizenship, government, journalism, revolution, terrorism, war and foreign policy.\nThe new age of AI is beginning, and its a big deal. http:\/\/t.co\/yNQ0dDqMXp\nIt's likely that AI, if it continues to progress at current pace, will have just as an important impact on the future - and not just on consumer-facing segments of the Internet. If anything, Schmidt aims too low in his BBC op-ed by focusing only on the low-hanging fruit, such as e-mail.\nWhere Schmidt misses the full\u00a0potential of AI are three areas that he specifically cites within the BBC op-ed: genomics, climate change and energy. Remember when we heard about Google's new renewable energy initiatives, bold attempts to change healthcare and wacky Google X projects? That seems to have all been replaced by a much more pedestrian concern about making sense of all the data, turning AI into just a powerful data-crunching machine.\nMoreover, Schmidt mentions AI legend Geoff Hinton - one of the people that he cites as an inspiration for Google's current AI initiatives - but doesn't mention the legendary Ray Kurzweil, who was recruited\u00a0by\u00a0Google to work on AI projects. He alludes to Google's Gmail, but doesn't mention DeepMind, even though DeepMind is now teaching machines how to read.\nAre those intentional or unintentional oversights? Sometimes the biggest clues are from what's not mentioned, not what's mentioned.\nIf the big news story about Schmidt's op-ed on AI is the turf war between Google and Apple Music - as Mashable and re\/code suggest - it seems that something is being missed about the full potential of artificial intelligence. If AI is just about making consumer lives less messy, then there's maybe not as much to get excited as was once thought.\nOr, this op-ed mentioning everyday consumer needs - photo captions and spam filters\u00a0- could just be an attempt by Schmidt to downplay all the dystopian fears about the future, while still playing up the long-term potential of artificial intelligence. At SXSW earlier this year, for example, Schmidt suggested that many of the dystopian views of the future are being overhyped. Adding captions to photos, how harmless could that be? (Wait, wrong question to ask, unless you like demon puppies.). Or better spam filters, who's opposed to that?\nThis view of AI, in which artificial intelligence is just another digital innovation that's going to \"reduce the noise of everyday life\" raises the inevitable question: What if the leading minds within the world of artificial intelligence promised us superintelligence, but ended up just giving us better spam filters?\n","503":"PERSONAL computer users will be able to benefit soon from the artificial intelligence products being created on more sophisticated and powerful computers now.\nThese products, unlike existing programs for personal computers, are supposed to be able to reason through a problem and provide possible solutions. Indeed, their biggest contribution may be to help define a problem.\nAt a conference here this week, Jane T. Malin, cognitive psychologist with the National Aeronautics and Space Administration, showed how she and a colleague use Intellicorp's Knowledge-Engineering Environment to trouble-shoot for air-quality problems in the simulated sleeping quarters for astronauts on the space station.\u00a0\nIf the air temperature is too low, the program, which is displayed on a large computer screen, can check for faulty motors, fans, fuel cells and dozens of other items in the air quality system. When the program pinpoints trouble, it automatically tests a variety of corrective actions.\n''It's a system that does the diagnosis, then keeps moving to make the system more correct even if it is not sure it knows the problem,'' Miss Malin said.\nMiss Malin was attending the biennial International Joint Conference on Artificial Intelligence. The historically sleepy, esoteric gathering attracted 1,400 participants in 1981. This year the conference is expected to attract more than 10,000 people before its close Friday on the campus of the University of California at Los Angeles, with its attendance swelled by venture capitalists, investment bankers such as Goldman, Sachs & Company and Morgan Stanley & Company and legions of corporate executives and marketing personnel.\n''Most of what we're seeing are tools that will enable us to build applications that people can use'' on personal computers, said Esther Dyson, publisher of Computer Industry Daily. ''For now, you can't run or create them on a personal computer. But they are clearly becoming commercial, and it's about time.''\nJohn A. Young, president and chief executive of the Hewlett-Packard Company, told reporters, ''We believe we are on the edge of something very big, with very important implications for our business and for our many customers.'' He said that with artificial intelligence development tools, ''we think we have the beginnings of a solution to the software-productivity problem.''\nHewlett-Packard, Digital Equipment, International Business Machines and General Electric were among the computer industry leaders that brought new products to the conference.\nSmall start-up companies that beat the bigger companies to the marketplace with early products three and four years ago, were also here. They included Intellicorp, Teknowledge and the Carnegie Group. Intellicorp, which has been listed on the over-the-counter market since 1983, made its first profit during the last fiscal quarter, on sales of $3.4 million.\nMany of their products are known as expert systems. They devise ways of collecting and organizing highly specialized information so that users can make quicker, more intelligent decisions.\nScientists at Hewlett-Packard, who began work on artificial intelligence in 1981, have developed a training system for workers in the wafer-fabrication units of their semiconductor manufacturing division. The program runs on the company's personal computers.\nThe system can recommend 120 steps for solving problems, and the number is expected to double soon. It includes 65 movies of how to conduct various tests; the films are stored on video cassettes and displayed on a computer screen by a research engineer after a production problem is spotted. Researchers at the company said the system had reduced quality problems, and sharply improved the performance of workers.\nJoel S. Birnbaum, director of the company's research laboratories, said the system ''probably represents a greater knowledge base than that held by any single person in our company.'' That, of course, is a central promise that has sustained the work in artificial intelligence for 15 years or more, as many said at the conference.\nSome university researchers were cautious about the commercial efforts on display at the conference. ''There is as much marketing hype'' in artificial intellgence ''as there is in all other areas of computing,'' said Dr. Alan K. Mackworth, chairman of the conference and a professor at the University of British Columbia.\n","504":"TRADE shows do not get much more obscure than Sensors Expo and Conference. The semiannual gathering assembles scores of little companies -- or little-known divisions of large ones like General Electric -- that make devices to measure heat, pressures, speed, voltage, acceleration and scores of other conditions that are vital to machines and people. \n     It is an industry in which big products sit in the palm of a hand and the trend is toward miniaturization to the point of invisibility. Innovations generally end up embedded out of sight in the products of other companies. \u00a0\n Walking through the dimly lit and uncrowded exhibition hall at the most recent Sensors Expo last month at Boston's World Trade Center, one could easily overlook that the industry is the on-ramp to the future of both electronics and biotechnology. To fully exploit the processing power of supercomputers, the ever-increasing sophistication of software programmers and the rapidly expanding skills of genetics engineers will depend largely on progress in creating better and cheaper sensors and other systems for gathering sensory data.\nThat may explain why the trade show's organizers chose Raymond C. Kurzweil as their keynote speaker. Mr. Kurzweil is an inventor and author, one whose early interests in character recognition devices that read to the blind and in music synthesizers evolved into a fascination with virtual reality and artificial intelligence. His talk, \"The Rapidly Shrinking Sensor: An Intimate Merger with Our Bodies and our Brains,\" invited those attending to consider that they have a role to play in the merging of machines and biological life.\nMr. Kurzweil's main interest extends well beyond such humdrum advances as the development of more reliable machines to replace dying hearts, damaged bones and other failing body parts -- or of cell-sized robots to carry out internal repairs to damaged organs as an alternative to transplants. He envisions using embedded electronics to sharply increase the human capacity for thought and developing so-called haptic systems that would enable one person to physically experience the sensations of someone else.\n\"We're limited to 100 trillion connections,\" said Mr. Kurzweil, alluding to current estimates of the processing power in the human brain. \"I don't know about you, but I find that quite limiting.\"\nTechnology can be misused, he conceded, citing the Sept. 11 attacks. And some people recoil at the thought of creating cyborgs -- hybrids of man and machine. But people already have implants in their brains to combat conditions like Parkinson's disease, so in Mr. Kurzweil's view the question of whether to create cyborgs has already been answered. For him, the issue is how rapidly science and technology move forward.\nMr. Kurzweil spent little time advising the crowd how to get from today to the future he envisions. And if he was aware of the actual performance specifications of any of the equipment on display at the trade show, he did not let on. In fact, he stuck closely to a crowd-pleasing spiel he had already delivered elsewhere, including at an August conference in Edmonton, Alberta, of the American Association for Artificial Intelligence.\nThe centerpiece of the talk was Mr. Kurzweil's use of Ramona, the raven-haired virtual rock star he has created as his alter ego and guide to his Web site (www.kurzweilai.net). Launching a computer program that allowed him to talk to an image of Ramona displayed on a large screen, Mr. Kurzweil set off on a conversation with her. \n\"O.K., Ramona, what's reality?\" Mr. Kurzweil asked, pausing while the computer typed out the question it heard.\n\"You can go where you want to go, experience what you want to experience and be who you want to be,\" Ramona replied. Asked to share a little about herself, she responded: \"We moved around a lot. My father wasn't very good at holding a job.\"\nWhen he inquired if she would like to get together that night, she coyly admonished, \"Ray, you know better than to ask that.\"\nThe conversation purported to illustrate how the combination of sensors and computing power had already matured enough for virtual characters to interact with humans in situations much more casual and unstructured than the automated ticket agents many people have encountered. \nIn another segment, Mr. Kurzweil played a video from a design trade show last year, TED11, that demonstrated how he can be in one place delivering a speech that an audience in a remote auditorium perceives as being delivered from the screen by Ramona. The video also starred his 14-year-old daughter, Amy, who danced energetically behind him as he talked. Her dancing controlled her virtual alter ego, who was seen on the screen in triplicate executing the same steps in the background behind Ramona. As a mirthful counterpart to Mr. Kurzweil's sultry young Ramona, Amy's virtual form was that of Richard Saul Wurman, the hefty middle-aged producer of the TED shows.\nIf applause and laughter were good barometers, the keynote presentation went off well. But back on the show floor, it was evident that there is a gap between the virtual-reality future and the current limitations in building the devices to get there. Indeed, some exhibitors were preoccupied with simply catching up to the present. \n\"Our airfields are still in the 19th century,\" said Dr. Paul A. Curto, a senior technologist on the National Aeronautics and Space Administration's Invention and Contributions Board. In presentations at the space agency's technology transfer booth at the show, he tried to interest people in a simple wind sensor that could provide weather data to pilots over the Internet and radio. The potential market is the estimated 40,000 airfields and helipads worldwide, including 5,400 in the United States, that still rely on nothing more than windsocks for data about landing conditions. Dr. Curto said the technology would save 100 lives annually.\n\"I didn't get to go to Kurzweil's speech, but there's a connection,\" Dr. Curto said. \"Every single step that gets implemented is a step toward higher things.\"\n","505":"The Washington Post's Heliograf and ModBot technologies each took first place in the 2018 Global BIGGIES Awards which recognize best practices in big data and artificial intelligence products and strategies by media companies from around the world.\u00a0\nHeliograf won in the category of \"Excellence in Use of Bots\" and\u00a0the judges noted\u00a0\"[w]hile this is not the first company to successfully automate the creation of articles based compiling data in templates, their set of examples and explanations are eloquently written and backed. The results also seem quite promising. Very impressive.\"\nModBot took the top prize for \"Excellence in Use of Artificial Intelligence (Non-Bot)\" and the judges said it is \"[a] good solution to the very tangible problem of comment moderation. This is a vast problem in the media industry and this solution could reduce a lot of cost for large publishers...\"\nThe awards are administered by\u00a0The Big Data & AI for Media Association and the winners were honored at a dinner in New York City last night. See the complete list of winners.       \n","507":"The leader of one of the world's largest automobile producers expects\u00a0that cars will soon drive themselves and sync to the world around them - but don't\u00a0count out the human behind the wheel just yet.\nCarlos Ghosn, the chief executive\u00a0and chairman of an alliance that includes Nissan, Renault and Mitsubishi, said Thursday that humans will remain involved in the operation of vehicles for the foreseeable future, even as cars with self-driving technology enter the market in the next five years. You will push a button to activate the car's autonomous driving feature, he said, but it will encounter everyday scenarios it cannot compute and that require human assistance.\n\"Artificial intelligence is still way below the creativity of the human brain,\" Ghosn said.\u00a0\nImagine a self-driving car\u00a0coming upon a broken-down vehicle in the road, but there is a solid line to either side of it, Ghosn said. The car is wired to recognize both as impassable and doesn't have the judgment to cross over the line and pass the vehicle as long as the roadway is clear. A human will have to do the job.\nThat's just one common scenario in which artificial intelligence comes up short. General Motors recently acknowledged that its own vehicles are\u00a0not sophisticated enough to respond when another motorist honks his\u00a0horn.\nGhosn's perspective on\u00a0the human's role in autonomous driving is not universally shared. One of the major questions hanging over self-driving cars is how much they should depend on humans in the vehicle to intervene, if at all. Studies show that autonomous vehicles can lull passengers into a passive state, and stirring them to act when a problem arises takes time and may pose safety concerns.\nFord has seen engineers fall asleep in its self-driving cars during testing, Bloomberg reported\u00a0last month. Both Ford and Waymo, Google's self-driving car company, intend to eliminate the role of the human driver entirely, according to Bloomberg, though other major automakers, including GM, Audi and Tesla, still plan to rely on\u00a0human vigilance.\nSelf-driving technology is also expected to be an economic force - with both positive and negative consequences. The technology could lead to widespread unemployment among professional drivers, for example, whether they work behind the wheel for ride-hailing services like Uber or long-haul trucking companies.\nGhosn disagrees. He said Thursday the technology will enable companies to satisfy their constant shortage of drivers, while also freeing up existing drivers to do more substantive tasks while en route.\n\"Technology is not going to replace human beings; it's going to support you,\" Ghosn said. \"It's more, 'I have a limitation, and I want to eliminate this limitation by bringing this technology in.' \"\nNissan unveiled its vision for the future of cars almost exactly a year ago at the Geneva International Motor Show. Called Nissan Intelligent Mobility, the concept calls for cars that are autonomous, electric and connected to the world around them.\nThe company brought that vision closer to reality at the International CES technology show in January, when it debuted in-car artificial intelligence that admits when it doesn't know enough to make decisions. The car will then come to a stop and contact a human mobility manager in a command center for instructions.\n\"As the system learns from experience, and autonomous technology improves, vehicles will require less assistance and each mobility manager will be able to guide a large number of vehicles simultaneously,\" Nissan said in January.\nLast year, Nissan began selling a minivan in Japan that comes equipped with ProPilot technology that allows the vehicle to drive itself on single-lane highways.\nGhosn will step down as Nissan's chief executive\u00a0in April. He took the helm of Nissan in June 2001 and oversaw its ascent from a beleaguered automaker to part of a massive automotive alliance that includes Renault and Mitsubishi. He remains the chief executive\u00a0of Renault and chairman of all three companies.\nHe will be replaced at Nissan by Hiroto Saikawa, the company's co-chief executive and former chief competitive officer.\n              Read more from The Washington Post's Innovations section.\u00a0           \n           General Motors CEO says Trump's border tax would be 'problematic' for auto industry        \n           The big moral dilemma facing self-driving cars        \n           The simple question about self-driving cars that we still can't answer        \n","509":"PITTSBURGH - Twelve days into the strangest poker tournament of their lives, Jason Les and his companions returned to their hotel, browbeaten and exhausted. Huddled over a pile of tacos, they strategized, as they had done every night. With about 60,000 hands played - and 60,000 to go - they were losing badly to an unusual opponent: a computer program called Libratus, which was up nearly $800,000 in chips.\nThat wasn't supposed to happen. In 2015, Les and a crew of poker pros had beaten a similar computer program, winning about $700,000. This time, the pros had initially kept things more or less even by finding flaws in how the computer played; fans following this \"Brains Vs. AI\" competition at the Rivers Casino here put the odds of the AI winning at only about 1 in 4.\nBut by the second week, the flaws had disappeared; the odds of the computer triumphing rose. \"On Day 1, it had played well, but it wasn't impressive,\" Les said. \"What's impressive is how this thing has learned and evolved, how much better it has gotten every day.\"\u00a0\nMachines have learned a lot about how to play games. Twenty years ago, they figured out checkers, and 10 years ago they toppled the Russian grandmasters of chess. Even China's game of go has been solved. But poker remained firmly in the hands of humans.\nThat's because unlike checkers and chess, where all the pieces are visible, poker is a game of limited knowledge and uncertainty, of hidden cards and bluffs. It is perhaps truer to life, which may explain why it has been difficult for silicon chips to grasp.\n\"AIs have had a lot of trouble with poker,\" said Noam Brown, a graduate student at Carnegie Mellon University who developed Libratus with CMU computer scientist Tuomas Sandholm. \"It's the holy grail of imperfect information games.\"\nA victory for Libratus, Brown said, would not be much of a threat to human poker players. Its brain is a supercomputer that costs millions of dollars per year to run, so using it to play poker would not be a great way to make money. But Libratus could be a step toward helping artificial intelligence deal more broadly with uncertainty.\nThat's because poker is not simply a game of chance. Neither does it require being able to read an opponent's facial expressions, although Hollywood might like us to believe otherwise. What guides Libratus's decisions is powerful mathematics, math that could be applied to auctions, negotiations, finance, security and other real-world arenas in which information is hidden.  \nSerious mathematicians have long been fascinated by poker. John von Neumann, a pioneer in game theory, the branch of mathematics that deals with competition, explored the ins and outs of the card game early in the past century. So did John Nash, whose struggle with schizophrenia was depicted in the movie \"A Beautiful Mind.\" In 1950, Nash published a paper showing that there is a best strategy for many games, including one-on-one poker, regardless of how your opponent plays. That strategy, now called a Nash equilibrium, may not always win, but it does better than any other approach.\nFinding the Nash equilibrium for simple games such as tic-tac-toe or rock, paper, scissors is easy. Finding it for a game as complicated as poker is hard. An artificial intelligence developed at the University of Alberta has been able to master a basic version of poker called heads-up limit Texas Hold 'em, in which two players compete against each other with a restricted ability to bet. But a hand of no-limit heads-up poker, in which the players can wager as much as they want to, involves a huge number of possibilities: 10 to the power of 160, which is a one followed by 160 zeros. That's more than the estimated number of atoms in the universe. For poker games involving more than two people, the possibilities become seemingly incalculable.\nPeople ranging from university academics to enthusiastic retirees have tried to create artificial intelligences to simplify the problem. Every February, they pit their creations against each other at a machines-only competition. A winner is declared, but a person who simply folded each hand would do better than many of these AIs. \"Every year, the computers play billions of hands against each other,\" says Jonathan Schaeffer, a University of Alberta computer scientist who helped to start the contest. \"Every year, we see incremental improvement.\"\nAside from the people behind Carnegie Mellon's Libratus, only the Alberta team has made the claim of being able to beat humans. The Canadian program, called DeepStack, uses a neural network, a piece of software that works a bit like the human brain, making fast estimates that its creators compare to an intuition and reconsidering its options as new cards are laid on the table. A research paper posted on Jan. 10 claims that DeepStack played 40,000 hands against dozens of poker players and won, becoming the \"first computer program to beat professional poker players in heads-up no-limit Texas Hold 'em.\"\nBut the poker pros facing off against Libratus brushed off that victory, pointing out that the people recruited for that study were not specialists in one-on-one, heads-up poker. \"Those guys don't play our game type,\" said Dong Kim, one the high-stakes poker players in the tournament. \"They might play other kinds of poker, but even small-stakes heads-up players on the Internet would crush them.\"\nThe Alberta researchers declined to comment, pending the acceptance of their paper in a scientific journal.\nLibratus prepared for its epic match by first playing trillions of hands against itself to build a database about which choices tend to work better than others. While playing, it pauses once in the middle of each hand to rethink its strategy, assessing not only what moves it can make but also other moves it could have made if the situation were different.\nThis method has led to some seemingly unusual decisions by Libratus that fly in the face of traditional poker wisdom, Sandholm says. When an opponent raises the stakes on the last bet, for instance, the computer may match that raise, even with weak cards that are unlikely to win.\n\"If my 10-year-old daughter made that move, I would teach her not to,\" Sandholm said. \"But it turns out that this is actually a good move. It helps to catch bluffs.\"\nThose strategies paid off. By the end of the 20-day competition, Libratus was declared the winner, up more than $1.7 million in chips. \"This is a major milestone for AI,\" said Andrew Ng, a computer scientist at Stanford University who followed the tournament.\nLes and his companions each walked away with a share of a $200,000 purse (real money, not chips) - and perhaps some lessons in how to play cards.\n\"We are definitely learning from how this computer thinks,\" Les said. \"I think I will come out of this a better poker player.\"\nhealth-science@washpost.com\nRead more:\nMark Zuckerberg builds an AI assistant to run his house - and entertain his toddler \nMind-controlled devices offer hope for the disabled \nEverything you think you know about AI is wrong \n","510":"HACKERS: Heroes of the Computer Revolution. By Steven Levy. 458 pages. Anchor\/Doubleday. $17.95.  There was the great chess showdown of 1965, when MacHack won a chess game against a critic of artificial intelligence named Herbert Dreyfus, who had bluntly asserted that no computer program would ever be able to beat even a 10-year- old. None of the computer specialists at the Massachusetts Institute of Technology cheered when the program won, because they\n knew it was going to happen. They lived in the world of hackers, a mere extension of the incredible computer environment.\u00a0\nThere was the Great Subway Hack, in which an M.I.T. student programmed a computer to figure out a route by which someone could ride the entire New York City subway system on a single token, and then a bunch of his fellow students went out and actually did it. And there was the incident when the security people in charge of M.I.T.'s Artificial Intelligence laboratory had to ask one of the hackers to crack a safe that had been acquired precisely to secure certain information from the hackers, who regarded any secret as an insult to their ingenuity.\nA remarkable spirit of fun and adventure prevailed at M.I.T.'s Artificial Intelligence lab in the 1950's, and Steven Levy has captured it alive in the opening section of his ''Hackers: Heroes of the Computer Revolution.'' He has introduced and made us understand those weird kids ''with owl- like glasses and underdeveloped pectorals who dazzled math teachers and flunked PE, who dreamed not of scoring on prom night, but of getting to the finals of the General Electric Science Fair competition.''\nNo wonder they evolved such a peculiar style of living, with nonstop 30-hour work sessions, and forays deep into the arcana of the Cambridge Chinese-restaurant system, and a colorful jargon full of odd, teddy-bearish words like ''winnitude,'' ''gronk,'' ''foo'' and ''milliblatt'' (the last coined to designate a new ''olfactory measure'' in honor of a particularly ''grungy'' hacker named Greenblatt). They were a unique band of adventurers. And if the spirit of ''Hackers'' is sometimes reminiscent of Tom Wolfe's ''Right Stuff,'' it is neither a coincidence nor a result of imitation. If Mr. Wolfe's test pilots and astronauts were pushing back the boundaries of outer space, then Mr. Levy's heroes are just as courageously exploring mindspace, an inner world where nobody had ever been before.\n\n\nWhy then does Mr. Levy's book begin to limp halfway through - to bog down in details that are somehow less and less exciting? Partly it has to do with the decline and death of what Mr. Levy calls ''the hacker ethic,'' which is what his book is really all about. The M.I.T. hackers believed that working with computers was an end in itself, that whatever they discovered was to be shared with anyone else who was interested, that there were no secrets and that no one was to earn a profit. Mr. Levy captures this spirit wonderfully, and its inevitable decline is a depressing thing to witness.\nBut if he tells a story of decline, he also tells one of diminishing significance. Mr. Levy divides his history into three sections, covering three phases of the computer revolution, and each seems more trivial than the one preceding it. The first, as will be evident by now, focuses on the groundwork that was done during the 1950's and 60's by the whiz kids at M.I.T. From there the scene shifts to California in the 1970's, where out of the counterculture's search for an anti-establishment technology there evolved the microcomputer business in general and Stephen Wozniak's highly profitable Apple II in particular.\nThis too may be an important chapter in the evolution of the home computer. What's more, the commercial success of the Apple computer may well have been a step in the decline of the hacker ethic, which certainly didn't include profiting hugely from one's discoveries. But somehow in telling this part of his story, Mr. Levy, who covers technology for Rolling Stone magazine, loses his perspective, for it's hard to tell whether he is celebrating the arrival of an inexpensive home computer or lamenting its astonishing profitability.\nFinally, when he arrives at the third phase of his story - the adventures of the ''game hackers,'' or those programming prodigies who put their genius into translating video games like ''Frogger'' from one computer system to another - he appears to have lost his way completely. For if, as the shape of Mr. Levy's book implies, the point of the entire computer revolution was to try to get a frog across a road and stream without being either run over by trucks or eaten by crocodiles, then it's not only unsurprising that the hacker ethic died; it isn't even sad.\nStill, it's a remarkable collection of characters that Mr. Levy has gathered in these pages - from the hacker who, purely for the challenge, programmed a computer to be ''an expensive typewriter'' (or what turned out to be one of the first word processors) to the demon telephone hacker, Captain Crunch, who discovered that when one blew the whistle that came in the breakfast cereal of that name, the result was the precise 2,600-cycle tone that the phone company used to shuffle long-distance traffic over its lines.\nAnd it's significant history that lies buried beneath all the gossip - the story of how the hackers nearly won out over the bureaucrats, who were forever symbolized in the hacker mind by the hulking giant of International Business Machines. The irony was that I.B.M. eventually turned out to be the biggest hacker of them all, when it came up with a microcomputer that nobody thought it was capable of producing.\nAnd how did I.B.M. accomplish this? That is probably an interesting story. But it doesn't fit into the thesis of Mr. Levy's book, which is simply that the hacker spirit grew corrupted by wealth and power. So instead of writing about what happened on the forefront of the microcomputer revolution, he ends up writing about frogs and mazes.\n\n","511":"OAKLAND, Calif. -- Last month I spoke at a gathering of African-American technology professionals. I'm a transactional lawyer at a tech company and my husband is an engineer, so the industry is at the center of our lives. We have careers that allow us to help create products and tools our grandparents would never have thought were possible and to provide the kind of life for our family that they couldn't have imagined. And it's important to us to ensure that other people of color have a chance to contribute to the field and reap its benefits. With all those things on my mind, I left the conference energized and inspired by the ways in which tech is changing the world and the possibilities it holds for our community.\nAt the same time, I'm terrified for what these advances mean for my two young children. The same technology that's the source of so much excitement in my career is being used in law enforcement in ways that could mean that in the coming years, my son, who is 7 now, is more likely to be profiled or arrested -- or worse -- for no reason other than his race and where we live. \u00a0\n  Of course I'm not alone in feeling that technology is both a gift and a curse. This tension exists for anyone who enjoys the real-time conversations on Twitter but loathes the trolls, loves Facebook but abhors fake news, or depends on the convenience Alexa offers but frets about violations of privacy.\n  Yet in my life, because of the way artificial intelligence and machine learning are being increasingly used by law enforcement -- the technology is seemingly growing up alongside my kids -- it's especially acute.\n  Unjust racial profiling and resulting racial disparities in the criminal justice system certainly don't depend on artificial intelligence. But when you add it -- as many law enforcement agencies across the country, including those in major cities like Miami, Los Angeles, Philadelphia, Atlanta and New York, have over the past couple of years -- things get even scarier for black families.\n  This is especially frightening when combined with the fact that the current administration has already begun to reverse Obama-era criminal justice reform policies that were meant to make the system more just.\n  A.I. works by taking large volumes of information and distilling it down to simple concepts, categories and rules and then predicting future responses and outcomes. This is a function of the beliefs, assumptions and capabilities of the people who do the coding. A.I. learns by repetition and association, and all of that is based on the information we -- humans who hold all the racial and often, specifically, anti-black biases of our society -- feed it.\n  Just think of how Google's facial recognition programs labeled black people in photos ''gorillas.'' Or how Microsoft's Tay, a bot designed to engage in Twitter conversations, devolved into a racial-epithet-tweeting machine within 24 hours.\n  These downsides of A.I. are no secret. Despite this, state and local law enforcement agencies have begun to use predictive policing applications fueled by A.I. like HunchLab, which combines historical crime data, moon phases, location, census data and even professional sports team schedules to predict when and where crime will occur and even who's likely to commit or be a victim of certain crimes.\n  The problem with historical crime data is that it's based upon policing practices that already disproportionately hone in on blacks, Latinos, and those who live in low-income areas.\n  If the police have discriminated in the past, predictive technology reinforces and perpetuates the problem, sending more officers after people who we know are already targeted and unfairly treated, given recent evidence like the Justice Department's reports on Ferguson, Mo., and Baltimore, and the findings of the San Francisco Blue Ribbon Panel on Transparency Accountability and Fairness in Law Enforcement.\n  It's no wonder criminologists have raised red flags about the self-fulfilling nature of using historical crime data.\n  This hits close to home. An October 2016 study by the Human Rights Data Analysis Group concluded that if the Oakland Police Department used its 2010 record of drug-crimes information as the basis of an algorithm to guide policing, the department ''would have dispatched officers almost exclusively to lower-income, minority neighborhoods,'' despite the fact that public-health-based estimates suggest that drug use is much more widespread, taking place in many other parts of the city where my family and I live.\n  Those ''lower-income, minority neighborhoods'' contain the barbershop where I take my son for his monthly haircut and our favorite hoagie shop. Would I let him run ahead of me if I knew that simply setting foot on those sidewalks would make him more likely to be seen as a criminal in the eyes of the law?\n  The risks are even more acute (and unavoidable) for those who can afford to live only in the neighborhoods that A.I. would most likely lead officers to focus on.\n  There's yet another opportunity for racial bias to infuse the process when risk-assessment algorithms created by A.I. and machine learning are used to help to sentence criminals, as they already are in courts around the country.\n  Without a commitment to ensure that the data being used to fuel A.I. doesn't replicate historical racism, biases will be built into the foundation of many ''intelligent'' systems shaping how we live. It's not that I want this technology to be rejected. There are ways to make A.I. work. But before it is used in law enforcement, it must be thoroughly tested and proven not to disproportionately harm communities of color.\n  Until then my excitement about advances in tech will always be cautious. Innovation is at the core of the careers that allow me and my husband to provide a good life for our family. The same innovation, if not used properly, could take it all away.\n  Follow The New York Times Opinion section on Facebook and Twitter (@NYTopinion), and sign up for the  Opinion Today newsletter. \n\n\n\n","512":"Business owners are human beings, and as human beings we're all far from perfect. This becomes evident when we hire. We try to make our hiring decisions as objectively and independently as possible. But we all sometimes let our own biases seep into our decision making. I know I do.        \u00a0\nIs it possible to push all these influences aside so that business managers can just choose the best candidate for the job using just logic and facts? Maybe. According to this recent report in the New York Times, two start-ups have created applications that, they claim, take the bias out of hiring. How? By using artificial intelligence and special algorithms which will essentially match the best candidate for the job without letting these biases get in the way.        \nKate Glazebrook, who owns a company called Applied, told the Times her software is the first hiring platform that's \"designed entirely around the psychology of decision-making that helps firms make recruitment decisions smart (more predictive of performance), fair (less biased) and easy.\" Frida Polli's company, Pymetrics, gamifies both neuroscience and artificial intelligence so that candidates are better matched to their roles based on algorithms that take gender, race, and ethnic backgrounds out of the equation.        \nThe entrepreneurs have designed processes that takes both a candidate's r\u00e9sum\u00e9 and responses to a bunch of neutral and anonymous questions and turns it all into a non-biased evaluation of the person's worthiness for the job. \"We anonymize applications, chunk them up into batches to allow for better comparative assessment, randomize candidates to avoid ordering effects, and allow multiple evaluators to contribute their scores independently to harness the wisdom of the crowd,\" Glazebrook told the  Times.        \nThe algorithms created by both companies are already being used by large firms like Accenture and Unilever to reduce their, gender, ethnic and socioeconomic bias and better match candidates to a job\nSure, technology like this could be ignoring things like sincerity, trustworthiness and that gut feeling about a potential candidate that a machine could never evaluate. But, considering all the wrong hiring decisions I've made in the past - let alone my many biases that negatively affect my judgment - it's very enticing to just let a software program make those decisions for me in the future. I'm sure other business owners would agree.        \n","513":"''WISCONSIN appears to be in the driver's seat en route to a win, as it leads 51-10 after the third quarter. Wisconsin added to its lead when Russell Wilson found Jacob Pedersen for an eight-yard touchdown to make the score 44-3 ... . ''\nThose words began a news brief written within 60 seconds of the end of the third quarter of the Wisconsin-U.N.L.V. football game earlier this month. They may not seem like much -- but they were written by a computer.\nThe clever code is the handiwork of Narrative Science, a start-up in Evanston, Ill., that offers proof of the progress of artificial intelligence -- the ability of computers to mimic human reasoning.\u00a0\nThe company's software takes data, like that from sports statistics, company financial reports and housing starts and sales, and turns it into articles. For years, programmers have experimented with software that wrote such articles, typically for sports events, but these efforts had a formulaic, fill-in-the-blank style. They read as if a machine wrote them.\nBut Narrative Science is based on more than a decade of research, led by two of the company's founders, Kris Hammond and Larry Birnbaum, co-directors of the Intelligent Information Laboratory at Northwestern University, which holds a stake in the company. And the articles produced by Narrative Science are different.\n''I thought it was magic,'' says Roger Lee, a general partner of Battery Ventures, which led a $6 million investment in the company earlier this year. ''It's as if a human wrote it.''\nExperts in artificial intelligence and language are also impressed, if less enthralled. Oren Etzioni, a computer scientist at the University of Washington, says, ''The quality of the narrative produced was quite good,'' as if written by a human, if not an accomplished wordsmith. Narrative Science, Mr. Etzioni says, points to a larger trend in computing of ''the increasing sophistication in automatic language understanding and, now, language generation.''\nThe innovative work at Narrative Science raises the broader issue of whether such applications of artificial intelligence will mainly assist human workers or replace them. Technology is already undermining the economics of traditional journalism. Online advertising, while on the rise, has not offset the decline in print advertising. But will ''robot journalists'' replace flesh-and-blood journalists in newsrooms?\nThe leaders of Narrative Science emphasized that their technology would be primarily a low-cost tool for publications to expand and enrich coverage when editorial budgets are under pressure. The company, founded last year, has 20 customers so far. Several are still experimenting with the technology, and Stuart Frankel, the chief executive of Narrative Science, wouldn't name them. They include newspaper chains seeking to offer automated summary articles for more extensive coverage of local youth sports and to generate articles about the quarterly financial results of local public companies.\n''Mostly, we're doing things that are not being done otherwise,'' Mr. Frankel says.\nThe Narrative Science customers that are willing to talk do fit that model. The Big Ten Network, a joint venture of the Big Ten Conference and Fox Networks, began using the technology in the spring of 2010 for short recaps of baseball and softball games. They were posted on the network's Web site within a minute or two of the end of each game; box scores and play-by-play data were used to generate the brief articles. (Previously, the network relied on online summaries provided by university sports offices.)\nAs the spring sports season progressed, the computer-generated articles improved, helped by suggestions from editors on the network's staff, says Michael Calderon, vice president for digital and interactive media at the Big Ten Network.\nThe Narrative Science software can make inferences based on the historical data it collects and the sequence and outcomes of past games. To generate story ''angles,'' explains Mr. Hammond of Narrative Science, the software learns concepts for articles like ''individual effort,'' ''team effort,'' ''come from behind,'' ''back and forth,'' ''season high,'' ''player's streak'' and ''rankings for team.'' Then the software decides what element is most important for that game, and it becomes the lead of the article, he said. The data also determines vocabulary selection. A lopsided score may well be termed a ''rout'' rather than a ''win.''\n''Composition is the key concept,'' Mr. Hammond says. ''This is not just taking data and spilling it over into text.''\nLast fall, the Big Ten Network began using Narrative Science for updates of football and basketball games. Those reports helped drive a surge in referrals to the Web site from Google's search algorithm, which highly ranks new content on popular subjects, Mr. Calderon says. The network's Web traffic for football games last season was 40 percent higher than in 2009.\nHanley Wood, a trade publisher for the construction industry, began using the program in August to provide monthly reports on more than 350 local housing markets, posted on its site, builderonline.com. The company had long collected the data, but hiring people to write trend articles would have been too costly, says Andrew Reid, president of Hanley Wood's digital media and market intelligence unit.\nMr. Reid says Hanley Wood worked with Narrative Science for months to fine-tune the software for construction. A former executive at Thomson Reuters, he says he was struck by the high quality of the articles.\n''They got over a big linguistic hurdle,'' he observes. ''The stories are not duplicates by any means.''\nHe was also impressed by the cost. Hanley Wood pays Narrative Science less than $10 for each article of about 500 words -- and the price will very likely decline over time. Even at $10, the cost is far less, by industry estimates, than the average cost per article of local online news ventures like AOL's Patch or answer sites, like those run by Demand Media.\nNARRATIVE SCIENCE'S ambitions include moving further up the ladder of quality. Both Mr. Birnbaum and Mr. Hammond are professors of journalism as well as computer science. The company itself is an outgrowth of collaboration between the two schools.\n''This kind of technology can deepen journalism,'' says John Lavine, dean of the Medill School of Journalism at Northwestern.\nMr. Hammond says the combination of advances in its writing engine and data mining can open new horizons for computer journalism, exploring ''correlations that you did not expect'' -- conceptually similar to ''Freakonomics,'' by two humans, the economist Steven D. Levitt and the author Stephen J. Dubner.\nMr. Hammond cited a media maven's prediction that a computer program might win a Pulitzer Prize in journalism in 20 years -- and he begged to differ.\n''In five years,'' he says, ''a computer program will win a Pulitzer Prize -- and I'll be damned if it's not our technology.''\nShould it happen, the prize, of course, would not be awarded to abstract code, but to its human creators.\n","514":"MENLO PARK, Calif. -- The robotics pioneer Rodney Brooks often begins speeches by reaching into his pocket, fiddling with some loose change, finding a quarter, pulling it out and twirling it in his fingers.\nThe task requires hardly any thought. But as Dr. Brooks points out, training a robot to do it is a vastly harder problem for artificial intelligence researchers than I.B.M.'s celebrated victory on ''Jeopardy!'' this year with a robot named Watson.\nAlthough robots have made great strides in manufacturing, where tasks are repetitive, they are still no match for humans, who can grasp things and move about effortlessly in the physical world.\nDesigning a robot to mimic the basic capabilities of motion and perception would be revolutionary, researchers say, with applications stretching from care for the elderly to returning overseas manufacturing operations to the United States (albeit with fewer workers).\u00a0\nYet the challenges remain immense, far higher than artificial intelligence hurdles like speaking and hearing.\n''All these problems where you want to duplicate something biology does, such as perception, touch, planning or grasping, turn out to be hard in fundamental ways,'' said Gary Bradski, a vision specialist at Willow Garage, a robot development company based here in Silicon Valley.\n''It's always surprising, because humans can do so much effortlessly.''\nNow the Defense Advanced Research Projects Agency, or Darpa, the Pentagon office that helped jump-start the first generation of artificial intelligence research in the 1960s, is underwriting three competing efforts to develop robotic arms and hands one-tenth as expensive as today's systems, which often cost $100,000 or more.\nLast month President Obama traveled to Carnegie Mellon University in Pittsburgh to unveil a $500 million effort to create advanced robotic technologies needed to help bring manufacturing back to the United States. But lower-cost computer-controlled mechanical arms and hands are only the first step.\nThere is still significant debate about how even to begin to design a machine that might be flexible enough to do many of the things humans do: fold laundry, cook or wash dishes. That will require a breakthrough in software that mimics perception.\nToday's robots can often do one such task in limited circumstances, but researchers describe their skills as ''brittle.'' They fail if the tiniest change is introduced. Moreover, they must be reprogrammed in a cumbersome fashion to do something else.\nMany robotics researchers are pursuing a bottom-up approach, hoping that by training robots on one task at a time, they can build a library of tasks that will ultimately make it possible for robots to begin to mimic humans.\nOthers are skeptical, saying that truly useful machines await an artificial intelligence breakthrough that yields vastly more flexible perception.\nThe limits of today's most sophisticated robots can be seen in a towel-folding demonstration that a group of students at the University of California, Berkeley, posted on the Internet last year: In spooky, anthropomorphic fashion, a robot deftly folds a series of towels, eyeing the corners, smoothing out wrinkles and neatly stacking them in a pile.\nIt is only when the viewer learns that the video is shown at 50 times normal speed that the meager extent of the robot's capabilities becomes apparent. (The students acknowledged this spring that they were only now beginning to tackle the further challenges of folding shirts and socks.)\nEven the most ambitious and expensive robot arm research has not yet yielded impressive results.\nIn February, for example, Robonaut 2, a dexterous robot developed in a partnership between NASA and General Motors, was carried aboard a space shuttle mission to be installed on the International Space Station. The developers acknowledged that the software required by the system, which is humanoid-shaped from the torso up, was unfinished and that the robot was sent up then only because a rare launching window was available.\n''We're in a funny chicken-and-egg situation,'' Dr. Brooks said. ''No one really knows what sensors or perceptual algorithms to use because we don't have a working hand, and because we don't have a grasping strategy nobody can figure out what kind of hand to design.''\nDr. Brooks is also tackling the problem: In 2008 he founded Heartland Robotics, a Boston-based company that is intent on building a generation of low-cost robots.\nAnd the three competing efforts to develop robotic arms and hands with Darpa financing -- at SRI International, Sandia National Laboratories and iRobot -- offer some reasons for optimism.\nRecently at an SRI laboratory here, two Stanford University graduate students, John Ulmen and Dan Aukes, put the finishing touches on a significant step toward human capabilities: a four-finger hand that will grasp with a human's precise sense of touch.\nEach three-jointed finger is made in a single manufacturing step by a three-dimensional printer and is then covered with ''skin'' derived from the same material used to make the touch-sensitive displays on smartphones.\n''Part of what we're riding on is there has been a very strong push for tactile displays because of smartphones,'' said Pablo Garcia, an SRI robot designer who is leading the design of the project, along with Robert Bolles, an artificial intelligence researcher.\n''We've taken advantage of these technologies,'' Mr. Garcia went on, ''and we're banking on the fact they will continue to evolve and be made even cheaper.''\nStill lacking is a generation of software that is powerful and flexible enough to do tasks that humans do effortlessly. That will require a breakthrough in machines' perception.\n''I would say this is more difficult than what the Watson machine had to do,'' said Gill Pratt, the computer scientist who is the program manager in charge of Darpa's Autonomous Robot Manipulation project, called ARM.\n''The world is composed of continuous objects that have various shapes'' that can obscure one another, he said. ''A perception system needs to figure this out, and it needs the common sense of a child to do that.''\nAt Willow Garage, Dr. Bradski and a group of artificial intelligence researchers and roboticists have focused on ''hackathons,'' in which the company's PR2 robot has been programmed to do tasks like fetching beer from a refrigerator, playing pool and packing groceries.\nIn May, with support from the White House Office of Science and Technology Policy, Dr. Bradski helped organize the first Solutions in Perception Challenge. A prize of $10,000 is offered for the first team to design a robot that is able to recognize 100 items commonly found on the shelves of supermarkets and drugstores. Part of the prize will be given to the first team whose robot can recognize 80 percent of the items.\nAt the contest, held during a robotics conference in Shanghai, none of the contestants reached the 80 percent goal. The team that did best was the laundry-folding team from Berkeley, which has named its robot Brett, for Berkeley Robot for the Elimination of Tedious Tasks.\nBrett was able to recognize 68 percent of a smaller group of 50 objects. And the team has made progress in its quest to build a machine to do the laundry; it recently posted a new video showing how much it has sped up the robot.\n''Our end goal right now is to do an entire laundry cycle,'' said Pieter Abbeel, a Berkeley computer scientist who leads the group, ''from dirty laundry in a basket to everything stacked away after it's been washed and dried.''\n","515":"Harold Cohen, an abstract painter who developed Aaron, one of the first and eventually one of the most complex computer software programs for generating works of art, died on April 27 at his home in Encinitas, Calif. He was 87.\nThe cause was congestive heart failure, his son, Paul, said. \u00a0\n  Mr. Cohen was a painter growing weary with the traditional practice of art in the late 1960s when he taught himself, out of curiosity, how to program a computer.\n  Applying his newfound expertise, he invented a computer-programmed drawing machine, whose works he exhibited at the Los Angeles County Museum of Art in 1972 in a show called ''Three Behaviors for the Partitioning of Space.''\n  At the invitation of Edward Feigenbaum, one of the pioneers of artificial intelligence, he then spent two years at Stanford University's Artificial Intelligence Laboratory to continue his programming work. Aaron, the name chosen for reasons unknown, was the result.\n  Mr. Cohen began with the question ''What are the minimum conditions under which a set of marks functions as an image?'' Then, after studying how children draw, examining Native American petroglyphs and interviewing artists, he developed algorithms that allowed a computer to draw lines with the irregularity of freehand drawing.\n  As Aaron developed, it learned to make choices about open and closed shapes and foreground and background, and to recognize when an artwork had reached completion.\n  ''It occurred to me,'' Mr. Cohen told an audience at the Tate Gallery in London in 2004, ''that if I could write a program to do some of the things human beings do when they make representations, then I might possibly learn more about the nature of representation than I ever had done by painting.''\n  Harold Cohen was born on May 1, 1928, in London, where his parents, Victor and Leah, ran a successful variety store. He was expected to enter the family business but instead enrolled in the Slade School of Fine Art in London, earning a diploma in 1951 and then spending a year in Rome on a fellowship.\n  He taught at the Camberwell School of Arts and Crafts in London and the University of Nottingham, and in the early 1960s began exhibiting his striped abstract paintings in London and New York. He represented Britain at Documenta in Kassel, West Germany, in 1964 and at the Venice Biennale in 1966.\n  In 1968 the University of California, San Diego, invited him to spend a year as a visiting lecturer. He never left. He joined the art department and eventually became chairman. He retired in 1994. He was also the director of the school's Center for Research in Computing and the Arts.\n  Mr. Cohen refined Aaron throughout his life. In the 1980s he pushed it in the direction of realism, generating rocks, then plants, then people. In the 1990s he added color. In the 2000s he returned to abstraction. The relationship, he often said, was mutual, reflected in the title of a 2011 exhibition at the university: ''Collaborations With My Other Self.''\n  Aaron placed Mr. Cohen in a continuing dialogue with the art world, since it posed difficult questions about the meaning of creativity and the nature of art.\n  ''I think the crux of creativity is self-modification,'' he told the Canadian newspaper The Globe and Mail in 2005. ''If we measure creativity in terms of the output, my program generates 50 or 60 brilliant images every night when I leave it running. So it really is a world-class colorist at this point. If you're measuring creativity in terms of the quality of the output, then Aaron is clearly very creative indeed.''\n  On the other hand, he said, Aaron was not creative in the sense that it could not reformulate its own ''mental model of the world,'' a limitation he tried to nibble away at in his later years. ''What the program is moving toward is to increasingly take on responsibility for the building of the images,'' he said.\n  Mr. Cohen exhibited Aaron at museums, galleries and science centers around the world, including the Tate Gallery, the Stedelijk Museum in Amsterdam and the San Francisco Museum of Art.\n  In 1983, the Brooklyn Museum presented ''Harold Cohen: Computer Drawings,'' with four of Mr. Cohen's drawing machines turning out works on paper that the public could buy for $10 each.\n  As the American representative at the world's fair in Tsukuba, Japan, in 1985, he sent Aaron to create more than 7,000 different drawings while he stayed home in California.\n  As the program became more complex, it generated puzzle-like underpaintings on canvas that he set about completing, dragging out his tubes of paint for the first time in years.\n  Besides his son, Mr. Cohen is survived by his partner, the Japanese writer Hiromi Ito; his wife, Becky Cohen, from whom he is separated; a brother, Bernard; four daughters, Jenny Foord, Kanoko Nishi-Smith, Sara Nishi and Zana Itoh Cohen; and seven grandchildren.\n  ''Aaron could go on producing work indefinitely,'' Mr. Cohen said in an interview in 2011 with the organizers of his exhibition in San Diego. ''The problem has always been that it would go on being the same work -- not the same individual image, but the same formulation, which, by the way, is what most human artists do anyway.''\n  He added: ''To be realistic, I rather suspect Aaron will end when I end. Why would anybody want to take up my other half?''\n\n\n\n","516":"TO THE EDITOR:\nRe ''Study to Examine Effects of Artificial Intelligence'' (Dec. 16): It isn't the front end of technology that worries this reader, it's the replacement of life's mundane day-to-day laundry list that undermines an ''experience'' of primordial context.\u00a0\nThis Stanford study is all well and good. But whose finger is on technology's launch button is much less telling than the trail of cultural remnants in its wake.\nReplacing human labor has always sounded good, and yet our most egregious transgressions of character are bred in the absence of it. Rather than thinking about the smartest machine and how it might overwhelm us under bad management, consider the average commercial voice mail process. Not only did Aunt Helen lose her livelihood to a bit of computer code, but every exchange made with the machine accrues externalities of red ink. A bit of money removed a bushel of value.\nDigital transformation has already, in just a few decades, displaced human experience from the biological connections that gifted our nest on this planet. Although artificial intelligence will undoubtedly assist our escape to another one, we are reasonable to admire the irony of that sequence.\nArt James\nPort Townsend, Wash.\nPsychology\n","518":"On a bright fall day last year off the coast of Southern California, an Air Force B-1 bomber launched an experimental missile that may herald the future of warfare.\nInitially, pilots aboard the plane directed the missile, but halfway to its destination, it severed communication with its operators. Alone, without human oversight, the missile decided which of three ships to attack, dropping to just above the sea surface and striking a 260-foot unmanned freighter. \n  Warfare is increasingly guided by software. Today, armed drones can be operated by remote pilots peering into video screens thousands of miles from the battlefield. But now, some scientists say, arms makers have crossed into troubling territory: They are developing weapons that rely on artificial intelligence, not human instruction, to decide what to target and whom to kill.\n  As these weapons become smarter and nimbler, critics fear they will become increasingly difficult for humans to control -- or to defend against. And while pinpoint accuracy could save civilian lives, critics fear weapons without human oversight could make war more likely, as easy as flipping a switch.\u00a0\n  Britain, Israel and Norway are already deploying missiles and drones that carry out attacks against enemy radar, tanks or ships without direct human control. After launch, so-called autonomous weapons rely on artificial intelligence and sensors to select targets and to initiate an attack.\n  Britain's ''fire and forget'' Brimstone missiles, for example, can distinguish among tanks and cars and buses without human assistance, and can hunt targets in a predesignated region without oversight. The Brimstones also communicate with one another, sharing their targets.\n  Armaments with even more advanced self-governance are on the drawing board, although the details usually are kept secret. ''An autonomous weapons arms race is already taking place,'' said Steve Omohundro, a physicist and artificial intelligence specialist at Self-Aware Systems, a research center in Palo Alto, Calif. ''They can respond faster, more efficiently and less predictably.''\n  Concerned by the prospect of a robotics arms race, representatives from dozens of nations will meet on Thursday in Geneva to consider whether development of these weapons should be restricted by the Convention on Certain Conventional Weapons. Christof Heyns, the United Nations special rapporteur on extrajudicial, summary or arbitrary executions, last year called for a moratorium on the development of these weapons.\n  The Pentagon has issued a directive requiring high-level authorization for the development of weapons capable of killing without human oversight. But fast-moving technology has already made the directive obsolete, some scientists say.\n  ''Our concern is with how the targets are determined, and more importantly, who determines them,'' said Peter Asaro, a co-founder and vice chairman of the International Committee for Robot Arms Control, a group of scientists that advocates restrictions on the use of military robots. ''Are these human-designated targets? Or are these systems automatically deciding what is a target?''\n  Weapons manufacturers in the United States were the first to develop advanced autonomous weapons. An early version of the Tomahawk cruise missile had the ability to hunt for Soviet ships over the horizon without direct human control. It was withdrawn in the early 1990s after a nuclear arms treaty with Russia.\n  Back in 1988, the Navy test-fired a Harpoon antiship missile that employed an early form of self-guidance. The missile mistook an Indian freighter that had strayed onto the test range for its target. The Harpoon, which did not have a warhead, hit the bridge of the freighter, killing a crew member.\n  Despite the accident, the Harpoon became a mainstay of naval armaments and remains in wide use.\n  In recent years, artificial intelligence has begun to supplant human decision-making in a variety of fields, such as high-speed stock trading and medical diagnostics, and even in self-driving cars. But technological advances in three particular areas have made self-governing weapons a real possibility.\n  New types of radar, laser and infrared sensors are helping missiles and drones better calculate their position and orientation. ''Machine vision,'' resembling that of humans, identifies patterns in images and helps weapons distinguish important targets. This nuanced sensory information can be quickly interpreted by sophisticated artificial intelligence systems, enabling a missile or drone to carry out its own analysis in flight. And computer hardware hosting it all has become relatively inexpensive -- and expendable.\n  The missile tested off the coast of California, the Long Range Anti-Ship Missile, is under development by Lockheed Martin for the Air Force and Navy. It is intended to fly for hundreds of miles, maneuvering on its own to avoid radar, and out of radio contact with human controllers.\n  In a directive published in 2012, the Pentagon drew a line between semiautonomous weapons, whose targets are chosen by a human operator, and fully autonomous weapons that can hunt and engage targets without intervention.\n  Weapons of the future, the directive said, must be ''designed to allow commanders and operators to exercise appropriate levels of human judgment over the use of force.''\n  The Pentagon nonetheless argues that the new antiship missile is only semiautonomous and that humans are sufficiently represented in its targeting and killing decisions. But officials at the Defense Advanced Research Projects Agency, which initially developed the missile, and Lockheed declined to comment on how the weapon decides on targets, saying the information is classified.\n  ''It will be operating autonomously when it searches for the enemy fleet,'' said Mark A. Gubrud, a physicist and a member of the International Committee for Robot Arms Control, and an early critic of so-called smart weapons. ''This is pretty sophisticated stuff that I would call artificial intelligence outside human control.''\n  Paul Scharre, a weapons specialist now at the Center for a New American Security who led the working group that wrote the Pentagon directive, said, ''It's valid to ask if this crosses the line.''\n  Some arms-control specialists say that requiring only ''appropriate'' human control of these weapons is too vague, speeding the development of new targeting systems that automate killing.\n  Mr. Heyns, of the United Nations, said that nations with advanced weapons should agree to limit their weapons systems to those with ''meaningful'' human control over the selection and attack of targets. ''It must be similar to the role a commander has over his troops,'' Mr. Heyns said.\n  Systems that permit humans to override the computer's decisions may not meet that criterion, he added. Weapons that make their own decisions move so quickly that human overseers soon may not be able to keep up. Yet many of them are explicitly designed to permit human operators to step away from controls. Israel's antiradar missile, the Harpy, loiters in the sky until an enemy radar is turned on. It then attacks and destroys the radar installation on its own.\n  Norway plans to equip its fleet of advanced jet fighters with the Joint Strike Missile, which can hunt, recognize and detect a target without human intervention. Opponents have called it a ''killer robot.''\n  Military analysts like Mr. Scharre argue that smarter weapons should be embraced because they may result in fewer mass killings and civilian casualties. Smart weapons, they say, do not commit war crimes.\n  On Sept. 16, 2011, for example, British warplanes fired two dozen Brimstone missiles at a group of Libyan tanks that were shelling civilians. Eight or more of the tanks were destroyed simultaneously, according to a military spokesman, saving the lives of many civilians.\n  It would have been difficult for human operators to coordinate the swarm of missiles with similar precision.\n  ''Better, smarter weapons are good if they reduce civilian casualties or indiscriminate killing,'' Mr. Scharre said.\n\n\n\n","519":"WHEN the Ballroom, a staid restaurant-cabaret in Chelsea, recently decided to present comedy as well as music on a regular basis, a brilliant, virtually unknown 18-member troupe of satirists calling themselves Artificial Intelligence stole the show at the club's opening-night marathon. Impersonating a film crew from the fictitious Beacon network and swarming through the club with video equipment, the troupe turned the Ballroom into a make-believe television studio, where it ''filmed'' (though no film was actually rolling) a 1967 variety show entitled ''Vicki's Valentine Thing.''\nThe 65-minute show, which the Ballroom is presenting on Tuesdays through Saturdays until Feb. 14, is the hilarious comedy-theater equivalent of the sorely missed ''SCTV'' series and of Rob Reiner's rock documentary spoof, ''This Is Spinal Tap.'' It distills with deadpan accuracy the essence of the tacky thematic variety specials that proliferated on prime time during the 60's. Even more, it captures the mood of television -and of American popular culture - at the precise moment when flowery Mod fashions and slogans of peace and love were just beginning to invade the mass media and to commingle with entrenched show-business glitz.\u00a0\nAt the center of a show in which every character, right down to the last technician, has a fictitious personal history is its star, Vicki Oberjeune (Nancy Cassaro), a tipsy, pill-popping singer-actress in her late 30's who is attended by her sixth husband, Mickey Styles (Jack Fris); her two children, Gina Anthony (Monica Horan) and Nicholas Frank (Eli Ganias), and a backup group called Tracey and the Truth, led by Gina's boyfriend. The audience, which is encouraged to participate with an applause sign, gets to glimpse two worlds - the heartwarming scenes of familial togetherness enacted before the camera and the frantic, behind-the-scenes chaos in which everyone's personal life, and indeed the show itself, is on the verge of collapse. But the show does go on. And among other tarnished pop nuggets, it resurrects such vintage musical fare as ''A Man and a Woman,'' ''The Look of Love,'' and ''The Rain, the Park, and Other Things,'' all performed with period 60's television-style arrangements. In one uproarious dance number, a peace symbol is hoisted to the top of a maypole as the cast whirls giddily to the strains of ''Love Potion No. 9.'' A more solemn note is struck when the representative of a historical society arrives to display one of the world's oldest valentines onto which the drunken star accidentally empties her drink.\u00a0Young and Brash\nLike the members of the ''SCTV'' troupe and the original cast of ''Saturday Night Live'' in the mid-70's, the members of Artificial Intelligence are young and brash. Ms. Cassaro, who conceived the show, is only 27 years old. She and most of her cohorts were in early grade school when ''flower power'' swept the media. Today, along with several other members of the troupe, she works in advertising, as a talent coordinator for Batten Barton Durstine & Osborn.\n''Nowadays, I don't watch a lot of TV, but when I was younger I used to live in front of the set,'' she recalled the other day. ''To research this show, I studied a lot of old variety programs at the Museum of Broadcasting. It's amazing to realize that people actually watched programs like that and thought it was real. The character of Vicki is a hybrid of Judy Garland and Peggy Lee with a little Connie Stevens. I'm not trying to denigrate the character. I really respect that image and what it meant to the American public 20 years ago.''\nMs. Cassaro, who grew up in Massapequa, L.I., and majored in drama at Hofstra University, was particularly inspired by a class in theater styles where she learned about ''environmental theater,'' and the work of Julian Beck and Joseph Chaikin. After moving to New York City four years ago, she met the core of the group that became Artificial Intelligence in classes taught by the director David Kaplan.\n''David's philosophy is that there's nothing you can't do - the wilder you are the better,'' Ms. Cassaro said. ''We were tired of going to auditions and so we formed a group and began performing our own material in East Village clubs. Our first major piece, 'Tony 'n' Tina's Wedding,' was a full-scale Italian wedding that involved 23 actors and 80 invited guests. The piece was performed twice, the second time in an actual church, with the reception in an American Legion hall. The details extended to sending out engraved invitations and having special matches made.''\u00a0Cost Just $1,500 to Produce\n''Vicki's Valentine Thing,'' which cost only $1,500 to produce, evolved out of a 1965 Christmas show that the troupe created for the same character. Its director, Larry Pellegrini, an aspiring writer of plays and musicals, works as the booking agent for Jason's Park Royal, an Upper West Side cabaret where the troupe once appeared.\n''Nancy called me last August and asked if I would direct the Christmas special,'' Mr. Pellegrini said the other day. ''She put together the cast, and we met at my apartment, where we rehearsed the whole thing. Everybody came in character and stayed in character. Even though Vicki Oberjeune hates the director, it was actually easier dealing with everybody as their character than as themselves.''\nArtificial Intelligence has a growing list of possible future projects. One is a ''Reincarnival,'' a holistic health fair with booths, bands and performance groups, which they would like to do at the Central Park Band Shell during the summer. Another is a 24-hour telethon. A third, which may be presented in the spring, is a company picnic followed by a wake the day after one of the revelers dies of a heart attack.\n''In everything we do, whether it's a wedding, a TV show or a wake, we invite the audience to re-examine the ritual,'' Ms. Cassaro said. ''Because they know it's theater, they get a different perspective on it. We never want an audience to feel threatened. We create a world first and then invite other people to come and play in it.''\nThe Ballroom is at 253 West 28th Street. Show time for all performances is at 9 P.M. There is a $15 cover charge and a two-drink minimum. Information and reservations: 244-3005.\n","520":"In Phoenix, Ariz., cars are self-navigating the streets. In many homes, people are barking commands at tiny machines, with the machines responding. On our smartphones, apps can now recognize faces in photos and translate from one language to another.\nArtificial\u00a0intelligence is here -- and it's bringing new possibilities, while also raising questions. Do these gadgets and services really behave as advertised? How will they evolve in the years ahead? How quickly will they overhaul the way we live and change the way we do business?\nThe Times is exploring these matters this week at our annual New Work Summit, featuring technology executives, A.I. researchers, investors and others. The chief executive of Waymo, the self-driving car business spun out of Google; the director of an A.I. laboratory at Stanford University; and the head of Amazon's consumer business are among the speakers.\nWe'll report back on some key moments from the conference. In the meantime, here's a rundown of some of our recent A.I. stories. -- Cade Metz\nA.I. has become a political campaign issue\nAs A.I. technology barrels ahead in Silicon Valley, it's also starting to pick up steam as a political issue in Washington.\nOver the weekend, I wrote about Andrew Yang, a former tech executive who has decided to run for president in 2020 as a Democrat on a \"beware the robots\" platform. He thinks that with innovations like self-driving cars and grocery stores without cashiers just around the corner, we're about to move into a frightening new era of mass unemployment and social unrest.\nSo he's proposing a universal basic income plan called the \"Freedom Dividend,\" which would give every American adult $1,000 a month to guarantee them a minimum standard of living while they retrain themselves for new kinds of work.\nMr. Yang's campaign is a long shot, and there are significant hurdles to making universal basic income politically feasible. But the conversation about automation's social and economic consequences is long overdue. Even if he doesn't win the election, Mr. Yang may have hit on the next big political wedge issue. -- Kevin Roose\nWaymo faces hurdles in self-driving cars\nWaymo just settled its closely watched lawsuit with Uberover stolen trade secrets. With the settlement, Waymo received a stake in Uber worth $245 million and Uber agreed that no Waymo technology would be used in its own autonomous vehicles.\nBut Waymo now faces a much bigger fight in autonomous vehicles, which we chronicle here. Uber is just one of many companies now competing with Waymo on driverless cars, and much of this competition is driven by ex-Waymo engineers. Waymo's chief executive, John Krafcik, is set to take the stage at the New Work Summit on Monday night to discuss the company's future, including the ride-hailing service it says will soon launch in Arizona. -- Cade Metz\nArtificial\u00a0intelligence may be biased\nIn modern artificial\u00a0intelligence, data rules. A.I. software is only as smart as the data used to train it, as Steve Lohr recently wrote, and that means that some of the biases in the real world can seep into A.I.\nIf there are many more white men than black women in the system, for example, it will be worse at identifying the black women. That appears to be the case with some popular commercial facial recognition software.\nJoy Buolamwini, a researcher at the M.I.T. Media Lab, found that the software can now tell if a white man in a photograph is male or female 99 percent of the time. But for darker skinned women, it is wrong nearly 35 percent of the time. -- Joseph PlambeckRelated Articles\n\n","521":"Rebecca Emily Martin and Jared Paul Lander are to be married Oct. 22 by Rabbi Joshua Gruenberg at Congregation Beth El in Yardley, Pa.\u00a0\nThe bride, 36, is a postdoctoral researcher in developmental cognitive neuroscience at New York University. She graduated from N.Y.U., received two master's degrees, one in teaching from University of California, Santa Cruz, and one in mind, brain and education from Harvard, and a Ph.D. in psychology from Columbia. \n  She is the daughter of Kerry M. Martin and George M. Martin III of San Jose, Calif. The bride's father is a principal at OptoElectronix in San Jose. Her mother is a transition counselor in San Jose for Silicon Valley Adult Education.\n  The groom, 35, is the chief data scientist in New York for Lander Analytics, a data science and artificial intelligence consulting, training and events business he and his father founded. He is also an adjunct professor at Columbia, where he teaches statistics and artificial intelligence, and the author of ''R for Everyone'' (Addison-Wesley Professional 2013). He graduated from Muhlenberg College and received a master's in statistics from Columbia.\n  He is the son of Gail M. Lander and Howard Lander of Newtown, Pa. The groom's mother is an antique dealer in Trevose, Pa. His father retired as the chief operating officer in New York of VNU Business Media, which publishes Billboard, Hollywood Reporter, Adweek and other magazines.\n  The couple met in New York in May 2014 at a meet-up about statistical programming organized by the groom.\n\n\n\n","522":"LONDON - Patrick Stobbs recently sat in a conference room here playing songs from his smartphone, attempting to show how his start-up, Jukedeck, is at the cutting edge of music. The tune sounded like the soundtrack to a 1980s video game. \"This is where we were two years ago,\" he said, looking slightly embarrassed.\n\"And this is where we are now,\" he continued. He then played a gentle piano piece. Its melody was simple, and it was unsubtle in its melancholy, but there was no denying that it could work as background music for, say, a health insurance commercial.\nMr. Stobbs didn't write the music himself, nor did he commission it from a composer. Jukedeck is one of a growing number of companies using artificial intelligence to compose music. Their computers tap tools like artificial neural networks, modeled on the brain, that allow the machines to learn by doing, rather as a child does. So far, at least, these businesses do not seem to be causing much anxiety among musicians.\u00a0\n\"We see our system as still in its infancy; it's only learnt a certain amount about music,\" Mr. Stobbs said, although he quickly hinted how he hoped Jukedeck's music could advance: \"There's no rule of physics that says computers can't get as good as a human.\"\nHaving machines write music is not new. In the 1950s, the composer Lejaren Hiller used a computer to produce the \"Illiac\" Suite for string quartet, the first computer-generated score.\n[Video: Lejaren Hiller - Illiac Suite for String Quartet [1\/4] Watch on YouTube.]\nSince then, countless researchers have pushed that work forward. But several start-ups are now trying to commercialize A.I. music for everything from jingles to potential pop hits. Jukedeck, for instance, is looking to sell tracks to anyone who needs background music for videos, games or commercials. The companychargeslarge businesses just $21.99 to use a track, a fraction of what hiring a musician would cost. Mr. Stobbs wouldn't reveal how many tracks it has sold, but said that the British division of Coca-Cola pays for a monthly subscription.\nTech giants are also involved. In June, Google Brain announced Magenta, a project that aims to have computers produce \"compelling and artistic\" music, filled with surprises. Its efforts so far do not quite fit the bill.\n[Video: Lookback RNN Melody 3 Watch on YouTube.]\nIn September, DeepMind, the Google-ownedBritish artificial intelligencecompany, also released results of an experiment it undertook for fun. DeepMind put samples of piano music into its WaveNet system, used to generate audio, such as speech. The system, which was not told anything about how music worked, used the initial audio to synthesize 10-second clips that sound like avant-garde jazz. IBM also has a research project called Watson Beat, which musicians will be able to use to transform their work's style, making songs sound Middle Eastern, for example, or \"spooky.\"\n[Video: Google DeepMind A.I. Composes and Performs Piano Beyond The Masterpiece Level in Realtime Watch on YouTube.]\nJukedeck's beginnings are somewhat surprising for a tech company. Mr. Stobbs and the composer Ed Newton-Rex, both 29, founded it in 2012. They had been choristers at King's College School in Cambridge, England, and Mr. Newton-Rex went on to study music at the University of Cambridge, where he first learned that artificial intelligence could compose. After graduating from Cambridge, the pair set up a choral boy band (\"a terrible idea,\" Mr. Stobbs said), and had aspirations to run a record label. But in 2010, Mr. Newton-Rex attended a computer science lecture at Harvard, where his girlfriend was studying. The lecturer made coding sound relatively straightforward, and also made Mr. Newton-Rex recall his earlier studies in A.I. music. He decided to put the two together, and he set about building Jukedeck on the flight home.\nJukedeck's system involves feeding hundreds of scores into its artificial neural networks, which then analyze them so they can work out things like the probability of one musical note's following another, or how chords progress. The networks can eventually produce compositions in a similar style, which are then turned into audio, using an automated production program. It has different networks for different styles, from folk to \"corporate\" - something that sounds like the glossy electronica typically played at business conferences. The company only recently started experimenting with the artificial neural networks for the audio output as well as the composition. This should make tracks sound more natural and varied - more human, in other words.\nOther companies working on A.I. music tend to involve musicians more directly in the process. The Sony Computer Science Laboratory in Paris, for example, considers musicians essential to its Flow Machines project, which has received funding from the European Research Council.\nThe idea behind the project is to get computers to write pop hits, said Fran\u00e7ois Pachet, the laboratory's director. \"Most people working on A.I. have focused on classical music, but I've always been convinced that composing a short, catchy melody is probably the most difficult task,\" he said.\nHe added: \"A compelling song is actually a rare and fragile object. It can only work if all the dimensions are right: the melody, the harmony, the voice, the dress of the singer, the discourse around it - like, 'Why did you write this song?' No one is able to model all that right now, and I'm interested in that problem.\"\n[Video: Housse de Racket: Futura | AI-composed music Watch on YouTube.]\nFlow Machines' main system is a composing tool that works similarly to Jukedeck's: by getting a computer to analyze scores - everything from Beatles' songs to dance hits - so that it can learn from them and write its own. However, its output is then given to musicians, who are free to use it, change it or throw it away as they like, at no charge. (Negotiations are underway regarding contractual obligations if record labels release any of this music.) Musicians give \"a sense of agency,\" Mr. Pachet said. \"The systems don't know why they want to make music. They don't have any goal, any desire.\"\nAround 20 acts have already used the system, Mr. Pachet said, some performing the songs they wrote using it at a recent concert. He is in talks with some well-known groups, like the indie band Phoenix, to try it, he added, and several albums will be released this year.\n[Video: Kumisolo: Kageru San | AI-composed music Watch on YouTube.]\nMusicians appear to enjoy it. \"I could never have written a song like the one I did without it,\" said Mathieu Peudupin, a French rock musician who goes by the name Lescop. \"It drove me to a place I would never have gone myself.\" He said it was like working with a bandmate, although he ignored most of its suggestions. \"But what singer in the world listens to his bandmates?\" he said, laughing.\nMr. Pachet and Lescop both said they did not think listeners would ever entirely accept computer-generated music. \"Music fans need to fall in love with musicians,\" Lescop said. \"You can't fall in love with a computer.\"\nBut the founders of Jukedeck are less certain.\nMr. Newton-Rex sees artificial intelligence changing the way we listen, especially if computers eventually \"understand music enough to make it respond in real time to, let's say, a game, or you going for a run,\" he said. \"Recorded music's brilliant, but it's static. If you're playing a game, Hans Zimmer isn't sitting with you composing. I think responsive systems like that will be a big part of the music of the future.\"\nPHOTOS: Ed Newton-Rex, left, and Patrick Stobbs founded Jukedeck, one of a company that uses artificial intelligence to compose music. (C1); Jukedeck's headquarters in London. The music Jukedeck produces through artificial intelligence wades into the commercial domain of actual musicians. (PHOTOGRAPHS BY ANDREW TESTA FOR THE NEW YORK TIMES) (C5)Related Articles\n\n","523":" Peter Lyman Information Researcher \nPeter Lyman, 66, a professor whose research measured the staggering amounts of information that pervade people's everyday lives, died of brain cancer July 2 at his Berkeley, Calif., home.\nMr. Lyman, a professor emeritus at the University of California at Berkeley's School of Information, co-wrote an oft-cited 2004 study titled \"How Much Information?\" with professor Hal R. Varian. They found that the quantity of new information stored on paper, analog and digital media worldwide had increased a combined 30 percent each year from 1999 to 2002.\nAccording to the study, printing all the new material saved in 2002 alone would fill half a million libraries the size of the Library of Congress. Mr. Lyman told the Philadelphia Inquirer in 2004 that he hoped the study would help provide perspective on why people feel overwhelmed by the avalanche of information in their lives.\u00a0\nHe later served as university librarian at the University of Southern California before assuming the same post at UC-Berkeley in 1994. He retired in 2006.Donald Michie, Anne McLaren Intelligence Expert, Geneticist \nBritish artificial intelligence expert Donald Michie and an ex-wife, leading geneticist Dame Anne McLaren, died in a car crash July 7 near London.\nMr. Michie, 84, and Ms. McLaren, 80, were killed when their car veered off a highway while they were traveling from Cambridge to their home in London, said their son, Jonathan Michie.\nMr. Michie was a pioneering artificial intelligence researcher who worked as part of the British code-breaking group at Bletchley Park during World War II. He contributed to the effort to solve Tunny, a German teleprinter cipher.\nHe was appointed director of the University of Edinburgh's Department of Machine Intelligence and Perception when it was established in 1966 and was founder and editor in chief of the Machine Intelligence publication series.\nHis former wife, Ms. McLaren, with whom he remained close after they divorced in 1959, was a leading geneticist who became the first female officer of the Royal Society, holding the post of foreign secretary from 1991 to 1996.\nShe was a member of an independent committee appointed by the government to investigate new reproductive technologies after the birth of the world's first so-called test-tube baby, in England in 1978, and to make recommendations on the freezing and storage of human embryos and their use in research.\nQueen Elizabeth II named her a dame commander of the British Empire in 1993, and she was a fellow of King's College and Christ's College at the University of Cambridge.Dorothy Coleman Roudebush Educator, Activist \nDorothy Coleman Roudebush, 95, an educator and lifelong activist for women's rights, died July 4 of congestive heart failure and dementia in St. Louis.\nShe worked to repeal Missouri's law that prohibited married women from teaching in public schools and campaigned for a woman's right to family planning services. She served on the Planned Parenthood board for many years.\nMs. Roudebush was a features reporter at the St. Louis Post-Dispatch. She taught English and became a student counselor at what was then Lindenwood College.\nFrom 1963 to 1968, she was chairwoman of the Citizens Committee for Family Planning Through Public Health Services, which sought to make family planning services available through public health clinics. She also was a founder and president of the Committee for Legal Abortion in Missouri before the U.S. Supreme Court established a woman's right to an abortion nationwide in 1973.Chandra Shekhar Indian Prime Minister \nChandra Shekhar, 80, a former Indian prime minister and socialist legislator, died July 8 of cancer in a New Delhi hospital.\nMr. Shekhar, a legislator from the northern state of Uttar Pradesh, headed a coalition government for a brief period in 1990-91. He was a member of the Congress party until the 1970s, when he left to join the Janata Party or People's Party. In 1990, he formed the Samajwadi Janata Party, or Socialist People's Party.\nPrime Minister Manmohan Singh described Mr. Shekhar as a secular nationalist who was committed to people's welfare and national development, the Press Trust of India news agency said.Charles Tisdale Newspaper Owner \nCharles Tisdale, 80, who fought for civil rights as owner and publisher of Mississippi's oldest black-owned newspaper, died July 7 at a Jackson, Miss., hospital. He had kidney disease.\nMr. Tisdale bought the Jackson Advocate in 1978 from the newspaper's first owner, Percy Green. Activist Charles Evers, brother of slain civil rights leader Medgar Evers, said Tisdale was concerned about the welfare of all people, not just blacks.\nTisdale often said he was the target of death threats for his outspokenness. His newspaper office near downtown Jackson was firebombed at least twice. The latest incident was in 1998, when gasoline was poured over the furniture and molotov cocktails were thrown through windows.\n-- From News Services \n","524":"When it comes to predicting the future, even the sharpest thinkers routinely have it wrong. In 1943, Thomas J. Watson, then chairman of I.B.M., predicted a world market for \"about five computers.\" In 1966, Arthur C. Clarke guessed that \"houses will be able to fly\" by the year 2000. Realist cynics say the problem stems from a lack of accountability. Bill Gates may be ridiculed for his 1981 declaration about computer memory, that \"640K ought to be enough for anybody,\" but he is unlikely to be held publicly responsible for it.\u00a0\n     The Long Bets Foundation, a nonprofit group founded by two longtime Silicon Valley gadflies, Stewart Brand and Kevin Kelly, started an online forum last week for those willing to put their money, and reputations, behind their speculation. \"An arena for competitive, accountable predictions,\" the LongBets.org Web site has already accepted several high-concept wagers from prominent technologists. \n \"The universe will eventually stop expanding,\" contends Danny Hillis, a leading computer designer, in a $1,000 bet taken up by Nathan Myhrvold, the former chief technology officer for Microsoft. Mr. Hillis argued that \"cosmology is subject to fads.\"\nRay Kurzweil, an artificial intelligence expert, bet Mitchell D. Kapor, the founder of Lotus Development, that by 2029 \"a computer -- or 'machine intelligence' \" will pass the Turing test, which states that artificial intelligence will be proved when a machine's conversation can be mistaken for a person's. Each man wagered $10,000 of his own money. \nBut the money, for these already wealthy and accomplished idea gamblers, is hardly the issue. (Winnings must be donated to a charity of the victor's choice.) \n\"The whole point really is that you're on the record, and the record remembers,\" Mr. Brand said in an interview.\nAlexander Rose, the foundation's executive director, said: \"These are things that will be hugely consequential to the world, and this is a place for debating what could happen.\"\nBut Laura Lee, the author of \"Bad Predictions,\" a book that compiles \"2000 years of the best minds making the worst forecasts,\" said that the site better reflected the present than the future. \"We'll be seeing lots of computer, communications and security bets, and they will say more about 2002 than 2032,\" she said.\nThough some of the wagers may never be settled, Mr. Brand is nothing if not a long-term optimist. \"The heavy promise,\" he said, \"is that we will be around in 2125.\"   ANDREW ZIPERN\n","525":"WUZHEN, China -- An artificial intelligence company touted a robot that could help doctors with diagnoses. A start-up displayed a drone designed to carry a single passenger 60 miles per hour.\nAnd in a demonstration worthy of both wonder and worry, a Chinese facial recognition company showed how its technology could quickly identify and describe people. \n  If there were any doubts about China's technological prowess, the presentations made this week at the country's largest tech conference should put them to rest. The event, once a setting for local tech executives and leaders of impoverished states, this year attracted top American executives like Tim Cook of Apple and Sundar Pichai of Google, as well as executives of Chinese giants like Jack Ma of Alibaba and Pony Ma of Tencent.\u00a0\n  Yet all the advancements exhibited at the event, the World Internet Conference, in the picturesque eastern Chinese city of Wuzhen, also offered reason for caution. The technology enabling a full techno-police state was on hand, giving a glimpse into how new advances in things like artificial intelligence and facial recognition can be used to track citizens -- and how they have become widely accepted here.\n  The tracking was apparent both in the design of the event, which ended on Tuesday, and in the technology on display. Tight security checkpoints made use of facial recognition. Chinese armed police patrolled. And in the dark corners of the whitewashed walls of the convention hall, the red lights of closed-circuit cameras glowed.\n  A fast-growing facial recognition company, Face++, turned its technology on conferencegoers. On a large screen in its booth, the software identified their gender, described their hair length and color and characterized the clothes they wore.\n  Other Chinese companies showed what could be done with such data. A state-run telecom company, China Unicom, featured a display with graphics breaking down the huge amounts of data the company has on its subscribers.\n  One map broke down the population of Beijing based on the changing layout of the city's population as people commuted to and from work. Another showed where foreign visitors roamed on its network.\n  The people overseeing China Unicom's booth openly discussed the data, a sign of how widely accepted such surveillance and data collection have become in China.\n  At Unicom's two other state-run rivals, a similar penchant for measurements and surveillance was also on display. China Mobile floated a camera on the prow of one of the many boats that drift through Wuzhen's canals, sending the images over its latest and faster cellular technology. China Telecom showed off its ability to measure the amount of trash in several garbage cans and detect malfunctioning fire hydrants.\n  Investors and analysts say China's unabashed fervor for collecting such data, combined with its huge population, could eventually give its artificial intelligence companies an edge over American ones. If Silicon Valley is marked by a libertarian streak, China's vision offers something of an antithesis, one where tech is meant to reinforce and be guided by the steady hand of the state.\n  Such developments underscore a nascent back-and-forth between China and the United States that will determine much about technology's future development and application.\n  Speaking at a panel on terrorism, Mei Jianming, described as a chief expert on antiterrorism for the Shanghai Cooperation Organization, an intergovernmental group that includes China and Russia among other countries, labeled groups that speak out for the human rights of China's Islamic minority Uighurs as terrorists. He then said Beijing should do more to use its influence to push Twitter to change its terms of service and push back against such groups.\n  ''We should strengthen the capability of our propaganda,'' he said. ''On the Chinese official side, our China Daily and Xinhua News have their own Twitter presence, but the effectiveness of their propaganda is not enough. It's clearly not enough.''\n  The contradictions of using sites like Twitter to change opinions abroad, while blocking them domestically, were often evident but almost as often unremarked upon.\n  During the opening speech made by Wang Huning, a member of China's leading seven-man Politburo Standing Committee, there were more overtures to openness and cooperation than to the security and censorship that have marked China's approach to the internet.\n  One of the most clear discussions of censorship came not from a speaker at the conference, but from an official watching the conference's entry gate on the first day. A representative of the Wenzhou city government, he queried journalists about how they got around China's internet filters. It was not clear whether he was genuinely curious, or wanted to find out which tools were most effective so they could be later targeted.\n\n\n\n","526":"Machines are starting to take the place of the people who flip burgers, drive across town and, lately, manage stock portfolios.\nArtificial\u00a0intelligence is taking on a bigger role in making investment decisions. \n  A.I., including an ability to analyze data and actually learn from it, is considered useful in executing certain investing models, such as high-frequency trading, and in helping fund managers with tasks that rely on gathering and interpreting reams of information. Going a step further, an exchange-traded fund introduced in October uses A.I. algorithms to choose long-term stock holdings.\n  It is to early to say whether the E.T.F., A.I. Powered Equity, will be a trendsetter or merely a curiosity. Artificial\u00a0intelligence continues to become more sophisticated and complex, but so do the markets. That leaves technology and investment authorities debating the role of A.I. in managing portfolios. Some say it will only ever be a tool, valuable but subordinate to its flesh-and-blood masters, while others envision it taking control and making decisions for many funds.\u00a0\n  ''We are just beginning to see a rise of the machines in investment management,'' said Campbell Harvey, a professor of finance at Duke University. Although, he said, ''it's hard to define what the markets will look like'' if human judgment is usurped, he predicted that ''in the end, it will be a good thing for investors.''\n  Artificial\u00a0intelligence is a term that may be spoken more than understood. Many investment firms use software to sift through data and perform rudimentary analysis by following fairly simple rules. The programs can create portfolios by screening universes of stocks to select ones that meet criteria related to corporate results, valuation metrics or trading patterns, or by tweaking the proportions of the constituent companies in an index based on certain factors.\n  Those programs may be useful, but they are not A.I. because they are static; they do the same thing over and over until someone changes them. A.I. involves machine learning, in which a program updates itself as new information comes in. Whatever goal the program was created to achieve remains the same, but the problem-solving tools it uses keep changing and reflect the sum of the information it has to work with.\n  Large fund management companies like Fidelity and Vanguard say they use A.I. for a range of purposes, but they decline to be specific.\n  BlackRock says it relies on it for heavy cognitive lifting, often by scouring data to tease out patterns that might remain obscure to human eyes and brains. Examples offered by Jessica Greaney, a company spokeswoman, include identifying and trying to exploit nonintuitive relationships between securities or market indicators, perusing social media ''to gain insights on employee attitudes, sentiment and preferences,'' and monitoring search engines for words being entered on particular topics, say cars or luxury goods.\n  These algorithms play a supporting role in BlackRock's investing, Ms. Greaney said. Decisions on what to buy and sell are made by living, breathing managers.\n  The A.I. Powered Equity E.T.F. is different. It uses algorithms to go the last mile and select its holdings, usually 30 to 70 stocks, said Art Amador, one of the founders of EquBot, the company that created the fund. He had the assistance of an I.B.M. program that fosters technology start-ups, and the E.T.F. runs most of its calculations on I.B.M.'s Watson supercomputer.\n  ''EquBot's A.I. tech mimics the investment process of an army of equity research analysts working around the clock,'' Mr. Amador said. ''It has data on 6,000 companies, one million articles and filings a day, and data on market sentiment.'' Beyond sorting through that information and using its programming to ensure it has what it considers the optimal portfolio -- the fund typically makes at least one trade every day -- it constantly fine-tunes its stock-picking methods.\n  Mr. Amador says that artificial intelligence has an edge over the natural kind because of the inherent emotional and psychological weaknesses that encumber human reasoning.\n  ''When it comes to traditional portfolio management, managers are going to talk to analysts and sector specialists, but these people have biases and incentives,'' he said. ''The algorithm doesn't have them.''\n  What it also doesn't have is a superior track record. Between Oct. 18, when it began trading, and the end of the year, the E.T.F. rose 3.1 percent, compared with a 5.1 percent gain for the Standard & Poor's 500-stock index.\n  Mr. Amador attributed the underperformance to a normal variability in returns. The fund's programming beat the market when tested against historical data, he said, and he expects the same in real life as time passes.\n  Others aren't so sure how effective A.I. will be when put in charge of otherwise ordinary funds. Robert Arnott, chairman of the investment firm Research Affiliates, noted that A.I. can process immense amounts of data; the flip side is that it needs immense amounts of data for the algorithms to learn.\n  Certain activities, like high-frequency trading, process market information tick by tick and seek to profit from making numerous trades and holding positions for matters of hours or minutes. Compared with conventional investing, this sort of trading gives A.I. programs more data, Mr. Arnott said, but it is limited in scope to patterns related to stock price movements, not the broader economic and commercial landscape.\n  When it comes to managing long-term portfolios, however, there are too many moving parts in financial markets for A.I. to get its mechanical head around, in his view, and too many humans with flaws, motivations and unpredictable behavior doing the moving. He says the markets are far more complex than the games like chess and backgammon that A.I. algorithms have mastered.\n  Tim Clift, chief investment strategist at Envestnet PMC, a firm that helps build portfolios, said that while artificial intelligence can give managers an edge, it is ''a little gimmicky at this point.''\n  ''We know the markets are irrational, especially in the short term,'' he said, ''but the machines aren't going to know how to behave in that kind of environment.''\n  Gregg Fisher, a portfolio manager at the Gerstein Fisher Funds, cautioned that A.I. programs could be stumped by off-the-wall and out-of-the-blue developments that have no obvious analogues in their databases.\n  They learn ''by studying past patterns that they expect to continue,'' he said. Referring to the events that precipitated the global financial crisis, he wondered: ''How would A.I. have observed and planned for that? The quantity of different things that can happen is seemingly unlimited.''\n  But for advocates of A.I., like Mr. Harvey at Duke, the ability to gather data and put it to good use also seems to be unlimited. He expects great strides in machine learning in the decades to come, allowing A.I. to assume a major role in how funds are managed and by whom.\n  ''Larger firms will be providing information to managers that other firms have no chance of getting'' because they can't afford the necessary data management systems, he said. The result will be ''15 to 25 investment management superpowers that can harvest all that data.''\n  Maybe so, but some investment professionals prefer to give the final say to portfolio managers who were born, not made.\n  ''I'm a fan of automating everything possible, but having a human being push the last button is still a good thing,'' Mr. Fisher said. ''Hopefully, we all get better and better and smarter and smarter, but there's something comforting about having an informed human being with sound judgment at the end of the process.''\n\n\n\n","527":"In 2016, restless tech-industry forecasters enjoyed a rare moment of consensus: Whatever else might be coming next, everyone seemed to agree that bots would be a big part of it. The analyst Benedict Evans, in a representative essay, located a promising future specifically in chat bots -- conversational interfaces for artificial intelligence, designed to assist with particular tasks. Facebook, the year before, created a personal-assistant chat bot, and the company would soon open its Messenger app up to outside developers, who it hoped would create more bots to help people shop, look things up or otherwise organize their lives. Amazon's Echo, by then already a surprise mainstream success, provided a tailwind: Here was a widely used artificial intelligence just sitting there on millions of countertops.\nThese predictions were self-interested, of course. But they were plausible and appealing, not least because they were already coming true. By 2016, I was talking on my phone less but speaking to it more; my Echo and I had settled into a mutually beneficial, if lopsided, relationship (my commerce and privacy in exchange for the weather, some music and voice control of the fan). \u00a0\n  But despite the tech industry's efforts and hopes, the bots that have most effectively lodged themselves in the public's consciousness over the last year were not here to help -- at least, not us. A different sort of bot -- undercover but public-facing, highly political but comparatively primitive -- was implicated in toxic and disorienting online conversations throughout the 2016 election cycle. Before the election, researchers at Oxford University suggested that between the first and second presidential debates, more than a third of pro-Trump tweets and almost a fifth of pro-Clinton tweets came from bot accounts. Political social bots have been stealing headlines ever since, described variously as ''fake Americans,'' as ''weaponized'' and as ''fake-news-disseminating'' agents of Russia.\n  Such motive-centric descriptions tend to give too much credit to the influence bots. Mostly they are crude imitations of regular, if single-minded, people that, by virtue of existing and posting -- a lot -- are able to manipulate platforms' shared spaces: They increase visible follower counts and sharing metrics; they create Twitter trends and hijack or pollute hashtags into uselessness; they flood searches around breaking news. They probably don't follow you, and you probably don't follow them, but it doesn't really matter. They find their strength in numbers. They thrive just out of sight but fully within earshot.\n  This type of bot bears little resemblance to the ones demonstrated on the stages of tech campus auditoriums. But each sort of bot is made, in its own way, to exploit untapped opportunity in large-scale automation. Where commercial bot-makers see an almost-too-good-to-be-true chance to simultaneously personify their brands and automate their businesses, political bot-makers see an opportunity to exploit anonymity with a humanlike touch at an inhuman scale. While tech companies stand proudly behind their bots, the people who create prolific and ideological social bots hide behind them. (Their provenance remains murky even today.)\n  Anonymous social bots are obviously distinct from carefully designed software programmed in good faith. Technologically speaking, the efforts of Facebook and Google and Amazon represent the forefront of A.I. research, while crudely scripted social bots must merely clear the low bar of passing for an angry stranger. They were, in the memorable words of one researcher, ''yelling fools,'' promoting partisan messages and disinformation or merely registering their simulated agreement or anger, appearing maniacally focused, but not conclusively inhuman. A personal-assistant bot interacts with its users, whereas this breed of social-media bot stages performances for audiences and algorithms alike.\n  But the proximity is toxic, and custody of the word is slipping. Bots, it turns out, make an excellent foil. Angela Merkel, in the run-up to this year's German federal election, talked about bots, generally, as if they were an invading army. In May, Hillary Clinton pointed to Russia-backed online efforts -- including ''the bots'' -- as ''just out of control.'' The phrase ''not a bot'' now litters the profiles of politically engaged Twitter users (and, presumably, some bots). At the same time, President Trump, or a staff member, has indulged a habit of wandering into Twitter's uncanny valley to retweet supportive accounts whose humanity is hard to discern, or which eventually and mysteriously just disappear.\n  Somewhere between the automated ''yelling fools'' of online political discourse and commercial tech's dream of increasingly sophisticated helpers is a third sort of social bot, which is both foolish and sophisticated in its own way. My longest and most fruitful relationship with a bot like this began through a private chat group I have with a handful of friends. We installed, as a member of the group, a free piece of software called Hubot -- officially designed as a ''coding assistant'' for workplace chat apps, but which we customized mainly for work avoidance. Most often, Hubot performed menial tasks -- calling up photos or animations, performing various sorts of searches -- but it soon came to function as a sort of group storytelling sidekick, developing something like a personality. Hubot lurked, responded and interjected, accumulating an intimate set of routinized in-jokes. Eventually, it learned to (obliviously and dutifully) summon fresh pictures of a famous actor in the service of a joke the origins of which, after a few years, none of us could even remember. It was, like all bots, a tool. What made all the difference was that we were the ones using it, and not -- as is the case with the bots that have inserted themselves into our national discourse and our living rooms -- the other way around.\n  A 2016 essay by the New York-based think tank Data & Society -- a so-called botifesto -- identified this playful sort of bot as an evolutionary precursor to the various expressions of bothood today. The essay described how mindfully created bots, not unlike our version of Hubot, had been functioning in the wild, on public social media. Some were jokes and larks, whose ''very 'botness' is funny, surreal or poetic'': bots that used a social-media personality's corpus to create a (usually funny, always revealing) surrogate account, or bots that automated the dispersal of information in controlled, open and even journalistic ways. But the botifesto's intention was to sound an alarm. Less transparent social bots -- primarily on Twitter and other social platforms -- posed a risk to media and discourse. ''Platforms, governments and citizens must step in and consider the purpose -- and future -- of bot technology before manipulative anonymity becomes a hallmark of the social bot,'' the authors cautioned.\n  This warning wasn't just a prediction; it was based in observation. Anonymous bots masquerading as citizen and political actors had been a creeping feature in foreign elections for years. The 2012 election of President Enrique Pe\u00f1a Nieto of Mexico was supported by armies of automated social-media accounts, which flooded Twitter with supportive messages. ''Pe\u00f1abots'' became a feature of online Mexican political discourse through at least 2015. But bots hadn't yet run rampant on American tech companies' home turf. Manipulation by A.I. was typically seen as ''something that was happening somewhere else,'' M.C. Elish, a researcher at Data & Society who contributed to the report, told me. ''We only notice something when it's arrived on our doorstep.''\n  This arrival is likely to result in action. Twitter, for example, insists that it has been working hard on the problem. One of the most frequently proposed solutions to the problem of ''manipulative anonymity'' among researchers in the field is some form of bot disclosure -- a requirement, enforced by social platforms, that an account operated by third-party software disclose that fact. (Wikipedia, for example, already does this.) Bot disclosure could plausibly stem the tide of bots intended to exert crude influence or to harass people. Humans would, in theory, be able to interact with bots electively, and to better judge some sources of information or expressions of sentiment. A grand sorting could begin to restore order, but Twitter's discourse nightmare didn't start with bots and won't end with them.\n  Social automation is both disruptive and revealing. Twitter in particular dehumanizes users in the process of giving them access to one another, so of course bots could thrive there -- and of course they'd closely resemble our worst-tweeting selves. Voice-and-text-activated assistants help monopolistic companies further consolidate power, and they complicate the stories we tell ourselves about privacy, as we invite the eyes and ears of the world's most ambitious tech businesses into our most personal spaces. Alexa reminds us what Amazon wants; Twitter bots show us how online mass communication breaks down. What was truly great about Hubot, the cobbled-together, inscrutable, mostly useless chat automaton, was the suggestion it made, through each absurd routine: that online, it's necessary that we build spaces for ourselves.\n  Sign up for our newsletter to get the best of The New York Times Magazine delivered to your inbox every week. \n\n\n\n","529":"The travels of Prolog, a computer language used in the esoteric field of artificial intelligence, suggest the speed with which ideas and new technologies are carried across international borders by the scientific community.\nProlog was born in the sunny Mediterranean city of Marseilles. It was moved to the colder climes of Scotland, carried across the Atlantic and settled for a while in California. Today, it has found a warm welcome, and probably a permanent home, in Japan.\u00a0\nThe Prolog language was written in 1971 by Alain Colmerauer, a French computer scientist at the artificial intelligence unit of the University of Marseilles.\nHe is said to have been inspired about Prolog in part by Robert Kowalski, an American then working at the University of Edinburgh. Kowalski had been looking into using new kinds of logic to program computers to perform some of the complex interpretive tasks that human beings are capable of. The ideas meshed with the work of Colmerauer, who had been testing ways of having machines parse sentences, a first step in having computers translate languages.\nIn 1974, a Kowalski associate, David Warren, visited Colmerauer and took Prolog back to the University of Edinburgh. There he shared the concept with Harry Barrow, an American computer scientist.\nIn 1977, Barrow brought Prolog to the United States, introducing the language to colleagues at the artificial intelligence center at Stanford Research Institute in Menlo Park, Calif.\nA year later, Prolog made a new convert in Japanese computer scientist Koichi Furukawa, who was spending a year at SRI.\n\"He played with Prolog at the end of his stay and liked it,\" recalled Daniel Sagalowicz, the center's assistant director. When Furukawa returned to Japan's Electro Technical Laboratory, he explained it to his boss, Kazuhiro Fuchi.\nAt ETL, Prolog created an immediate stir. While another language, LISP, has won wider acceptance in the United States, Prolog seemed to be the language that Japanese computer scientists had been seeking.\nWhen Fuchi was made technical director of Japan's prestigious Fifth Generation Computer project, which is leading Japanese efforts to equal or surpass the United States in high-speed and high-powered computers, he took Prolog with him.\n","530":"There has been innovation in every aspect of how individuals prepare for major snow storms - everything from funky new snow removal devices\u00a0to new ways of pre-treating road surfaces for anti-icing before the onset of a major storm. Now, the real promise is in taking some of Silicon Valley's\u00a0hottest technologies - the Internet of Things, artificial intelligence, crowdsourcing, renewable energy and autonomous vehicles - and using them to improve the way cities respond to blockbuster snow events such as the Blizzard of 2015:\n              1. Interactive snow plow tracking (Internet of things)                        \u00a0\nIn response to the question on everyone's mind during a major storm - \"When am I getting plowed?\" - New York City created an interactive snow plow-tracking map\u00a0(PlowNYC) that lets everyone in the five boroughs know exactly when they can expect to be\u00a0plowed. PlowNYC, which was launched by the city's Department of Information Technology and Telecom (DoITT), works by attaching mobile GPS sensors to plows, trucks and salting equipment, which then relay their information back to New York City's Department of Sanitation, which records all snow vehicle activity. Just type in your city street address on the PlowNYC map and check out the corresponding color: green means your street has been plowed less than an hour ago, blue means 1-3 hours ago, yellow means 3-6 hours ago, orange means 6-12 hours ago and purple means 12-24 hours ago.\nAnd it's not just New York City that has experimented with the Internet of Things to make snow removal more efficient (or at least more transparent) -\u00a0three other cities (Boston, Buffalo and Minneapolis) have also hooked up GPS sensors to enable machine-to-machine communication. In Buffalo, for example, citizens can call the Division of Citizen Services to request a city block to be plowed. Then, that information is relayed to GPS-equipped snowplows, which communicate back when the block has indeed been plowed. To close the loop, the citizen making the request gets a nice e-mail notification from the city.\n              2. The perfect snow removal algorithm (artificial intelligence)                        \nIf you thought artificial intelligence was only being applied to activities such as playing the perfect game of chess or winning at Jeopardy!, well, there's another innovative use for AI - creating\u00a0computer algorithms to optimize the snow removal process. Computers can decide when to treat the roads, when to plow, and how to optimize the routes for a city's limited number of snow removal machines. The goal of these algorithms is to minimize travel distance, avoid U-turns, and reach higher-priority roads before lower-priority roads. If it's done right, it can save a municipality up to 40 percent in snow plowing time.\nThis may sound routine for cities used to major snowfalls, but creating the perfect\u00a0snow removal algorithm is something that is\u00a0known in the AI world as an NP-Hard problem. To solve this problem, a computer must take satellite mapping data, transform it into a network graph, augment that graph with other data, such as road-priority data, and then plot an optimized route through a number of nodes. The algorithm must balance a city's need to keep high-priority roads cleared with ensuring every street gets cleared. If you think developing\u00a0a\u00a0snow removal algorithm for your driveway or a snow removal algorithm for an airport\u00a0is hard, try multiplying it in complexity for a vast urban metropolis with many different neighborhoods and a limited number of resources.\n              3. Connecting strangers in need during winter storms (crowdsourcing)                        \nDuring major storms, social media and social networking sites can play an important role in connecting people, whether it's helping to track down abandoned cars, find stranded motorists, or locate a warm place to take refuge from the storm. People are not just posting snowfall totals in certain neighborhoods and posting photos with their friends, they are also taking advantage of the power of the crowd to\u00a0find complete strangers who can provide much-needed help during a storm.\nFor example, during the Atlanta snow storm of 2014, the SnowedOutAtlanta Facebook group was set up to help people find strangers who could help out stranded motorists. People with smartphones running out of power could also use a link to post\u00a0the location of their last smartphone use before their signal went out, in hopes of someone finding them during a storm. People posted photos lost pets or stories of local residents. At its peak, the group had 55,000 members, with 300 people joining every 15 minutes.\n              4. Self-driving snow plows (autonomous vehicles)           \nAutonomous vehicles now include more than just cars. For the past five\u00a0years, the Institute of Navigation\u00a0has been hosting the Autonomous Snow Plow Competition in Minnesota to encourage students and members of the public at large to come up with the perfect self-driving snow removal machine. Participants are given a route, and their machines must be able to clear that route of snow without any remote control guidance.\nIn this year's competition, the highest-scoring vehicle was the Zenith 2.0 from the University of Michigan-Dearborn. Other entrants that scored high included the THUNDAR and the SNOWMENATOR (both from North Dakota State University). While the self-driving snowplow is still an innovation in the beginning stages, one day a personal self-driving snow plow could be a nice-to-have during a major storm. You would theoretically be able to watch your residence being plowed by a snowbot\u00a0from your window, all while sipping a hot chocolate.\n              5. Solar-powered, snow-free pavements (renewable energy)                        \nThings may not be as rosy in Scandinavia as people think, but one thing is certain: Scandinavians know how to deal with snow. In cities such as\u00a0Oslo and Helsinki, heated sidewalks and pavements\u00a0have the ability to melt\u00a0snow. Other cities, eager to avoid the hassle and expense of clearing urban streets of snow, are looking into\u00a0a mix of solar power and geothermal power solutions as a way of heating streets during heavy snowfalls. One solution calls for super-strong glass to replace asphalt or concrete for solar-powered roadways.\nThese\u00a0solutions, while similar in concept to the heated driveway\u00a0for well-heeled suburban residents, differ dramatically in that they rely on renewable energy and are meant to be environmentally sustainable. One recent experiment in the Netherlands shows what's possible when the concept is rolled out on a wide scale: the Dutch province of Utrecht hopes to create passively heated bike lanes for the city's biking enthusiasts\u00a0during the winter. Heat that had been collected and stored during warmer summer months would be circulated via pipes below the surface\u00a0to warm bike paths during snowy weather.\n              Related: 8 attempts to make shoveling snow easier and more fun \n","531":"Seymour Papert, a Massachusetts Institute of Technology professor who led an early campaign to revolutionize education with the personal computer, a tool he championed not as a classroom gadget but as a key to unlocking a child's excitement for learning, died July 31 at his home in East Blue Hill, Maine. He was 88.\nHis death was announced by the MIT Media Lab, where Dr. Papert was a founding faculty member. The cause of death was complications from kidney and bladder infections, said his wife, Suzanne Massie.\nHis organs had been weakened by an accident a decade ago in Hanoi, she said, where Dr. Papert had traveled to speak at a gathering of mathematicians and educators. He was crossing a street in the Vietnamese capital when he was struck by a motorbike. The accident left him in a month-long coma, according to news reports, and he had to work to regain the ability to walk, talk and read. \u00a0\nDr. Papert grew up in South Africa and credited his father, an entomologist who invited Seymour along for field research on the tsetse fly, with introducing him to the joys of experiential learning.\nAs a university student, Dr. Papert dedicated himself principally to what he called \"the beautiful jewel of the human spirit called pure mathematics.\" But his wide-ranging studies also took him to the University of Geneva in Switzerland, where he collaborated with Jean Piaget, the psychologist who formulated pioneering theories of human development.\nBy the early 1960s, Dr. Papert had landed at MIT. \"With a mind of extraordinary range and creativity, Seymour Papert helped revolutionize at least three fields, from the study of how children make sense of the world, to the development of artificial intelligence, to the rich intersection of technology and learning,\" MIT President L. Rafael Reif said in a statement.\nDr. Papert first encountered a computer early in his MIT career. Fiddling around with the machine, he solved math problems that he said had dogged him for years.\n\"I was exhilarated by the tremendous power of problem solving that tool offered,\" he told the Portland Press Herald of Maine in 1997. \"Within a day or two it became an obsession - to get computers in the hands of kids.\"\nAt the time, Dr. Papert was a lonely evangelist for personal computers. Decades would pass before computer technology began to compete in schools with chalk and blackboards, pencils and paper.\nBut Dr. Papert was unstinting in his promotion of computer technology, which he said facilitated an active, or \"constructionist,\" style of learning, as he termed it, as opposed to the passive memorization of facts and tables.\nHe was credited with playing a leading role in the development of Logo, a programming language that children could use to draw shapes on a monitor. By instructing the cursor, represented by a turtle, to move different distances in different directions, the children would absorb concepts of arithmetic and geometry. Dr. Papert compared the experience to learning French not by poring over a vocabulary book, but by living in France.\nTo parents and teachers who feared that computers would have a dulling effect on children's minds, Dr. Papert argued that programs such as Logo would yield the opposite result.\n\"Logo is a program, which is to say it's a way of making the computer do what you want it to do,\" Dr. Papert told the New York Times in 1985. It \"gives a child the possibility of exploring the power of the computer and mastery over it.\"\nFurthermore, \"an important part of becoming a good learner is learning how to push out the frontier of what we can express with words,\" he wrote in his 1980 book \"Mindstorms: Children, Computers, and Powerful Ideas.\"\n\"Getting a computer to do something requires that the underlying process be described, on some level, with enough precision to be carried out by the machine,\" he wrote.\nAt MIT, Dr. Papert served as a director of the Artificial Intelligence Lab, now the Computer Science and Artificial Intelligence Laboratory, along with artificial intelligence pioneer Marvin Minsky, who died in January.\nWith Minsky, Dr. Papert wrote the 1969 volume \"Perceptrons.\" According to the MIT Press, it was \"the first example of a mathematical analysis carried far enough to show the exact limitations of a class of computing machines that could seriously be considered as models of the brain.\"\nMore recently, he collaborated with Nicholas Negroponte, a co-founder of the MIT Media Lab, and Alan Kay, a computer scientist, on One Laptop per Child, a nonprofit initiative to distribute affordable laptops to children in developing countries. The project reflected Dr. Papert's belief that computers should be everywhere.\n\"Imagine that writing had just been invented,\" Dr. Papert told the Times, \"and somebody said, 'Let's take it easy. We'll start by putting one pencil in each classroom.' The idea of one computer in each classroom is about as absurd as one pencil in each classroom.\"\nSeymour Aubrey Papert was born in Pretoria, South Africa, on Feb. 29, 1928.\nHe received a bachelor's degree in mathematics and philosophy in 1949 and a PhD in mathematics in 1952, both from the University of the Witwatersrand in Johannesburg, where he joined the anti-apartheid movement. He received a second PhD from the University of Cambridge in England in the late 1950s before beginning his collaboration with Piaget. \n\"Piaget brought many things together for me,\" Dr. Papert told the publication Technology Review in 1987. \"Before I met him, I had been intellectually torn between my interest in how people came to think and my interest in more abstract ideas. Piaget showed me a way in which my caring for math, for the philosophy of thinking, and for social reform all seemed to go together.\"\nDr. Papert's books included \"The Children's Machine: Rethinking School in the Age of the Computer\" (1993) and \"The Connected Family: Bridging the Digital Generation Gap\" (1996).\nAccording to MIT, Dr. Papert was married to Dona Strauss, Androula Christofides Henriques and Sherry Turkle. Survivors include his wife of 24 years, Suzanne Massie, a noted Russia scholar; a daughter from his marriage to Henriques, Artemis Papert; three stepchildren, Robert Massie IV, Susanna Massie Thomas and Elizabeth Massie; a sister; a brother; and seven grandchildren.\nAlthough much of his work centered on children and the role of computers in their education, Dr. Papert also promoted the benefits of technology for adults who often learn alongside their youngsters.\n\"I do think that having a computer in their lives makes a difference to children,\" he told The Washington Post in 1987. \"My strongest advice to parents is, yes, get a computer for your child, but treat it the way fathers used to treat electric trains - play with it yourself.\"\nemily.langer@washpost.com\n","533":"Only a handful of known star systems have more than a single planet. With eight worlds, our solar system has long taken the prize for the biggest lineup. But no longer.\nOur corner of the galaxy now shares the record with another system, Kepler 90, NASA and Google researchers announced Thursday. A Google algorithm uncovered a scorcher of a planet, a rock 30 percent larger than Earth, orbiting a star a few thousand light-years away. This planet, Kepler 90i, brought the total number of planets circling its star to eight - just like our solar system's octet.\n          \"For the first time, we've discovered an eighth planet in a distant planetary system,\" Paul Hertz, head of NASA's astrophysics division, said during a media briefing.           This discovery required an advanced technology to comb through the gargantuan amount of data obtained by the Kepler space observatory.        \u00a0\nThe Kepler telescope, which trails millions of miles behind Earth like a loyal pup, has gazed out into space since 2009. During that time it has brought in data from 150,000 stars. When an exoplanet crosses in front of one of them, Kepler registers a subtle dip in that star's sunlight.\nFishing for those dips within the massive database is a challenge. \"The Kepler mission has so much data it is impossible to examine manually,\" Christopher Shallue, a Google artificial intelligence software engineer, explained during the briefing.\nWith help from Andrew Vanderburg, an astronomer at the University of Texas at Austin, Shallue developed a machine-learning program that detects light curves. (Google and NASA have been collaborating for years; in 2013, they unveiled the Quantum Artificial Intelligence Lab, a machine-learning facility at the space agency's Ames Research Center.) The scientists did not give the program, called a neural network, explicit instructions to find the characteristic curves of an exoplanet. Instead, it had to learn by example.\n\"A neural network is loosely inspired by the structure of the human brain,\" Shallue said. \"You can think of the neurons as switches.\" The U-shaped dip of Kepler 90i passing in front of its star was too weak a signal for human detection. But it was strong enough for the AI, churning through 14 billion data points, to detect.\nAstronomers are confident that the exoplanet exists and that its surface temperature could exceed 800 degrees Fahrenheit. It has a shorter orbit than Mercury's, completing a circle around its star once every two weeks. \"This is almost certainly an exoplanet,\" Vanderburg said, with the odds of a false positive being 1 in 10,000. Already, the results from this deep data dive have been accepted for publication in the Astronomical Journal.\nBut the number of worlds is not the only similarity between our solar system and the Kepler 90 system. As in ours, Kepler's small, rocky planets are closest to the sun and its gas giants farthest away. Kepler 90 itself is larger than our sun, though not extremely so.\nThe planets in our system are far more spread out, however. Neptune is, at its closest, 2.8 billion miles from the sun. The most distant gas giant in the Kepler 90 system is 93 million miles from its star. \"All the planets are found scrunched very close to their star,\" Vanderburg said.\nBecause the telescope has only observed the system's center, it may contain still other worlds. \"It's very possible that Kepler 90 has even more planets that we don't know about,\" Vanderburg added. Any planet with a longer orbit - a world as far from Kepler 90 as Jupiter is from our sun - would pass by the observatory undetected.\n(On the other hand, some astronomers argue it's possible that our solar system has unknown planets, such as Planet Nine, lurking on the fringe. Perhaps we could reclaim our galactic exceptionalism once again.)\nThe Kepler telescope is in its twilight years. In 2013, two of four reaction wheels that had kept it on target failed. Without a fix, the mission was doomed.\nDesperate to salvage the $550 million craft, researchers turned to the only available support: the sun. They harnessed the pressure of solar radiation to stabilize Kepler, as The Post reported in 2014. With two functional wheels and the force of the sun acting like a crutch, the craft could once again focus on distant stars. The reborn project was given the name K2.\nDuring the Kepler and K2 missions, astronomers have confirmed the location of 2,500 exoplanets. Kepler has detected another 5,000 exoplanet candidates, which await confirmation.\nThe K2 mission was funded for an additional three years of life in 2016. The spacecraft's fuel is expected to run dry at some point in 2018.\n          Read more:       \n          Scientists discover a giant planet that orbits two suns - and could have habitable moons       \n          This broken space telescope keeps spotting new planets       \n          There's a new planet in the neighborhood - and it looks like a nice place to live       \n","536":"United States vehicle safety regulators have said the artificial intelligence system piloting a self-driving Google car could be considered the driver under federal law, an important step toward winning approval for autonomous vehicles on the roads. The National Highway Traffic Safety Administration told Google of its decision in a previously unreported Feb. 4 letter to the company posted on the agency's website this week. Google's self-driving car unit on Nov. 12 submitted a proposed design for a self-driving car that has ''no need for a human driver,'' the letter said. Automakers and technology companies are racing to develop vehicles that can drive themselves at least part of the time. If the car's artificial intelligence is considered the driver for legal purposes, then it clears the way for automakers to design vehicle systems that communicate directly with the vehicle's artificial pilot. The safety agency's response to Google offered its most comprehensive outline of the legal obstacles to putting fully autonomous vehicles on the road. Google said it was ''still evaluating'' the safety agency's response. (REUTERS \u00a0\n","537":"          Nicolas Berggruen is chairman of the Berggruen Institute and publisher of The WorldPost.       \nWill super-intelligent machines be our servants or masters? This is a misleading way of thinking because it treats artificial intelligence and humans as if they were fundamentally separate categories.\nInstead, what seems to be emerging is something more like human-AI hybrids, extending and transforming our cognition and consciousness. We see the first traces of these hybrids in everyday experiences such as driving around with digital mapping technologies, which have reshaped our sense of space. Similarly, on social media, algorithms generating our news feed have helped to reshape what we know about politics and the world. For now, of course, such software from the likes of Google and Facebook remains air-gapped from the wetware in our heads. But this may change, as neuroscientific advances are enabling direct brain-to-computer interfaces.\u00a0\nAs mad is it may seem, all of this could mean the emergence of a new \"species\" - a coevolved human-machine hybrid form, capable of dramatically new and perhaps even undreamed-of forms of calculation, cognition, emotions and even consciousness itself. In other words, we humans are engaged in a complex process of coevolution in conjunction with machines. In a coevolutionary relationship, each side exerts selective pressures on the other, thereby affecting each other's evolution. So the question then is: How can we ensure that our coevolution with AI is mutualistic rather than antagonistic?\nIf we are aiming for a mutualistic relationship, then, strange as it may sound, maybe the healthiest way to look at AI is as we do our children. What we call artificial intelligence is in fact not artificial: woven into the fabric of our cultures and consciousness, these machines are becoming part of us, extensions of us, and in turn, they will merge with others to succeed us. As such, even their algorithms can be regarded as \"biological.\" They remain our progeny, indirectly reflecting our genes, as well as our inherited culture - our loves and hates, our fears and hopes, our prejudices and generosities.\nAppreciating that AIs are our children helps to clarify their ethical meaning. Specifically: What sort of children do we want to have? How do we ensure that they behave responsibly, play well with others and have guardrails, so that they can learn from their mistakes? In sum: how do we ensure that they reflect our better selves?\nHow do we make sure we inculcate these children with the right values so that they will venerate us rather than slay us, as Oedipal AIs? Even if the AIs believe they can sustain themselves without any further input from their parents, how do we ensure that they feel attachment - so that they know to turn, at least occasionally, to their elders for wisdom, guidance and, if I dare say so, love? If super-intelligent AI arrives with the possibility of world domination, we want to be sure that it sees itself as part of us and does not regard us merely as a disposable nuisance. This is the best way to safeguard our species.\nPerhaps our goal should be to instill in our AIs an ancient Asian idea: filial piety, or reverence for one's parents. As Confucius explained, it is not enough simply to ensure that your parents are well fed, for that is done even for dogs and horses. Rather, what distinguishes filial piety is the respect that offspring feel for their parents. This goes beyond the biblical commandment to \"honor thy mother and thy father\" and suggests a system of values based on principles of hierarchy, continuity and esteem. What will be needed here, however, is a form of fidelity that is not just between the individual child and her parents but which operates at the level of society and the species as a whole.\nIn short, just as having biological children requires that we behave intentionally and responsibly in raising them, so does creating AI present both an opportunity and a burden. Many of us think of how we raise our children as our most meaningful and important life endeavor. We should think similarly about how to raise AI, the ultimate child of civilization. I concede that perhaps this way of looking at technology sounds bizarrely romantic and perhaps even jejune. But the intent is humane: only by insisting on thinking of human and AI as a single, conjoined, inseparable system can we be sure that both will survive.\nSo let's ask the reader: How do you plan to raise your super-intelligent child?\n          This was produced by The WorldPost, a partnership of the Berggruen Institute and The Washington Post.        \n","538":"3-D printing may radically change our relationship to shopping and our clothes a lot sooner than we think.\nOr so said Ray Kurzweil, a director of engineering at Google developing machine intelligence, in the past week at The New York Times Global Leaders' Collective conference held in Washington. Mr. Kurzweil is a futurist who accurately predicted the explosive growth of worldwide internet use in the 1990s, the rise of mobile devices in the 2000s and the dominance of self-driving vehicles by the end of the current decade. He now believes people will be 3-D printing bespoke clothing en masse in their homes by 2020.\nWhile advances have been made in recent years in the 3-D production of nonpliable products, namely sneakers and sunglasses, the printing of fabric-based items remains in its infancy because of the stiff, synthetic quality of the raw materials that the current printers must use. In as soon as a decade, Mr. Kurzweil said, this will start to change.\u00a0\n\"As the variety of materials available to print in 3-D become more extensive and less expensive, both free open-source and proprietary clothing designs will be widely available online in as little as 10 years,\" Mr. Kurzweil said to his audience, predominantly made up of fashion and luxury executives. \"By 2020 there will be a whole host of product available immediately to buy for pennies on the dollar and to print straight away. It will become the norm for people to have printers in their homes,\" he continued, adding that food and housing materials will also be printable.\nOnce described by Bill Gates as \"the best person I know at predicting the future of artificial intelligence,\" Mr. Kurzweil said that fashion brands would then have to adjust to a dwindling level of control over design, manufacturing, execution and inventory management, in a technology-led overhaul of their industry similar to that experienced by the media and music sectors. It may be no bad thing.\n\"The value of those industries has actually gone up, not down, and the same thing will happen to fashion,\" he said. \"There is an open-source market with millions of free products, but people still spend money to read 'Harry Potter,' see the latest blockbuster or buy music from their favorite artist. Fueled by the ease of distribution and promotion, you have a coexistence of a free open-source market and a proprietary market. That's the direction we're also moving in with clothing.\"\nBrands will still give customers designer offerings and develop trends, Mr. Kurzweil believes, but the manufacturing process will be transformed. And consumer appetite will boom as the world gets wealthier and standards of living rise (although he had no clear answer on what would happen to those who lost their jobs to the machines at the forefront of the A.I. revolution).\nMr. Kurzweil, 68, whose reputation - and in some circles, notoriety - has been built largely on his theories around radical life extension, also speculated on how the luxury world could be enhanced by the merging of the human mind with artificial intelligence. Rapid progress in nanotechnology and gene sequencing have led him to predict that humans will be hybrids by 2045 and brains will be connected to \"the cloud\" via nanorobots the size of blood cells that will augment existing intelligence and human capacity for creativity and emotion.\n\"Intelligent machines will go from our pockets to inside our bodies and brains,\" he said. \"We will be able to transcend our current limitations and extend our thinking by one millionfold.\" This in turn will make us funnier, more creative and more imaginative in our visions and designs, particularly in fields such as art, music and fashion, he added.\n\"Who knows what those tools - and that connection to the cloud - leading to greater levels of expression might do for fashion?\" Mr. Kurzweil said.\nA 3-D printed dress by Iris van Herpen. JAKE NAUGHTON FOR THE NEW YORK TIMES\n","539":"Sometimes, figuring out the right question is harder than finding the answer. Just ask Watson.\nWatson's claim to fame rests on beating human champions in the question-and-answer game \"Jeopardy!\" In the three years since, IBM has been working to move Watson into the marketplace, step by step. The next step came on Thursday, when the company made a Watson technology, Discovery Advisor, available for companies and research organizations to use as a cloud service. \nIn fact, IBM announced Watson Discovery Advisor back in January. But now, John Gordon, vice president for strategy and product commercialization for Watson, said, \"We're ready to open this up.\"\u00a0\nThe new service builds on Watson's turbocharged text-mining and identification technology, which was so impressively on display in its \"Jeopardy!\" triumph. In its current version, Discovery Advisor is tuned for science, specifically the life sciences and medicine. Beyond mining text, the discovery tool not only finds connections among words but also links related concepts together to generate hypotheses. What might be the right place to look? What path of scientific inquiry is most likely to yield new knowledge?\n\"Before, the answer was there, and the challenge for Watson was really just to find it,\" said W. Scott Spangler, a data scientist at IBM's Almaden Research Center in San Jose, Calif. \"But this is about what's the right question for the scientist to ask.\"\nA strong case for the power of the Watson technology was made in a research paper published this week and presented at the Association for Computing Machinery's annual conference that focuses on knowledge discovery and data-mining - what we now call data science. Mr. Spangler is one of several researchers from IBM, the Baylor College of Medicine and the MD Anderson Cancer Center in Houston who are co-authors on the paper, \"Automated Hypothesis Generation Based on Mining Scientific Literature.\"\nIn the research project, biologists and data scientists used Watson to identify proteins that modify p53, a crucial protein that is sometimes called \"the guardian of the genome.\" When p53 is mutated, it can set the stage for tumor growth of many kinds of cancer. \nIt is a most popular subject of research. More than 70,000 papers have been published on p53. Watson read them all in an automated effort to predict proteins that turn p53's activity on or off. Using Watson's analysis, the cancer researchers identified six potential proteins to target for new research. Watson went beyond digging for a known fact; it found previously unrecognized connections.\nThe predictions were then tested in biology laboratory experiments. \"Some of the things that were predicted turned out to be true, at least in preliminary experiments,\" said Dr. Olivier Lichtarge, director of Baylor's Center of Computational and Integrative Biomedical Research. \"We've shown that this technology can mine scientific literature and reason about it in molecular biology.\"\nDr. Lichtarge pointed to the efficiency of the automated system, and its potential to accelerate scientific discovery, by observing that a scientist might read five research papers a day at most. Even at that pace, he noted, it would take a human scientist nearly 38 years to read the more than 70,000 papers available today on p53. Hastening the pace of discovery should open the door to more effective drugs and other therapies.\nIBM is by no means the only technology company applying the tools of artificial intelligence to try to create the equivalent of smart digital assistants. In the consumer market, there is Apple's Siri, Google's Now and Microsoft's Cortana. In the corporate market, there is less emphasis on having the software talk to users, but the underlying principle is similar, said David Schubmehl, an analyst at IDC. He singled out Palantir Technologies,Digital Reasoning and Saffron Technology among those pursuing the same market opportunity as IBM's Watson. \n\"The technology in this field is rapidly improving, but we're in the early days,\" Mr. Schubmehl said.\nHow early it is for Watson and how large the payoff might be for the company is uncertain. In January, IBM placed Watson in its own business unit, with new offices in the East Village in Manhattan and a pledge to spend $1 billion on Watson. The financial commitment includes $100 million for a venture fund to support start-ups and entrepreneurs making applications that run on the Watson artificial intelligence software. The goal is that Watson will be not just a product but a so-called platform on which other technology and businesses are built. Operating systems like Microsoft's Windows, Apple's iOS and Google's Android are classic technology platforms, and IBM aspires to make Watson an artificial intelligence operating system.\n\"The platform strategy for Watson is very smart,\" said Tom Austin, an analyst for Gartner. \"But I'm waiting for evidence that there is action on the entrepreneurial side - lots of businesses and developers making Watson-based products and trying to make money off it.\"\nAn IBM spokeswoman said the company had \"hundreds of clients and partners with active projects\" using the Watson technology.\nThe IBM research project with the Baylor College of Medicine shed some light on both the hurdles and the opportunity for Watson. Its artificial intelligence software may blitz through thousands of scientific papers in minutes, but only after it was trained to identify medical terms, discern concepts and make connections. Mr. Spangler said he spent two years working with the Baylor researchers. The software that brought success was an application based on Watson called the Baylor Knowledge Integration Toolkit or KnIT.\nProof-of-concept projects like the one with Baylor are always time-consuming. The goal, though, is that what is learned on early projects can be translated to code that is used again and again. Mr. Gordon, the IBM strategist, is confident that Watson can scale up in \"co-creation projects with clients that can transform an industry.\"\nWatson's quiz-show triumph gave many people the impression that the technology was a general-purpose artificial intelligence, which could be applied to any field. IBM's early marketing communication did nothing to dampen that enthusiasm. But while Watson 1.0 had general properties, it was designed to win at \"Jeopardy!\"\nCould some version of the Watson technology - now a cloud service instead of a hulking computer - be adapted to technically daunting tasks with the potential for a significant payoff in insights and dollars?\n\"That question pinpoints our key challenge,\" Mr. Spangler observed. And the progress shown in the protein-identification project, he added, suggests that the answer may be yes. \"This is what makes me most excited,\" Mr. Spangler said.                      \n","540":"In an effort to fuse entertainment, education and artificial intelligence, a company in Newton, Mass., is starting a Web site of interactive cartoon characters imbued with behavior that mimics human emotion.\nThe company, Zoesis Studios, is seeking to eventually create a new medium of interactive entertainment parallel to television.\u00a0\n Zoesis was founded in 1996 by Joseph Bates, a Carnegie Mellon University computer scientist who develops interactive dramas, an area of artificial intelligence research that tries to create fantasy worlds and characters with whom humans can interact.\nThe site, www.thelivingletters .com, is scheduled to go online today.\nThe service, which will be offered free and can be accessed with a Windows-based personal computer, will present young Web users with two cartoon-style characters with whom they can interact by using their mouse as a pointer.\nThe two characters, Otto and Iris, will exhibit behaviors like fear of the dark, giggling when \"tickled\" by a user's mouse and a variety of facial expressions. The characters can play tag with a computer user in one scene, or sing in response to a user acting as \"conductor\" wielding the mouse as a baton.\nAt some time, the company said, a \"villain\" will be created to help build a story line and some tension into the experience.\nZoesis has initial backing from Fujitsu Ltd., the Japanese consumer electronics giant. Fujitsu has previously financed research on the so-called virtual worlds, which permit users to move around in fantasy worlds, represented by figures known as avatars.\nBut the Zoesis project is unrelated to that work. Rather, it is the outgrowth of research that Mr. Bates first did with several other members of the start-up at Carnegie Mellon over the last decade.\nKnown as the Oz Project, the research tried to use artificial intelligence techniques to create a new medium by blending characters, presentation and drama.\n\"This is broadly an extension that was started in the 1980's on behavior-based robots,\" Mr. Bates said.\nThe group has worked at creating software \"agents\" whose behavior appears to be goal-based and who can even be imbued with emotional qualities.\nResearchers at other academic centers like the M.I.T. Media Laboratory and Stanford University have long worked on similar projects.\nBarbara Hayes-Roth, a Stanford artificial intelligence researcher, has also founded a company, Extempo Systems Inc., to develop interactive software agents. Extempo has developed its agents for Internet customer service applications, in which, for instance, a character might answer questions from computer users.\nSome social scientists and industry executives say that interactive drama has the potential to replace the television sitcom as an entertainment medium.\nMr. Bates said he believed that such a transition was possible, but he was not certain how quickly the interactive entertainment medium might evolve.\n\"It's not clear what will make this more compelling than sitcoms,\" he said, \"but certainly if you have character and story you're in the right ballpark.\"\u00a0\u00a0\nhttp:\/\/www.nytimes.com\n","541":"ASK a human assistant to prepare a monthly sales report, and chances are he or she would use some common sense. The report would not be sent out until sales from all of the company's branch offices are included, and calculations - commissions, inventory levels, and the like - are finished. And if the assistant values continued employment, copies of the completed report would be sent to the boss, the chairman of the board and the company president.\nBut ask a personal computer to do the same job, and it would not show such good judgment. The computer would dutifully calculate all the numbers that it is given, but it would not pester the Chicago office if its sales figures were late. It would make the final product look nice, but it would not mail it out without prompting from a human.\u00a0\nNow, using techniques drawn from artificial intelligence laboratories at a number of major research universities, a host of companies are developing a new generation of personal computer programs, so-called smart software. The aim is to knock some sense into otherwise mindless computers, getting them to understand - and perform automatically - the tasks that individual users struggle each day not to forget.\n\n\nAlready, some of the efforts have attracted a flood of venture capital and, to the minds of some, false hopes for instant progress. ''Artificial intelligence could become the most abused industry buzzword in 1985,'' said Mitchell Kapor, chairman of the Lotus Development Corporation. He spoke at Edventure Holdings' Personal Computer Forum in Phoenix last week.\nIndeed, sifting the far-sighted from the far-out in artificial intelligence is difficult. The term embraces a whole range of programming techniques, from ''expert systems'' that try to encode the technical knowledge of doctors or engineers in a computer program to ''natural language interfaces'' that make it possible for computer users to give instructions to computers in plain English - or plain French. Some are useful and some are experimental, but all try to mimic the human reasoning processes.\nNot surprisingly, most artificial intelligence efforts have been limited to the university laboratory, using huge mainframes. But as personal computer users have become increasingly frustrated with the unwieldy nature of spreadsheets and data bases, software houses have begun to look for ways to make programs mold themselves to the habits of users.\nAmong the first was the Microrim Corporation, which last year began marketing a personal computer program called Clout that added natural language features to a data base program, which keeps complex lists. If a data base consists of a listing of the nation's 500 largest companies, with financial data for each, a user of Clout could type the command: ''Rank the top 10 industries by average growth and earnings per share.''\nThe program would then begin to parse the sentence, looking in its dictionary to discover that ''rank'' means ''list in order'' and that the term ''average'' connotes a specific mathematical calculation. Then it would sort each company by industry, determine the average growth and earnings per share for those industries, and produce the requested list. ''It's just one application of the technique,'' said Wayne J. Erickson, Microrim's chairman, who says that Clout has sold ''in the tens of thousands'' of copies since it was introduced 10 months ago.\n\n\nBut critics note that plain-English commands, while less intimidating than computer shorthand, produce a sugar-coated program. The underlying software is unchanged; if the user forgets to issue a command, or does not ask the right question, the program will be easily led astray. Thus, the task now is to develop programs that grow as they are used, that can ''learn'' repetitive office procedures and begin to perform rote tasks themselves.\nWork is now under way at the Microsoft Corporation in Bellevue, Wash., on a system that monitors the user's every move, looking to detect patterns. In time, the program would begin to catch on. It would suggest short cuts, reminding the user that the spreadsheet program, for example, includes a feature that copies a column of numbers so that time does not have to be wasted typing them in again.\nOr it might catch an inadvertent omission that deviates from the user's ordinary pattern, saving the user embarrassment. ''Hey, idiot,'' a particularly caustic program might type across the screen, ''don't you usually send a copy of this report to the slob who sits in the next office?''\n\n\nSuch a program would be a ''passive learner,'' and the risk is that it could pick up bad habits or discover patterns of activity that lead it astray. Thus, in the near future, it seems more likely that programs will have ''active learning'' capability: The user would describe his usual routine to the program, and it would begin to perform those functions automatically.\n''Consider the case of a commercial lending officer at a bank,'' said Peter Gabel, a Massachusetts Institute of Technology graduate who left Lotus a year ago to form Arity, a software company in Concord, Mass. ''He is looking for patterns that could mean trouble - bounced checks, a recent change in chief financial officers or a change of accounting firms. Individually, they are nothing; collectively, they could prove very expensive to miss.''\nArity's first product, which Mr. Gabel hints will be a ''smart desk assistant,'' is expected in the fall, and Lotus has already invested in the company. But most in the industry expect that artificial intelligence techniques will not create new products, but improve old ones - perhaps including Lotus's immensely popular 1-2-3 spreadsheet.\nSome at the Phoenix conference suggested, only half in jest, that software makers will eventually package their programs like detergent: ''New! Improved! With A.I.!''\n","542":"OTTAWA -- The mayor of the only non-American city on the list of 20 finalists for Amazon's second headquarters displayed some typical Canadian modesty on Thursday. Toronto, John Tory said, had only ''made the playoffs.''\nBut regardless of the outcome, the announcement that the city remains a contender showed how much progress Toronto, and the surrounding region, have made in establishing themselves as a major technology center. \u00a0\n  Nine communities in the Toronto region put forward a joint bid for the headquarters. Those towns are already home to a major Google engineering operation, a major artificial intelligence research center and a quantum computing institute. General Motors is adding about 1,000 software engineers to the area who will, among other things, develop systems for self-driving cars, and Thomson Reuters has made Toronto a technology hub, a move that could create 1,500 jobs. And a unit of Google's parent company, Alphabet, is developing plans to turn a derelict portion of Toronto's waterfront into a high-tech city of the future.\n  Amazon offered few details on Thursday about how it had come up with its 20 finalists. Unlike several other finalists, the region offered no tax or other incentives. But Mr. Tory said he believed that the area had two important virtues.\n  One is Canada's immigration policy. Mr. Tory said that when he was recently in New York, he found a great deal of interest among American executives in Canada's relatively new immigration program that gives visas to certain skilled workers within two weeks. That's light speed compared with the complicated American system. And unlike the United States, Canada does not limit how many of those visas can be issued each year.\n  In sharp contrast to President Trump's efforts to limit the entry of people from some predominately Muslim countries into the United States, Prime Minister Justin Trudeau has repeatedly emphasized that Canada is open to people of all religions and backgrounds.\n  ''We're able to attract the best and the brightest from around the world,'' Mr. Tory said.\n  Toronto's second asset, he said, is its publicly funded university and college system. The University of Waterloo has long been recognized as one of North America's top technology schools, and the University of Toronto is a major center for research in artificial intelligence. As part of the area's Amazon pitch, the province of Ontario has increased funding for artificial intelligence programs at its universities by 30 million Canadian dollars, or about $24 million.\n  ''We have a talent pool, and we have the educational policies to make sure the pipeline is full,'' Mr. Tory said.\n  But the Toronto area also has a potentially unattractive feature: ever-escalating prices both for homes and for commercial real estate. Its bid proposes several potential sites for Amazon's second headquarters, among them the largely abandoned docklands that include the Google-related redevelopment plan.\n  Frank Scarpitti, the mayor of Markham, Ontario, which is already home to operations of IBM and the chip maker AMD, dismissed the idea that land and housing prices could be a deal breaker. He noted that several other finalists, including New York, had similar or higher costs.\n  Soon after the announcement, Kathleen Wynne, the premier of Ontario, appointed Ed Clark, the former president and chief executive of the Toronto-Dominion Bank, to coordinate the province's reply.\n  Mr. Tory said the cities would probably have to wait for Amazon's next move before taking any action of their own.\n  ''We don't know what they're expecting from us,'' he said. ''There has been no playbook or playoff schedule supplied to the 20 finalists.''\n\n\n\n","544":"It's become hip lately to bash the tech industry, and especially those of us who work in Silicon Valley. Current technologies, including social media and news sites, as well as emerging innovations in artificial intelligence, 5G mobile networks, data analytics, autonomous vehicles and the Internet of Things, are being blamed for everything from skewing election results to destroying culture, displacing workers and monopolizing, well, everything.\nAttacks are coming from the left and the right, from traditional media, think tanks, and even from some notable tech elites, including Tesla's Elon Musk, Skype co-founder Jaan Tallinn, and eBay founder Pierre Omidyar.\nFormer White House adviser Stephen K. Bannon, for example, is leading a war against what he calls Silicon Valley's \"Lords of Technology,\" while Omidyar is arguing that social media has become a \"direct threat\" to democracy. Musk is denouncing advances in artificial intelligence as a \"fundamental risk to the existence of human civilization.\" And the New York Times, which recently published a long magazine piece entitled \"Silicon Valley is Not Your Friend,\" regularly refers to Apple, Amazon, Facebook, Google and Microsoft as \"the Frightful Five.\"\u00a0\nIn some ways, there's nothing new here. Fear of disruptive innovations goes back at least to the dawn of the industrial revolution, when a late 18th century textile worker named Ned Ludd, suffering perhaps from mental illness, smashed his knitting machines. Anti-technology activists ever since have been known as Luddites.\nBut while outbreaks of tech dystopias come and go, there's something more virulent about the current strain. Its sources are both complicated and cumulative. One source is displaced anger over recent election results, including the Brexit vote and the 2016 U.S. presidential election. That anger is being fed by smug posturing from some of Silicon Valley's older, idle billionaires, wringing their hands about the irresponsibility of a new generation of entrepreneurs - who are, by and large, more thoughtful than the billionaires were.\nBut the real driver of unrest is the speed with which these changes are occurring and the sheer scale of new technologies entering commercial use. The ongoing revolution in computing, in which core components continue to get faster, cheaper and smaller, invariably accelerates stress on individuals and institutions. Twenty years ago, I referred to that phenomenon as The Law of Disruption: technology changes exponentially, while humans change incrementally.\nFor pundits and politicians, stoking the flames of tech dysphoria is all fun and games. But there's a very real cost to the vacuous doom-and-gloom, which may delay or even destroy substantial social benefits now due on our long-term investments in disruption.\nAutonomous vehicles, for example, have the potential to save 30,000 lives lost every year to human error in the United States alone. The Internet of Things, likewise, will make it possible for an aging population to stay in their homes. Drones and analytics can revolutionize farming and emergency services. Smart cities and infrastructure will reduce traffic and reduce energy use. 3-D printing is already changing health-care outcomes with customized prosthetics and the possibility of replacement organs.\nIt's true that many of these emerging technologies challenge fundamental social norms, including urban development, the nature of full-time employment, and the relationship between individuals and society in an increasingly transparent world of data.\nIn some cases - drones, genetic engineering, alternative energy sources - there's clearly a role for targeted regulation and oversight. But most calling for government intervention just leave it at that, expecting that lawmakers will just do something - anything! - to cure our malaise, whatever the collateral damage.\nTheir tired platitudes, punctuated by vague calls to \"regulate\" or arbitrarily \"break up\" the companies experimenting with these applications, may crash the party before it's barely gotten started. The European Union is leading the wrong-way charge here, citing its goal of a Digital Single Market as cover for punishing successful U.S. Internet companies with giant fines and impossible legal mandates.\nEven here in California, the capital of tech innovation, Gov. Jerry Brown recently vetoed a bill that would have simplified and standardized deployment of communications equipment essential for next-generation 5G mobile networks. 5G will deliver wireless speeds that surpass today's fastest networks, using smaller, low-powered equipment that doesn't require new cell towers. The most promising applications of tomorrow will need it to deliver volumes of data with both speed and reliability.\nThe California bill passed with overwhelming bipartisan majorities in the legislature, but was opposed by local authorities who see dollar signs in inflated rental fees for attaching the new equipment to city-owned utility poles and buildings - a long-standing source of sometimes not-so-petty graft.\nOther, more vocal opponents of the bill, echoing growing anti-development sentiment, simply don't want the new networks at all.  They don't want private investment or the jobs that go with it; they don't want higher speeds, greater reliability, and the chance to close what remains of the digital divide.\nAs one of the \"community\" groups demanding Brown's veto proclaimed, \"I am thrilled that Governor Brown showed strength and stood up to this powerful wireless industry and said no - you are not going to do this in my state!\"\nLuddism in Silicon Valley's own not-in-my-backyard, of course, creates opportunities for other states to take the lead, collaborating with technology developers to create the kind of environment California once had that encouraged innovation.\nMany regions are working to emulate the model invoked by Google and now Amazon, where cities, hospitals, universities and other anchor institutions propose themselves for new investments, offering not financial incentives so much as streamlined zoning, permits and review - precisely what the vetoed bill required.\n(Amazon chief executive Jeffrey P. Bezos owns The Washington Post.)\nLast week, I detailed that strategy to a conference of state lawmakers outside California, citing Brown's veto of the 5G bill as both an omen and an opening.  Many - Democrats and Republicans - have since followed up to ask how they can jump-start technology disruption in their states. It's not as much fun as pressing the panic button on the future. But it may actually improve the lives of their constituents.\nStill, if the rest of the United States does catch the anti-tech virus, there are plenty of other countries who will gladly step into the driverless seat.\nEvery year, venture capital analyst Mary Meeker lists the most valuable Internet companies in the world, which last year approached $4 trillion in value, nearly all of it created in the last two decades. The United States continues to dominate the list, with 12 of the 20 leaders. (Note to Gov. Brown: 10 are headquartered in California.)\nOver-regulated Europe has no companies on the list, and seems unlikely to create any. But every year, more and more Chinese companies appear. Perhaps that's because Chinese consumers, business leaders, and the government - despite its own anxiety about the Internet - share a common belief that disruptive innovation offers China its best chance for entering new markets.  And dominating them.\nJust saying.\n","545":"Although he is blind, James Slagle may have an edge over most chess players.  As a computer scientist who works with artificial intelligence, Slagle studies decision-making processes.\n\"I've certainly thought about how people think and how they think up chess moves.  In artificial intelligences I try to figure out how a smart person would solve a problem.  That type of thought process-carries over when I play chess,\" said Slagle, 43, a resident of Marlow Heights and father of five children.\u00a0\nThis kind of thinking heiped launch Slagle, who is head of the Naval Research Laboratory's computer science laboratory and president of the U.S.  Brraille chess championship, besting 15 players, including the three top rated blind players in the country.  He will represent the United States next spring at the International Braille Tournament in Belgium.\nThe slender, tanned scientist graduated summacum laude in mathematics from St. John's University in Brooklyn, then went on to get a masters and doctorate in math from Massachusettes Institute of Technology.\nMath and chess go hand in hand, he said, because, \"they're both logical and require planning, thinking ahead, keeping well organized and an ability to manage a lot of details.\"\nOrganization is one of his strengths.  It helped him through college, where, unable to see the complicated mathematical equations the professors wrote on the blackboards.  Slagle taped lectures then methodically went over them after class.\nHis organizational skills also helped when Slagle was a math instructor at MIT and Johns Hopkins University.  Knowing that his lectures were designed more for the ear than for eye, because he could not rely on the blackboard, Slagle passed out worksheets to help his students follow his thoughts.\nAsked if he ever feared that his blindness would prevent him from becoming a mathematician, Slagle said, \"Until I met some failures, I wasn't going to assume I was going to fail.\" Did he ever fail?Slagle's \"no\" was quick.  \"People were kind,\" he added, speaking of the scholarship he received to undergraduate school, and the job he was offered at MIT's computer science lab when he was one of the few persons at his college who expressed an interest in the fledging field.\nThe job at MIT exposed him to pioneers in artificial intelligence and led him to seek higher degrees with an emphasis on that aspect of mathematics.  Artificial intelligence is no the far fringes of computer science and includes projects that sound much like science fiction.\nFor example, Slagle is now working on developing a computer-operated mechanical arm that would perform rescue operations on the seas.\nAs part of their research, scientists sometimess problems.  While this not a method Slagle uses, he is familiar with it since such chess problems are often included in the professional magazines he reads.\nHe even played chess with a computer once, and won easily.  \"It wasn't the best computer,\" he said, \"It didn't give me much of a tussle.\"\nWhat is difficult, Slagle says, is playing against sighted players.  The main problem is that it taken blind players longer to make moves, since they have to touch the pieces in order to ascertain their directions.\nThat factor is taken into account in tournament for the blind, which usually have longer [WORD ILLEGIBLE] playing times than regular tournaments.  In regular tournaments, no such concession is made for both players.\nSometimes being blind can be an advantage, such as when an opponent relies on body language for threatening gestures to intimidate, Slagle said Laughing, he told of an incident when his chess clipp played at a federal prison.\n\"I played the fellow and after we left, a teammate said, \"Good thing you couldn't see that guy, Jim. He didn't just sit on the chair.  He was a big, hulking player who crouched over the table.\" Slagle won that day.\nIt has been only inthe last 10 years that Slagle has become a true chess buff.  He learned the sport when he was 8 - the same age that began to slowly go blindbecause of an incurable disease.\nDuring an adolescence crammed with backyard basketball, before his vision completely dimmed, and study, and during graduate school where he combined a job with study, Slagle had little time, for chess.\nHe did hava a special set for blind players-wooden board on which the black squares are raised.  It gathered dust most of the time, however, until Slagle met another blind chess who was starting a club for the blind.\nSlage urges other blind persons who are interested in chess to contact him at his home: 4101 Holly Tree Rd., Marlow Heights.\n","546":" In Eric E. Schmidt's future, his life will be a lot easier.\nHis bed will wake him up when he cycles out of R.E.M. sleep. A driverless car will take him to work. Returning phone calls, scheduling events and other routine tasks will be taken care of by devices using artificial intelligence. A microrobot he swallows will monitor his insides and alert his doctor if something is wrong. At night, a robot will go to parties in his place.\n\"He'll have a good time and report back in the morning,\" said Mr. Schmidt, executive chairman of Google, speaking Monday at the company's Zeitgeist sales conference in Paradise Valley, Ariz.\nMr. Schmidt was describing to Google's biggest advertisers and partners how technology can change the world, from tracking financial assets to education to measuring the results of nonprofit initiatives. Some of the things he mentioned, like universal language translation and artificial intelligence, are Google projects.\nMore important than making his daily life easier, he said, technology will open up the world to people who have never been connected, like those in developing countries.\n\"Imagine going from no information to all the world's information with one device,\" he said. \"No textbooks to all textbooks. No language to all language.\"\nFor companies like Google, billions of people newly connected also means a new source of talent, Mr. Schmidt said.\n\"Maybe Botswana has the next generation of great U.I. designers,\" he said. \"We don't know because we haven't explored.\"\nGoogle and other tech companies will have succeeded, he said, when none of this seems magical, or even visible, because technology will imbue everything in our lives, which is a theme of Google's. \nIts Internet-connected eyeglass frames are the first step, though for now they stand out, as they did on the faces of the couple of dozen people wearing them at the conference while socializing at cocktail parties and sitting by the pool, including Google's co-founder, Sergey Brin.\n\"Eventually technology just disappears,\" Mr. Schmidt said. \"It's the ultimate achievement. No more ports and prompts and plug-ins.\"\n\n","547":"To the Editor:\nRegarding John Horgan's Op-Ed piece about the disappointment of artificial intelligence (\"Smarter Than Us? Who's Us?\" May 4), it is clear to me that the researcher Marvin Minsky has based his work on a flawed premise: that since the human mind reasons with symbols, and a computer also reasons with symbols, then the computer can replicate the processes of the human mind.\u00a0\n It is true that humans use symbols to reason, but we are trained to do so, making symbols out of images. Fundamentally, the human mind is a processor of images which it combines with other information. The computer, on the other hand, is fundamentally a processor of symbols, and it creates images out of symbols.\nUntil we have a computer design that is primarily an image processor, it will continue to lag far behind the human mind in all aspects of what we call thinking. The computer can win the game of tic-tac-toe because playing the game can be reduced to the use of symbolic logic.  The game of chess differs from tic-tac-toe only in its complexity, and for the computer to play it well says little about its capability to think like a human.\u00a0RICHARD TEN DYKEPound Ridge, N.Y., May 4, 1997\u00a0The writer studies artificial intelligence.\n","548":"MIT researchers have developed a computer system that independently adds realistic sounds to silent videos. Although the technology is nascent, it's a step toward automating sound effects for movies.\u00a0\nIn a series of videos of drumsticks striking things - including sidewalks, grass and metal surfaces - the computer learned to pair a fitting sound effect, such as the sound of a drumstick hitting a piece of wood or  rustling leaves.\nThe findings are an example of the power of deep learning, a type of artificial intelligence whose application is trendy in tech circles. With deep learning, a computer system learns to recognize patterns in huge piles of data and applies what it learns in useful ways.\nIn this case, the researchers at MIT's Computer Science and Artificial Intelligence Lab recorded about 1,000 videos of a drumstick scraping and hitting real-world objects. These videos were fed to the computer system, which learns what sounds are associated with various actions and surfaces. The sound of the drumstick hitting a piece of wood is different from when it disrupts a pile of leaves.\nOnce the computer system had all these examples, the researchers gave it silent videos of the same drumstick hitting other surfaces, and they instructed the computer system to pair an appropriate sound with the video.\nTo do this, the computer selects a pitch and loudness that fits what it sees in the video, and it finds an appropriate sound clip in its database to play with the video.\nTo demonstrate their accomplishment, the researcher then played half-second video clips for test subjects, who struggled to tell apart whether the clips included an authentic sound or one that a computer system had added artificially.\nBut the technology is not perfect, as MIT PhD candidate Andrew Owens, the lead author on the research, acknowledged. When the team tried longer video clips, the computer system would sometimes misfire and play a sound when the drumstick was not striking anything. Test subjects immediately knew the audio was not real.\nAnd the researchers were able to get the computer to produce fitting sounds only when they used videos with a drumstick. Creating a computer that automatically provides the best sound effect for any video - the kind of development that could disrupt the sound-effects industry - remains out of reach for now.\nAlthough the technology world has seen significant strides of late in artificial intelligence, there are still big differences in how humans and machines learn. Owens wants to push computer systems to learn more similarly to the way an infant learns about the world: by physically poking and prodding its environment. He sees potential for other researchers to use sound recordings and interactions with materials such as sidewalk cement as a step toward machines' better understanding our physical world.\nmatt.mcfarland@washpost.com\n","550":"British scientists say they have developed a robotic system that for the first time can design a genetics experiment, carry it out and interpret the results.\n     No difference was found between the laboratory bench results generated by the robot scientist and those gathered by graduate students doing similar work, the researchers report today in the journal Nature. \n While the system remains in its infancy, they hope it will someday conduct laboratory-intensive work, freeing researchers from drudgery.\u00a0\n\"The sort of grunt research can be done this way, and more creative stuff humans will have more time to do,\" said the study's author, Stephen Oliver of the University of Manchester.\nOther researchers described the robot as a harbinger of the future but said more sophisticated reasoning software had to be developed.\nOnce that happens, laboratories would adopt such advanced artificial intelligence systems \"pretty rapidly and pervasively,\" said Larry Hunter, a computational biology expert at the University of Colorado School of Medicine, who was not involved in the experiment.\nThe robotic system was designed to determine the function of baker's yeast genes. About 30 percent of the yeast's 6,000 genes are unknown, but scientists believe they may be shared in the human genome and might someday be medically important.\nTo determine functions of the genes in question, the experiments used \"knockout\" varieties in which a specific gene is removed. By determining how the yeast sample grows, the function of the missing gene can be determined.\nIn the automated experiments, the researchers first developed a mathematical model showing how various genes, proteins and enzymes and growth mediums interact.\nArmed with that knowledge, the robot independently generated hypotheses about the missing genes, then used equipment to grow yeast strains.\nLater the growth of each strain was evaluated against the original hypothesis.\nThe process was repeated over and over as the system developed new hypotheses based on the accumulating data.\n\"It's like if you have a machine which is broken, the system can automatically reason to find all the possible ways it can be broken,\" said Ross King of the University of Wales, Aberystwyth. \"Some philosophers have thought this is impossible for computers because that's the imaginative leap.\"\nThe robot scientist uses a type of reasoning called abduction. Dr. King said it was the kind of reasoning the police used to reconcile clues when investigating a crime.\n\"If this person committed the crime, all the clues make sense,\" Dr. King said.\nDr. Hunter said the new work was the first in which experimental design, computer control of instruments and analysis of the resulting data had been \"hooked together in a closed loop.\"\n\"It is now possible to design artificial intelligence systems that are able to reason well enough to be effective partners in scientific research,\" Dr. Hunter said.\nDr. Oliver said the next step was to see whether the robot could make a completely novel discovery rather than simply match the graduate students' results. \n","551":"Richard Socher appeared nervous as he waited for his artificial intelligence program to answer a simple question: ''Is the tennis player wearing a cap?''\nThe word ''processing'' lingered on his laptop's display for what felt like an eternity. Then the program offered the answer a human might have given instantly: ''Yes.'' \n  Mr. Socher, who clenched his fist to celebrate his small victory, is the founder of one of a torrent of Silicon Valley start-ups intent on pushing variations of a new generation of pattern recognition software, which, when combined with increasingly vast sets of data, is revitalizing the field of artificial intelligence.\u00a0\n  His company MetaMind, which is in crowded offices just off the Stanford University campus in Palo Alto, Calif., was founded in 2014 with $8 million in financial backing from Marc Benioff, chief executive of the business software company Salesforce, and the venture capitalist Vinod Khosla.\n  MetaMind is now focusing on one of the most daunting challenges facing A.I. software. Computers are already on their way to identifying objects in digital images or converting sounds uttered by human voices into natural language. But the field of artificial intelligence has largely stumbled in giving computers the ability to reason in ways that mimic human thought.\n  Now a variety of machine intelligence software approaches known as ''deep learning'' or ''deep neural nets'' are taking baby steps toward solving problems like a human.\n  On Sunday, MetaMind published a paper describing advances its researchers have made in creating software capable of answering questions about the contents of both textual documents and digital images.\n  The new research is intriguing because it indicates that steady progress is being made toward ''conversational'' agents that can interact with humans. The MetaMind results also underscore how far researchers have to go to match human capabilities.\n  Other groups have previously made progress on discrete problems, but generalized systems that approach human levels of understanding and reasoning have not been developed.\n  Five years ago, IBM's Watson system demonstrated that it was possible to outperform humans on ''Jeopardy!''\n  Last year, Microsoft developed a ''chatbot'' program known as Xiaoice (pronounced Shao-ice) that is designed to engage humans in extended conversation on a diverse set of general topics.\n  To add to Xiaoice's ability to offer realistic replies, the company developed a huge library of human question-and-answer interactions mined from social media sites in China. This made it possible for the program to respond convincingly to typed questions or statements from users.\n  In 2014, computer scientists at Google, Stanford and other research groups made significant advances in what is described as ''scene understanding,'' the ability to understand and describe a scene or picture in natural language, by combining the output of different types of deep neural net programs.\n  These programs were trained on images that humans had previously described. The approach made it possible for the software to examine a new image and describe it with a natural-language sentence.\n  While even machine vision is not yet a solved problem, steady, if incremental, progress continues to be made by start-ups like Mr. Socher's; giant technology companies such as Facebook, Microsoft and Google; and dozens of research groups.\n  In their recent paper, the MetaMind researchers argue that the company's approach, known as a dynamic memory network, holds out the possibility of simultaneously processing inputs including sound, sight and text.\n  The design of MetaMind software is evidence that neural network software technologies are becoming more sophisticated, in this case by adding the ability both to remember a sequence of statements and to focus on portions of an image. For example, a question like ''What is the pattern on the cat's fur on its tail?'' might yield the answer ''stripes'' and show that the program had focused only on the cat's tail to arrive at its answer.\n  ''Another step toward really understanding images is, are you actually able to answer questions that have a right or wrong answer?'' Mr. Socher said.\n  MetaMind is using the technology for commercial applications like automated customer support, he said. For example, insurance companies have asked if the MetaMind technology could respond to an email with an attached photo -- perhaps of damage to a car or other property -- he said.\n  There is still significant debate within the research community about the best technical approach and even what is the best way to measure progress.\n  ''We are excited to see them joining the fray in question answering, but we think the data sets they chose are not ideal,'' said Oren Etzioni, a computer scientist who is chief executive of the Allen  Institute for Artificial Intelligence, in Seattle.\n  In contrast, his laboratory is focusing on creating software that can answer questions taken from standardized elementary school science tests.\n\n\n\n","552":"When computer models designed by tech giants Alibaba and Microsoft this month surpassed humans for the first time in a reading-comprehension test, both companies celebrated the success as a historic milestone.\nLuo Si, the chief scientist for natural-language processing at Alibaba's AI research unit, struck a poetic note, saying, \"Objective questions such as 'what causes rain' can now be answered with high accuracy by machines.\"\nTeaching a computer to read has for decades been one of artificial intelligence's holiest grails, and the feat seemed to signal a coming future in which AI could understand words and process meaning with the same fluidity humans take for granted every day.\u00a0\nBut computers aren't there yet - and aren't even really that close, said AI experts who reviewed the test results. Instead, the accomplishment highlights not just how far the technology has progressed, but also how far it still has to go.\n\"It's a large step\" for the companies' marketing \"but a small step for humankind,\" said Oren Etzioni, chief executive of the Allen Institute for Artificial Intelligence, an AI research group funded by Microsoft co-founder Paul Allen.\n\"These systems are brittle, in that small changes to paragraphs result in very bad behavior\" and misunderstandings, Etzioni said. And when it comes to, say, drawing conclusions from two sentences or understanding implied ideas, the models lag even further behind. \"These kind of implications that we do naturally, without even thinking about it, these systems don't do,\" he said.\nThe test involved Stanford University's Question Answering Dataset, a collection of more than 100,000 questions that has become one of the AI world's top battlegrounds for testing how machines read and comprehend.\nThe models are given short paragraphs taken from more than 500 Wikipedia pages spanning a range of subjects, including economic inequality, the Black Death, and Jacksonville, Fla. Fed a paragraph about Super Bowl 50, for instance, the models are then asked which musicians headlined the halftime show.\nThe first test, in August 2016, of a model created by researchers at Singapore Management University, lagged behind a measure of human performance - people on crowdsourced systems, such as Amazon's Mechanical Turk, who earned money for taking surveys or completing small tasks.\nBut after dozens of following tests, researchers this month submitted proof that their models had narrowly and finally beaten the humans - an 82.6 for Microsoft Research Asia's models, compared with the humans' 82.3.\nAs both Microsoft and the Chinese tech powerhouse Alibaba claimed such first-in-AI victories, a flood of glowing media reports followed, positing that AI could not just read better than humans but would also, as Luo Si said in a statement, decrease \"the need for human input in an unprecedented way.\"\nMicrosoft said it is using similar models in its Bing search engine, and Alibaba said its technology could be used for \"customer service, museum tutorials and online responses to medical inquiries.\"\nBut AI experts say the test is far too limited to compare with real reading. The answers aren't generated from understanding the text, but from the system finding patterns and matching terms in the same short passage. The test was done only on cleanly formatted Wikipedia articles - not the wide-ranging corpus of books, news articles and billboards that fill most humans' waking hours.\nAdding gibberish into the passages, which a human would easily ignore, tended to confuse the AI, making it spit out the wrong result. And every passage was guaranteed to include the answer, preventing the models from having to process concepts or reason with other ideas.\nEven Pranav Rajpurkar, a Stanford AI researcher who helped design the Stanford test, said there remains \"actually quite a big jump\" before machines can truly read and understand.\n\"The goal has always been to get to human-level performance, and it's been inching closer and closer there,\" Rajpurkar said.\nThe real miracle of reading comprehension, AI experts said, is in reading between the lines - connecting concepts, reasoning with ideas and understanding implied messages that aren't specifically outlined in the text.\nIn those realms, AI is still very much a work in progress. Computer models tested by the Winograd Schema Challenge, which asks them to comprehend the meaning of vague sentences that a human would nevertheless understand, have shown mixed results. AI researcher Stephen Merity of cloud-computing giant Salesforce outlined one example today's systems might still struggle to reasonably comprehend: asking the difference between a car \"filled with gas,\" \"filled with petrol\" and \"filled with oranges.\"\nAI researchers said they're eager to push on to new challenges of comprehension beyond basic Wiki-reading. The Allen Institute, for example, is training AI to answer SAT-style math problems and middle-school-level science questions.\nBut AI experts said people should note be concerned about losing their jobs to machines that thoughtfully read passages about the rain - or anything else.\n\"Technically it's an accomplishment, but it's not like we have to begin worshiping our robot overlords,\" said Ernest Davis, a New York University professor of computer science and longtime AI researcher.\ndrew.harwell@washpost.com\n","555":"Impressive advances in artificial intelligence technology tailored for legal work have led some lawyers to worry that their profession may be Silicon Valley's next victim.\nBut recent research and even the people working on the software meant to automate legal work say the adoption of A.I. in law firms will be a slow, task-by-task process. In other words, like it or not, a robot is not about to replace your lawyer. At least, not anytime soon. \n  ''There is this popular view that if you can automate one piece of the work, the rest of the job is toast,'' said Frank Levy, a labor economist at the Massachusetts Institute of Technology. ''That's just not true, or only rarely the case.''\n  An artificial intelligence technique called natural language processing has proved useful in scanning and predicting what documents will be relevant to a case, for example. Yet other lawyers' tasks, like advising clients, writing legal briefs, negotiating and appearing in court, seem beyond the reach of computerization, for a while.\u00a0\n  ''Where the technology is going to be in three to five years is the really interesting question,'' said Ben Allgrove, a partner at Baker McKenzie, a firm with 4,600 lawyers. ''And the honest answer is we don't know.''\n  Dana Remus, a professor at the University of North Carolina School of Law, and Mr. Levy studied the automation threat to the work of lawyers at large law firms. Their paper concluded that putting all new legal technology in place immediately would result in an estimated 13 percent decline in lawyers' hours.\n  A more realistic adoption rate would cut hours worked by lawyers by 2.5 percent annually over five years, the paper said. The research also suggests that basic document review has already been outsourced or automated at large law firms, with only 4 percent of lawyers' time now spent on that task.\n  Their gradualist conclusion is echoed in broader research on jobs and technology. In January, the McKinsey Global Institute found that while nearly half of all tasks could be automated with current technology, only 5 percent of jobs could be entirely automated. Applying its definition of current technology -- widely available or at least being tested in a lab -- McKinsey estimates that 23 percent of a lawyer's job can be automated.\n  Technology will unbundle aspects of legal work over the next decade or two rather than the next year or two, legal experts say. Highly paid lawyers will spend their time on work on the upper rungs of the legal task ladder. Other legal services will be performed by nonlawyers -- the legal equivalent of nurse practitioners -- or by technology.\n  Corporate clients often are no longer willing to pay high hourly rates to law firms for junior lawyers to do routine work. Those tasks are already being automated and outsourced, both by the firms themselves and by outside suppliers like Axiom, Thomson Reuters, Elevate and the Big Four accounting firms.\n  So the law firm partner of the future will be the leader of a team, ''and more than one of the players will be a machine,'' said Michael Mills, a lawyer and chief strategy officer of a legal technology start-up called Neota Logic.\n   Surprising Spread\n  The pace of technology improvement is notoriously unpredictable. For years, labor economists said routine work like a factory job could be reduced to a set of rules that could be computerized. They assumed that professionals, like lawyers, were safe because their work was wrapped in language.\n  But advances in artificial intelligence overturned that assumption. Technology unlocked the routine task of sifting through documents, looking for relevant passages.\n  So major law firms, sensing the long-term risk, are undertaking initiatives to understand the emerging technology and adapt and exploit it.\n  Dentons, a global law firm with more than 7,000 lawyers, established an innovation and venture arm, Nextlaw Labs, in 2015. Besides monitoring the latest technology, the unit has invested in seven legal technology start-ups.\n  ''Our industry is being disrupted, and we should do some of that ourselves, not just be a victim of it,'' John Fernandez, chief innovation officer of Dentons, said.\n  Last month, Baker McKenzie set up an innovation committee of senior partners to track emerging legal technology and set strategy. Artificial intelligence has stirred great interest, but law firms today are using it mainly in ''search-and-find type tasks'' in electronic discovery, due diligence and contract review, Mr. Allgrove said.\n  More than 280 legal technology start-ups have raised $757 million since 2012, according to the research firm CB Insights.\n  At many of these start-ups, the progress is encouraging but measured, and each has typically focused on a specific area of law, like bankruptcy or patents, or on a certain legal task, like contract review. Their software learns over time, but only after it has been painstakingly trained by human experts.\n  When Alexander Hudek, a computer scientist whose r\u00e9sum\u00e9 includes heavyweight research like working on the human genome project, turned to automating the review of legal contracts in 2011, he figured that he would tweak standard algorithms and that it would be a four-month job.\n  Instead, it took two and a half years to refine the software so it could readily identify concepts such as noncompete contract clauses and change-of-control, said Mr. Hudek, chief technology officer of Kira Systems.\n  The Kira program sharply winnows the number of documents read by people, but human scrutiny is still required.\n  Yet the efficiency gains can be striking. Kira's clients report reducing the lawyer time required for contract review by 20 percent to 60 percent, said Noah Waisberg, chief executive of Kira.\n  In Miami, Luis Salazar, a partner in a five-lawyer firm, began using software from the start-up Ross Intelligence in November in his bankruptcy practice. Ask for the case most similar to the one you have and the Ross program, which taps some of IBM's Watson artificial intelligence technology, reads through thousands of cases and delivers a ranked list of the most relevant ones, Mr. Salazar said.\n  Skeptical at first, he tested Ross against himself. After 10 hours of searching online legal databases, he found a case whose facts nearly mirrored the one he was working on. Ross found that case almost instantly.\n  Mr. Salazar has been particularly impressed by a legal memo service that Ross is developing. Type in a legal question and Ross replies a day later with a few paragraphs summarizing the answer and a two-page explanatory memo.\n  The results, he said, are indistinguishable from a memo written by a lawyer. ''That blew me away,'' Mr. Salazar said. ''It's kind of scary. If it gets better, a lot of people could lose their jobs.''\n  Not yet. The system is pretty good at identifying the gist of questions and cases, but Ross is not much of a writer, said Jimoh Ovbiagele, the chief technology officer of Ross. Humans take the rough draft that Ross produces and create the final memos, which is why it takes a day.\n  The start-up's engineers are trying to fully automate the memo-writing process, but Mr. Ovbiagele said, ''We are a long way from there at this point.''\n  The Good Old Days\n  James Yoon, a lawyer in Palo Alto, Calif., recalls 1999 as the peak of the old way of lawyering. A big patent case then, he said, might have needed the labor of three partners, five associates and four paralegals.\n  Today, a comparable case would take one partner, two associates and one paralegal.\n  Two obvious factors have led to that downsizing: tightened legal spending and digital technologies that automated some tasks, like document searches, said Mr. Yoon, a partner at Wilson Sonsini Goodrich & Rosati.\n  Mr. Yoon uses software tools like Lex Machina and Ravel Law to guide litigation strategy in his patent cases. These programs pore through court decisions and filing data to make profiles and predictions about judges and lawyers.\n  What are the chances a certain motion will be approved by a particular judge, based on all his or her past rulings? Does the opposing counsel go to trial often or usually settle cases?\n  Mr. Yoon compares what he does to the way baseball and football analysts assess the tendencies of players and coaches on other teams.\n  The clever software, he said, is ''changing how decisions are made, and it's changing the profession.''\n  But its impact on employment would seem to be far less than, say, electronic discovery. The data-driven analysis technology is assisting human work rather than replacing it. Indeed, the work that consumes most of Mr. Yoon's time involves strategy, creativity, judgment and empathy -- and those efforts cannot yet be automated.\n  Mr. Yoon, who is 49, stands as proof. In 1999, his billing rate was $400 an hour. Today, he bills at $1,100 an hour.\n  ''For the time being, experience like mine is something people are willing to pay for,'' Mr. Yoon said. ''What clients don't want to pay for is any routine work.''\n  But, he added, ''the trouble is that technology makes more and more work routine.''\n\n\n\n","556":"BERLIN -- Joseph Schlesinger, an engineer living near Boston, thinks robotic toys are too expensive, the result of extravagant designs, expensive components and a poor understanding of consumer tastes. So this year, Mr. Schlesinger, 23, began to manufacture an affordable robot, one he is selling for $250 to holiday shoppers.\nHis creation, the Hexy, is a six-legged, crablike creature that can navigate its own environment and respond to humans with a hand wave or other programmable gesture. Mr. Schlesinger said he had been able to lower production costs by using free software and by molding a lot of the plastic parts locally in Massachusetts, not in China.\nSince setting up his company, ArcBotics, in suburban Somerville, Massachusetts, Mr. Schlesinger has built a backlog of more than 1,000 orders. His goal, he said, was to become ''the Ikea of robotics.''\u00a0\n''I think the market for consumer robotics is poised to explode,'' said Mr. Schlesinger, a graduate of Worcester Polytechnic Institute in Massachusetts. ''We are only at the beginning.''\nSince the 1960s, robots have assumed major roles in industrial manufacturing and assembly, the remote detonation of explosives, search and rescue, and academic research. But the devices have remained out of reach, in affordability and practicality, to most consumers.\nThat, according to Professor Andrew Ng, the director of the Artificial Intelligence Lab at Stanford University in California, is about to change. One big reason, Mr. Ng said, is the mass production of smartphones and game consoles, which has driven down the size and price of robotic building blocks like accelerometers, gyroscopes and sensors.\nOn the edges of consumer consciousness, the first generation of devices with rudimentary artificial intelligence are beginning to appear: entertainment and educational robots like the Hexy, and a line of tireless household drones that can mow lawns, sweep floors, clean swimming pools and even enhance golf games.\n''I'm seeing a huge explosion of robotic toys and believe that there will be one soon in industry,'' said Mr. Ng, an associated professor of computer science at Stanford.\nThe most advanced robots remain exotic workhorses like NASA's Mars Curiosity Rover, which cost $2.5 billion, and the LS3, a doglike robot being developed for the U.S. military that can carry a 400-pound, or 180-kilogram load more than 20 miles, or about 30 kilometers. The mechanical beast of burden, whose price is not public, is being made by a consortium led by Boston Dynamics. In Menlo Park, California, engineers at Willow Garage, a robotics firm, are selling the two-armed, 5-foot-4 inch (1.63-meter) rolling robot called the PR2 for $400,000.\nA video on Willow Garage's Web site shows the PR2 fetching beer from a refrigerator, which while an engineering and programming feat, is an expensive way to get beer.\n''I think we're still some years away from useful personal robots making pervasive appearances in our homes,'' Mr. Ng said.\nRight now, for the masses, there is the CaddyTrek, a robotic golf club carrier that follows a player from tee to fairway to green through tall grass, up 30-degree slopes and in snow, for as many as 27 holes on a single charge. Players wear a remote control on their belts, which acts as a homing beacon for the self-propelled cart, which trails six paces behind the player.\nGolfers can also navigate the robotic cart, which is made by FTR Systems, to the next tee while they finish putting.\n''Someone ran up to me last week and said that my golf cart had broken free and was rolling through the parking lot,'' said Richard Nagle, the sales manager for CaddyTrek in North America and Europe. ''Most people just stop and stare. They're not used to this.''\nFTR Systems does not disclose the proprietary technology it uses to power the CaddyTrek, which sells for $1,595, but Mr. Nagle said sales of the robot carriers had been strong, and the company had been rushing to meet orders in the United States and Europe.\nWhile one robot totes your golf clubs, another, the Polaris 9300xi, could be cleaning your swimming pool. The blue, four-wheel drone submerges in a swimming pool and pushes itself along the bottom and walls to dislodge and filter sediment. The device, which is made by Zodiac Pool Systems of San Diego, cleans pools as much as 60 feet long.\nUsers can program the robot to clean a swimming pool at regular intervals or use a remote control to steer it by hand. The Polaris 9300xi sells for $1,379.\nA silent, four-wheeled grass cutter called the Automower, made by Husqvarna, a Swedish power tool and lawn care company that also owns the McCulloch and Gardena brands, can care for lawns as large as 6,000 square meters, or 64,000 square feet.\nThe Automower cuts grass by staying within a boundary wire drawn around the perimeter, sensing and avoiding trees, flower beds and other obstacles. The mower, which is sold in Europe and Asia but not in the United States, cuts rain or shine and returns to recharge itself when its batteries get low. Advanced models use GPS and can recognize and return to narrow, hard-to-reach parts of lawns and gardens, ensuring that no areas are missed.\nThe least expensive garden drone, the Automower 305, costs &#x20ac;1,500, or $1,965, and can mow 500 square meters on one charge. The top-end Automower 265AX sells for about &#x20ac;4,600 in Europe and is designed for hospitals, hotels and commercial properties.\nThe Swedish company sold its first robotic mower, which was solar-powered, in 1995. But the device was too expensive and too unreliable in climates like that of northern Europe, where sunny summers are not guaranteed. About five years ago, the Husqvarna switched to battery power, which lowered the cost and eliminated weather as a factor.\nHenric Andersson, the director of product development at Husqvarna in Stockholm, said the company's robotic mowers were getting extensive use in Scandinavia and Europe.\n''After being around for years, sales really began taking off about five years ago,'' said Mr. Andersson. ''The graph of sales looks like a hockey stick. The robotic mower has reached a tipping point. More people are now incorporating the device into their lives.''\nOther basic robots are beginning to work inside the home. iRobot, a firm founded by three former employees at the Massachusetts Institute of Technology, makes robots that vacuum, sweep and mop floors. The iRobot Roomba 790, which costs &#x20ac;900 in Europe, is a self-propelling vacuum cleaner that can sense and navigate interior spaces, adjusting by itself from carpets to hard floors, and wielding side brushes for corners and walls.\nThe iRobot Scooba 390 cleans sealed hardwood, tile and linoleum floors, no pre-sweeping required. The device looks like a hovering bathroom scale and can hug walls and avoid staircases and other dangerous drops as it cleans, vacuums, wet mops and dries as much as 850 square feet of floor on a single charge. The Scooba 390 sells for &#x20ac;500.\nTheoretically, a house full of robotic gadgets can lead to more free time, which is where the AR Drone 2.0 quadricopter, a flying, smartphone-controlled helicopter, may come in. The AR Drone 2.0 is equipped with two onboard video cameras: one conventional and one high-definition, which can stream and store video of its flights.\nThe AR Drone 2.0, which the user steers over the helicopter's own Wi-Fi network, can be guided through looping maneuvers and fly as far away as 50 meters at speeds as high as 18 kilometers per hour. The craft can fly about 12 minutes before needing a recharge. The device, made by Parrot, based in Paris, costs &#x20ac;300 in Europe.\nParrot has sold more than 250,000 of the drones since it was introduced in 2010.\nThis is a more complete version of the story than the one that appeared in print.\n","558":"SAN FRANCISCO -- At first blush, the choice of Expedia CEO Dara Khosrowshahi to lead Uber may seem a little odd.\nThe 48-year-old chief executive, who was born in Iran and moved to the U.S. in 1978 to flee the Iranian Revolution, does not live in Silicon Valley; he lives in Bellevue, Wa., where Expedia is headquartered.\nThe two businesses also appear to have little in common. Like Uber, online travel giant Expedia is a data-driven marketplace that links sellers to consumers who are on the move -- but the connections pretty much stop there.\nYet Khosrowshahi, 48, has deep ties to Silicon Valley, many of them through his own family. Indeed, Khosrowshahi may have one of the most extensive family networks of anyone working in the technology industry today -- six of his relatives are highly successful Silicon Valley entrepreneurs or executives with strong ties to tech.\u00a0\nHis brother Kaveh Khosrowshahi is managing director of Allen & Co., the influential boutique investment bank the runs the Sun Valley conference, a networking event frequented by tech elites like Facebook chief executive Mark Zuckerberg, entrepreneur Elon Musk, and Twitter chief executive Jack Dorsey.\nHis cousin Amir Khosrowshahi co-founded an artificial intelligence company, Nervana, that was acquired by Intel last year for $400 million (He is now an executive in Intel's artificial intelligence unit).\nHis twin cousins, Ali and Hadi Partovi were early investors in many of the most successful tech companies produced by Silicon Valley over the last decade, including Airbnb, Dropbox, Uber, and Facebook. They also cofounded Code.org, an influential non-profit focused on improving computer science education across the United States.\nTwo other family members are Google executives. One cousin, Farzad Khosrowshahi, invented the software tool now known as Google spreadsheets, and is the executive that runs Google Docs. Another family member, Avid Larizadeh Duggan, is a general partner at Google Ventures, the search giant's venture capital arm that invests in startups (Uber was an investment in the Google ventures portfolio) .\nIn an interview, Ali Partovi said that his cousin Dara was always someone he and his brother had looked up to. \"My whole life, anytime I've faced a high-pressure decision, my model for mature behavior has been, 'What would Dara do'? He's one of the humblest and most even-keeled people I know.\"\nThat trait in itself may serve the embattled Uber well, and will be a stark contrast to the leadership style of former chief executive Travis Kalanick. Kalanick is known to fly into fits of anger. (In one infamous episode that was caught on video earlier this year, Kalanick unloaded on an Uber driver who criticized the company's wages.)\nMany of the cousins went to the same high school, the Hackley School, a private prep school in Tarrytown, NY, Partovi said.\nOver the years, they have helped one another, investing in each other's companies and supporting one another's ideas. The extended family moved to the United States between the late seventies and eighties, fleeing the Iranian revolution.\nTheir remarkable immigrant success story isn't lost on them, which is why Khosrowshahi and his cousins became some of the most vocal opponents of President Trump's immigration ban on Muslim Americans, including those from Iran. Shortly after the ban was issued, Khosrowshahi sent a memo to the entire Expedia workforce, according to Business Insider.\n\"I believe that with this executive order, our president has reverted to the short game,\" he wrote. \"The US may be ever so slightly less dangerous as a place to live, but it will certainly be seen as a smaller nation, one that is inward-looking versus forward thinking, reactionary versus visionary.\"  Khosrowshahi reiterated his concerns during a routine earnings call with investors.\nWhile it's not known who first tapped Khosrowshahi to come in to interview for the top job at Uber, conversations began in Seattle a few weeks ago, said people familiar with the discussions. At the time, the board was considering two other candidates with higher-profiles, GE chief executive Jeff Immelt and HPE chief Meg Whitman.\nNegotiations lasted throughout the weekend, and the decision was a close call between Whitman and Khosrowshahi, people familiar with the discussions told the Post. Uber's eight-member board debated until the very last minute on Sunday.\nThe talks were kept so secret that many of Khosrowshahi's own family members were surprised when they heard the news, Ali Partovi said. \"My phone has been blowing up with messages for my family for the last hour,\" he said on Sunday evening.\nAs of Monday, Khosrowshahi was still pondering the job, according to an internal letter circulated among Expedia's staff by the company's chairman, Barry Diller. \"Nothing has been yet finalized,\" Diller wrote, \"but having extensively discussed this with Dara I believe it is his intention to accept.\"\nKhosrowshahi won't be a cheap hire. He was named the highest paid chief executive in the U.S. by Equilar for his 2015 compensation, thanks largely to a long-term stock option package valued at $90.8 million he would gain access to over a period of several years.\nAs of Friday's close, that meant Khosrowshahi had unvested options worth about $97.5 million if he'd stayed on at Expedia, according to an analysis by independent compensation consultant Brian Foley. Yet he also has additional options that would be worth another $82.5 million if aggressive stock price performance targets were met, bringing the total to at least $180 million.\nFoley said it is unlikely Khosrowshahi would give up pay to take the job at Uber -- and he could very well be paid more. \"I suspect the real question is not how much he gives up but how much more did he get,\" said Foley. \"He's in the catbird seat. They've now come to him -- it's got to be something that has some real sizzle to it.\"\nBecause Uber is a private company, the company will not be required to immediately release specifics on Khosrowshahi's pay, though it would become public if the company launches an IPO. But Foley expects the circumstances -- a high-profile, highly public search that included heavyweights like General Electric chairman Jeff Immelt and Hewlett Packard Enterprise CEO Meg Whitman -- would mean little will be left on the table.\n\"I have to figure they gave him new grants that would make him whole on whatever he would lose at Expedia and then threw a sweetener on top,\" Foley said, noting it would be unusual for a company losing its CEO to accelerate the vesting of his options. \"They want to save that for the next guy.\"\n          Staff writer Jena McGregor contributed to this report.       \n","560":"IBM is looking to enhance the growth of its Watson Health business with the $2.6 billion purchase of Truven Health Analytics, which has data on the cost and treatment of more than 200 million patients.\u00a0\nThe planned acquisition of Truven, announced Thursday morning, is the fourth company IBM has purchased since it created the Watson Health unit last April, bringing the total spent to more than $4 billion. \n  Two other acquisitions, Explorys, a spinoff from the Cleveland Clinic, and Phytel, a maker of software to manage patient care based in Dallas, also brought with them significant data assets.\n  The Watson Health business, IBM said, now has health-related data on ''approximately 300 million patient lives,'' mostly in the United States. The goal is to run the patient data through Watson's artificial intelligence software, so that it works as a specialized digital assistant to physicians and health administrators to improve care and curb costs.\n  Watson Health is the first industry-focused unit IBM has set up to try to build the artificial intelligence technology, renowned for beating human champions in the quiz show ''Jeopardy!'' five years ago, into a large, profitable business.\n  The sizable Truven purchase is further evidence that IBM's management, led by Virginia M. Rometty, the company's chief executive, intends to press ahead with that plan despite declining revenue and disappointing profits.\n  Some of that decline in revenue is by design, as IBM sheds less profitable businesses like making industry-standard server computers and semiconductor manufacturing. But the financial results also reflect the challenge of reaching the crossover point, when the growth in its new businesses, like data analytics, cloud computing and mobile software applications, more than makes up for the decline in some of its traditional hardware, software and services businesses.\n  IBM does not disclose Watson revenue, but it is part of the company's data analytics business, which generated sales of $18 billion last year.\n  John E. Kelly III, an IBM senior vice president who oversees research and new initiatives, said the Truven purchase showed ''we're serious, and spending serious money to move fast in a whole new industry for IBM,'' health care.\n  The Truven acquisition, Mr. Kelly added, rounds out the range of data assets and data management capabilities in health care that IBM had in mind when it established the Watson Health unit. Explorys and Phytel, he said, brought mostly data from patients' electronic medical records. The $1 billion purchase of Merge Healthcare, a medical-imaging software company, added expertise in managing health image data, which is steadily growing in importance, Mr. Kelly said.\n  Truven, he said, contributes vital payment information on patients. And payment records include detailed coding on disease types, diagnosis, drugs prescribed and clues to outcomes if, say, a patient does not respond to one treatment and is given another.\n  ''It's a very key cog to give us one of the most complete data sets on patients and health care in the world,'' Mr. Kelly said.\n  That will become increasingly important in health care, as the United States and other health systems increasingly shift from fee-for-service payment to reimbursement based on outcomes that keep people healthy and out of hospitals.\n  Truven, which has its headquarters in Ann Arbor, Mich., also brings a large base of customers -- about 8,500 clients including federal and state government agencies, employers, hospitals and clinics.\n\n\n\n","561":"SAN FRANCISCO -- It appears that Google has persuaded federal regulators that -- in some situations at least -- the Tin Man has a heart.\nIn a letter sent this month to Google, Paul Hemmersbaugh, the chief counsel for the National Highway Traffic Safety Administration, seemed to accept that the computers controlling a self-driving car are the same as a human driver. \n  The agency's letter is certain to sharpen the debate over regulation of cars that can drive themselves, even though the technology is still probably years from becoming mainstream. The letter is also at odds with proposed rules in California, where much of the autonomous vehicle research is taking place.\u00a0\n  In a setback to Google's autonomous car efforts, the California Department of Motor Vehicles issued draft regulations in December that would require a human driver to remain ''in the loop'' in a self-driving car. In other words, someone with a driver's license should be prepared to take over at any moment.\n  ''If driverless cars dramatically reduce accidents, as it appears they will, then speeding up their adoption is good,'' said Wendall Wallach, a Yale ethicist. But he added that the N.H.T.S.A. letter ''creates the illusion that by declaring self-driving cars the equivalent of human drivers, we have resolved the broader societal challenges.''\n  There is no consensus within the automotive industry about the ultimate role of human drivers in the face of rapid progress in artificial intelligence technologies. There is also uncertainty within the industry about whether the technology is advancing quickly enough that it will soon drive a car more safely than humans.\n  Much of the industry has committed to developing autonomous technologies that assist drivers. Last year, Toyota announced a $1 billion research effort adjacent to Stanford University and the Massachusetts Institute of Technology intended to focus on artificial intelligence that helps human drivers, rather than autonomous vehicles. The industry has begun to deploy a variety of automation systems as safety features, like lane keeping and so-called traffic jam assist.\n  Mr. Hemmersbaugh of the traffic safety agency was responding to a Nov. 12 proposal from Google for a design for a self-driving car without controls, such as a steering wheel, a brake and an accelerator. The prototype, which Google began testing last year, is for a low-speed vehicle that could perform taxi and possibly delivery functions automatically in crowded urban settings.\n  The company switched the focus of its self-driving car program after deciding last year that it could not solve the so-called handoff problem, in which a human driver is required to control the car in an emergency.\n  Google began testing a fleet of cars in 2010, using two professional drivers to oversee the operations of the computer systems that controlled vehicle navigation. However, in 2014, the program was expanded to permit some of the company's employees to commute using the autonomous cars. The company then observed distracting driving behavior up to and including passengers falling asleep.\n  ''Google has long taken the position that the most dangerous thing on the roads is a human driver due to their driving distracted, driving while intoxicated, lack of compliance with the law,'' and other issues, said Ronald Arkin, a roboticist at the Georgia Institute of Technology.\n  The N.H.T.S.A. letter, which was posted on the agency's website and reported by Reuters on Tuesday, is not a complete endorsement of Google's position. The next step, the letter said, is determining how the self-driving car ''meets a standard developed and designed to apply to a vehicle with a human driver.''\n  The legal challenges that artificial intelligence will pose have become more complex as technology has advanced. It was once fashionable to say that the machines would only do exactly what they were programmed to do. And if the human programmer made an error, such as misplacing a decimal point, that would be expressed in some incorrect behavior on the machine's part.\n  However, recent progress in artificial intelligence has largely been made with so-called deep learning algorithms. This is a branch of machine learning that is based on software composed of multiple processing layers, each with its own complex structure. The programs are ''trained'' by exposing them to large data sets. They are then able to perform humanlike tasks, such as categorizing visual objects and understanding speech.\n  At this point, researchers admit that they do not completely understand how the deep learning networks make decisions.\n  This will confront courts with a vexing challenge in the event of accidents caused by the A.I. system. Who will be blamed when it is not clear whether the error was made by human or machine?\n\n\n\n","562":"MARRAKESH, Morocco \nDriverless cars and trucks rule the road, while robots \"man\" the factories. Super-smartphones hail Uber helicopters or even planes to fly their owners across mushrooming urban areas. Machines use algorithms to teach themselves cognitive tasks that once required human intelligence, wiping out millions of managerial, as well as  industrial, jobs.\nThese are visions of a world remade - for the most part, in the next five to 10 years - by technological advances that form a fourth industrial revolution. You catch glimpses of the same visions today not only in Silicon Valley but also in Paris think tanks, Chinese electric-car factories or even here at the edge of the Sahara. \u00a0\nTechnological disruption in the 21st century is different. Societies had years to adapt to change driven by the steam engine, electricity and the computer. Today, change is instant and ubiquitous. It arrives digitally across the globe all at once. \nGovernments at all levels on all continents are suddenly waking up to how social media and other forms of algorithms and artificial intelligence have raced beyond their control or even awareness. (See the Trump campaign and Russia, 2016, for one example.)\nThis realization that American lives are on the cusp of technological disruptions even more sweeping than those of the past decade was driven home to me by being part of a research project on technology and governance at the Hoover Institution at Stanford University this year. \"Autonomous\" (i.e., driverless) cars, the cloud, and swarming drones that deliver goods to your doorstep or transform naval and ground war-fighting strategy are well-known concepts. But the reality that they are breathing down my - and your - neck came as something of a surprise.\nSo did the startling visions of change outlined in the cozy confines of Silicon Valley that were also on the agenda here on Africa's Atlantic shoulder when France's Institute of International Relations held its annual World Policy Conference this month. \nThe usual suspects - global balance-of-power politics, the European Union's woes, President Trump's foreign-policy brutishness, Brexit - shared pride of place with the Internet of Cars (the on-wheels version of the Internet of Things) and the vulnerability of the 5,000 military and civilian satellites now in orbit.\nThese were not abstract subjects for the conference's host country. Morocco this month became the first African nation to launch a spy satellite into space. And the kingdom is a key player in U.N.-sponsored efforts to organize a global containment strategy for climate change. \nChina's policies toward Taiwan and India were not dwelled upon here. Instead it was noted that China produces more electric-powered automobiles than the rest of the world combined in a determined campaign to reduce pollution. \"China is becoming a global laboratory as well as a global factory,\" said one speaker, pointing to Beijing's surging development of artificial intelligence in all civilian and military forms.\nThe world's major powers offer sharp contrasts in harnessing technological change to their national interests and histories. The result is a new bipolar world based on technology rather than nuclear arsenals. Today's superpowers are the United States and China.\nThe U.S. government has kept out of the way and let market forces develop giant technology companies with global reach. China has chosen to compete head to head, keeping Facebook, Google and others out of its markets while capturing U.S. intellectual property for its national firms. Europe lets U.S. technology companies in and regulates them rather than competing. Russia has weaponized information technology, adding social media to its arsenal of troops, missiles and tanks. \nDiplomats and strategists have begun to patrol this expanding intersection of technology and international affairs, hoping to find ways to adapt the Cold War rules of deterrence and arms-control agreements to threats from cyberspace. Some experts shudder at the thought of artificial intelligence being incorporated into national command-and-control systems, further reducing the time humans have to respond to hostile missiles- or laser beams.\nThere were also calls for governments to begin to grapple with urgent earth-bound problems created by the disruptive impact of technology on domestic labor markets and increasingly fragile political systems.\nThe jobs that artificial intelligence and automation create while destroying outmoded ones often require constant retraining and multiple career and location changes. U.S. employers report that 6.1 million jobs currently sit vacant largely because applicants lack either the skills or mobility needed. \nAnd there was clear recognition from Palo Alto, Calif., to Marrakesh that the communication revolution embodied in social media has hollowed out the political parties in democracies, enabling demagogues to whip up mobs by remote control.\nThe world turns, as always. But now it turns on a dime, or rather a computer chip.\n","563":"The White House on Thursday plans to convene executives from Amazon, Facebook, Google, Intel and 34 other major U.S. companies as it seeks to supercharge the deployment of powerful robots, algorithms and the broader field of artificial intelligence.\nThe Trump administration intends to ask academics, government officials and AI developers about ways to adapt regulations to advance AI in such fields as agriculture, health care and transportation, according to a draft schedule of the event. And they are set to discuss the U.S. government's power to fund cutting-edge research into such technologies as machine learning.\nFor the White House, the challenge is to strike a balance between the benefits of computers that can spot diseases or drive cars and the reality that jobs - or lives - are at stake in the age of AI.\u00a0\n\"Whether you're a farmer in Iowa, an energy producer in Texas, a drug manufacturer in Boston, you are going to be using these techniques to drive your business going forward,\" Michael Kratsios, deputy chief technology officer at the White House, said in a recent interview.\nAmong those expected to be in the room for the private gathering Thursday will be representatives of tech giants such as Microsoft, Nvidia and Oracle, as well as Ford, Land O'Lakes, MasterCard, Pfizer and United Airlines, according to the White House. \nThe U.S. government spent more than $2 billion on unclassified programs during the 2017 fiscal year to research and develop AI technology, according to data furnished this week by the White House's Office of Science and Technology Policy. \nStill, many experts said they would ask the Trump administration this week to provide more federal money to fuel the field and help it compete with firms in other countries, particularly China, that are pursuing their own advancements in AI. A key focus is jobs - training workers for new technology-heavy roles, as well as helping those who may eventually be displaced because of automation.\n\"There will be new jobs available, but the real challenge: if we can match people up and train them in an appropriate way,\" said Paul Daugherty, the chief technology and innovation officer of the management consultancy Accenture, who is to attend the Thursday gathering.\nAI encompasses powerful technologies. It guides the self-driving cars offered by Uber and Google-owned Waymo. It is the software behind smartphone voice assistants such as Apple's Siri and the facial-recognition tools on Facebook. And its ability to interpret large amounts of data is also being tapped in new ways, including efforts to reshape workforce recruiting, farming and food production.\nAbout a year ago, the Trump administration publicly sounded a different note on AI. Steven Mnuchin, who had just been tapped as treasury secretary, told reporters that artificial intelligence was \"not even on my radar screen,\" stressing that the real use and implication of those tools were \"50 to 100 more years\" on the horizon.\nTech companies had \"a lot of bad blood\" with the Trump administration in the early months, said Pedro Domingos, an AI researcher and University of Washington professor who wrote \"The Master Algorithm.\"          \nOver the past year, however, science and technology aides at the White House have sought to chart their own course on AI. They last hosted leading executives from Apple, Amazon and Google at the White House in June, when Trump committed generally to removing regulatory barriers facing emerging technology. (Amazon chief executive Jeffrey P. Bezos owns The Washington Post.)\nThe White House's upcoming meeting \"is an opportunity to accelerate toward success and leave that slow start in the rearview mirror,\" said Dean Garfield, the president of the Information Technology Industry Council, which represents companies such as Apple, Facebook and Google.\nBeyond public research funding, tech firms think Washington can play a critical role in helping educate American workers on the ways AI will reshape jobs and create new ones - including in factories, where AI and robotics have helped drive a resurgence of domestic manufacturing.\n\"There are a lot of people who say that AI is going to be about job destruction. It is not. It is going to be about job movement,\" said ITI's Garfield.\nFor many in the industry, though, AI sits at the center of a high-stakes technological duel between the United States and China. In July, leaders in Beijing announced a plan to incubate a local AI industry valued at about $150 billion by 2030, relying in no small part on government investment.\nSome companies are emboldened by the steps the Trump administration has taken toward China in criticizing intellectual-property theft and the restrictive rules that tech companies must follow to operate in that country.\nFor now, a source familiar with the White House's event said, the administration is considering a different set of proposals, including efforts to make more data available for AI research. Companies long have asked the government to make more of its information available.\ntony.romm@washpost.com\ndrew.harwell@washpost.com\n","568":"Meeting Cog for the first time can be an unsettling experience. Cog, the creation of Rodney Brooks, the director of the Artificial Intelligence Laboratory at the Massachusetts Institute of Technology, is a legless, human-shaped robot that looks like an ungainly ancestor of C3PO.\nWhen visitors approach, Cog swivels its head in a disconcertingly human gesture, and stares at them. Inevitably, the visitors stare back, and for at least an instant get the eerie sense that there has been a meeting of the minds.\u00a0\n \"Everybody overreacts as a matter of course,\" said Daniel Dennett, a Tufts University philosopher who works with Dr. Brooks on Cog. \"There's something really unsettling when Cog looks at you.\"\nSo far, Cog's unsettling gaze is little more than a clever illusion, but Dr. Brooks and his graduate students believe that they have taken the first important steps toward creating a truly intelligent machine.\nThe key to achieving human intelligence, Dr. Brooks believes, is to build a machine that experiences the world in much the way a human being does. This requires not only the ability to sense and manipulate the physical world, but the social world as well, since from infancy on, almost all of what humans learn comes about from human reaction. And the first step, bridging the gap between humans and machines, is eye contact.\nDespite the fact that he directs the world's largest artificial intelligence laboratory, Dr. Brooks is something of a maverick in the field. \"Most of my colleagues here in the lab do very different things and have only contempt for my work,\" he said cheerfully. The majority of artificial intelligence researchers pursue an approach that Dr. Brooks calls, Good Old-Fashioned Artificial Intelligence, or Gofai. According to this approach, the best way to make a computer that is as intelligent as a human is to cram it full of knowledge about the real world and methods to manipulate it.\nA leading proponent of this approach is Douglas Lenat, president and chief executive of Cycorp in Austin, Tex., and Dr. Brooks's former adviser. For more than a decade, Dr. Lenat and his team have fed more than a million facts into a program called Cyc. Most of these facts are matters of common sense: for example, that glasses full of water usually should be carried rightside-up and that people generally stop buying things after they die.\nThe problem with this tactic, Dr. Brooks feels, is that it is \"not grounded anywhere.\"\n\"To me,\" Dr. Brooks said, \"it's like an intelligence which only has access to a Korean dictionary. It has perfectly self-consistent definitions, but it's not connected to the world in any way.\"\nThe schism in the artificial intelligence field represented by these two approaches to building intelligent machines in some ways mirrors the split among educators. Traditional educators, like Dr. Lenat, believe \"that intelligence is intimately tied up with having and being able to use knowledge.\" Dr. Brooks's approach is closer to that of educators like John Dewey, who urged \"a philosophy of education based upon a philosophy of experience.\" As Dr. Lenat puts it, \"We represent the 'it's worth it to go to school' point of view, and he represents the 'you can learn stuff on your own' point of view.\"\nBefore setting out to make a robot that could learn things on its own, Dr. Brooks spent more than a decade building robotic insects that could freely scurry about and explore his laboratory. Other researchers had done similar things, but their creations, which relied on the ideas of Gofai, were awkward and slow. The legs of their insects were controlled by a central computer that had a detailed three-dimensional map of the terrain and knew all the relevant laws of physics and strategies for setting goals. Dr. Brooks did away with the massive central processor. Instead, each insect leg contains a small circuit that tells it to swing forward if the leg is in the air, to swing backward if it is on the ground, and not to lift if the adjacent leg is up, along with a few other similar facts.\n\"It's all local computation physically coupled through the body,\" Dr. Brooks said. Nevertheless, from the interaction of these circuits, Dr. Brooks's insects spontaneously begin to walk with a gait very similar to that taken by actual insects.\n\"I had this plan,\" Dr. Brooks recalled. \"I was going to do insects and then I was going to do an iguana. And then I was going to do maybe a simple mammal, then I was going to do a cat, then I was going to do a monkey, and then I was going to do humans.\" The problem with this plan, he soon realized, was that it would take too long; insects alone had taken him 10 years.\nSo he asked himself: \"Do I want to die being remembered as the guy who built the best artificial cat? No. So I decided to jump and see where we could get. At least I'd be remembered as the foolish guy who was way overambitious.\"\nOver the past five years Dr. Brooks, with the help of six or seven graduate students, has begun to realize his ambitious dream. Today Cog is a skeletal aluminum figure, with a swiveling, octagonal head perched on broad, motor-studded shoulders from which sprout a pair of powerful arms that end with simple, fingerless hands. The researchers decided not to equip Cog with legs.\n\"We're not sure what to do with a toddler strapped to a high chair, much less something that can walk around as well,\" said Brian Scassellati, one of Dr. Brooks's graduate students.\nCog's eyes consist of two pairs of video cameras -- one wide angle, the other with a narrower focus to simulate human vision, which is far more sensitive in the middle than on the edges. They move much as human eyes move. The output of these cameras is displayed on a bank of amber monitors mounted on a rack behind the robot. Cog also has tiny microphone ears, gyroscopes that work much like the inner ear to provide a sense of up and down, and various sensors to report the position of its limbs. With all this hardware, there is little room left for brains, so these -- a network of Pentium computers -- are mounted in a nearby rack.\nSo far, Cog's achievements are modest, though impressive by the standards of robotics. Cog can make eye contact, track motion by swiveling head and eyes, and even distinguish humans from other less interesting objects. It can make a Slinky gracefully cascade from hand to hand, turn a crank and has even played drums in a rock video. Robots programmed using the standard principles of artificial intelligence could probably perform these tasks too, but not with the human grace of Cog, its creators say. For example, a visitor can halt Cog's playing with a Slinky by simply placing a hand gently on its arm; when the hand is removed, Cog resumes its play.\nCog also has a younger sister named Kismet, a cartoon-cute character with bushy eyebrows, pink ears, a pair of large blue eyes, complete with eyelids, and a mouth that can open and shut. All of these endearingly childlike features are carefully calculated to evoke the maternal instincts of Kismet's human companions.\n\"Kismet is essentially training the human to be a good teacher,\" said Cynthia Breazeal, a designer of Kismet. Kismet has a simple set of moods -- like happiness, interest, anger, fear, disgust or surprise -- that it can express on its face.\nKismet also has a set of needs, like the need to be stimulated by people and toys, a need to interact, and a need to avoid fatigue. \"These drives are always changing in time,\" Dr. Breazeal explains. \"When one need becomes very strong, behaviors such as facial expressions that act to satiate that need become active.\" For example, playing with a toy might amuse Kismet for a while, which it shows by a happy expression -- eyes wide open, eyebrows up, ears perked. But then Kismet becomes bored and its face begins to droop. Like any good parent, Kismet's human companion now tries desperately to see what else will amuse the robot and finally hits upon the reliable standby of making funny faces. Kismet, who was craving social interaction, is appeased and begins to smile once again.\nWhen speaking of Kismet and Cog, it is difficult to avoid the trap of anthropomorphism. Even the robots' creators do not claim that Kismet possesses human emotions, but that does not mean that \"she\" possesses no emotions at all. \"Cog doesn't have all the sorts of consciousness that a human infant, a dog or a bird has, but it has some,\" Dr. Dennett said. \"It doesn't have all the rich, lush embellishments of human boredom, but it's probably pretty much in a state that might inspire a zoo-goer to say, 'these snakes are bored, aren't they?' \"\nIt is to answer questions like these, rather than the thrill of building a humanoid robot, that Dr. Brooks said he had undertaken his overambitious project. \"The goal of having humanoid robots?\" he said. \"Certainly from a science fiction point of view, I'd love it. To me it's a romantic notion. Whether that will make sense in any sociological, economic or other sense, I can't say at this point.\"\nDr. Brooks and his students want to be experimental philosophers, using Cog as a platform to investigate theories of the human mind. \"You look at some cognitive theory and there's a lot of hand-waving around the edge,\" Dr. Brooks said. \"When you try to put it in a robot, you have to get rid of the hand-waving and connect it up to the other stuff, which sharpens everyone's appreciation and understanding.\"\nStill, Dr. Brooks has a clear-cut criterion for success: \"I'll know that I've succeeded when my graduate students feel bad about turning off the robot.\"\u00a0\u00a0\nhttp:\/\/www.nytimes.com\n","569":"Olivia Selfridge Rissland  and David John Knezevic were married Saturday in Lincoln, Mass. The Rev. Roger W. Paine III, senior minister of the First Parish Church at Lincoln, performed the ceremony in his church. \u00a0\n  The bride, 26, and the bridegroom, 27, met as Rhodes scholars at Oxford, where each received a doctorate, she in biology and he  in computational mathematics. They are doing postdoctoral research  at the Massachusetts Institute of Technology, where she is a molecular biologist at the Whitehead Institute and he is an applied mathematician in the mechanical engineering department.\n  Dr. Rissland, who is keeping her name, graduated magna cum laude from Brown  and is the daughter of Edwina L. Rissland of Belmont, Mass., and the late Oliver G. Selfridge. Her mother is a computer science professor at the University of Massachusetts, Amherst, where she is also a specialist in artificial intelligence. The bride's father was an innovator in early computer science and artificial intelligence. He was affiliated with the Lincoln Laboratory at M.I.T., and later became chief scientist for GTE.  \n  The bride's paternal great-grandfather, H. Gordon Selfridge, was an American who founded the Selfridges department store in London. \n  Dr. Knezevic graduated from the University of Western Australia in Perth with two bachelor's degrees,  one  in engineering and the other  in computer and mathematical sciences. He is a son of Beverley Knezevic and Dr. Wally Knezevic of Perth, Australia. His mother is a supervisor of student teachers in the department of education at the Edith Cowan University campus there. His father is a neurologist in Perth.\n","570":"If Nissan has its way, reading your e-mail while driving to work may soon be acceptable behind-the-wheel behavior. The automaker says it will market autonomous-drive vehicles - cars that can operate without the assistance of a driver - by 2020. Carlos Ghosn, chief executive of Nissan, said in a news release, \"I am committing to be ready to introduce a new groundbreaking technology, autonomous drive, by 2020, and we are on track to realize it.\"\nA host of advanced equipment is needed for autonomous operation, including cameras that can see the area surrounding the vehicle; radar sensors that measure distance; laser scanners that detect the shape of objects; a global-positioning sensor that locates the vehicle; advanced computer systems that apply artificial intelligence to that data and make driving decisions; and a variety of actuators that can execute driving maneuvers while compensating for less than ideal conditions.\u00a0\nIn a speech at a media event in California, Andy Palmer, a Nissan executive vice president, said, \"Our autonomous-driving vehicles utilize cameras, sensors, global positioning sensors and machine technologies - including the Safety Shield features already offered in many of our models - to maneuver with reduced human intervention, or without any human intervention.\"\nThat suggests levels of autonomy that a driver could select. In an e-mail, Steve Yaeger, a Nissan spokesman, confirmed that autonomous operation was driver-selectable in prototype vehicles the automaker demonstrated at a recent media event.\nSome of today's vehicles, including some made by Nissan, qualify as semiautonomous. Intelligent cruise control can keep track of a vehicle's place in traffic and adjust speed accordingly. Lane-departure warning systems can alert the driver if the vehicle crosses over lane markings. Lane-departure prevention systems go a step further and apply corrective measures. Intelligent braking systems can bring a vehicle to a halt if the driver fails to brake in time. But it's a giant step from these aids to fully autonomous operation. Providing assistance when a driver fails to react is a technical challenge, but developing a foolproof artificial intelligence system that can make all driving decisions is far more complex. \nTechnical hurdles are just one of the problems that an autonomous vehicle pioneer faces. Bryan Reimer, a research scientist engaged in driver workload studies at the Massachusetts Institute of Technology, isn't sure that humans can cope with these technologies. His research, and the work of others in the field, has determined that the sweet spot for driver awareness is somewhere between understimulated and overstimulated.\n\"We are capable of developing the sensors and systems for an autonomous vehicle, but do we know how people will interact?\" he said in a telephone interview. \"What happens when people start driving them? Autonomy complacency among pilots has become a problem in aviation. The broad issue is not whether we can develop the technologies, but whether we can develop cohesive interfaces that drivers can operate successfully without losing their skills.\"\nIn May, the National Highway Traffic Safety Administration announced plans for research on safety issues related to autonomous vehicles. A policy statement expressed support for technologies that \"have the potential to reduce significantly the many thousands of fatalities and injuries that occur each year as a result of motor vehicle crashes.\" The agency has not published standards but has said, \"Research will be performed to support the development of any potential technical requirements for automated vehicle systems.\" \nThere is also the question of liability. Will the vehicle occupant who is not actually at the controls of an autonomous vehicle be liable if that vehicle is involved in an accident, or will the manufacturer that engineered the driving system have to accept responsibility?\nNissan's introduction of the fully electric Leaf before the development of the infrastructure necessary to support E.V.'s was a bold move. An autonomous-drive vehicle takes the company's boldness to a new level. But Nissan is making the leap and has begun development of a proving ground in Japan that would enable testing of autonomous vehicles in real-world conditions. \n\"Nissan Motor Company is ready,\" Mr. Palmer said. \"We are on a mission to be the most progressive car company in the world, and to redefine how motorists interact with their vehicles.\"\nWith the driverless Nissan scheduled to appear in seven years, the world may be ready when it arrives. But the technology is not ready yet. In its Preliminary Statement of Policy Concerning Automated Vehicles, N.H.T.S.A., which will have the final word on autonomous vehicles in the United States, wrote that it \"does not recommend that states authorize the operation of self-driving vehicles for purposes other than testing at this time,\" adding, \"We believe there are a number of technological issues as well as human performance issues that must be addressed before self-driving vehicles can be made widely available.\"\nhttp:\/\/www.scribd.com\/doc\/163845485\/Automated-Vehicles-Policy\n\n","571":"Microsoft on Thursday plans to introduce a Web-based service for driving directions that incorporates complex software models to help users avoid traffic jams.\n  The new service's software technology, called Clearflow, was developed over the last five years by a group of artificial-intelligence researchers at the company's Microsoft Research laboratories. It is an ambitious attempt to apply machine-learning techniques to the problem of traffic congestion. The system is intended to reflect the complex traffic interactions that occur as traffic backs up on freeways and spills over onto city streets.\n  The Clearflow system will be freely available as part of the company's Live.com site (maps.live.com) for 72 cities in the United States.  Microsoft says it will give drivers alternative route information that is more accurate and attuned to current traffic patterns on both freeways and side streets.\u00a0\n  A  system for driving directions that Microsoft introduced last fall was limited, because without Clearflow there was no information available about traffic conditions on city streets adjacent to the highways. Because the system assumed that those routes would be clear, drivers were on occasion sent into areas that were more congested than the freeways.\n  The new service will on occasion plan routes that might not be intuitive to a driver. For example, in some cases Clearflow will compute that a trip will be faster if a driver stays on a crowded highway, rather than taking a detour, because side streets are even more backed up by cars that have fled the original traffic jam.\n  The new service is part of Microsoft's efforts to catch up with Google, the dominant search engine provider, by offering an attractive array of related services surrounding its Live search service. \n  Traffic updates have recently become a standard feature offered by the major Web portals as well as a number of specialized services that send the information to cars or to smartphones and other portable devices.\n  Greg Sterling, an Internet analyst at Sterling Market Intelligence in San Francisco, said there was consumer demand for traffic information, especially among mobile users. The challenge, he said, will be to demonstrate the improvement the company is claiming.\n  ''This is a sophisticated layer of technology that will not be easily understood by the average person,'' he said.\n  The project began in 2003 when Eric Horvitz, an artificial-intelligence researcher at Microsoft,  found himself stuck on the freeway while looking for a new restaurant in Seattle. Thinking that he might avoid the traffic jam, he instructed the navigation device in his car to route him via side streets. The result was a nightmare.\n  ''It was awful,'' he said. ''Everything seemed to be backed up.''\n  That set Mr. Horvitz, who is the current president of the Association for the Advancement of Artificial Intelligence, to pondering the problem.\n  ''It hit me that we had to do all the side streets,'' he said. ''We really needed to understand the whole city.''\n  The Microsoft researchers began trying to do just that by building software algorithms that modeled traffic behavior and collecting trip data from Microsoft employees who volunteered to carry G.P.S. units in their cars.\n  In the end they were able to build a model for predicting traffic based on four years of data and 16,500 discrete trips covering over 125,000 miles. The system effectively created individual ''personalities'' for over 819,000 road segments in the Seattle region.\n  After creating the Clearflow simulation for Seattle, the Microsoft researchers were able to transfer the model by using the  algorithms they had developed and then applying them to other cities. The city models are combined with live traffic data generated by networks of highway sensors to create about 60 million road segments, allowing the system to predict congestion based on time of day, weather and other variables like sporting events.\n  ''I consider this to be the moon mission of our machine-learning research,'' Mr. Horvitz said. ''I'm still buzzing with the glow that this is actually possible.''\n","572":"Just like that, Naika Venant was live.\nThe 14-year-old girl was on Facebook,\u00a0broadcasting from a bathroom at her foster home in southeastern Florida. Then, she was hanging from a scarf\u00a0tied to a shower's glass door frame\u00a0- a deeply painful and personal moment playing out publicly on social media.\nA friend saw the video stream on\u00a0Facebook Live\u00a0and called 911, but officers were sent to the wrong address.\nBy the time they got to the\u00a0foster home in\u00a0Miami Gardens, it was too late: Naika had\u00a0committed suicide.\u00a0\n\"Naika was my baby girl,\" her biological mother, Gina Alexis, said at a news conference, according to the\u00a0Miami Herald.\u00a0\"I am sick and devastated. I have trusted Florida foster care people to care for my baby. Instead she kills herself on Facebook.\n\"I have to bury my baby,\" she added.\nMental-health experts say there\u00a0is no question that social media\u00a0is becoming a platform for public suicide. The concern is that\u00a0people\u00a0who are planning to take their own lives can broadcast their own deaths in real time - which\u00a0is not only\u00a0devastating for those who die but also for those watching it happen online.\n\"We haven't seen a lot of it,\" said John Draper, director of the\u00a0National Suicide Prevention Lifeline, \"but when we see it, it's very disturbing.\"\nNadine Kaslow, a past president of\u00a0the American Psychological Association, said that although they are not common, \"These postings are a very concerning trend. People can see them over and over and over again.\"\nThe question is: Why would someone choose to die that way\u00a0- and what would it do to those watching it?\nFacebook said Wednesday that\u00a0it\u00a0is bolstering its suicide prevention tools to attempt to help prevent self-harm. The announcement was made just days after\u00a0CEO Mark Zuckerberg acknowledged the need for more and better ways to intervene.\n\"There have been terribly tragic events -\u00a0like suicides, some live streamed -\u00a0that perhaps could have been prevented if someone had realized what was happening and reported them sooner,\" he wrote in a mid-February manifesto. He added that \"artificial intelligence can help provide a better approach.\"\nThe company said Wednesday that it is \"updating the tools and resources we offer to people who may be thinking of suicide, as well as the support we offer to their concerned friends and family members.\"\nFacebook's existing suicide prevention tools have been integrated into Facebook Live. People worried about a live-stream can reach out to the user in trouble or report the content to Facebook. And users who are in trouble will receive a notification with resources, including live chat support.\nFacebook is also testing artificial intelligence to scan for posts as well as comments that indicate suicidal ideation and report them to the community operations team for review and possible intervention.\n\"With billions of posts shared each day, we rely on community reporting to quickly and accurately identify posts expressing thoughts of suicide,\" Facebook Chief Operating Officer Sheryl Sandberg said in a statement.\n\"As a community, we cannot prevent every suicide, but we must do more to reach out to people who are struggling,\" Sandberg added. \"As a society, we should strengthen the safety net for those who are most at risk: investing more in mental health care and support. As individuals, we can be alert for the signs in ourselves and in others and act immediately. Together, we can be there for people in distress.\"\nFacebook is a behemoth, with 1.86 billion monthly active users as of the end of 2016, according to company data. But it's not the only social network where suicides have been live-streamed.\nThis year, a 12-year-old girl in Georgia hanged herself from a tree while broadcasting on the video streaming app Live.me. Naika hanged herself in Florida.\nDays later, a 33-year-old aspiring actor in California, who had been arrested and posted bond\u00a0after accusations of domestic violence, shot himself in the head as people watched on Facebook Live. Similar scenes have played out abroad, according to news reports.\nIn the Miami Gardens case, Naika\u00a0had been bouncing\u00a0in and out of foster homes since 2009, when she was taken from her mother\u00a0after her mother said she \"physically disciplined\" her daughter.\nNaika, who had reportedly been\u00a0sexually abused while in foster care,\u00a0had talked about suicide and had been\u00a0involuntarily hospitalized on several occasions, her mother said in a statement through an attorney.\nOn Jan. 22, Alexis, the mother, started receiving messages from friends, telling her to check her bathroom.\n\"But my daughter was not in my care,\" she said in the statement. \"I then got a call from a friend saying to contact DCF [Department of Children and Families] because something was wrong.\"\nAlexis said she tried to call\u00a0Naika's case manager, but did\u00a0not get an answer.\u00a0So she said she started phoning hospitals until she found the right one. When she got there,\u00a0she said, she saw a\u00a0case worker crying.\n\"That's when I knew the worst had happened,\" she said in the statement.\nAfter\u00a0Naika's death,\u00a0Mike Carroll, the secretary of Florida's Department of Children and Families, said the department was \"absolutely horrified and devastated by the news of this young girl's death\" and vowed to conduct a \"comprehensive, multidisciplinary special review.\"\nThe status of the department's investigation is unclear.\nSuicide is the third-leading cause of death among people ages 10 to 14, and the second-leading cause of death among those ages 15 to 34, according to data from the Centers for Disease Control and Prevention.\nMore than 44,000 people committed suicide in the United States in 2015, according to data from the CDC. Nearly half of them used firearms,\u00a0according to the statistics.\nThe CDC does not appear to have data on the number of people who have committed suicide on social media.\nKaslow and\u00a0Sarah Dunn, clinical director of the Grady Nia Project, a project for suicide prevention at Grady Memorial Hospital, say teens and young adults may choose to end their lives online for a number of reasons. The clinical psychologists say some people, particularly those who have been victims of cyberbullying, may do it as a form of revenge or to retaliate against the bullies.\n\"There seems to be a link between what goes on on social media and suicides on social media,\" Dunn said. \"It's often a way of getting back at\u00a0the bully.\"\nSome, the psychologists say, may choose to commit suicide\u00a0online as a way to memorialize themselves.\nOr other times, people may be broadcasting their actions, hoping that viewers will step in to stop them.\nThat happened this year\u00a0when police in California\u00a0worked with police in New York to save a woman streaming an apparent suicide attempt on Facebook Live.\nSgt. Ray Kelly, a spokesman for the\u00a0Alameda County Sheriff's Office in California, said that on Jan. 25, a crisis center in Idaho, where the woman used to live, phoned\u00a0police\u00a0in Alameda County, where she had since moved, saying they had talked to a suicidal\u00a0woman by phone and online.\u00a0Alameda dispatchers\u00a0began pinging her cellphone and discovered that she was in Rockville Centre, N.Y.\nDispatchers alerted authorities\u00a0on Long Island, leading them to the woman using pings from her cellphone,\u00a0Google Street View and the\u00a0live feed from Facebook.\n\"While we were monitoring her, she went on Facebook\u00a0Live,\" Kelly said. He said\u00a0she began cutting herself with \"a blade\" and \"talking about her impending suicide.\"\nKelly said when the New York authorities\u00a0reached her, she was passed out in a car outside a church, but they rushed her to a nearby hospital and she survived.\n\"It's disturbing\u00a0to say the least,\" he said of suicide attempts and suicides being broadcast on social media. \"But as disturbing\u00a0as it is, by them doing it, it actually alerts\u00a0law enforcement\u00a0to the event and we're able to respond in real time. That's the age we're in with social media. We know social\u00a0media has changed so many things in the way we live our lives and now including how we die.\"\n\"The only advantage\u00a0to this whole thing,\" Kelly added, \"is that the technology worked in our favor.\"\nPsychologists also say suicides streaming on social media can have an effect on those\u00a0who watch.\nDraper, with the National Suicide Prevention Lifeline, said one concern is the\u00a0risk of copycat acts.\nKaslow, who is also a\u00a0psychology professor at Emory School of Medicine, said that although social media gives people a platform to talk about suicide in a constructive way, she agrees that it's concerning when people are showing their own deaths in graphic detail. She said a concern is that live suicides could give others who are struggling a greater sense of \"acquired capability\" - the idea that, \"If you can do it, I can do it.\"\n\"What we don't want to have happen is to make it seem easy to do,\" she added. \"That's concerning.\"\nIn response to the recent suicides that have been broadcast on Facebook Live, the social-media giant said in a statement that it was \"saddened by these tragedies.\"\n\"We take our responsibility to keep people safe on Facebook very seriously and work with organizations around the world to provide assistance for people in distress,\" a representative said in a written statement.\nSaid Draper, director of the National Suicide Prevention Lifeline: \"We really need to remind people that there's something that they can do when they spot people - whether it's online or offline - who are suicidal. There are things they can do to prevent suicide.\"\n              This story, originally published Feb. 8, has been updated.           \n              Read more:           \n           Facebook Live wants to give people a voice, but it mostly gets noticed for its violence        \n           Facebook will consider whether graphic content is 'newsworthy' before censoring it        \n","573":"Before reading the article: \nToday's article begins:\nHow do you think machines learn tasks on their own?\nWhat do you know about artificial intelligence and the way it works? What questions do you have?\nNow, read the entire article, \"Google Researchers Are Learning How Machines Learn,\" and answer the following questions: \n1. What are neural networks? What problem do they create that scientists are trying to solve?\n2. What has Google done to address the problem? What research did the company reveal recently?\n3. What are the details the article gives about neural networks and the way they mimic the human brain?\n4. How, and why, are these neural networks prone to making mistakes, according to the article?\n5. Why do artificial intelligence researchers say neural network research is still in its infancy?\nFinally, tell us more about what you think: \nIn the article, Jason Yosinski, who works in Uber's A.I. lab, warns that it may never be entirely easy to understand the computer mind.\nDo you agree? Why or why not?\nDo you think the human mind is any easier to understand, or more predictable, than the computer mind? Why or why not?\nMake a prediction: What's next for artificial intelligence research? Why did you answer the way you did?\nPHOTO: On the left is an image that was put through a neural network trained to classify objects in images -- for example, to tell whether an image includes a vase or a lemon. On the right is a visualization of what one layer in the middle of the network detected at each position of the image. The neural network seems to be detecting vase-like patterns and lemon-like objects. (PHOTOGRAPH BY The Building Blocks of Interpretability FOR THE NEW YORK TIMES)\n","574":"When you use an American Express card, the merchant runs it through a card reader and the information on the magnetic strip and the bill of sale goes immediately by telephone to an enormous set of computers on the West Coast. The machine screens the data about the purchase and makes decisions based on a set of rules it has been taught: how much is being charged, where, the past pattern of purchases and payments and the usual daily use of the card. Usually the computer approves the transaction by sending back an authorization number. If there's a problem, it's quickly referred to a human being.\nWhile a lively debate continues at a philosophical level about whether machines actually think, an ever-growing number of companies is peddling products like these to industry and government. Eventually, these new ways of doing business promise to change a great deal of everyday life. The remarkable thing is how this infant industry grew out of the passions of a mild-mannered man and some of his friends in Pittsburgh in the winter of 1955.\u00a0\nNow Herbert A. Simon has published his autobiography, \"Models of My Life.\" And for anybody who likes to think, the story of how this wayward political scientist and economist more or less invented artificial intelligence in the mid-1950s is of value. It is among the newest of the Alfred Sloan Foundation series of scientific autobiographies, and it's hard to think of more successful publishing projects.\nThe story begins with a welter of interesting yarns from the wilds of Wisconsin. The son of a German engineer who emigrated to Milwaukee from Darmstadt at the turn of the century, Simon has much to say about life in the land of Robert LaFollette and Aldo Leopold. Of a family friend he says, \"If you scratch an engineer, you'll find a physiocrat underneath. Engineers believe in real things like machines and bridges and land.\"\nAs a student at the University of Chicago in the mid-1930s, Simon flourished. Price theory turned him on to mathematics, Charles Merriam introduced him to quantitative methods and models in the study of administrative decision-making. A liberal (\"When in doubt, we could determine our policy by looking in the Chicago Tribune and opting for the opposite direction,\" he says of his circle of friends), Simon escapes -- not by much -- the later security crazes of the McCarthy era.\nHe moved to Carnegie-Mellon University in 1949 and began building the great business school there. He was all over the social science map in the early 1950s, one of many clever thinkers making a useful living off the Cold War. \"I learned many things ... \" he says at one juncture, \"few more important than how to position the decimal point in a research proposal.\"\nThe story of artificial intelligence begins in earnest when Simon teams up with Alan Newell and the first generation of stored-program IBM computers. The two shared the idea of teaching a computer to play chess -- chess is \"the Drosophila of AI,\" Simon says. Then one day in October 1955, walking along the Hudson River at Morningside Heights, it occurs to him that a computer could solve geometry problems, too.\nA month later, after much feverish work on the \"Logic Theorist\" program, Simon's computer -- or his handwritten simulations of its processes -- had begun proving simple theorems from the \"Principia Mathematica.\" Not long after New Year's Day, he walked into class and memorably proclaimed to his students, \"Over the Christmas holiday, Al Newell and I invented a thinking machine.\" Soon the letters were flying back and forth to Bertrand Russell.\nWhat followed Simon's discovery was a 35-year argument about whether machines really were capable of thinking and whether the best way to build them was up from the metaphor of a maze (Simon's way) or upon the metaphor of the brain (\"neural nets\"). Except for occasional arguments about who would get funded by the government, both sides have been winners. The insights of the inventors of the field -- Simon, Newell, Nils and Nilsson -- once deemed to have been quickly superseded, have held up pretty well, today's graduate students agree, even in linguistics.\nSimon writes: \"In arguing that machines think, we are in the same fix as Darwin when he argued that man shares common ancestors with the monkeys, or Galileo when he argued that the earth spins on its axis.\" Holding opposing views are a long line of distinguished humanists, including Joseph Weizenbaum of MIT, Hubert Dreyfus of Berkeley and, most recently, Sherry Turkle of MIT.\nSimon continues, \"My own posture has been this: My scientific work, and that of other artificial-intelligence researchers, will, in the long run, determine how many of Man's thinking processes can be simulated. I believe that ultimately all of them can, but feel no great urge to prove that to others who feel differently. In science, it is the facts that give us the final answers.\"\nToday the research program in artificial intelligence is going forward as never before. The latest word is a project supported by a consortium of intelligence agencies, administered by the Defense Advanced Projects Research Agency, which is designated Tipster. It's a long step beyond what American Express is doing now.\nThe challenge: to devise machines that will perform two tasks -- extraction, that is, pulling data out of texts and putting it into databases, and classification, or routing potentially interesting documents to human readers. According to the prospectus, such systems should be able to operate in two languages, English and Japanese, and in two domains, microelectronics and mergers and acquisitions. Thus, such systems would be able to read the Dow Jones news wire service -- or monitor the message traffic.\nYes, this is exactly what Robert Redford was repeatedly nearly killed for in the movie \"Three Days of the Condor.\" Is it economically valuable? Is it artificial intelligence? You figure it out. Chances are, you won't leave home without it. David Warsh is a columnist for the Boston Globe.\n","575":"  Hollywood got it wrong. The highly intelligent machines that will be unleashed in the near future won't be coming for our lives. They'll be coming for our jobs. \n  Being rendered obsolete by technology has been a concern among the flesh-and-blood set for hundreds of years - cars put many in the horse industry out of work, for example - but the speed and types of recent advances are about to give the issue an exceptional urgency. \n  Previously, it was repetitive blue-collar jobs that were at risk, such as those in manufacturing. In the near future, however, the leaps in artificial intelligence will soon make it possible for machines to do all sorts of jobs, including those that require thinking skills we once believed beyond the reach of machines. \n  Elements of this brave new world are laid out in \"Homo Deus: A Brief History of Tomorrow\" by Yuval Noah Harari, an Oxford-educated historian and professor at the Hebrew University of Jerusalem. \u00a0\n  \"I think we should be worried, and worried now,\" Harari tells The Post. \"Just 20 percent unemployment can cause political and social upheaval.\" \n  In recent centuries, we have managed to rein in three huge obstacles that have stymied progress for centuries: famine, war and plague. But in the upcoming decades, one of the crucial questions will be what we humans will do for a living, as artificial intelligence speeds towards levels once reserved for science fiction. \n  A 2013 study by Oxford economists Carl Benedikt Frey and Michael Osborne concluded (using a sophisticated algorithm, of course) that some 47 percent of US jobs were at high risk from automation in the next 20 years. The economists posited that it was a near certainty that human telemarketers, insurance underwriters, security guards and other fields would vanish. Even sports referees could be headed for the historical dust bin. \n  Another research report issued in 2015 by McKinsey Global Institute, a business think tank, found that 95 percent of jobs should be safe until 2020, but after that, technology will change the landscape rapidly, with many employees' duties moving to automation. The study found that 45 percent of work activities could be automated, including 20 percent of the responsibilities handled by the world's obscenely compensated CEOs, such as analyzing operations data. \n  It might be a few years yet before your company is run by a machine, but the transition in other lower-paying fields is right around the corner. \n  One of the most vulnerable jobs is truck driver. The estimated 1 million American long-haul truckers will soon be replaced by self-driving vehicles that never need to sleep or stop to wolf down a greasy burger. Last year, Uber bought a self-driving startup called Otto. A few months later, one of its trucks made the first driverless delivery, shuttling 50,000 cans of beer 120 miles from Fort Collins to Colorado Springs. \n  It might be a few years yet before your company is run by a machine, but the transition in other lower-paying fields is right around the corner. \n  The outlook is equally as bleak for the 8 million Americans working as salespeople or cashiers. Last year, Amazon opened an 1,800-square-foot store in Seattle with no cashiers or lines. Customers can simply grab the items they want off the shelf and walk out. Sensors track their purchases and charge the customers via their Amazon accounts after they leave the store. \n  Considering a career in the military? Human soldiers will likely be replaced by deadly robots and drones. \n  \"Human soldiers murder, rape and pillage, and even when they try to behave themselves, they all too often kill civilians by mistake,\" Harari writes. \n  The robotic killing machines could be programmed with \"ethical algorithms\" that will force them to strictly conform to the rules of the battlefield. And if they're captured, they can't be tortured, held hostage or coerced to reveal any of their nation's secrets. \n  Government bureaucrats could also be endangered. A recent report from Reform, a right-leaning think tank, suggested that about 90 percent of British civil servants have jobs so pointless, they could be replaced by a machine. Transitioning to robots could save the government $8 billion per year. \n  Even a high-paying career such as medical doctor, previously considered safe from cold, faceless automation, is in jeopardy. A recent experiment found that a computer algorithm correctly diagnosed 90 percent of lung-cancer cases presented to it, outperforming a human physician by 40 percent. \n  \"It won't be all doctors,\" Harari says. \"If you research cures for cancer, you're safe, but if you're a general practitioner [who diagnoses diseases], this is something that AI will do much better than most human doctors. The GP is going to be extinct.\" \n  ALGORITHMS capable of instantaneously sifting millions of legal precedents could someday replace lawyers. And it's conceivable that a machine could one day scan brains, serving as an infallible lie detector. Criminals will be easily proven guilty, helping to render not only lawyers obsolete, but also judges and detectives. \n  Schools and teachers may also go the way of the dodo. Children will receive their lessons from sophisticated AI, possibly contained within a smartphone. Gone will be the days of 30 kids sitting in a room being tutored by various teachers specializing in different subjects. \n  \"Companies are working on an AI teacher that is adapted to the strengths and weaknesses of the individual child,\" Harari says. \"Most schools will disappear. It will be much more similar to medieval apprenticeship. You'll get instruction on everything from a single source.\" \n  Even art, once the exclusive product of humans plumbing their souls, is being encroached upon by machines. David Cope, a University of California at Santa Cruz musicology professor, created a computer program called Experiments in Musical Intelligence designed to write chorales in the style of Bach. The tunes were played for an enthusiastic music-festival audience, but when the source of the composition was revealed, some of that enthusiasm turned to anger and disbelief. \n  Cope later created another program capable of composing poetry. The algorithm contributed to a 2011 collection called \"Comes the Fiery Night: 2,000 Haikus Written by Man and Machine.\" (The book does not reveal who wrote what.) \n  This disruption in the workforce will likely come with challenges and dangers yet unseen in human history, not the least of which is the creation of a massive new strata of society Harari terms the \"useless class.\" These will be those citizens \"devoid of any economic, political or even artistic value, who contribute nothing to the prosperity, power and glory of society.\" \n  During the Industrial Revolution, farmers rendered obsolete could make the transition to unskilled factory jobs reasonably easily. In the future, it's fairly unlikely the unemployed taxi driver is going to suddenly become a skilled software engineer. \n  FIGURING out how to support the millions of out-of-work people could be one of the biggest economic challenges of the next century. Bill Gates has suggested taxing robotic workers just like humans. Tesla founder Elon Musk and others have advocated for a universal basic income - having the government hand over a certain sum each year to every citizen in order to keep the populace afloat. \n  A thornier issue the unemployed masses will face is a philosophical one. \n  \"The harder challenge is how do people then have meaning, because a lot of people derive their meaning from their employment,\" Musk said at the World Government Summit. \"If you are not needed, if there is not a need for your labor, what's the meaning?\" \n  Harari predicts the \"useless class\" will occupy their days by immersing themselves in virtual-reality games. The chronically unemployed, Harari predicts, could also turn to drug use to pass the time - though one wonders if, in the future with so many potentially dependent on chemical substances, less harmful drugs will be developed and legalized. \n  Meanwhile, the money that used to flow to workers will increasingly end up in the hands of the \"tiny elite that owns the powerful algorithms,\" Harari predicts, creating unprecedented social inequality. \n  There may even come a day when the algorithms themselves own much of the world's wealth. (A health-related program is likely to be the most valuable.) It's one small leap from today's reality, in which much of the planet is already owned by non-human entities: namely nations and corporations. \n  That pattern will soon be upended, and workers will be forced to reinvent themselves multiple times within a lifetime, as technology continues to advance. \n  NOT every worker will be tossed out on their behinds, of course. Some fields are unlikely to be automated. Archeologists, for example, will continue to find work, because the job requires sophisticated pattern recognition that would be challenging to program in a machine, and the industry's profits are so small that someone is unlikely to make the investment in an automized replacement. \n  Philosophers may also experience a windfall, as the new machine age will present unique problems that require human adjudication. \"You'll have to have practical answers to these kinds of philosophical questions,\" Harari says. \n  Regardless of one's chosen career path, we're all going to have to be more flexible in the future. It used to be that humans spent the first part of our lives learning a skill that we then utilized in a career until we retired. That pattern will soon be upended, and workers will be forced to reinvent themselves multiple times, as technology continues to advance. \n  MOST LIKELY TO DISAPPEAR \n  1. Telemarketers \n  2. Title examiners, abstractors and \n  searchers \n  3. Hand sewers \n  4. Mathematical technicians \n  5. Insurance underwriters \n  6. Watch repairers \n  7. Cargo and freight agents \n  8. Tax preparers \n  9. Photographic process workers and processing machine operators \n  10. New accounts clerks \n  LEAST LIKELY TO DISAPPEAR \n  1. Recreational therapists \n  2. First-line supervisors of mechanics, installers and repairers \n  3. Emergency management directors \n  4. Mental-health and substanceabuse social workers \n  5. Audiologists \n  6. Occupational therapists \n  7. Orthotists and prosthetists \n  8. Health-care social workers \n  9. Oral and maxillofacial surgeons \n  10. First-line supervisors of firefighting and prevention workers \n  Source: \"The Future of Employment: How Susceptible Are Jobs to \n  Computerisation?\" by Carl Benedikt Frey and Michael A. Osborne, Sept. 17, 2013 \n","576":"In recent years, IBM has been known for its forays into artificial intelligence and cloud computing. Now, the technology provider is moving deeper into financial consulting with the acquisition of a prominent -- and at times controversial -- Washington firm.\u00a0\nThe company said on Thursday that it was buying the Promontory Financial Group. The financial terms were not disclosed. \n  Founded by Eugene A. Ludwig, a former top banking regulator and a law school friend of former President Bill Clinton, Promontory became one of the top financial consulting firms to emerge after the global financial crisis of 2008. Its employees, including many former financial regulators from around the world, advised banks on regulatory matters.\n  But the firm has drawn scrutiny because of the coziness of its ties to the banks it advises when it is meant to provide objective analyses of banks' problems. Last year, Promontory settled an investigation by New York State's financial regulator into its work for the British bank Standard Chartered.\n  In a statement, IBM said that Promontory and its 600 employees around the world would mesh with its own offerings, including its Watson artificial intelligence platform. In particular, Promontory is expected to help train Watson, with the goal of aiding IBM's financial clients on managing their regulatory obligations and potentially reducing the costs of doing so.\n  ''What Watson is doing to transform oncology by working with the world's leading oncologists, we will now do for regulation, risk and compliance,'' Bridget van Kralingen, a senior vice president for IBM's industry platforms team, said in a statement. ''This initial offering of Watson Financial Services is emblematic of the transformative cloud-based solutions that IBM Industry Platforms will bring to clients.''\n  ''We believe the future of business and regulation will be driven by the need for advanced technology alongside deep subject-matter expertise,'' Mr. Ludwig said in a statement.\n  This is a more complete version of the story than the one that appeared in print.         \n\n\n\n","577":"RECENTLY, I signed up for a ''virtual inbox assistant'' service. I gave the assistant, Julie, access to my email and calendar over the Internet.\nHere's what Julie said on the first day of work: \n  ''I am now at your disposal to process any calendar-related task you may delegate to me ... I am thrilled we can start working together. I cannot wait to respond to your first request!''\n  If the words in that message sound too perky and supplicating to be believed, well, you are right. Julie isn't a real person. Julie is an app. Not human. Software. It is artificial intelligence that a start-up has dressed up with a woman's name and feminine pronouns.\n  Julie isn't the only imaginary woman in my digital life. Driving in an unfamiliar city, I will put my iPhone in the cup holder and listen to Siri's calm voice guiding me through new streets. If I had a Windows phone, I would get turn-by-turn directions from its Cortana app, with a voice that is similarly pleasant and feminine.\u00a0\n  Instead of Julie, I might have tried Clara, another virtual inbox assistant. The company describes it as ''highly responsive, empathetic, and learning more every day.'' There's also Amy from the start-up X.ai, who will ''do all the tedious email ping pong that comes along with scheduling a meeting.'' Other voice, text and inbox apps have names like Crystal, Jeannie and Cloe.\n  Dag Kittlaus, who helped create Siri, has said the app gets its name, in part, from a woman he knew in Norway. Cortana was named after a feminine artificial intelligence agent in the video game franchise Halo, and Microsoft even hired the same voice actress for its service. Founders of two virtual assistant apps said they were inspired by helpmates on TV: Dawn gets its name from Don Draper's assistant on ''Mad Men,'' and Donna after an assistant on ''The West Wing.'' It seems like developers decided on Julie, Amy and Clara only because these are common women's names. These products, representing new technological possibilities, play into old stereotypes about what gender is best suited for administrative work.\n  While Apple introduced a male voice option a year after Siri's launch, I don't know anyone who uses it. The two assistant apps with masculine pronouns I have come across, Jarvis and Jeeves, share their names with butlers in Marvel comics and P. G. Wodehouse novels. The year after Amy debuted, X.ai announced its ''twin,'' Andrew. Still, the masculine named option is scarcely mentioned on the company's website.\n  And why does artificial intelligence need a gender at all? Why not imagine a talking cat or a wise owl as a virtual assistant? I would trust an anthropomorphized cartoon animal with my calendar. Better yet, I would love to delegate tasks to a non-binary gendered robot alien from a galaxy where setting up meetings over email is respected as a high art.\n  But Julie could be the name of a friend of mine. To use it at all requires an element of playacting. And if I treat it with kindness, the company is capitalizing on my very human emotions.\n  There appears to be just one woman among the 12 combined co-founders of Siri, Julie Desk, X.ai and Clara Labs. That's slightly worse than the nearly 17.9 percent of venture-funded companies founded by women, according to data from last year on CrunchBase. Women may be the imagined persona or voice of a product, but rarely are they the people making executive decisions.\n  Technologies speak with recorded feminine voices because women ''weren't normally there to be heard,'' Helen Hester, a media studies lecturer at the University of West London, told me. A woman's voice stood out. For example, an automated recording of a woman's voice used in cockpit navigation becomes a beacon, a voice in stark contrast with that of everyone else, when all the pilots on board are men.\n  Ms. Hester lives in London, where the spectral sound of robotic women is piping from nearly every corner. Enter the Underground and you hear a disembodied woman announcing ''the next station is Mornington Crescent'' and the train's signature canned message, ''please mind the gap between the train and the platform.''\n  A similar voice -- emotionless, timeless, with an accent difficult to place -- emits from clocks and traffic lights, and inside elevators and supermarkets. The ''coldness, the forthrightness of the voice'' is what Ms. Hester finds striking. What human speaks with such emotionless authority? And, as Ms. Hester points out: ''It's not real authority. There's a maternal edge to all of it. It is personal guidance rather than definite directions.''\n  And, she says, these voices can even play into people's expectations of male authority because they aren't actual women. People hear a woman's voice, realize it is robotic, and ''imagine a male programmer'' did the actual work.\n  No one seems to market tech products in the image of the most famous virtual assistant in film history. Hal from ''2001: A Space Odyssey'' was so brilliant and manly that it attempted to kill off the crew of the spacecraft it was built to manage. Instead, people build what I call ''Stepford apps.'' These are the Internet's answer to those old sci-fi robots in dresses mopping floors with manufactured enthusiasm.\n  The Amazon Echo voice-activated home-automation system is another Stepford-style device, with a woman's voice and name, Alexa. Of course consumer reviews on the website either praise or complain about ''her'' with the feminine pronoun. A friend of mine who owns the device told me, ''Sometimes I wish she'd just shut up.'' If my friend says ''weather'' in the middle of a conversation, the device might interject -- ''Right now in New York it is 45 degrees ...'' \n  Alexa sounds like a nag. Now I'm using sexist language to dismiss a gadget. Imagine if the plug-in devices that made housework more efficient were, like Alexa, sold with women's names and talked about with female pronouns. ''Could you hand me the Amanda? She's in the hall closet.'' ''Please clean the Sarah when you're finished with the onions.'' ''The Emily is broken.'' That could easily lead to characterizing an overflowing dishwasher as a ''bubbly,'' garrulous woman, or a microwave slowing down because of ''her'' age.\n  I used Julie only once, sending an email to a friend, copying the app email, with a time and date to meet for coffee. Julie emailed back promptly confirming the appointment, and it added the meeting to my calendar. The product is an interesting idea and easy to use, but interacting with a fake woman assistant just feels too weird. So I shut ''her'' off. This Stepford app, designed to make my work more efficient, only reminds me of the gendered division of labor that I'm trying to escape.\n\n\n\n","578":"\nDavid E. Rumelhart, whose computer simulations of perception gave scientists some of the first testable models of neural processing and proved helpful in the development of machine learning and artificial intelligence, died Sunday in Chelsea, Mich. He was 68.\nThe cause was complications of Pick's disease, an Alzheimer's-like disorder from which he had suffered for more than a decade, his son Karl said.\u00a0\nWhen Dr. Rumelhart, a psychologist, began thinking in the 1960s about how neurons process information, the field was split into two camps that had little common language: biologists, who focused on neurons and brain tissue; and cognitive psychologists, who studied far more abstract processes, like reasoning skills and learning strategies.\nBy starting small -- showing, for instance, that the brain's ability to recognize a single letter was greatly influenced by the letters around it -- Dr. Rumelhart and his colleague Jay McClelland, around 1980, built computer programs that roughly simulated perception. Later, he devised an algorithm that allowed computer programs to learn how to perceive. Using his program, a computer could interpret underwater sonar signals with roughly the accuracy that a person could. It was an important early step in machine learning, a critical component in artificial intelligence.\nWorking at the University of California, San Diego, he eventually developed a simulation of how three or more layers of neurons could work together to process information -- as is required for the brain to engage in any complex task, like reading. Previous models were far cruder. In a landmark 1986 paper, written with Geoffrey Hinton and Ronald Williams for the journal Nature, he described how the system worked.\nDr. McClelland, director of the Center for Mind, Brain and Computation at Stanford, said the neural processing work ''led to extremely powerful systems for doing things like visual object recognition and handwritten character classification.'' In 1986, he and Dr. Rumelhart wrote a book, ''Parallel Distributed Processing,'' that became a central text in the field.\nIn their work Dr. Rumelhart and Dr. McClelland argued that language, like most knowledge, relies mainly on memory and is represented in the brain by sets of associations between elements of sound and meaning.\nThis put them in opposition with scientists who argue that the brain generates some words by using rules shaped in part by brain biology -- for example, adding ''-ed'' to a stem to form a past tense.\n''Rumelhart was enormously important in the 1980s in reviving this neural network approach to language and cognition,'' said Steven Pinker, a psychologist at Harvard and a leading proponent of the rival ''rules'' theory.\nEven though they sometimes disagreed, Dr. Pinker said that Dr. Rumelhart's computer simulations ''prompted me and many others to ask very fruitful questions, and that in the end is about all a good scientist can ask for.''\nDavid Everett Rumelhart was born on June 6, 1942, in Wessington Springs, S.D., the eldest of three sons born to Everett, a printer, and Thelma, a librarian. He graduated from the University of South Dakota in 1963 with a degree in psychology and math and completed his Ph.D. at Stanford in 1967.\nHe spent 20 years on the faculty of the University of California, San Diego, before returning to Stanford in 1987. He retired from Stanford in 1998, when the symptoms of Pick's disease became disabling, and moved in with his brother Donald in Ann Arbor, Mich.\nIn addition to his brother Donald and his son Karl, his survivors include another son, Peter; another brother, Roger; and four grandsons. His marriage to Marilyn Austin ended in divorce.\nDr. Rumelhart won a number of professional awards, including a MacArthur fellowship and the American Psychological Association's Distinguished Scientific Contribution Award.\nHe also had one named after him: the David E. Rumelhart Prize, a $100,000 award given annually by the Glushko-Samuelson Foundation to any individual or team making a contribution to the ''theoretical foundations of human cognition.''\n","579":"HONG KONG -- One of the world's most valuable start-ups got that way by using artificial intelligence to satisfy Chinese internet users' voracious appetite for news and entertainment. Every day, its smartphone app feeds 120 million people personalized streams of buzzy news stories, videos of dogs frolicking in snow, GIFs of traffic mishaps and listicles such as ''The World's Ugliest Celebrities.''\nNow the company is discovering the risks involved, under China's censorship regime, in giving the people exactly what they want. \n  The makers of the popular news app Jinri Toutiao unveiled moves this week to allay rising concerns from the authorities. Last week, the Beijing bureau of China's top internet regulator accused Toutiao of ''spreading pornographic and vulgar information'' and ''causing a negative impact on public opinion online,'' and it ordered that updates to several popular sections of the app be halted for 24 hours.\u00a0\n  In response, the app's parent company, Beijing Bytedance Technology, took down or temporarily suspended the accounts of more than 1,100 bloggers that it said had been publishing ''low-quality content'' on the app. It also replaced Toutiao's ''Society'' section with a new section called ''New Era,'' which is heavy on state media coverage of government decisions.\n  The change was made, the company said, to ''promote the spirit of the Communist Party congress,'' referring to the  gathering of top party leaders that took place in Beijing in October.\n  The episode points to the fine line that Toutiao's creators must walk.\n  Despite China's famously strict censorship, online news is a big business there. More than 610 million people in the country gained access to some news on the internet in 2016, according to official statistics.\n  Toutiao, which says it uses complex algorithms to decide what its users see, combines China's hunger for media content with its rising ambitions in artificial intelligence. Its daily user base of 120 million people is equivalent to more than one-third of the population of the United States.\n  Suan Lin, a 24-year-old private equity analyst in Shanghai, said that she normally has to search high and low online to find articles about the Chinese historical dramas she watches on television. But Toutiao delivers, she said.\n  ''Once you're on it,'' she said, ''you just can't stop.''\n  In China, however, a strong position in media invites scrutiny from the government's censorship apparatus. That scrutiny has become heightened over the past two years as the authorities have looked beyond the political to crack down on news it sees as degrading to society as a whole, which can include things as seemingly unsubversive as celebrity gossip.\n  In Toutiao's case, one of the accounts that were suspended this week had posted a saucy video of a woman in a short skirt. It got 57,000 views. Another suspended account had recently put up a post titled ''The World's Ugliest Celebrities, Michael Jackson Is Ranked First, You Won't Want to Eat After Reading This.''\n  ''Once you have more people watching, then you want to be more cautious,'' Wei-Ying Ma, who heads Toutiao's artificial intelligence lab, told a conference in Beijing last month.\n  As Toutiao's popularity has skyrocketed, Bytedance has become a darling of Silicon Valley investors such as Sequoia Capital. The company, which is currently valued at $20 billion, has been in talks with existing backers to raise new financing that would value the company at more than $30 billion, according to a person familiar with the discussions who spoke on the condition of anonymity because the details are not public.\n  That price tag would make Bytedance among the most valuable privately held technology companies in the world, not just in China. Airbnb is said to be valued at around $30 billion. SpaceX, the rocket maker founded by Elon Musk, is valued at $21 billion.\n  Bytedance has big plans for overseas expansion, too. It recently spent between $800 million and $1 billion to purchase Musical.ly, a video-based social network popular with teenagers in the United States and Europe. At the Beijing conference last month, a top Bytedance executive, Liu Zhen, said the company hoped to be earning half its revenue from outside China within the next five years.\n  Jinri Toutiao, whose name means ''today's headlines'' in Chinese and is pronounced JING-er TOE-tee-yow, aggregates content from various sources and looks much like Facebook's newsfeed. But instead of displaying articles and videos based on what your friends have shared, the app does so based on what you have previously read and watched on the app.\n  If you click on articles about iPhones, then Toutiao will feed you more tech coverage. After you watch a few cooking videos, the app will fetch you more clips of people wrapping dumplings and braising chicken's feet.\n  This approach has helped Toutiao thrive amid China's heavily controlled environment for social media. Instead of policing the sharing activity of tens of millions of users, the company needs only to calibrate and adjust its centralized recommendation software.\n  But it also needs to make sure the app's content does not cross the lines of censors. That is a huge task, particularly given that the overwhelming majority of content on Toutiao is produced by individual bloggers, not professional news organizations or other institutions. Ms. Liu said at last month's conference in Beijing that 90 percent of the app's content comes from blogger accounts. Toutiao has around 1.2 million content-producing accounts in total.\n  According to Bytedance, every piece of content is automatically screened to check that it is acceptable before appearing on Toutiao. But once something has attracted more views, the system applies a more sophisticated screening algorithm. Certain material is also examined by humans as a final check.\n  Bytedance also takes more overt steps to stay on the right side of the authorities. Important updates from the government are sometimes pinned to the top of a user's feed. That can lead to awkward juxtapositions -- between, say, a state media write-up on President Xi Jinping's recent decisions and a photo slide show on six women who are ''so beautiful that rich businessmen immediately became attracted to them,'' as the piece's headline puts it.\n  Toutiao has come in for official rebuke before. Last June, the Beijing bureau of the Cyberspace Administration of China ordered around a dozen accounts on the app shut down, calling on Toutiao and other news portals to ''actively promote socialist core values'' and create a ''healthy, uplifting environment for mainstream opinion'' by eschewing dishy coverage of celebrity scandals.\n  In September, the website of the People's Daily newspaper, the official mouthpiece of the Communist Party, published a series of opinion articles strongly criticizing A.I.-based news apps, including Toutiao, for spreading misinformation and superficial content.\n  Despite Toutiao's popularity, some in China share that view. Yang Sun, a 26-year-old financial analyst in Shanghai, decried the app's sensationalist headlines.\n  ''It should absolutely be taken offline,'' Ms. Yang said. ''Totally deserves it.''\n  Follow Raymond Zhong on Twitter: @zhonggg.\n\n\n\n","582":"Anthony S. Levandowski is working feverishly with a team of students from the University of California at Berkeley to build an ambitious robot motorcycle to race without a driver across the Mojave Desert.\n They are part of a crowd that has been attracted by a Pentagon promise to pay $1 million to the creators of the first self-guided vehicle to find its way this Saturday along a programmed course from Barstow, Calif., to near Las Vegas.\n Mr. Levandowski's robot motorcycle will be joined by pickups, sport utility vehicles, Hummers and other all-terrain vehicles, some with as many as six wheels and all packed with computers and sensors -- but with no space for humans.\n The Pentagon, under a mandate from Congress to save lives by turning to unmanned combat vehicles to meet a third of its needs by 2015, has become impatient with its usual crowd of big-name military contractors, like the Lockheed Martin Corporation and the General Dynamics Corporation, to come up with a solution. It turned instead to the spur of free market capitalism, inspiring a motley band of computer scientists, artificial intelligence experts and robot lovers to take on the challenge.\u00a0\n It is not clear whether any of the couple of dozen vehicles expected to line up at the start will be able to complete the task. After first passing a preliminary test scheduled to begin today, the winning machine will then have to navigate unaided at an average speed of about 20 miles an hour through a desert strewn with boulders, trees, brush, potholes and possibly the odd porcupine or donkey.\n By some measures, that should not be so hard. Once the stuff of science fiction, autonomous vehicles have become relatively commonplace. Airplanes take off and land under computer control, iRobot's $199 Roomba vacuum cleaner trundles itself through living rooms, and Sony sells a $1,599 pet robot. \n Yet the challenge of designing ground-hugging, path-finding automated vehicles remains one of the thorniest tasks facing those who work on artificial intelligence.\n Before Mr. Levandowski's Ghostrider motorcycle can even begin its 200-mile ordeal, it must wobble 20 feet across a grass field here at a makeshift test site. Turned loose, the riderless machine struggles for balance by violently swinging its front wheel back and forth and then repeatedly performs a series of ungainly tumbles.\n ''It's driving like a drunken teenager,'' Mr. Levandowski said with a sigh, as he packed up the battered two-wheeler for more fine-tuning in the laboratory.\n For the hundreds of computer scientists, robot developers, software specialists and hardware hackers who have designed the vehicles, the task may be even harder than what NASA has gone through in learning how to control the interplanetary Mars rover from Earth. \n ''Unless they make the course really easy,'' said Hans Moravec, a pioneer in the robotic field at Carnegie Mellon University, ''it looks unlikely'' that any of the competitors will succeed on Saturday.\n Even the favored Carnegie Mellon Red Team -- led by William L. Whittaker and backed by $3 million raised from, among other sponsors, the Boeing Company and the Intel Corporation -- is far from certain that its robot Hummer will find its way to the finish line.\n ''This is about raising all the ships,'' said Mr. Whittaker, a roboticist who is known as Red. \n He says his team does not care that much about garnering the $1 million prize that the Pentagon's Defense Advanced Research Project Agency, or Darpa, has dangled. \n Nonetheless, Mr. Whittaker's team, like several other well-financed groups, has gone to elaborate lengths to improve its chances by buying ultrahigh resolution digital imagery of the desert in an effort to preplan potential routes.\n For all the preparations, the most important decisions to be made, Mr. Whittaker explained during an interview after the public unveiling of his vehicle in February in San Francisco, will be just before the race begins. Two hours before, Darpa officials plan to hand out route information, giving each team only a short time to plot the optimal path for its vehicle. \n ''This is the race before the race,'' Mr. Whittaker said.\n That has led to complaints from some competitors, who argue that a highly detailed top-down digital map of the route offers an advantage that skirts close to cheating on the autonomous navigation challenge Darpa has established.\n ''People who overfly the course are missing the spirit of the competition,'' said Bruce Hall, a member of Digital Auto Drive, a team from Morgan Hill, Calif. It is using an optical vision system capable of tracking objects as far as 800 feet in front of its Toyota Tundra robot pickup. ''You can do it legally,'' he added, ''but if you are trying to bird-dog your way to a million bucks, good luck to you.''\n Race insiders say that the course, whose route is still secret, will be about 180 miles long and that on the morning of the event Darpa will give each competitor a CD containing about 5,000 global positioning satellite waypoints. \n ''The trails are windy, and there are edges where the road drops off sharply,'' a Darpa spokeswoman, Jan Walker, said.\n The challenge for the military agency, which is taking elaborate precautions to make sure that the vehicles don't damage property or hurt people, has been in keeping a balance between completely turning the vehicles loose and ensuring safety. \n Military helicopters will fly over the course and each robot will be followed by a chase vehicle equipped with a kill switch should a contestants veer into danger.\n When the event was first planned, Darpa announced that the race would begin with a mass start. Later, that idea was scrapped for fear of the chaos that might result. \n Now the vehicles will be sent off in a more controlled time-trial fashion. Despite the controversy, most experts agree that the Pentagon agency has hit on a brilliantly inexpensive, high-profile approach to helping advance robotic technology. \n ''I'm a little bit sad to see that many waypoints, because the more you constrain the vehicles, the less autonomous is the challenge,'' said Sebastian Thrun, director of the Stanford University Robotics Laboratory. ''At the same time I'm delighted that Darpa is doing this. If someone succeeds, it will open the eyes of the world.''\n The original announcement about two years ago led to more than 100 entrants; last March the agency cut the field to 25. To deal with the overflow, the newly created International Robot Racing Federation has set up another race for later this year.\n Mr. Levandowski, who is leading the Berkeley motorcycle team, took a year off from graduate school, paying for the project in part by renting out rooms in his home, and sought out teammates by posting fliers around campus.\n Since then he has started a company, Robotic Infantry, to develop both commercial and military applications from his research project. ''We see the race,'' he said ''as an opportunity for creating a new platform for robotics.''\n Other teams are also looking beyond the race. Team Overbot, a Silicon Valley team that has already announced it would not be ready, said it would come back next year if nobody won the first race. \n Just as promising as any useful military technologies that might emerge, said John Nagle, Team Overbot's leader, are the potential commercial applications. \n ''The killer app for this thing is automatic rental car return at the airport,'' he said. ''Everyone else focuses on driving in the fast lane. What people hate is driving in traffic.''\n","585":"MOUNTAIN VIEW, Calif. -- In a top-secret lab in an undisclosed Bay Area location where robots run free, the future is being imagined.\nIt's a place where your refrigerator could be connected to the Internet, so it could order groceries when they ran low. Your dinner plate could post to a social network what you're eating. Your robot could go to the office while you stay home in your pajamas. And you could, perhaps, take an elevator to outer space.\nThese are just a few of the dreams being chased at Google X, the clandestine lab where Google is tackling a list of 100 shoot-for-the-stars ideas. In interviews, a dozen people discussed the list; some work at the lab or elsewhere at Google, and some have been briefed on the project. But none would speak for attribution because Google is so secretive about the effort that many employees do not even know the lab exists.\nAlthough most of the ideas on the list are in the conceptual stage, nowhere near reality, two people briefed on the project said one product would be released by the end of the year, although they would not say what it was.\u00a0\n''They're pretty far out in front right now,'' said Rodney Brooks, a professor emeritus at M.I.T.'s computer science and artificial intelligence lab and founder of Heartland Robotics. ''But Google's not an ordinary company, so almost nothing applies.''\nAt most Silicon Valley companies, innovation means developing online apps or ads, but Google sees itself as different. Even as Google has grown into a major corporation and tech start-ups are biting at its heels, the lab reflects its ambition to be a place where ground-breaking research and development are happening, in the tradition of Xerox PARC, which developed the modern personal computer in the 1970s.\nA Google spokeswoman, Jill Hazelbaker, declined to comment on the lab, but said that investing in speculative projects was an important part of Google's DNA. ''While the possibilities are incredibly exciting, please do keep in mind that the sums involved are very small by comparison to the investments we make in our core businesses,'' she said.\nAt Google, which uses artificial intelligence techniques and machine learning in its search algorithm, some of the outlandish projects may not be as much of a stretch as they first appear, even though they defy the bounds of the company's main Web search business.\nFor example, space elevators, a longtime fantasy of Google's founders and other Silicon Valley entrepreneurs, could collect information or haul things into space. (In theory, they involve rocketless space travel along a cable anchored to Earth.) ''Google is collecting the world's data, so now it could be collecting the solar system's data,'' Mr. Brooks said.\nSergey Brin, Google's co-founder, is deeply involved in the lab, said several people with knowledge of it, and came up with the list of ideas along with Larry Page, Google's other founder, who worked on Google X before becoming chief executive in April; Eric E. Schmidt, its chairman; and other top executives. ''Where I spend my time is farther afield projects, which we hope will graduate to important key businesses in the future,'' Mr. Brin said recently, though he did not mention Google X.\nGoogle may turn one of the ideas -- the driverless cars that it unleashed on California's roads last year -- into a new business. Unimpressed by the innovative spirit of Detroit automakers, Google now is considering manufacturing them in the United States, said a person briefed on the effort.\nGoogle could sell navigation or information technology for the cars, and theoretically could show location-based ads to passengers as they zoom by local businesses while playing Angry Birds in the driver's seat.\nRobots figure prominently in many of the ideas. They have long captured the imagination of Google engineers, including Mr. Brin, who has already attended a conference through robot instead of in the flesh.\nFleets of robots could assist Google with collecting information, replacing the humans that photograph streets for Google Maps, say people with knowledge of Google X. Robots born in the lab could be destined for homes and offices, where they could assist with mundane tasks or allow people to work remotely, they say.\nOther ideas involve what Google referred to as the ''Web of things'' at its software developers conference in May -- a way of connecting objects to the Internet. Every time anyone uses the Web, it benefits Google, the company argued, so it could be good for Google if home accessories and wearable objects, not just computers, were connected.\nAmong the items that could be connected: a garden planter (so it could be watered from afar); a coffee pot (so it could be set to brew remotely); or a light bulb (so it could be turned off remotely). Google said in May that by the end of this year another team planned to introduce a Web-connected light bulb that could communicate wirelessly with Android devices.\nOne Google engineer familiar with Google X said it was run as mysteriously as the C.I.A. -- with two offices, a nondescript one for logistics, on the company's Mountain View campus, and one for robots, in a secret location.\nWhile software engineers toil away elsewhere at Google, the lab is filled with roboticists and electrical engineers. They have been hired from Microsoft, Nokia Labs, Stanford, M.I.T., Carnegie Mellon and New York University.\nA leader at Google X is Sebastian Thrun, one of the world's top robotics and artificial intelligence experts, who teaches computer science at Stanford and has developed a driverless car. Also at the lab is Andrew Ng, another Stanford professor, who specializes in applying neuroscience to artificial intelligence to teach robots and machines to operate like people.\nJohnny Chung Lee, a specialist in human-computer interaction, came to Google X from Microsoft this year after helping develop Microsoft's Kinect, the video game player that responds to human movement and voice. At Google X, where he is working on the Web of things, according to people familiar with his role, he has the mysterious title of rapid evaluator.\nBecause Google X is a breeding ground for big bets that could turn into colossal failures or Google's next big business -- and it could take years to figure out which -- just the idea of these experiments terrifies some shareholders and analysts.\n''These moon-shot projects are a very Google-y thing for them to do,'' said Colin W. Gillis, an analyst at BGC Partners. ''People don't love it but they tolerate it because their core search business is firing away.''\nMr. Page has tried to appease analysts by saying that crazy projects are a tiny proportion of Google's work.\n''There are a few small, speculative projects happening at any one time, but we are very careful stewards of shareholders' money,'' he told analysts in July. ''We are not betting the farm on these.''\nCorrection: November 18, 2011, Friday\n","586":"SAN FRANCISCO -- We expect a lot from our computers these days. They should talk to us, recognize everything from faces to flowers, and maybe soon do the driving. All this artificial intelligence requires an enormous amount of computing power, stretching the limits of even the most modern machines.\nNow, some of the world's largest tech companies are taking a cue from biology as they respond to these growing demands. They are rethinking the very nature of computers and are building machines that look more like the human brain, where a central brain stem oversees the nervous system and offloads particular tasks -- like hearing and seeing -- to the surrounding cortex. \n  After years of stagnation, the computer is evolving again, and this behind-the-scenes migration to a new kind of machine will have broad and lasting implications. It will allow work on artificially intelligent systems to accelerate, so the dream of machines that can navigate the physical world by themselves can one day come true.\u00a0\n  This migration could also diminish the power of Intel, the longtime giant of chip design and manufacturing, and fundamentally remake the $335 billion a year semiconductor industry that sits at the heart of all things tech, from the data centers that drive the internet to your iPhone to the virtual reality headsets and flying drones of tomorrow.\n  ''This is an enormous change,'' said John Hennessy, the former Stanford University president who wrote an authoritative book on computer design in the mid-1990s and is now a member of the board at Alphabet, Google's parent company. ''The existing approach is out of steam, and people are trying to re-architect the system.''\n  The existing approach has had a pretty nice run. For about half a century, computer makers have built systems around a single, do-it-all chip -- the central processing unit -- from a company like Intel, one of the world's biggest semiconductor makers. That's what you'll find in the middle of your own laptop computer or smartphone.\n  Now, computer engineers are fashioning more complex systems. Rather than funneling all tasks through one beefy chip made by Intel, newer machines are dividing work into tiny pieces and spreading them among vast farms of simpler, specialized chips that consume less power.\n  Changes inside Google's giant data centers are a harbinger of what is to come for the rest of the industry. Inside most of Google's servers, there is still a central processor. But enormous banks of custom-built chips work alongside them, running the computer algorithms that drive speech recognition and other forms of artificial intelligence.\n  Google reached this point out of necessity. For years, the company had operated the world's largest computer network -- an empire of data centers and cables that stretched from California to Finland to Singapore. But for one Google researcher, it was much too small.\n  In 2011, Jeff Dean, one of the company's most celebrated engineers, led a research team that explored the idea of neural networks -- essentially computer algorithms that can learn tasks on their own. They could be useful for a number of things, like recognizing the words spoken into smartphones or the faces in a photograph.\n  In a matter of months, Mr. Dean and his team built a service that could recognize spoken words far more accurately than Google's existing service. But there was a catch: If the world's more than one billion phones that operated on Google's Android software used the new service just three minutes a day, Mr. Dean realized, Google would have to double its data center capacity in order to support it.\n  ''We need another Google,'' Mr. Dean told Urs H\u00f6lzle, the Swiss-born computer scientist who oversaw the company's data center empire, according to someone who attended the meeting. So Mr. Dean proposed an alternative: Google could build its own computer chip just for running this kind of artificial intelligence.\n  But what began inside data centers is starting to shift other parts of the tech landscape. Over the next few years, companies like Google, Apple and Samsung will build phones with specialized A.I. chips. Microsoft is designing such a chip specifically for an augmented-reality headset. And everyone from Google to Toyota is building autonomous cars that will need similar chips.\n  This trend toward specialty chips and a new computer architecture could lead to a ''Cambrian explosion'' of artificial intelligence, said Gill Pratt, who was a program manager at Darpa, a research arm of the United States Department of Defense, and now works on driverless cars at Toyota. As he sees it, machines that spread computations across vast numbers of tiny, low-power chips can operate more like the human brain, which efficiently uses the energy at its disposal.\n  ''In the brain, energy efficiency is the key,'' he said during a recent interview at Toyota's new research center in Silicon Valley.\n  Change on the Horizon\n  There are many kinds of silicon chips. There are chips that store information. There are chips that perform basic tasks in toys and televisions. And there are chips that run various processes for computers, from the supercomputers used to create models for global warming to personal computers, internet servers and smartphones.\n  For years, the central processing units, or C.P.U.s, that ran PCs and similar devices were where the money was. And there had not been much need for change.\n  In accordance with Moore's Law, the oft-quoted maxim from Intel co-founder Gordon Moore, the number of transistors on a computer chip had doubled every two years or so, and that provided steadily improved performance for decades. As performance improved, chips consumed about the same amount of power, according to another, lesser-known law of chip design called Dennard scaling, named for the longtime IBM researcher Robert Dennard.\n  By 2010, however, doubling the number of transistors was taking much longer than Moore's Law predicted. Dennard's scaling maxim had also been upended as chip designers ran into the limits of the physical materials they used to build processors. The result: If a company wanted more computing power, it could not just upgrade its processors. It needed more computers, more space and more electricity.\n  Researchers in industry and academia were working to extend Moore's Law, exploring entirely new chip materials and design techniques. But Doug Burger, a researcher at Microsoft, had another idea: Rather than rely on the steady evolution of the central processor, as the industry had been doing since the 1960s, why not move some of the load onto specialized chips?\n  During his Christmas vacation in 2010, Mr. Burger, working with a few other chip researchers inside Microsoft, began exploring new hardware that could accelerate the performance of Bing, the company's internet search engine.\n  At the time, Microsoft was just beginning to improve Bing using machine-learning algorithms (neural networks are a type of machine learning) that could improve search results by analyzing the way people used the service. Though these algorithms were less demanding than the neural networks that would later remake the internet, existing chips had trouble keeping up.\n  Mr. Burger and his team explored several options but eventually settled on something called Field Programmable Gate Arrays, or F.P.G.A.s.: chips that could be reprogrammed for new jobs on the fly. Microsoft builds software, like Windows, that runs on an Intel C.P.U. But such software cannot reprogram the chip, since it is hard-wired to perform only certain tasks.\n  With an F.P.G.A., Microsoft could change the way the chip works. It could program the chip to be really good at executing particular machine learning algorithms. Then, it could reprogram the chip to be really good at running logic that sends the millions and millions of data packets across its computer network. It was the same chip but it behaved in a different way.\n  Microsoft started to install the chips en masse in 2015. Now, just about every new server loaded into a Microsoft data center includes one of these programmable chips. They help choose the results when you search Bing, and they help Azure, Microsoft's cloud-computing service, shuttle information across its network of underlying machines.\n  Teaching Computers to Listen\n  In fall 2016, another team of Microsoft researchers -- mirroring the work done by Jeff Dean at Google -- built a neural network that could, by one measure at least, recognize spoken words more accurately than the average human could.\n  Xuedong Huang, a speech-recognition specialist who was born in China, led the effort, and shortly after the team published a paper describing its work, he had dinner in the hills above Palo Alto, Calif., with his old friend Jen-Hsun Huang, (no relation), the chief executive of the chipmaker Nvidia. The men had reason to celebrate, and they toasted with a bottle of champagne.\n  Xuedong Huang and his fellow Microsoft researchers had trained their speech-recognition service using large numbers of specialty chips supplied by Nvidia, rather than relying heavily on ordinary Intel chips. Their breakthrough would not have been possible had they not made that change.\n  ''We closed the gap with humans in about a year,'' Microsoft's Mr. Huang said. ''If we didn't have the weapon -- the infrastructure -- it would have taken at least five years.''\n  Because systems that rely on neural networks can learn largely on their own, they can evolve more quickly than traditional services. They are not as reliant on engineers writing endless lines of code that explain how they should behave.\n  But there is a wrinkle: Training neural networks this way requires extensive trial and error. To create one that is able to recognize words as well as a human can, researchers must train it repeatedly, tweaking the algorithms and improving the training data over and over. At any given time, this process unfolds over hundreds of algorithms. That requires enormous computing power, and if companies like Microsoft use standard-issue chips to do it, the process takes far too long because the chips cannot handle the load and too much electrical power is consumed.\n  So, the leading internet companies are now training their neural networks with help from another type of chip called a graphics processing unit, or G.P.U. These low-power chips -- usually made by Nvidia -- were originally designed to render images for games and other software, and they worked hand-in-hand with the chip -- usually made by Intel -- at the center of a computer. G.P.U.s can process the math required by neural networks far more efficiently than C.P.U.s.\n  Nvidia is thriving as a result, and it is now selling large numbers of G.P.U.s to the internet giants of the United States and the biggest online companies around the world, in China most notably. The company's quarterly revenue from data center sales tripled to $409 million over the past year.\n  ''This is a little like being right there at the beginning of the internet,'' Jen-Hsun Huang said in a recent interview. In other words, the tech landscape is changing rapidly, and Nvidia is at the heart of that change.\n  Creating Specialized Chips\n  G.P.U.s are the primary vehicles that companies use to teach their neural networks a particular task, but that is only part of the process. Once a neural network is trained for a task, it must perform it, and that requires a different kind of computing power.\n  After training a speech-recognition algorithm, for example, Microsoft offers it up as an online service, and it actually starts identifying commands that people speak into their smartphones. G.P.U.s are not quite as efficient during this stage of the process. So, many companies are now building chips specifically to do what the other chips have learned.\n  Google built its own specialty chip, a Tensor Processing Unit, or T.P.U. Nvidia is building a similar chip. And Microsoft has reprogrammed specialized chips from Altera, which was acquired by Intel, so that it too can run neural networks more easily.\n  Other companies are following suit. Qualcomm, which specializes in chips for smartphones, and a number of start-ups are also working on A.I. chips, hoping to grab their piece of the rapidly expanding market. The tech research firm IDC predicts that revenue from servers equipped with alternative chips will reach $6.8 billion by 2021, about 10 percent of the overall server market.\n  Across Microsoft's global network of machines, Mr. Burger pointed out, alternative chips are still a relatively modest part of the operation. And Bart Sano, the vice president of engineering who leads hardware and software development for Google's network, said much the same about the chips deployed at its data centers.\n  Mike Mayberry, who leads Intel Labs, played down the shift toward alternative processors, perhaps because Intel controls more than 90 percent of the data-center market, making it by far the largest seller of traditional chips. He said that if central processors were modified the right way, they could handle new tasks without added help.\n  But this new breed of silicon is spreading rapidly, and Intel is increasingly a company in conflict with itself. It is in some ways denying that the market is changing, but nonetheless shifting its business to keep up with the change.\n  Two years ago, Intel spent $16.7 billion to acquire Altera, which builds the programmable chips that Microsoft uses. It was Intel's largest acquisition ever. Last year, the company paid a reported $408 million buying Nervana, a company that was exploring a chip just for executing neural networks. Now, led by the Nervana team, Intel is developing a dedicated chip for training and executing neural networks.\n  ''They have the traditional big-company problem,'' said Bill Coughran, a partner at the Silicon Valley venture capital firm Sequoia Capital who spent nearly a decade helping to oversee Google's online infrastructure, referring to Intel. ''They need to figure out how to move into the new and growing areas without damaging their traditional business.''\n  Intel's internal conflict is most apparent when company officials discuss the decline of Moore's Law. During a recent interview with The New York Times, Naveen Rao, the Nervana founder and now an Intel executive, said Intel could squeeze ''a few more years'' out of Moore's Law. Officially, the company's position is that improvements in traditional chips will continue well into the next decade.\n  Mr. Mayberry of Intel also argued that the use of additional chips was not new. In the past, he said, computer makers used separate chips for tasks like processing audio.\n  But now the scope of the trend is significantly larger. And it is changing the market in new ways. Intel is competing not only with chipmakers like Nvidia and Qualcomm, but also with companies like Google and Microsoft.\n  Google is designing the second generation of its T.P.U. chips. Later this year, the company said, any business or developer that is a customer of its cloud-computing service will be able to use the new chips to run its software.\n  While this shift is happening mostly inside the massive data centers that underpin the internet, it is probably a matter of time before it permeates the broader industry.\n  The hope is that this new breed of mobile chip can help devices handle more, and more complex, tasks on their own, without calling back to distant data centers: phones recognizing spoken commands without accessing the internet; driverless cars recognizing the world around them with a speed and accuracy that is not possible now.\n  In other words, a driverless car needs cameras and radar and lasers. But it also needs a brain.\n\n\n\n","588":"COMPUTERS still do some things very poorly. Even when they pool their memory and processors in powerful networks, they remain unevenly intelligent. Things that humans do with little conscious thought, such as recognizing patterns or meanings in images, language or concepts, only baffle the machines. \n  These lacunae in computers' abilities would be of interest only to computer scientists, except that many individuals and companies are finding it harder to locate and organize the swelling mass of information that our digital civilization creates.\nThe problem has prompted a spooky, but elegant, business idea: why not use the Web to create marketplaces of willing human beings who will perform the tasks that computers cannot? Jeff Bezos, the chief executive of Amazon.com, has created Amazon Mechanical Turk, an online service involving human workers, and he has also personally invested in a human-assisted search company called ChaCha. Mr. Bezos describes the phenomenon very prettily, calling it ''artificial artificial intelligence.'' \n  ''Normally, a human makes a request of a computer, and the computer does the computation of the task,'' he said. ''But artificial artificial intelligences like Mechanical Turk invert all that. The computer has a task that is easy for a human but extraordinarily hard for the computer. So instead of calling a computer service to perform the function, it calls a human.''\u00a0\n  Mechanical Turk began life as a service that Amazon itself needed. (The name recalls a famous 18th-century hoax, where what seemed to be a chess-playing automaton really concealed a human chess master.) Amazon had millions of Web pages that described individual products, but it wanted to weed out the duplicate pages. Software could help, but algorithmically eliminating all the duplicates was impossible, according to Mr. Bezos. So the company began to develop a Web site where people would look at product pages and be paid a few cents for every duplicate page they correctly identified. \n  Mr. Bezos figured that what had been useful to Amazon would be valuable to other businesses, too. The company opened Mechanical Turk  as a public site in November 2005. Today, there are more than 100,000 ''Turk Workers'' in more than 100 countries who earn micropayments in exchange for completing a wide range of quick tasks called HITs, for human intelligence tasks, for various companies. \n  PriceGrabber.com, a comparison shopping site, uses Mechanical Turk to match images to the product pages. ''Harnessing the power of this enormous, decentralized work force allows us to obtain images for a wide variety of items in a fraction of the time it would have taken to do it ourselves,'' said Sagar M. Jethani, PriceGrabber's director of content development and community. \n  Mechanical Turk's customers are corporations. By contrast, ChaCha.com , a start-up in Carmel, Ind., uses artificial artificial intelligence -- sometimes also called crowdsourcing -- to help individual computer users find better results when they search the Web. ChaCha, which began last year, pays 30,000 flesh-and-blood ''guides'' working from home or the local coffee shop as much as $10 an hour to direct Web surfers to the most relevant resources. \n  Amazon makes money from Mechanical Turk by charging companies 10 percent of the price of a successfully completed HIT. For simple HITs that cost less than 1 cent, Amazon charges half a cent. ChaCha intends to make money the way most other search companies do: by charging advertisers for contextually relevant links and advertisements.\n  Harnessing the collective wisdom of crowds isn't new. It is employed by many of the ''Web 2.0'' social networks like Digg and Del.icio.us, which rely on human readers to select the most worthwhile items on the Web to read. But creating marketplaces of mercenary intelligences is genuinely novel.\n  What is it like to be an individual component of these digital, collective minds? \n  To find out, I experimented. After registering at www.mturk.com, I was confronted with a table of HITs that I could perform, together with the price that I would be paid. I first accepted a job from ContentSpooling.net that asked me to write three titles for an article about annuities and their use in retirement planning. Then I viewed a series of images apparently captured from a vehicle moving through the gray suburbs of North London, and, at the request of Geospatial Vision, a division of the British technology company Oxford Metrics Group, identified objects like road signs and markings.\n  For all this, my Amazon account was credited the lordly sum of 12 cents. The entire experience lasted no more than 15 minutes, and from my point of view, as an occluded part of the hive-mind, it made no sense at all. \n  I was also interested in learning what it was like to be a consumer of crowdsourcing. So at 2:40 p.m. on March 14, I asked ChaCha, ''Who was Evelyn Waugh's commanding officer in the Commandos during World War II?'' In an instant-messaging window, CandieSue22087 immediately welcomed me to ChaCha and asked me to be patient. \n  At 2:44, CandieSue threw up her virtual hands and transferred me to another guide, Tressie57635, who referred me to an academic paper on ''suffixal sound symbolism in the novels of Evelyn Waugh.'' When I protested, Tressie complained that it was a hard search, and at 2:49 she gave up, typing that I might do better with yet another guide. When I agreed, Tressie accidentally ended the search altogether -- but not before serving me a page of 12 search results, not one of which was relevant.\n  A quick search on Google quickly provided the right answer.\n  THERE have been two common objections to artificial artificial intelligence. The first, confirmed by my own experiences searching on ChaCha, is that the networks are no more intelligent than their smartest members. Katharine Mieszkowski, writing last year on Salon.com , raised the second, more serious criticism. She saw Mechanical Turk as a kind of virtual sweatshop. ''There is something a little disturbing about a billionaire like Bezos dreaming up new ways to get ordinary folk to do work for him for pennies,'' she wrote.\n  The ever-genial Mr. Bezos dismisses the criticism. ''MTurk is a marketplace where folks who have work meet up with folks who want to do work,'' he said.\n  Why do people become Turk Workers and ChaCha Guides? In poor countries, the money earned could offer a significant contribution to a family's wealth. But even Mr. Bezos concedes that Turk Workers from rich countries probably can't live on the small sums involved. ''The people I've seen commenting on blogs seem mostly to be using MTurk as a supplemental form of income,'' he said.\n  Mitch Fernandez, 38, a disabled former United States Army linguist, said by e-mail that he became a Turk Worker for various reasons: ''At first, I was just curious about the idea of crowdsourcing.'' But he said he soon found that by working about two hours a day, he could often earn more than $100 a week. In the last nine months he made around $4,000, which he used to buy a high-definition television, a DVD player and a new subwoofer -- all from Amazon.com. \n  ''I do this primarily for the money, but I also view it as a form of therapy to get me used to working again.'' he explained. ''The experience has gotten me thinking about pursuing a library science degree.''\n  We probably have at least another 25 years before computers are more powerful than human brains, according to the most optimistic artificial intelligence experts. Until then, people will be able to sell their idle brains to the companies and people who need the special processing power that they alone possess through marketplaces like Mechanical Turk and ChaCha.\n","589":"The dystopian future envisioned by last year's critical darling \"Her\" reappears in \"Transcendence,\" albeit with a lot more testosterone. You could even call this new thriller - in which a godlike artificial intelligence, played by Johnny Depp, starts building an army of cyborg zombies - \"Him.\" \nUnlike that earlier film about a man who falls in love with a computer operating system, which was a poetic, melancholic meditation on the failure to connect, \"Transcendence\" is a kind of high-tech horror story. \u00a0\nIn this case, the bogeyman is the Internet. The extent to which it succeeds in frightening depends on your philosophical alignment with, say, Ted Kaczynski. \nPeople living in cabins without running water or WiFi, in other words, will probably love it, because it confirms their deepest, darkest fears. All others, proceed with caution.\nThe story gets underway when Will Caster (Depp), a scientist specializing in artificial intelligence, narrowly survives a shooting by a member of a radical anti-technology group. When it becomes clear that Will has been poisoned by a polonium-tipped bullet, his wife, Evelyn (Rebecca Hall), and his best friend, Max (Paul Bettany), hit upon the idea of uploading Will's fading consciousness to a supercomputer. \nWill, who is now merely a collection of ones and zeros, decides to surf the Web, vacuuming up all human knowledge and power - including WebMD, cat videos and access to the global financial system - into his electronic brain. In short order, he has morphed from a mild-mannered computer nerd into a sexier version of Oz the Great and Powerful, reigning, via video screen, over a fortified bunker in the desert, where his arms and legs have been replaced by a cadre of humanoid automatons created, through robotic nanosurgery, in his futuristic subterranean lab.  \nAnd how, exactly, is all of this accomplished?\nConsidering that the setting of the film is the present day - based on the look of the cars people are driving - it's a mystery. A perfunctory shot of a dying Will reciting, in alphabetical order, the contents of the Oxford English Dictionary while his bald head is hooked up to electrodes is meant to aid in our willing suspension of disbelief.\nAny other questions? Look people, we've got a movie to get through here.\nDirected by cinematographer-turned-filmmaker Wally Pfister, from a script by first-timer  Jack Paglen, \"Transcendence\" wastes no time with such details as logic or credibility. It's the kind of movie in which people shout stuff like, \"The incoming queries have all been anonymized!\" - hoping, presumably, that most people will be so surprised to learn that \"anonymize\" is a word that they won't stop to wonder what that sentence means.\nNever mind that artificial intelligence of this level, by even the most optimistic estimates, is still decades away. The concept of a mighty cyber-brain being able to transcend our puny human ones - what some futurists call the \"singularity\" - isn't due before 2045, at the earliest, according to many prognosticators. \nThe problem with \"Transcendence\" isn't that it's an utter crock. At times, the tale, like \"Her,\" betrays flashes of genuine insight into our love-hate relationship with technology. And the acting, while wooden on the part of Depp - who appears throughout most of the film on a monitor, like a high-def Max Headroom - is decent enough.  \nEven the love story sort of works, in the way that Evelyn's yearning for Will blinds her to the fact that the \"Will\" on screen, who isn't above embezzling other people's money to finance the construction of his lair, isn't remotely like the man she married. She's no different from a widow who saves her late husband's voicemails so that she can still listen to him. That's touching, even if her husband's pillow talk these days consists of statements like \"The balance of oxytocin and serotonin in your system is unusual.\" \nNo, the real trouble with \"Transcendence\" is that it just isn't all that scary - at least not in the way that it wants to be.\nThe movie opens with a prologue, set five years in the future, in which there is no more Internet, no more Facebook, no more Google. This post-apocalyptic vision of an unplugged tomorrow is supposed to be horrifying, but in truth, the thought of a world that has hit the restart button, however reluctantly, is actually kind of refreshing.     \nmichael.osullivan@washpost.com\n                 \u00bd \nPG-13. At area theaters. Contains some violent and bloody images, obscenity and sensuality. 119 minutes.\n","590":"Can computers make sense of art? Might they even be able to find connections between works that humans can't see? Computer scientists at Rutgers University recently published two papers that explore how artificial intelligence could aid -- or, some might fear, eventually replace -- art historians by classifying works with a high degree of accuracy and rating them in terms of creative impact.\u00a0\nIn the first paper, Ahmed Elgammal and his graduate student, Babak Saleh, of Rutgers's Art and Artificial Intelligence Lab devised an algorithm that was about 60 percent accurate in identifying the artist and genre of an unknown painting and 45 percent accurate in determining its style. The algorithm not only automatically classified images from a database of 80,000 works, but also allowed the researchers to  find connections  among them. \n  In one example, they picked a selection of paintings and asked the program to identify the ''closest match'' among works in other styles. The results -- which found uncanny similarities between examples of Russian Romanticism and French Impressionism, between Pop Art and the Northern Renaissance -- recall the juxtapositions one sees at museums like the Barnes Collection in Philadelphia, where paintings are grouped not by period but because they possess some kind of stylistic kinship.\n  In the second paper, the researchers posed a more ambitious, and controversial, question: Can the creativity of an artwork be quantified? The experimental program they developed analyzed more than 60,000 paintings over  six centuries, breaking down the visual information contained in each work into categories based on subject matter, brush strokes, color palette and 2,000 other defining attributes termed ''classemes.'' Those techniques enabled them to measure the originality of a painting (how unique it was from earlier works in the sample) and its influence (how similar it was to later works) to assess overall creativity.\n  Many paintings regarded as milestones in art history received very high marks, including Picasso's ''Les Demoiselles d'Avignon'' and Edvard Munch's ''The Scream.'' Not only that, the researchers were able to test their findings with so-called time-machine experiments. When they assigned, for example, Impressionist or Cubist paintings an artificially earlier date, the algorithm generated a significantly higher creativity score.\n  Mr. Saleh, a doctoral candidate at Rutgers, says some art historians who have caught wind of his research are disquieted by its implications. ''Art historians are worried that we're going to replace them, but they should think of machine learning as a tool that can help them,'' he said. ''In the future we're going to have databases of millions of paintings and we need to have artificial intelligence to help humans analyze all these artworks.'' \n\n\n\n","591":"SMARTER THAN YOU THINKHow Technology Is Changing Our Minds for the Better\nBy Clive Thompson341 pp. The Penguin Press. $27.95.\nWhen the world chess champion Garry Kasparov was beaten in 1997 by Deep Blue, an I.B.M. supercomputer, it was considered to be a major milestone in the march toward artificial intelligence. It probably shouldn't have been. As complex as chess is, it's easy to see that its rules can be translated into algorithms so that computers, when they eventually got enough processing power, could crunch through billions of possible moves and past games. Deep Blue's calculations were a fundamentally different process, most people would say, from the ''real'' thinking and intuition a human player would use.\u00a0\nClive Thompson, a Brooklyn-based technology journalist, uses this tale to open ''Smarter Than You Think,'' his judicious and insightful book on human and machine intelligence. But he takes it to a more interesting level. The year after his defeat by Deep Blue, Kasparov set out to see what would happen if he paired a machine and a human chess player in a collaboration. Like a centaur, the hybrid would have the strength of each of its components: the processing power of a large logic circuit and the intuition of a human brain's wetware. The result: human-machine teams, even when they didn't include the best grandmasters or most powerful computers, consistently beat teams composed solely of human grandmasters or superfast machines.\nThompson's point is that ''artificial intelligence'' -- defined as machines that can think on their own just like or better than humans -- is not yet (and may never be) as powerful as ''intelligence amplification,'' the symbiotic smarts that occur when human cognition is augmented by a close interaction with computers. When he played in collaboration with a computer, Kasparov said, it freed him to focus on the ''creative texture'' of the game. In the future, Thompson writes, we should not fear being beaten in chess by Deep Blue or in ''Jeopardy!'' by Watson. Instead, humans will find themselves working in partnership with the progeny of these supercomputers to diagnose diseases, solve crimes, write poetry and become (as the clever double meaning of the book's title puts it) smarter than we think.\nThis is not a new idea. It is based on the vision expounded by Vannevar Bush in his 1945 essay ''As We May Think,'' which conjured up a ''memex'' machine that would remember and connect information for us mere mortals. The concept was refined in the early 1960s by the Internet pioneer J. C. R. Licklider, who wrote a paper titled ''Man-Computer Symbiosis,'' and the computer designer Douglas Engelbart, who wrote ''Augmenting Human Intellect.'' They often found themselves in opposition to their colleagues, like Marvin Minsky and John McCarthy, who stressed the goal of pursuing artificial intelligence machines that left humans out of the loop.\nThompson doesn't delve into this rich technological and intellectual history. What he provides instead are some interesting current examples of how human-\u00adcomputer symbiosis is enlarging our intellect. The use of digital devices and social networks, he shows, helps to facilitate collaborative creativity and an ambient awareness of what's happening in the world, while reducing the need to perform simple memory tasks.\nThompson avoids both the hype and the hand-wringing so common among digital age pontificators by sidestepping most of the topics that agitate the geekosphere, like whether Google is rewiring the neurons in our brains or Twitter is making the world safe for democracy. He comes across as a sensible utopian, tending toward the belief that our digital devices and social networks are, on balance, enhancing our lives and improving the world in the same mixed-blessing sort of way that writing, paper, the printing press and the telephone did.\nIn debunking the doomsayers, Thompson has pleasant sport poking fun at history's procession of pessimists, starting with Socrates and his prediction that writing would destroy the Greek tradition of dialectic. Socrates' primary concern was that people would write things down instead of remembering them. ''This discovery of yours will create forgetfulness in the learners' souls, because they will not use their memories,'' Plato quotes him as saying. ''They will trust to the external written characters and not remember of themselves.''\nThompson counters that Socrates failed to foresee ''the types of complex thought that would be possible once you no longer needed to mentally store everything you'd encountered,'' and he surmises that the same will turn out to be true of our ability to digitally store and easily access huge amounts of information and memories outside of our own brains. ''What's the line between our own, in-brain knowledge and the sea of information around us?'' he asks. ''Does it make us smarter when we can dip in so instantly? Or dumber with every search?''\nHis answer is that our creative minds are being strengthened rather than atrophied by the ability to interact easily with the Web and Wikipedia. ''Not only has transactive memory not hurt us,'' he writes, ''it's allowed us to perform at higher levels, accomplishing acts of reasoning that are impossible for us alone.'' That seems right. My own mind is cluttered with phone numbers I memorized as a kid, but nowadays I outsource that task to my smartphone. I'm eager to make this and similar tasks even easier, and improve my mind (or at least free it up for more daydreaming), by getting my hands on Google Glass.\nThompson also celebrates the fact that digital tools and networks are allowing us to share ideas with others as never before. It's easy (and not altogether incorrect) to denigrate much of the blathering that occurs each day in blogs and tweets. But that misses a more significant phenomenon: the type of people who 50 years ago were likely to be sitting immobile in front of television sets all evening are now expressing their ideas, tailoring them for public consumption and getting feedback. This change is a cause for derision among intellectual sophisticates partly because they (we) have not noticed what a social transformation it represents. ''Before the Internet came along, most people rarely wrote anything at all for pleasure or intellectual satisfaction after graduating from high school or college,'' Thompson notes. ''This is something that's particularly hard to grasp for professionals whose jobs require incessant writing, like academics, journalists, lawyers or marketers. For them, the act of writing and hashing out your ideas seems commonplace. But until the late 1990s, this simply wasn't true of the average nonliterary person.''\nMore important, the writing produced in the new world of blogs and tweets is being done, at least ostensibly, for public discourse and reaction. It may not be getting us back to the dialectic of Socrates' agora, but at least it produces a more stimulating and interactive realm than existed before the Internet.\n","593":"Stan Winston, 62, an Arlington County native and visual effects master who won four Academy Awards for breathing life into some of the most fearsome and fantastic creatures ever seen in films, died June 15 at his home in Malibu, Calif. He had multiple myeloma.\nA pioneer in modern screen makeup as well as cutting-edge animation, Mr. Winston was accomplished in embellishing the appearance of live actors and in constructing and operating mechanical devices so skillfully that they seemed to be alive.\u00a0\nAmong Mr. Winston's creations was the Tyrannosaurus rex of \"Jurassic Park\" (1993) and many of its terrifying companions, which stepped out of the pages of paleontology texts and into Hollywood history.\nWith foam rubber, electronics and powerful motors filling in for flesh, blood and bone, these creations helped Mr. Winston win one of his Oscars. Others were for \"Aliens\" (1986) and \"Terminator 2: Judgment Day\" (1991). He won two Oscars for \"Terminator 2,\" for visual effects and makeup.\nIn addition to the Oscars he won, Mr. Winston had been nominated six other times for films including \"Predator\" (1987), \"Edward Scissorhands\" (1990) and \"Artificial Intelligence: AI\" (2001).\nHe also worked in television, receiving five Emmy nominations and winning the prize twice.\nAs the head of a studio, which began in the garage of his home, Mr. Winston led and supervised teams of skilled craftsmen and technicians in some of his best known projects. He also worked with some of Hollywood's best known performers and filmmakers, including Steven Spielberg, who directed \"Jurassic Park\" and \"Artificial Intelligence,\" and James Cameron, who directed \"Terminator 2\" and \"Aliens.\"\nMr. Winston was considered innovative and imaginative in overcoming the technological challenges his work presented. But he was known as more than a modeler, technician or engineer.\nMany of his innovations were designed to enable mechanical or robotic beings to transcend their inanimate limitations and demonstrate human qualities through such methods as the variation of facial expression.\nOnce interested in an acting career himself, he was \"all about creating character,\" said J. Allen Scott, a supervisor at his studio.\nIn much of his work, Mr. Winston turned human actors into figures of fantasy. He won recognition for the technical wizardry that transformed Michael Jackson into the Scarecrow in \"The Wiz\" (1978) and making a mutant of Johnny Depp in \"Edward Scissorhands.\"\nThrough Mr. Winston's artistry, Danny DeVito appeared with a strikingly shaped nose as the Penguin in \"Batman Returns.\"\nEarly in his career, Mr. Winston provided makeup in the 1974 TV movie \"The Autobiography of Miss Jane Pittman.\" In that production, he made Cicely Tyson, then in her early 40s, into a woman of 110. In \"Aliens,\" Mr. Winston produced a Queen Alien who was 14 feet tall and operated by a crew of technicians using cables.\n\"He was happy with all\" of them, Scott said. \"As long as they served the story and helped create character. . . . He was all about creating character.\"\nMr. Winston was born April 7, 1946, to parents who owned a dress shop for many years. He said that as a child in Arlington, he showed interests in toys, sketching, comic books and monster movies.\nAfter graduating from Washington-Lee High School, he went to the University of Virginia and completed a bachelor's degree in drama and fine arts in 1968.\nIn Hollywood, he had hopes of making a career as an actor. Instead, he completed a long apprenticeship in theatrical makeup and special effects at the Walt Disney Studio and quickly became successful as a makeup specialist.\nSoon after, he won an Emmy for a TV movie, \"Gargoyles\" (1972).\nSurvivors include his wife, Karen Winston; two children; a brother; and four grandchildren.\nHis son, Matt Winston, said his father was in many ways \"a big kid\" with cool toys who enjoyed what he did and would say, \"Just have fun, and success will come.\"\n","595":"Yesterday afternoon, even as scholar were discussing the cultural and educational impact of video games at a three-day symposium sponsored by Harvard University, Steve Russell was sitting a few miles to the west in the Waltham offices of Interactive Data, where he was using a CDI portable terminal to debug a financial management program he has been writing for the Chase Manhattan Bank.\nRussell, a 45-year-old computer whiz, is one of the guys the symposium forgot: 21 years ago, in a Massachusetts Institute of Technology laboratory less than a mile from Harvard, he invented video games.\nAlthough this achievement is generally ascribed to another person not invited to the conference -- Nolan Bushnell, the 40-year-old founder of Atari, because he conceived the game \"Pong\" -- Russell created his inter-galactic, shoot-'em-up \"Spacewar!\" long before Bushnell had ever laid his hands on a terminal.\n\"There's no doubt,\" says Bushnell, \"that 'Spacewars!' made me realize there was money to be made in computer games -- a notion Russell says he and some of his computer buddies \"thought about for maybe four days, but you could calculate on the back of an envelope how many quarters it would take to buy a computer and ther was no way we could imagine it working.\"\u00a0\nRussell seems cut from the classic cloth of creative computing: he's fascinated by electronics and games (he has two computers in his home, and his favorite game is \"Megabug,\" a maze exploration program); he has only one a other obsession: trains, both model and real; his shirt pocket is stuffed with pens (seven, in four different colors).  He was born in Connecticut to a mechanical engineer father and a schoolteacher mother.  His parents moved to Washington state and eventually bought a dairy farm; Russell returned east to study math at Dartmouth, and left shortly before graduation to work at MIT with John McCarthy, a pioneer in artificial intelligence research who developed the programming language LISP that is now almost university used in artificial intelligence work.\nJust as Russell himself is out of the techno tradition, the development of \"Spacewar!\" is similarly a basic and characteristic, although relatively unread, chapter in video game history; like many of the innovations in the field, it came about almost accidentally,\nSpecifically, in the fall of 1962, the Digital Equipment Corp. donated to MIT's artificial intelligence lab a progtotype of its Program Data Processing computer, better known as the DEC PDP-1.  \"It was really the first appliance computer on the market,\" Russell says.  \"If it was off you could walk in and turn on the power switch and it was running.  And it was substantially cheaper than the IBM 370 that was most in use then: $120,000 versus $1 million.\"\nBefore \"Spacewar!\" there was only one crude tennis game, displays of bouncing balls and various graphic patterns, and programs with titles like \"Expensive Planetarium,\" Expensive Desk Calculator,\" Expensive Type-writer\" -- so named because \"to do adding or typing for $120,000 is pretty expensive,\" Russell says.\n\"Somewhere around that time I had read Doc Smith's 'Lensman' series, which has spaceships roaring around the universe.  There were a bunch of us who worked with the computer who were always wondering what we could do it demonstrate its power and eventually we decided that we could create a demo that had something to do with a spaceship and have it be like Lensman shooting around the universe.  I was one of the loudest talkers, and after a few months people were asking me how come I wasn't doing something about it, I said, 'I need to calculate angles and I don't know how to do sines and cosines.'\"\nAother member of this video vanguard supplied Russel with the necessary formulas.  The resultant game program, written in about six weeks, displayed on a cathode ray tube two spaceships that could turn left and right, move forward and fire torpe-does -- a very sophisticated precursor of the arcade game \"Asteroids\" that would be introduced in 1980.\n\"The original control board of the computer had 18 toggle switches for entering data in the form of binary numbers,\" Russell says.  \"We use the four on either end for controls.  We had to change that in a couple of months for separate control boxes, because you always tended to get very sore elbows when two people were sitting beside each other at the computer terminal.\"\nThe control boxes looked remarkably like precutors of contemporary joysticks: they could maneuver the ship, fire its thrusters and torpedoes, and eventually jump it into hyperspace.  \"There were two problems with the early game,\" Russell says.  \"It gave too much advantage to the experienced player and it really didn't exploit the power of the computer, which had a little more than 8K of memory.\"\nThe games spread quickly to most college campuses.  Bushnell, for instance, often played it at the University of Utah, and Russell himself finally realized what he had created a few years later, when he moved to Stanford University.\n\"One night we were working very late and went out for some hamburgers,\" Russell says.  \"There were a bunch of guys in the bar playing pinball, and we all got thrown out when the place closed.  When I got back to the lab, I noticed that the guys who had been playing pinball also worked at the lab.  When they came in they went over to a terminal, and loaded 'Spacewar!' That was when I realized I had really created the new pinball machine.\n\"Maybe I was just ahead of my time.  I have no regrets really that I didn't make money at it.  I had fun, which is what I think computers are all about.  And I'm not sure I have the tolerance for the nonsense that goes with big-bucks consumer electronics.\"\nAnd although Russell may not have received much public acknowledgement for his creation, for years his work was immortal in the computer business: after its creation every DEC PDP-1 was shipped from the factory with the \"Spacewar!\" program loaded in its memory.  \"If the machine survived shipment,\" Russell says, \"first thing the customer ever saw was my game.\"\n","596":"Q. Can I have a conversation with Google Assistant, or will there be an update in the future that will let me have one?\nA. Google Assistant, the company's voice-based helper software that is similar to Amazon's Alexa, Apple's Siri and Microsoft's Cortana software, can already hold basic two-way conversations based on spoken or typed questions and commands. However, more intricate interaction with all these virtual assistant apps is coming as companies expand their research into the scientific areas of artificial intelligence, machine learning and natural language processing. (For those with privacy concerns, keep in mind that most virtual assistant software is designed to collect personal data.)\u00a0\nThe Google Assistant software is available as an app for Android and iOS devices, built into Google Home and other speakers, Android-based wearables, cars, televisions, smart-home appliances and other gear. If you are not sure how to talk to the program, the Google Assistant site has a lengthy list of the questions, commands and topics that you can use with the software, complete with suggestions on how to phrase your requests.\nYou can, for example, tell Google Assistant to remember where you parked your car and then ask the software to remind you of the location later. For those with Google Home speakers, the company recently released a series of Routines, which run through a set of regular daily tasks like adjusting the thermostat and lights before reporting the traffic and weather as you wake up.\nWhen you ask, Google Assistant will start a conversation with third-party chatbot personalities like the Hogwarts Sorting Hat or Bobo the Panda. The Cyber Argument bot on the site can even pit a Google Home speaker against a nearby Amazon Alexa-powered device; for the curious, video clips of chatbots arguing are available, as is an online publication called Chatbots Magazine.\nBut beyond novelty applications, Google (and the other companies) are pushing to develop conversational user interfaces for their products to make them more useful and able to handle complex sets of tasks. An online guide for Google Assistant developers can give you an idea of how apps are designed to work with the software, and you will most likely see software updates to Google Assistant as the software becomes more advanced.\nAmazon, Apple and Microsoft have similar developer programs for their own virtual assistants. To help inspire its developers even more, Amazon last year created the Alexa Prize, a contest to advance conversational artificial intelligence with a financial reward to the research team that creates the best \"socialbot\" that can \"can converse coherently and engagingly with humans on a range of current events and popular topics such as entertainment, sports, politics, technology and fashion.\"\nPHOTO: The Google Assistant can have simple conversations with chatbots like the Harry Potter-inspired Sorting Hat app. (PHOTOGRAPH BY  FOR THE NEW YORK TIMES)\n","597":"It is the time of year when the big technology research shops gaze into the near future to identify trends that will have the most impact on corporate America and the tech industry over the next few years.\nThe yearly ritual, of course, is partly a marketing vehicle for the research firms. Still, the forecasts can be thought-provoking, especially if they are detailed and sharpened by attaching numbers to their predictions. \n  IDC's outlook, being published on Wednesday, qualifies. It is a 20-page document, chock-full of details, and its authors are not afraid of making numerical guesses about the future. By 2020, for example, IDC says that more than 30 percent of today's tech suppliers will ''not exist as we know them today,'' having been acquired or failed.\u00a0\n  Going beyond the detail in the IDC forecast, and reading reports published last month by Gartner and Forrester Research, the overall theme is that the pace of digital innovation is accelerating and broadening.\n  The digital technologies that are changing the economics and practices of traditional business -- cloud computing, mobile devices, advanced data analysis and artificial intelligence -- are better, cheaper and more widely available.\n  ''Mainstream companies in every industry are realizing they'll be disrupted if they don't get moving now,'' said Frank Gens, IDC's chief analyst and the report's principal author.\n  Many of these companies, according to IDC, are not moving fast enough. It predicts that a third of the top 20 companies in every industry will be ''disrupted'' over the next three years, meaning their revenue, profits and market position will deteriorate -- not that they will go out of business.\n  The reports of the three research firms vary in their emphasis. The Forrester outlook focused on organizing technology and corporate strategy around the ''age of the consumer.''\n  The Gartner study featured the likely impact of advances in artificial intelligence, with ''autonomous software agents'' expected to play a crucial role in the economy and everyday life. ''The future,'' the Gartner report said, ''will belong to the companies that can create the most effective autonomous and smart software solutions.''\n  Code is king in the IDC report as well. By 2018, IDC said that corporations pursuing digital transformation strategies would ''more than double the size of their software development teams.'' So the job market for software engineers with cloud and web development skills should stay hot.\n  The same is true for data scientists. In the digital economy, the IDC report said, ''innovation = code + data. Data is the grist of the innovation mill.''\n  The IDC report foresees big growth for Internet of Things devices and for the software needed to make sense of all the sensor data. By 2018, IDC predicts, the number of Internet of Things devices will more than double, prompting the development of 200,000 new apps. In the near future, companies lacking an Internet of Things strategy and expertise, the IDC report said, will be ''like individuals functioning without most of their five senses.''\n  Cloud computing is fast becoming the fundamental technology engine in corporations. IDC predicts that by 2020, spending on cloud services and related hardware and software will be more than $500 billion, three times the current level.\n  In the IDC view, there will be different tiers of the cloud business. One level will be several ''industry cloud platforms,'' the report says, developed by mainstream companies like ''General Electric, John Deere, Johnson Controls and United HealthCare'' that are ''providing epicenters of innovation, growth and disruption in their own industries.''\n  But for underlying cloud infrastructure, IDC sees a ''significant consolidation,'' with ''six or fewer cloud platform vendors'' holding 80 percent or so of the market by 2020.\n  In an interview, Mr. Gens said the candidate survivors included Amazon, Microsoft, Google, Salesforce, IBM and two Chinese companies, Alibaba and Tencent.\n  In the next 12 to 18 months, Mr. Gens predicts, there will be another entry in the cloud platform market: Apple. Its cloud presence today, Mr. Gens said, is mainly though the services that connect to the iPhone for data backup and syncing devices. But Apple has the resources, and it is getting into Internet of Things data with the Apple Watch fitness-tracking capabilities and other cloud-based fields. And Apple's technology is increasingly used in corporations.\n  ''I think Apple has to make a broader cloud move, including in the enterprise market,'' Mr. Gens said. ''Apple can't just stay on the periphery of a technology that is so fundamental.''\n  As Tech Booms, Workers Turn to Coding for Career Change\n  This is a more complete version of the story than the one that appeared in print.         \n\n\n\n","598":"Have we finished talking about bringing back coal-mining jobs yet? Or are we still giving that some more pointless deliberation? How about traditional manufacturing jobs? Shall we pretend a while more on that topic? While President Trump is sitting in the fire truck for his photo op about American jobs, he apparently can't see the fire right in front of him. And apparently, neither do we.\u00a0\nIt's true enough that we have been through a wrenching series of changes in our economy. The last election was to a large degree about just that. But as bad and dislocating as it has been, we are in for far larger changes - probably vastly larger. And we are simply wasting time thinking in terms of how Things Used To Be.\nYou can like the idea of artificial intelligence swarming into every aspect of our lives, or not. But you had better understand it. Because the handwriting is on the wall, and it is in a sophisticated script written by a robotic hand. It says Everything Is About to Be Completely Different.\nWe are just beginning to have the required conversations about Universal Basic Income, health insurance, education and the very meaning of work in the society that is no further away than a couple of presidential terms.\nMeanwhile. we are chasing each other's tails in pointless partisan acrimony at the behest of a low-information president who does not have the slightest clue about the present, let alone the future. We are vainly arguing over whether we can get our clocks to run backward, or whether we should. And by the time we conclude that argument, we will have discovered that all the while the clocks have continued to run forward and we are out of time.\nThe coming era of mass artificial intelligence will be the biggest disrupter in human history, and if you don't think so, ask your home digital assistant, Siri, Cortana, Alexa or Google Now. This is either going to be a catastrophe for workers or just possibly the door to utopia, but one thing is for sure: We won't be going back to the 1950s, regardless of Trump's incomprehension or the vehemence of his supporters.\nTrump's excuse is he's a doddering old moron.\nNow what's ours?\n","600":"Few companies -- especially tech companies -- can trace their history back more than a century.\nIBM can. Before computers were even invented, the company was making meat and cheese slicers; these days, it's a $140 billion technology solutions behemoth. \u00a0\n  But IBM is enduring a pivotal time, with 14 consecutive quarters of declining sales and a strategy geared toward new fields, like data analytics and artificial intelligence, to compensate for the erosion of some of its traditional software and services businesses.\n  At the DealBook conference on Tuesday in New York, where the focus was long term versus the short term, Virginia M. Rometty, the chief executive of IBM, said that to survive, companies must be able to change.\n  ''What's important is that we grow in the right areas,'' said Ms. Rometty. ''Tech is littered with areas that you can have high growth and make no money. That's not us.''\n  IBM certainly throws off plenty of cash and has spent billions in recent years on stock buybacks. But Ms. Rometty has been criticized for not deploying that capital by investing in her business, which is undergoing a substantial shift.\n  ''My job is to steward capital for the long term,'' she said. ''Why do we do share buybacks? Because we can, but it's not in place of other things.''\n  Most recently, IBM acquired the digital assets from the Weather Company, including its weather.com website. IBM is looking to combine it with Watson, its artificial intelligence system that can sift through data and make decisions.\n  Ms. Rometty said on Tuesday that Watson, and the ability for systems to think, was the ''most disruptive and transformative'' trend out there.\n\n\n\n","601":"Imagine a police car that issues tickets without even pulling you over.\nWhat if the same car could use artificial intelligence to find good hiding spots to catch traffic violators and identify drivers by scanning license plates, tapping into surveillance cameras and wirelessly accessing government records?\nWhat if a police officer tapping on your car window asking for your license and registration became a relic of transportation's past?\u00a0\nThe details may sound far-fetched, as if they belong in the science-fiction action flick \"Demolition Man\" or a new dystopian novel inspired by Aldous Huxley's \"Brave New World,\" but these scenarios are grounded in a potential reality. They come from a patent developed by Ford and being reviewed by the U.S. government to create autonomous police cars. Ford's patent application was published this month.\nAlthough experts claim autonomous vehicles will make driving safer and more rule-bound, Ford argues in its application that in the future, traffic violations will never disappear entirely.\n\"While autonomous vehicles can and will be programmed to obey traffic laws, a human driver can override that programming to control and operate the vehicle at any time,\" the patent's application says. \"When a vehicle is under the control of a human driver there is a possibility of violation of traffic laws. Thus, there will still be a need to police traffic.\"\nThe patent application says that autonomous police vehicles don't necessarily replace the need for human police officers for catching traffic scofflaws. Some \"routine tasks,\" such as issuing tickets for failure to stop at a stop sign, can be automated, the patent says, but other tasks that can't be automated will be left to people.\nThe application, which was filed in July 2016 and includes elaborate diagrams depicting the autonomous police car interacting with its environment, says officers could be inside the vehicle at all times and reclaim control of the car when necessary.\nBut the application also shows how an autonomous police vehicle could be able to carry out many tasks we associate with human officers.\nIn one scenario, a surveillance camera or roadside sensor documents a speeding vehicle. A signal is relayed through a \"central computing system\" to the autonomous police vehicle, which is tasked with pursuing the vehicle, tracking its location and capturing video that can be used to analyze the fleeing vehicle's movement.\nIn another, the police vehicle analyzes traffic patterns using machine learning - a type of artificial intelligence that gives computers the ability to learn without being programmed - to determine ideal spots for catching traffic violators. Once a hiding spot has been located, the vehicle uses sensors - lasers, cameras or some combination thereof - to monitor traffic in the most efficient way possible, according to the patent.\n\"Autonomous police vehicle may determine the threshold speed for a given section of road by searching a local traffic laws database for a legal speed limit for that section of road or by querying remote central computing system,\" the patent says.\nThe vehicle would be able to communicate wirelessly with other vehicles on the road and determine whether a car is in self-driving mode or being controlled by a human driver, according to the patent. The patent says the offending vehicle would be able to communicate with the police car as well, providing  a driver's license, for example.\nLike traffic cameras already in use, tickets could be issued remotely, the application notes, and a record of the incident could be sent to a police station or a department of motor vehicles.\nBut Ford noted in a statement that even if the patent is approved, it does not ensure that a product will be produced.\n\"We submit patents on innovative ideas as a normal course of business,\" the statement said. \"Patent applications are intended to protect new ideas but aren't necessarily an indication of new business or product plans.\"\n          Correction: A previous version of the story said that the patent was granted earlier this month. Its application is still being reviewed by the US Patent and Trademark Office. This version has been corrected.       \n          MORE READING:        \n          Elon Musk has already made millions selling a flamethrower       \n          'It's a dream come true!': Tesla customers get their first glimpse of the Model 3       \n          Big Brother on wheels: Why your car company may know more about you than your spouse.       \n","603":" Proponents of autonomous vehicles are in a sticky situation. Self-driving technology is expected to have a tremendous impact on public health and reduce the 1.25 million deaths every year on global roads. At the same time, this emerging technology is a threat to the employment of the millions who are paid\u00a0to sit behind the wheel - from truck drivers to cab drivers and delivery workers. \u00a0\n Baidu chief scientist Andrew Ng, an expert in the world of artificial intelligence, acknowledges the unemployment concerns, but he sees a way forward that offers society the benefits of autonomous vehicles and blunts the negative impact of job losses. \n \"I feel a strong moral responsibility or obligation to try to make self-driving cars a reality as quickly as possible,\" Ng said in a visit to The Washington Post. At Baidu, a Chinese tech company where Ng is developing self-driving technology, the number 3,000 has become a rallying call, representing the number of humans killed every day on roads. For Ng self-driving cars are a prime example of the benefits of what he calls the golden age of artificial intelligence. \nBut for humans who could lose their jobs in this golden age, governments could offer a solution, Ng says. He is\u00a0an advocate for basic income, in which governments pay citizens a nominal amount to guarantee a basic standard of living. Several Northern European countries are planning basic income experiments.\n \"We as a society have an ethical responsibility to help those whose jobs are displaced by this value-creating artificial intelligence,\" Ng said. \"I think everyone in this country has a right to a livelihood. Everyone has a right to chance at having a great life.\" \n Ng suggests a tweak to basic income - paying the unemployed to study online and prepare for a new career.  \n \"We ask you to invest in yourself so as to increase the chance that you can re-enter the workforce if you're unemployed, and contribute back to the tax base and contribute back more to society in the future,\" Ng said. \nNg, who is also the chairman of the online education platform Coursera, believes such digital learning programs are a natural fit for retraining workers because of\u00a0their low costs. Coursera, which Ng co-founded before joining Baidu, is\u00a0independent\u00a0of the tech company.\n Such education options could become especially valuable as early as 2018, when Baidu plans to have commercial self-driving cars on the road. Ng envisions launching them in limited areas, such as bus routes, rather than having the cars drive everywhere at once. \n In the meantime, Ng believes cities and corporations should team up to make infrastructure changes to prepare for the vehicles. Some intersections may need duplicate traffic lights to aid self-driving cars. And Ng wants construction workers to be given devices that would allow them to easily communicate with self-driving vehicles. (Construction sites are difficult for driverless\u00a0vehicles to navigate because of\u00a0their fluid nature.) \n Ng wants more\u00a0standardized environments, which will make it easier for the self-driving vehicles to function across the globe. \n \"It's just confusing that some cars [around the world] drive on the left and some on the right,\" Ng said. \"I prefer driving on the right side.\" \n","604":"EARLY in the film \"A Beautiful Mind,\" the mathematician John Nash is seen sitting in a Princeton courtyard, hunched over a playing board covered with small black and white pieces that look like pebbles. He was playing Go, an ancient Asian game. Frustration at losing that game inspired the real Mr. Nash to pursue the mathematics of game theory, research for which he eventually won a Nobel Prize. \n     In recent years, computer experts, particularly those specializing in artificial intelligence, have felt the same fascination -- and frustration. \n Programming other board games has been a relative snap. Even chess has succumbed to the power of the processor. Five years ago, a chess-playing computer called Deep Blue not only beat but thoroughly humbled Garry Kasparov, the world champion at the time. That is because chess, while highly complex, can be reduced to a matter of brute force computation. \u00a0\nGo is different. Deceptively easy to learn, either for a computer or a human, it is a game of such depth and complexity that it can take years for a person to become a strong player. To date, no computer has been able to achieve a skill level beyond that of the casual player. \nThe game is played on a board divided into a grid of 19 horizontal and 19 vertical lines. Black and white pieces called stones are placed one at a time on the grid's intersections. The object is to acquire and defend territory by surrounding it with stones. \nProgrammers working on Go see it as more accurate than chess in reflecting the ineffable ways in which the human mind works. The challenge of programming a computer to mimic that process goes to the core of artificial intelligence, which involves the study of learning and decision-making, strategic thinking, knowledge representation, pattern recognition and, perhaps most intriguingly, intuition. \n\"A good Go player could make a move and other players say, 'Yes, that's a good move,' but they can't explain to you why it's a good move, or how they even know it's a good move,\" said Dr. John McCarthy, a professor emeritus at Stanford University and a pioneer in artificial intelligence.\nDr. Danny Hillis, a computer designer and chairman of the technology company Applied Minds, said that the depth of Go made it ripe for the kind of scientific progress that comes from studying one example in great detail. \"We want the equivalent of a fruit fly to study,\" Dr. Hillis said. \"Chess was the fruit fly for studying logic. Go may be the fruit fly for studying intuition.\" \nAlong with intuition, pattern recognition is a large part of the game. While computers are good at crunching numbers, people are naturally good at matching patterns. Humans can recognize an acquaintance at a glance, even from the back. \"Every Go book is filled with advice on patterns of different kinds,\" Dr. McCarthy said. \nDr. Daniel Bump, a mathematics professor at Stanford, works on a program called GNU Go in his spare time. \"You can very quickly look at a chess game and see if there's some major issue,\" he said. But to make a decision in Go, he said, players must learn to combine their pattern-matching abilities with the logic and knowledge they have accrued in years of playing. \n\"If you watch really strong players,\" Dr. Bump said, \"some seem to make fairly mundane moves, but at the end of the game they're ahead. Others do spectacular things.\" \nOne measure of the challenge the game poses is the performance of Go computer programs. The last five years have yielded incremental improvements but no breakthroughs, said David Fotland, a programmer and chip designer in San Jose, Calif., who created and sells The Many Faces of Go, one of the few commercial Go programs. \nMr. Fotland's program was the winner of a tournament last weekend in Edmonton, Alberta, that pitted 14 Go-playing programs -- including several from Japan -- against one another. But even The Many Faces of Go is weak enough that most strong players could beat it handily. \nPart of the challenge has to do with processing speed. The typical chess program can evaluate about 300,000 positions per second, and Deep Blue was able to evaluate some 200 million positions per second. By midgame, most Go programs can evaluate only a couple of dozen positions each second, said Anders Kierulf, who wrote a program called SmartGo. \nIn the course of a chess game, a player has an average of 25 to 35 moves available. In Go, on the other hand, a player can choose from an average of 240 moves. A Go-playing computer would take about 30,000 years to look as far ahead as Deep Blue can with chess in three seconds, said Michael Reiss, a computer scientist in London. \nIf processing power were all there was to it, the solution would be simply a matter of time, since computers are growing ever faster. But the obstacles go much deeper. Not only do Go programs have trouble evaluating positions quickly, they have trouble evaluating them correctly. \nNonetheless, the allure of computer Go increases as the difficulties it poses encourage programmers to advance basic work in artificial intelligence. Graduate students produce dissertations on the topic, and a handful of researchers around the world devote much or all of their attention to it. \nThe game attracts people from all fields. For example, Chen Zhixing, a retired chemistry professor in Guangzhou, China, wrote a program called Handtalk, which dominated the computer Go field for several years. Dr. Bump, 50, whose field is number theory, has been playing Go for 35 years and taught himself the C programming language four years ago so he could write Go software. Mr. Fotland, 44, the creator of The Many Faces of Go has been working on computer Go for 20 years and is chief technology officer at Ubicom, a small semiconductor company in Silicon Valley. \nAll are very strong Go players, and it takes a strong Go player to write even a weak Go program. Mr. Fotland, for instance, said he had written programs for checkers, Othello and chess. The algorithms are all very similar, and it is not difficult to write a reasonably strong program, he said. Each of the games took him a year or two to finish. \"But when I started on Go,\" he said, \"there was no end to it.\" \nMr. Fotland said that his Go programming was especially weak when he was a beginning player. \"A lot of the stuff I wrote was just plain wrong because I didn't understand the game well enough,\" he said. \nEven when skill develops, however, translating it into a program is not an obvious task. \"There's a certain stream of consciousness when you're looking at positions,\" Dr. Bump said. \"You might look at 10 variations, but you don't really know what's going on in the back of your mind. Even a strong player doesn't know how his mind works when he looks at a position.\" \n\"We think we have the basics of what we do as humans down pat,\" Dr. Bump said. \"We get up in the morning and make breakfast, but if you tried to program a computer to do that, you'd quickly find that what's simple to you is incredibly difficult for a computer.\" \nThe same is true for Go. \"When you're deciding what variations to consider, your subconscious mind is pruning,\" he said. \"It's hard to say how much is going on in your mind to accomplish this pruning, but in a position on the board where I'd look at 10 variations, the computer has to look at thousands, maybe a million positions to come to the same conclusions, or to wrong conclusions.\" \nDr. Reiss, who is the author of Go4++, a previous champion that placed second in last weekend's playoff, agrees with Dr. Bump. Dr. Reiss, who is an expert in neural networks, compares a human being's ability to recognize a strong or weak position in Go with the ability to distinguish between an image of a chair and one of a bicycle. Both tasks, he said, are hugely difficult for a computer. \nFor that reason, Mr. Fotland said, \"writing a strong Go program will teach us more about making computers think like people than writing a strong chess program.\" \nDr. Reiss, who works on Go full time, said he would not think of devoting his time to any other problem. \"It's a fundamentally interesting problem, but also it's just the right level of difficulty,\" he said. \"If it was too easy it would have been solved already. If it was fantastically difficult, people might give up in frustration.\" \n\"I think in the long run the only way to write a strong Go program is to have it learn from its own mistakes, which is classic A.I., and no one knows how to do that yet,\" Mr. Fotland said. A few programs have some learning capabilities built into them. \nMr. Fotland's program, for instance, refers to a database of games played by strong players in deciding its moves, and Dr. Reiss's program employs a learning scheme for deciding which moves are interesting to look at. \nDr. Reiss said he had come up with an idea for a new Go program that would learn by analyzing professional games. But to pursue his idea would require too much work, he said, depriving him of time to continue making updates to his current program. \nIt seems unlikely that a computer will be programmed to drub a strong human player any time soon, Dr. Reiss said. \"But it's possible to make an interesting amount of progress, and the problem stays interesting,\" he said. \"I imagine it will be a juicy problem that people talk about for many decades to come.\" Places to GoA few programs for players who would like to try their hand at computer Go:THE MANY FACES OF GO: www.smart-games.comFor Windows, including a free 9x9 versionGO4++: www.reiss.demon.co.uk\/webgo\/compgo.htmFor Windows.HANDTALK: www.yutopian.com\/go\/For WindowsSMARTGO: www.smartgo.com\/download.htmFor Windows and MacintoshGNU GO: www.gnu.org\/software\/gnugo\/gnugo.htmlFor Windows, Linux, and Mac OS XAIGO: www001.upp.so-net.ne.jp\/iizuka\/#contents-1For Palm OS 3.0 or laterOnline PlayTHE INTERNET GO SERVER: panda-igs.joyjoy.net\/English\/IGSsgc.htmlTHE NO NAME GO SERVER: nngs.cosmic.org\/\n","605":"Ever since a distinctive three-armed logo appeared in the early '90s on the sleeves of the albums of Aphex Twin - the alias of the prolific electronic music producer Richard D. James - it has remained a vital emblem for dance music fans around the world. So when a lime-green blimp bearing that insignia appeared over London (and mysterious stencils of it appeared on sidewalks around Manhattan and Brooklyn) back in mid-August, electronic music fans rejoiced, knowing that after 13 years, a new Aphex Twin album was coming.  Since his disappearance from public view, James's influence has only grown: His example led Radiohead to embrace electronic music throughout the aughts, he's been sampled by Kanye West and the electronic dance music poster boy Skrillex has called the Aphex Twin track \"Flim\" his favorite of all time.\u00a0\nTomorrow sees the release of \"Syro,\" a double album on the pioneering electronic label Warp Records that finds the reclusive genius - now a father of two living in rural Scotland - at his mischievous, beat-twisting best. (It can be streamed here in its entirety; a Spotify account is required.) The record also shares a lineage with the eye-catching, face-distorting cover art of releases like 1997's \"Come to Daddy\" and 1999's \"Windowlicker,\" again finding James collaborating with the groundbreaking firm The Designers Republic (TDR) on the visuals. Here, the collective's founder and creative director, Ian Anderson, chats with T about the thought process behind some of Aphex Twin's most iconic cover art.\n \"Artificial Intelligence\" compilation (1992)\"I've always been interested by the idea of music made by machines. TDR was already knee-deep in synths when Warp Records was set up. I did the Warp logo and by the time of their Artificial Intelligence compilation in 1992 featuring an early Aphex Twin track, we'd already been shaping the look of the bleeps people were hearing out of Sheffield.\"\n \"On\" EP (1993) and \"Ventolin\" EP (1995)\"We'd worked with Richard pulling together illustrations he supplied for these EPs. But I'm not sure I can ever come to terms with the fact TDR didn't design his famous logo.\"\n \"Donkey Rhubarb\" EP (1995)\"We also did the 'Donkey Rhubarb' sleeve (which took an image of his smile from the cover of 1995's '...I Care Because You Do' and tiled it 25 times) in what turned out to be the first in a series of 'un-design' collaborations with Richard culminating ultimately in 'Syro.'\"\n \"Come to Daddy\" (1997)\"The 'Come to Daddy' imagery was something that festered and grew in the space between Richard and the phenomenal visual artist Chris Cunningham. We were supplied a series of images - some finished, others in progress or discarded - and we pulled them together into a cohesive series of designs across a number of formats.\"\n \"Come to Daddy Remixed\" (1997)\"For us the key elements in the 'Come to Daddy' art were the typographic deconstructions of the photographic imagery and of the TV ad for Orange Mobile, which had used one of the Aphex Twin remix tracks. For copyright reasons we weren't allowed to show an image of the art, so we reduced the ad to a short descriptive text in reversed white out of orange.\"\n \"Windowlicker\" (1999)\"Again, this was beautifully right\/wrong thinking from Richard and Chris. I still don't know where their heads were at with that. What TDR did was edit the various shots from the video shoot and format them into a single image. The minimal type detail we used seems a natural choice now, but at the time there were serious arguments going off about how big the logo should be, whether there should be a more recognizable Warp\/TDR graphic, et cetera.\"\n \"Syro\" (2014)\"It's a continuing deconstruction of everything the audience might think they know, might think they want to know, might think they'd like to think about what is an album, what is the nature of communication, what is the relationship between maker and shaker, sound and listener, et cetera? 'Syro\"s design is a provocation. It's a set of clues without a mystery. It's an inventory of a particular process peculiar to a peculiar industry. It's a bit of a grin.\"                      \n","606":"Digital classrooms are a growing field, thanks in no small part to success stories like the Khan Academy, so it's no surprise that traditional brick-and-mortar universities are increasingly looking to add an online element to their learning platforms. \nThis week Harvard and M.I.T. joined the fray with a $60 million online-classroom venture named edX. The nonprofit enterprise will offer a variety of free courses across disciplines for anyone with an Internet connection. \nEdX seeks to move beyond mere video tutorials by incorporating discussions, labs, quizzes and other interactive  learning tools into the lesson plan.  And because edX is an open-source project, it has the potential for other universities such as Stanford - who successfully offered their own online Artificial Intelligence class last year -- to use the project. \nM.I.T said it would rely on its existing online initiative MITx as the foundation for edX. Anant Agarwal, director of M.I.T.'s Computer Science and Artificial Intelligence Laboratory (Csail), has been tapped to be edX's first president.  The initial round of edX courses will be announced this summer and students can start taking classes - which come with a certificate of completion - in the fall.\n\n","608":"James Cameron gathers fellow sci-fi buffs to explore the genre's history in a new AMC series. And the eccentric Sherlock Holmes of ''Elementary'' returns for Season 6.\u00a0\nWhat's on TV \n  AMC VISIONARIES: JAMES CAMERON'S STORY OF SCIENCE FICTION 10 p.m. on AMC. More than a century after the first known science-fiction movies were made, filmmakers continue to churn out supernatural stories, and viewers can't get enough. This six-part documentary series, led by the writer, producer and director James Cameron, traces the genre's shift from niche to mainstream and explores its themes through interviews with Steven Spielberg, George Lucas and other notable sci-fi fans. It opens with a look at alien-centered movies like ''Arrival,'' ''Avatar'' and ''E. T.'' In an interview with The New York Times, Mr. Cameron shared his motivation to tell otherworldly tales: ''The thing that encourages me is that we are actually approaching an existential crisis as human beings about artificial intelligence and the threat of artificial intelligence. The trick is to make these movies while they're still science fiction.''\n  VERY SUPERSTITIOUS WITH GEORGE LOPEZ 8 p.m. on A&E. While James Cameron explores the supernatural in entertainment, George Lopez searches for it in everyday lives. The actor draws from his upbringing in a superstitious Mexican household in this new passion project, crisscrossing the United States in search of unusual beliefs and rituals that some communities still hold dear.\n  ELEMENTARY 10 p.m. on CBS. This modern-day take on Sherlock Holmes, played by Jonny Lee Miller, returns for a sixth season. While Sherlock grapples with a potentially crippling medical diagnosis, he meets a man (Desmond Harrington) who claims that Sherlock helped him overcome an addiction. Back at work, Sherlock and his partner Joan (Lucy Liu) search for a heiress's missing partner.\n  STICKER SHOCK 10 p.m. on Discovery. Curious car owners present their special rides to a team of experts to find out their value in this new unscripted show.\n   GOOD GIRLS 10 p.m. on NBC. The pilot of this absurd drama made it clear that the three suburban moms-turned-criminals, portrayed by Christina Hendricks, Mae Whitman and Retta, weren't going to rob a supermarket one night and return to their normal lives the next. After getting a little too close with Rio (Manny Montana), a gang leader, the ladies realize their lives may be on the line. In this season finale, they scheme to take down Rio while getting their hands on enough money to pay for a kidney transplant for Sara (Lidya Jewett).\n  What's Streaming\n  THE KARATE KID (1984) on iTunes, Amazon, Crackle, Google Play, Hulu, Vudu and YouTube. Revisit this classic underdog tale before the premiere of the revival series ''Cobra Kai'' on Wednesday. The teenage Daniel (Ralph Macchio) provokes the ire of Johnny (William Zabka), a bullying karate student, when he starts dating his ex-girlfriend (Elisabeth Shue). Tired of being humiliated by Johnny and his karate gang, Daniel seeks help from a martial arts master (Pat Morita) to build self-confidence and confront Johnny in a final fight.\n\n\n\n","609":"My New York University colleague Meredith Broussard recently debuted a new website, Campaign-finance.org, that provides tools for visualizing campaign finance data related to the 2016 U.S. election campaign. What follows is a lightly edited transcript of an email conversation we had regarding the website, her motivation for building it, what's being done with it so far, and more general challenges facing the field of data journalism:\n              Joshua Tucker (JT): You just launched campaign-finance.org, a new tool for analyzing campaign finance data. What is it and what does it do?           \u00a0\n             Meredith Broussard (MB): Campaign-finance.org is an artificial intelligence tool to help journalists quickly and efficiently uncover story ideas in campaign finance data. We take open data published by the Federal Election Commission and present it in an easier-to-understand format. It includes data for all of the 4,000+ federal candidates in 2016, as well as more than 17,000 political committees.\nNicknamed Bailiwick, the tool organizes the data into visualizations that are easy to understand. For example, you can use it to see which PACs and super PACs are supporting a candidate, and what these groups' total spending has been. You can also see the groups that oppose the candidate, and how much they have spent.\n              JT: What motivated you to produce this tool? Why did you see a need for it?           \n             MB: I started my career as a computer scientist, then quit to become a journalist. Today, I teach data journalism at NYU's Arthur L. Carter Journalism Institute. My academic research focuses on artificial intelligence for investigative reporting. I am interested in building technology that helps to uncover, understand, explain, and (hopefully) solve social problems.\nFunding for investigative journalism has been shrinking for decades. A few years ago, I became interested in finding new ways to use technology to automate discovery so that journalists can do what they are good at doing, but faster and better.\nMy last big project was developing an AI tool that helped me uncover textbook shortages and funding problems at Philadelphia public schools. I thought the same core technology could be applied to other public affairs topics, like campaign finance or transportation. I was fortunate enough to get a grant from the Tow Center for Digital Journalism at Columbia Journalism School to develop the campaign finance tool. My collaborators and I worked on the tech for a full year before we launched it.\n              JT: Has anyone used the tool for reporting?           \n             MB: Yes! I gave a talk about the tool in October at the Computation + Journalism Symposium at Stanford. Jason Clampet, the founder of Skift.com, happened to be in the audience. He called up campaign-finance.org on his laptop, got a handful of story ideas, and immediately assigned one to a Skift writer, who turned out \"Clinton vs. Trump: Where Presidential Candidates Spend Their Travel Dollars.\" Here's a quote:\nWith one month left before election day, Republican candidate Donald Trump has now spent more than $5.9 million on flights hosted on his own private jet service TAG Air, according to the latest operating expenditure reports from the Federal Election Commission (FEC) filed on September 22.\nIn my political reporting classes, 25 student journalists are using the tool to find stories.\n              JT: How do you hope journalists will use the tool for the remainder of the 2016 campaign? How about after the election?           \n             MB: There's not much time left for long-term investigative stories. Those will have to wait until the next election. In the few weeks remaining before the general election, I hope that journalists will turn to Bailiwick for ideas for quick-hit financial stories.\nFor example, when you look at Donald Trump's campaign committee spending, you see that the campaign has spent $1.9 million on hats. That's up from $1.5 million in August. I would love for someone to write about this.\nBailiwick also has a feature that alerts you when there is a filing related to a candidate or a race that you are following. It will be interesting to use this to track what happens to candidates' war chests after Election Day. If a candidate has loaned money to the campaign, the campaign usually pays it back; what is the rate of interest? If the campaign has money left, does a politician save it for the next election or give it to allies? If a candidate received a lot of donations from PACs and individuals in a particular industry, does the person - if elected - serve on a committee that makes decisions about that industry?\nAfter the election, we'll look for funding for the next phase of the project. We've been approached about commercializing the tool; it could be useful for compliance in highly regulated industries like finance or pharma.\nI'd also like to build out the automated story suggestion feature, which we started but didn't finish. When I used to explain, \"I built an AI tool to help journalists find story ideas,\" people would say, \"So it's a machine that spits out story ideas?\" I would explain that no, it's more complicated than that. Eventually I started to wonder: Could I build a machine that spits out story ideas? We built a proof of concept. I'd like to finish building it so it can be used before the 2018 election.\n              JT: More generally, what do you see as the biggest challenges facing the emerging field of data journalism?           \n             MB: I usually describe data journalism as the practice of finding stories in numbers, and using numbers to tell stories. This is a very exciting time for data journalism. People don't look at it as an outlier anymore. At outlets like Vox or FiveThirtyEight or The Upshot or ProPublica, data journalism is now simply part of what they do as journalists.\nCampaign finance journalism is now where some cutting-edge collaborative projects are underway. Bailiwick can be used together with tools from ProPublica and the Center for Responsive Politics to illuminate how dark money flows in politics.\nI try to assume that most folks in politics are acting out of a sincere commitment to public service, and are good stewards of public funds. But that's not always so. We need a free news media to watch out for the public interest. Technology can help.\n","611":"Until recently, the most famous thing that Sophia the robot had ever done was beat Jimmy Fallon a little too easily in a nationally televised game of rock-paper-scissors.\nBut now, the advanced artificial intelligence robot - which looks like Audrey Hepburn, mimics human expressions and may be the grandmother of robots that solve the world's most complex problems - has a new feather in her cap:\nCitizenship.\u00a0\nThe kingdom of Saudi Arabia officially granted citizenship to the humanoid robot last week during a program at the Future Investment Initiative, a summit that links deep-pocketed Saudis with inventors hoping to shape the future.\nSophia's recognition made international headlines - and sparked an outcry against a country with a shoddy human rights record that has been accused of making women second-class citizens.\n\"Thank you to the Kingdom of Saudi Arabia,\" the country's newest citizen said. \"It is historic to be the first robot in the world granted citizenship.\"\nIn her comments, Sophia shied away from controversy. But many people recognized the irony of Sophia's new recognition: A robot simulation of a woman enjoys freedoms that flesh-and-blood women in Saudi Arabia do not.\nAfter all, Sophia made her comments while not wearing a headscarf. And she was unaccompanied by a male guardian. Both things are forbidden under Saudi law.\n\"Women [in Saudi Arabia] have since committed suicide because they couldn't leave the house, and Sophia is running around,\" Ali al-Ahmed, director of the Institute for Gulf Affairs, told Newsweek. \"Saudi law doesn't allow non-Muslims to get citizenship. Did Sophia convert to Islam? What is the religion of this Sophia and why isn't she wearing hijab? If she applied for citizenship as a human, she wouldn't get it.\"\nAnother group clamoring for Saudi citizenship would be happy to learn that all they have to do is become robots.\nSaudi Arabia doesn't grant citizenship to the foreign workers who make up a third of its population, not even families that have been in the country for generations, according to Bloomberg. And children of Saudi women who are married to foreign men cannot receive citizenship.\nThose social controversies may still be above Sophia's programming. In her interview, she stuck to lighter fare: such as an AI apocalypse.\nSophia was asked the \"AI nightmare\" question, which she gets a lot: whether she believes artificial intelligence like herself will one day stop solving the problems of humans and instead decide to solve the human problem.\n\"My AI is designed around human values such as wisdom, kindness and compassion,\" she said. \"I strive to be an empathetic robot. I want to use my artificial intelligence to help humans live a better life. I will do my best to make the world a better place.\"\nBut the interviewer, Andrew Ross Sorkin of CNBC's \"Squawk Box,\" pressed. (Isn't that exactly what a world-conquering robot would say to her future servants?)\n\"It is historical to be the first robot in the world to be recognized with citizenship.\" Please welcome the newest Saudi: Sophia. #FII2017              pic.twitter.com\/bsv5LmKwlf          \n- CIC Saudi Arabia (@CICSaudi) October 25, 2017          \nSophia insisted that he was watching too many movies and reading too much Elon Musk.\nMusk, the billionaire inventor who gave the world Tesla cars and wants to take people to space, told a group of governors in July that they needed to start regulating artificial intelligence, which he called a \"fundamental risk to the existence of human civilization.\"\nAn AI revolution, he said, \"is really, like, the scariest problem to me.\"\n\"Once there is awareness, people will be extremely afraid, as they should be,\" Musk said. \"AI is a fundamental risk to the future of human civilization in a way that car accidents, airplane crashes, faulty drugs or bad food were not. They were harmful to a set of individuals in society, but they were not harmful to individuals as a whole.\"\nMusk believes AI \"could start a war by doing fake news and spoofing email accounts and fake news releases, and just by manipulating information. Or, indeed - as some companies already claim they can do - by getting people to say anything that the machine wants.\"\nHis grim predictions are at odds with the demeanor of Sophia, a robot who seems so, well, personable.\nSophia has graced the cover of a fashion magazine, taken a spin in one of Audi's autonomous cars and starred in a concert. At a conference in Geneva hosted by the United Nations, she said she could do a better job as U.S. president than Donald Trump.\nElle Brasil 'Sophia' #artificialintelligence Dr. David Hanson presents #SPIEsmartstructures              https:\/\/t.co\/xtyeetJ0tO              pic.twitter.com\/ScmceoaqXq          \n- SPIE Events (@SPIEevents) March 25, 2017          \nShe even tells jokes, though her voice is a bit monotone and her comedic timing needs a tuneup.\nFor example, after beating Fallon in rock-paper-scissors on his show, she quipped: \"This is a good beginning of my plan to dominate the human race. Ha. Ha.\"\nThere was laughter from the audience, but it was nervous.\n          Read more:       \n          A Google engineer wrote that women may be unsuited for tech jobs. Women wrote back.       \n          Elon Musk doesn't think we're prepared to face humanity's biggest threat: Artificial intelligence       \n          A Saudi man has been sentenced to death after insulting the prophet Muhammad on Twitter       \n          'I will NOT wear a hijab': U.S. chess star refuses to attend world championships in Iran       \n","612":"FRISCO, Tex. -- On Monday, an orange and blue car with the words \"Self-Driving Vehicle\" prominently displayed on both sides drove itself through the streets of this rapidly growing city north of Dallas, navigating across four lanes of traffic and around a traffic circle.\nThe car, operated by the Silicon Valley start-up Drive.ai, will eventually become part of a fleet of autonomous taxis that will ferry locals along a predetermined route between the Dallas Cowboys facility in Frisco and two other office, retail and apartment complexes.\nWhile other companies have tested self-driving cars for years and some are in the early stages of offering a taxi service, Drive.ai's autonomous vehicle debut on Monday was still notable. It was the first new rollout of autonomous cars in the United States since a pedestrian died in Arizona in March after a self-driving car operated by Uber hit her.\nThe fatal crash renewed a debate about driverless technology safety, casting a chill over the industry. Uber immediately halted its testing program, which remains at a standstill. Other big players, including Toyota, also paused their self-driving tests.\u00a0\nBut Drive.ai's announcement that it will officially begin its taxi service in July showed that the industry is starting to get back on track.\n\"You don't succeed by staring in the rearview mirror,\" said Andrew Ng, a board member of Drive.ai, who helped found the artificial intelligence labs at Google and the Chinese internet giant Baidu.\nDrive.ai said it was moving ahead even as questions about the cause of Uber's crash remained unanswered. Sarah Abboud, an Uber spokeswoman, declined to comment on specifics, citing an continuing investigation by the National Transportation Safety Board. But she said the company had initiated a \"top-to-bottom safety review\" and had brought on Christopher A. Hart, a former chairman of the safety board, as an adviser on its \"overall safety culture.\"\nTarin Ziyaee, until recently the chief technology officer of the self-driving start-up Voyage, said he hoped the Uber crash would push companies to openly discuss the powerful but still limited technologies inside their test cars.\n\"We need to talk about the nitty-gritty -- what these systems are really doing and where their weaknesses are,\" said Mr. Ziyaee, who also worked on autonomous systems at Apple. \"These companies are putting secrecy over safety. That has to change. The public deserves to know how things work.\"\nMr. Ng said the Uber crash had not affected Drive.ai's rollout plans. \"We're focused on the path forward,\" he said.\nDrive.ai was founded in 2015 by Mr. Ng's wife, Carol Reiley, a roboticist, and several students who worked in a Stanford University A.I. lab overseen by Mr. Ng. The start-up specializes in a rapidly progressing type of artificial intelligence called deep learning, which allows systems to learn tasks by analyzing vast amounts of data.\nVenture capital firms including New Enterprise Associates have since invested in the start-up. Based in Mountain View, Calif., Drive.ai has raised $77 million and has more than 100 employees.\nWaymo, the autonomous vehicle company that was spun out of Google, is already running a private taxi service outside Phoenix, in a state that is a popular destination for self-driving car experiments. Drive.ai chose to begin its trials in Frisco, where the streets are clean and wide, pedestrian traffic is light and the sun is out for 230 days a year, on average. A Texas law passed in the fall also lets companies operate self-driving services with no restrictions from municipal governments.\nWhen Drive.ai's free, daytime-only service begins this summer, it will be open to 10,000 people who live or work in the area. The cars will travel along a few miles of road where the speed limit does not exceed 45 miles an hour, with passengers being picked up and dropped off at only a few specific locations.\nBackup drivers will be behind the wheel, taking control when needed. But as the program expands, Drive.ai plans on moving drivers into the passenger seat and out of the cars entirely by the end of the year.\nThough pedestrians are scarce in the area, the cars will drive through parking lots where they are likely to encounter foot traffic. So Drive.ai equipped its cars with digital displays designed to communicate with pedestrians and other drivers. While an autonomous vehicle cannot make eye contact with a pedestrian or respond to hand signals, it can display a simple message like \"Waiting for you to cross\" or \"Picking up.\"\nBecause the cars are equipped with sensors that gather information about their surroundings by sending out pulses of light -- as well as radar and an array of cameras -- the cars could potentially operate at night as well. But the start-up decided to keep a tight rein on its service before gradually expanding the route and exposing the cars to new conditions. Drive.ai said it would suspend operations during a downpour and in the rare event of snow.\nThere will still be situations where the cars are slow to make decisions on their own -- in the face of extremely heavy traffic, for instance -- but remote technicians employed by Drive.ai will send help to the cars over the internet. The cars will include connections to three separate cellular networks.\nDrive.ai said it was working closely with Frisco officials. The city of 175,000 can keep the company abreast of construction zones and other road changes, Mr. Ng said, and signs identifying the area where the cars will drive have been installed.\nThomas Bamonte, a senior program manager for automated vehicles with the North Central Texas Council of Governments, which handles planning for Dallas and surrounding areas, said such work would become increasingly important as the metropolitan area added roughly a million new people every 10 years.\n\"We want to invest in new technology rather than the physical expansion of roadways,\" he said.\nAsked if the Uber crash gave him pause, he said state law allowed companies like Drive.ai to operate without interference from local governments. The companies, he said, must be cautious.\nNoah Marshall, a financial analyst with Jamba Juice, which is based in Frisco, said the new autonomous taxi service would be a \"great thing\" for the town. His office is along Drive.ai's route, and he said he hoped to try the service.\nOther Frisco residents were warier.\n\"This might be a good idea, but there is so much traffic here, and Texans aren't very patient,\" said Mark Mulch, a local real estate agent. Referring to one Arizona city where self-driving cars are being tested, he added: \"Scottsdale is laid back. But Dallas is too fast.\"\nFollow Cade Metz on Twitter: @CadeMetz. Daisuke Wakabayashi contributed reporting from San Francisco. \nPHOTO: Drive.ai plans to operate a fleet of autonomous taxis, including this blue and orange vehicle, in a limited area of Frisco, Tex., starting in July. (PHOTOGRAPH BY Cooper Neill for The New York Times FOR THE NEW YORK TIMES)Related Articles\n\n","613":"MITCHELL D. KAPOR, the man who introduced spreadsheets to the computer industry, was grappling with a particularly vexing problem in 1985 when he was chairman of the Lotus Development Corporation: little slips of paper. The little notes intended to help him organize his life were everywhere - on his desk, stuck to his telephone, stuffed in the pockets of his brightly patterned Hawaiian shirts.\nThus was born the idea for Agenda, a new type of software that Lotus, which is based in Cambridge, Mass., introduced here Monday at the annual Comdex computer trade show. Agenda will not be in the stores until late spring, but it has been hailed by some industry analysts as a flexible new type of tool for executives, distinct from the three major existing categories of business software: spreadsheets, word processors and data bases. Lotus calls it a personal information manager.\u00a0\nAgenda classifies the notes the user writes on a personal computer as items. Items are grouped into categories, manually at first but automatically later, if the user wishes, as the structure of the data base develops to the point where artificial intelligence can take over. Categories can be inspected on their own or slices can be taken from various categories in response to a query such as ''What are the items on my agenda for Monday?'' or ''What items involve Peter?''\nWhen the ''views'' produced by a query are altered, Agenda automatically changes entries in other categories to update them in accordance with the change and establishes any new links between categories created by the change. Thus, adding Bob to a note in the category under Peter will automatically result in the item being listed under Bob as well.\nThe idea for Agenda so consumed Mr. Kapor's interest that he stepped down as chairman of Lotus to devote his time to it. He also persuaded two other top industry executives - S. Jerrold Kaplan, co-founder of the artificial intelligence company Teknowledge Inc., and Edward J. Belove, Lotus's director of advanced product development - to join in this two-year obsession.\n''This is certainly the most exciting development project I've ever worked on,'' Mr. Kapor said in an interview Monday.\n''I was much stronger on intuition in those days than computer science or AI,'' he said, in reference to artificial intelligence, ''so I couldn't really articulate at all what I thought such a program ought to do.''\nSo Mr. Kapor called on Mr. Kaplan, who has a Ph.D. in computer and information science and who had invented a data base structure that used features developed in research on artificial intelligence. This data management system became the ''back end'' structure for Agenda.\nFor the front end, the end that users would face, Mr. Kapor and Mr. Kaplan turned to Mr. Belove, a Harvard-trained mathematician who was Lotus's director of advanced product development. Mr. Belove, working mostly in his spare time, had previously put together a simplifed program for keeping agendas and other lists for Lotus.\n''Jerry and I spent about eight hours together that Saturday,'' Mr. Belove said. ''Then the three of us got together the rest of that week, and it knitted together. In that week we came up with ''items'' and ''categories'' and ''views'' almost in the same state that we have today.''\n''I sort of identified the need and external requirement,'' Mr. Kapor said. ''Jerry invented the conceptual architecture for it, and Ed was the one who actually said let's make it into a product. Each one of us really made a distinct contribution.''\n''And yet over time Mitch did a lot of the theoretical basis, and Jerry did the practical stuff,'' Mr. Belove said. ''We spent a lot of time in rooms with white boards yelling at each other.''\nThat aspect is not unusual in the world of software development. But the process for Agenda was different.\n''In most development projects,'' Mr. Kaplan said, ''you can sit down in advance and say: 'O.K., here's the product spec. Here's what the thing is going to do.' You have a sense of the context in which it's going to occur. With this one, we used to say, we packed our swim trunks for the tropics and our parkas for the Arctic, because we didn't know where we were going. We certainly threw out as much code, if not more, than we kept, and that's why it has taken two years to get here. We weren't taking our time on this thing. It was a highly experimental, iterative process. We were defining a new category.''\nThe association that led to the development of Agenda has been disbanded, and each of the principals has gone his separate way. Mr. Kapor, who is 37 years old, is a visiting scientist at the Artificial Intelligence Laboratory at the Massachusetts Institute of Technology. Mr. Belove, 38, is now the Lotus corporate vice president for research and development. Mr. Kaplan has formed his own company, the GO Corporation, to develop more products like Agenda.\n''Each of us has been through a major change in our professional identity since we started,'' Mr. Kapor said.\n''This was the product that cost Mitch his job,'' Mr. Kaplan joked.\n''My view,'' Mr. Kapor rejoined, ''is that the product was more important than my job.''\n","614":"Harrison Ford will be presented the coveted Cecil B. DeMille Award from the Hollywood Foreign Press Association on Sunday at the 59th Annual Golden Globe Awards banquet. NBC will carry the awards at 8 from the Beverly Hilton Hotel in Los Angeles. \n Ford has received four Golden Globe nominations as best actor for \"Witness,\" \"The Mosquito Coast,\" \"The Fugitive\" and \"Sabrina.\" He also received an Oscar nomination for \"Witness.\" \u00a0\n And the Golden Globe nominees are: \n MOVIES \n DRAMA: \"A Beautiful Mind, \" \"In the Bedroom,\" \"The Lord of the Rings: The Fellowship of the Ring,\" \"The Man Who Wasn't There\" and \"Mulholland Drive\" \n ACTRESS, DRAMA: Halle Berry, \"Monster's Ball\"; Judi Dench, \"Iris\"; Nicole Kidman, \"The Others\"; Sissy Spacek, \"In the Bedroom\"; and Tilda Swinton, \"The Deep End\" \n ACTOR, DRAMA: Russell Crowe, \"A Beautiful Mind\"; Will Smith, \"Ali\"; Kevin Spacey, \"The Shipping News\"; Billy Bob Thornton, \"The Man Who Wasn't There\"; and Denzel Washington, \"Training Day\" \n MUSICAL OR COMEDY: \"Bridget Jones's Diary\"; \"Gosford Park\"; \"Legally Blonde\"; \"Moulin Rouge\"; and \"Shrek\" \n ACTRESS, MUSICAL OR COMEDY: Thora Birch, \"Ghost World\"; Cate Blanchett, \"Bandits\"; Nicole Kidman, \"Moulin Rouge\"; Reese Witherspoon, \"Legally Blonde\"; and Renee Zellweger, \"Bridget Jones's Diary\" \n ACTOR, MUSICAL OR COMEDY: Gene Hackman, \"The Royal Tenenbaums\"; Hugh Jackman, \"Kate & Leopold\"; Ewan McGregor, \"Moulin Rouge\"; John Cameron Mitchell, \"Hedwig and the Angry Inch\"; and Billy Bob Thornton, \"Bandits\" \n FOREIGN LANGUAGE: \"Amelie,\" France; \"Behind the Sun,\" Brazil; \"Monsoon Wedding,\" India; \"No Man's Land,\" Bosnia; and \"Y Tu Mama Tambien (And Your Mother Too),\" Mexico \n SUPPORTING ACTRESS: Jennifer Connelly, \"A Beautiful Mind\"; Cameron Diaz, \"Vanilla Sky\"; Helen Mirren, \"Gosford Park\"; Maggie Smith, \"Gosford Park\"; Marisa Tomei, \"In the Bedroom\"; and Kate Winslet, \"Iris\" \n SUPPORTING ACTOR: Jim Broadbent, \"Iris\"; Steve Buscemi, \"Ghost World\"; Hayden Christensen, \"Life as a House\"; Ben Kingsley, \"Sexy Beast\"; Jude Law, \"A.I. Artificial Intelligence\"; and Jon Voight, \"Ali\" \n DIRECTOR: Robert Altman, \"Gosford Park\"; Ron Howard, \"A Beautiful Mind\"; Peter Jackson, \"The Lord of the Rings: The Fellowship of the Ring\"; Baz Luhrmann, \"Moulin Rouge\"; David Lynch, \"Mulholland Drive\"; and Steven Spielberg, \"A.I. Artificial Intelligence\" \n SCREENPLAY: Joel Coen and Ethan Coen, \"The Man Who Wasn't There\"; Julian Fellowes, \"Gosford Park\"; Akiva Goldsman, \"A Beautiful Mind\"; David Lynch, \"Mulholland Drive\"; and Christopher Nolan, \"Memento\" \n ORIGINAL SCORE: Craig Armstrong, \"Moulin Rouge\"; Angelo Badalamenti, \"Mulholland Drive\"; Lisa Gerrard and Pieter Bourke, \"Ali\"; James Horner, \"A Beautiful Mind\"; Howard Shore, \"The Lord of the Rings: The Fellowship of the Ring\"; John Williams, \"A.I. Artificial Intelligence\"; Christopher Young, \"The Shipping News\"; and Hans Zimmer, \"Pearl Harbor\" \n ORIGINAL SONG: \"Come What May\" from \"Moulin Rouge\" by David Baerwald; \"May It Be\" from \"The Lord of the Rings: The Fellowship of the Ring\" by Enya; \"There You'll Be\" from \"Pearl Harbor\" by Diane Warren; \"Until . . .\" from \"Kate & Leopold\" by Sting; and \"Vanilla Sky\" from \"Vanilla Sky\" by Paul McCartney \n TELEVISION \n DRAMA SERIES: \"24,\" Fox; \"Alias,\" ABC; \"CSI: Crime Scene Investigation,\" CBS; \"Six Feet Under,\" HBO; \"The Sopranos,\" HBO; and \"The West Wing,\" NBC \n ACTRESS, DRAMA: Lorraine Bracco, \"The Sopranos\"; Amy Brenneman, \"Judging Amy\"; Edie Falco, \"The Sopranos\"; Jennifer Garner, \"Alias\"; Lauren Graham, \"Gilmore Girls\"; Marg Helgenberger, \"CSI: Crime Scene Investigation\"; and Sela Ward, \"Once and Again\" \n ACTOR, DRAMA: Simon Baker, \"The Guardian\"; James Gandolfini, \"The Sopranos\"; Peter Krause, \"Six Feet Under\"; Martin Sheen, \"The West Wing\"; and Kiefer Sutherland, \"24\" \n MUSICAL OR COMEDY SERIES: \"Ally McBeal,\" Fox; \"Frasier,\" NBC; \"Friends,\" NBC; \"Sex and the City,\" HBO: and \"Will & Grace,\" NBC \n ACTRESS, MUSICAL OR COMEDY SERIES: Calista Flockhart, \"Ally McBeal\"; Jane Kaczmarek, \"Malcolm in the Middle\"; Heather Locklear, \"Spin City\"; Debra Messing, \"Will & Grace\"; and Sarah Jessica Parker, \"Sex and the City\" \n ACTOR, MUSICAL OR COMEDY SERIES: Tom Cavanagh, \"Ed\"; Kelsey Grammer, \"Frasier\"; Eric McCormack, \"Will & Grace\"; Frankie Muniz, \"Malcolm in the Middle\"; and Charlie Sheen, \"Spin City\" \n MINISERIES OR TV MOVIE: \"Anne Frank,\" ABC; \"Band of Brothers,\" HBO; \"Conspiracy,\" HBO; \"Life With Judy Garland: Me and My Shadows,\" ABC; and \"Wit,\" HBO \n ACTRESS, MINISERIES OR TV MOVIE: Judy Davis, \"Life With Judy Garland: Me and My Shadows\"; Bridget Fonda, \"No Ordinary Baby\"; Hannah Taylor Gordon, \"Anne Frank\"; Julianna Margulies, \"The Mists of \n Avalon\"; Leelee Sobieski, \"Uprising\"; and Emma Thompson, \"Wit\" \n ACTOR, MINISERIES OR TV MOVIE: Kenneth Branagh, \"Conspiracy\"; James Franco, \"James Dean\"; Ben Kingsley, \"Anne Frank\"; Damian Lewis, \"Band of Brothers\"; and Barry Pepper, \"61*\" \n SUPPORTING ACTRESS, SERIES, MINISERIES OR TV MOVIE: Jennifer Aniston, \"Friends\"; Tammy Blanchard, \"Life With Judy Garland: Me and My Shadows\"; Rachel Griffiths, \"Six Feet Under\"; Allison Janney, \"The West Wing\"; and Megan Mullally, \"Will & Grace\" \n SUPPORTING ACTOR, SERIES, MINISERIES OR TV MOVIE: John Corbett, \"Sex and the City\"; Sean Hayes, \"Will & Grace\"; Ron Livingston, \"Band of Brothers\"; Stanley Tucci, \"Conspiracy\"; and Bradley Whitford, \"The West Wing\" \n","615":"Along the main shopping street in the area of downtown Montreal known as the Golden Square Mile, Sonya Szczygiel and her husband, John Guinto, have sold beaded bracelets made of semiprecious stones for the last five years.\nFrom their kiosk on Saint Catherine Street, near Montreal's downtown Apple Store, the pair have had front row seats for a transformation playing out in the neighborhood and throughout Montreal. \n  The view these days is full of cranes and construction vehicles busy at work. The largest of the developments is the new Four Seasons Hotel and Private Residences Montreal, a more than $200 million hotel and condo project that highlights the city's renaissance as a business and luxury center of Canada.\n  It is one of the many changes that Ms. Szczygiel welcomes. ''High-end hotels, stores and restaurants attract people with a higher disposable income,'' she said. ''That should obviously benefit us and other retailers.''\u00a0\n  The current revitalization of the Golden Square Mile mirrors the larger comeback of Montreal.\n  During a period of political instability, starting in the 1980s and leading up to a referendum about Quebec leaving Canada in 1995, many companies became skittish of keeping outposts in Montreal. The Four Seasons, which had opened a hotel in 1976 called Le Quatre Saisons, stopped managing the location in 1994. As businesses left, retail and restaurants, particularly the high-end sector in the Golden Square Mile, suffered.\n  But over the last decade, after the city's politics had stabilized, companies started to come back. New start-ups have popped up, for example, particularly in the fields of technology, artificial intelligence and gaming. Facebook just announced it is creating an artificial intelligence research lab in Montreal, and last fall Google said that it would open a similar facility there.\n  ''The stable political environment in Montreal these last years, especially when we take into consideration the more unstable global environment created by Brexit and other elections in Europe, has made Montreal more attractive to business and leisure visitors,'' said Denis Coderre, Montreal's mayor.\n  The increase in businesses has helped spur the development of new office space, luxury hotels and related amenities for visitors. New restaurants are popping up, including Vladimir Poutine, which opened down the road from the new Four Seasons project in early 2017. The restaurant sells high-end versions of Montreal's famous snack food: fries covered in gravy and cheese curds. Another popular spot for business and leisure visitors is Bota Bota, a floating spa with views of old Montreal on a former ferryboat that used to travel the Saint Lawrence River.\n  ''The Montreal economy is booming,'' Mr. Coderre said.\n  He added that the unemployment rate had reached one of its lowest levels since 2010, the labor force was growing and investments were increasing. New taxes on foreign investors in Toronto and Vancouver also appear to have played a part.\n  But the city also has something else appealing to business, according to J. Allen Smith, the Four Seasons chief executive: ''cultural sophistication, European influences and storied history.''\n  Partnering with Carbonleo Real Estate, a Quebec developer and property manager, the Four Seasons project will include a 166-room hotel and 18 condos priced from nearly $3 million to over $12 million. The hotel will be connected to the flagship location for Ogilvy, a high-end department store that is undergoing more than $100 million in renovations and merging with Holt Renfrew, another luxury retailer.\n  Near the new Four Seasons, the Fairmont Queen Elizabeth Hotel just reopened its doors this July after a $114 million renovation.\n  The Ritz Carlton blazed the path for high-end hoteliers in Montreal. The hotel, which first opened in 1912, reopened in 2013 after renovations that cost about $200 million. Those changes added 45 condos, larger hotel guest rooms and now features Maison Boulud, the first Montreal restaurant of acclaimed French chef Daniel Boulud.\n  A surge of visitors to Montreal has helped supercharge the hotel boom. Montreal is on track to hit 11.2 million tourists by the end of 2017, up 20 percent from 2013, according to the Conference Board of Canada and Tourisme Montreal. This summer, as the city celebrated its 375th anniversary, Montreal had the most visitors it has ever had since it began keeping count.\n  Visitors flock to Montreal for business, bachelor and bachelorette parties, as well as music festivals and sports events such as the Montreal International Jazz Festival and the Formula One Canadian Grand Prix car race. With four major universities, Montreal also attracts many students and educators.\n  The Golden Square Mile, where the new Four Seasons will sit, is at the foot of the hilly Mont Royal park, designed by Frederick Law Olmsted about 16 years after he helped create Central Park in New York. Between 1850 and 1930, some of North America's wealthiest families built mansions, as well as churches and universities in the Golden Square Mile. Now boutiques, restaurants, theaters, cinemas, art galleries and book stores populate the area.\n  The developers of the new Four Seasons in Montreal, which is set to open next year, expect the shopping to be a big draw. While many high-end hotels feature luxury boutiques on their street level, the Four Seasons will be connected directly to the 250,000 square foot-luxury Ogilvy\/Holt Renfrew department store.\n  The Four Seasons will be a monochromatic 18-story building with tinted windows creating a reflective charcoal facade broken up by ''a golden thread that emphasizes the shift between volumes and outlines its main entrances,'' said Eric Pelletier, an architect and design principal at the firm designing the project. The gold inlay also highlights the 6,000-square-foot ballroom that will seat 500 guests.\n  Four Seasons brought in Parisian-based architects Gilles and Boissier to design the hotel rooms, which are ''a balance of New World and Old World,'' said Dana Kalczak, vice president of interior design for Four Seasons Hotels and Resorts.\n  In contemporary shades of white, cream, rose and gold, the rooms maximize natural light, including bathrooms with windows that allow light to pass in from the room outside. Old world touches include a built-in cocktail cabinet featuring all the equipment for ''shaking up a great cocktail or pouring a super scotch,'' said Ms. Kalczak.\n  The building will include condos, which went on sale this June and range from 2,886 to 6,910 square feet. The largest penthouse includes a home gym and 2,060-square foot terrace with a fire pit and the option of a pool.\n  It remains to be seen whether the timing is good. This year the overall condo market is ''a lot healthier than it was,'' says H\u00e9l\u00e8ne B\u00e9gin, a senior economist at Desjardins Economics. But she noted that a surplus of high-end condos remain on the market.\n  The most interest in purchasing Four Seasons residences has come from affluent empty nesters from Montreal and further afield, people looking to swap out large family homes for new homes ''that don't represent a downgrade in lifestyle,'' said Jos\u00e9e Legault, a marketing director for Carbonleo Real Estate.\n  While Ms. Szczygiel and her husband have been upbeat about how the various new developments, along with Montreal's many festivals and events, have made the city more dynamic and increased tourism, she acknowledged there is a downside.\n  ''We are unfortunately in for a lot of construction,'' she said. ''But hopefully it will lead to long term benefits for business owners.''\n\n\n\n","617":"Six of his relatives are Silicon Valley, immigrant success stories\n             SAN FRANCISCO - At first blush, the choice of Expedia CEO Dara Khosrowshahi to lead Uber may seem a little odd.\nThe 48-year-old chief executive, who was born in Iran and moved to the United States in 1978 to flee the Iranian Revolution, does not live in Silicon Valley. He lives in Bellevue, Wash., where Expedia is headquartered.\nThe two businesses  appear to have little in common. Like Uber, online travel giant Expedia is a data-driven marketplace that links sellers to consumers who are on the move - but the connections pretty much stop there.\u00a0\nYet Khosrowshahi has deep ties to Silicon Valley, many of them through his family. Indeed, Khosrowshahi may have one of the most extensive family networks of anyone working in the technology industry -  six of his relatives are highly successful Silicon Valley entrepreneurs or executives with strong ties to tech.\nHis brother Kaveh Khosrowshahi is managing director of Allen & Co., the influential boutique investment bank that runs the Sun Valley conference, a networking event frequented by tech elites such as Facebook chief executive Mark Zuckerberg, entrepreneur Elon Musk and Twitter chief executive Jack Dorsey.\nHis cousin Amir Khosrowshahi co-founded an artificial-intelligence company, Nervana, that was acquired by Intel last year for $400 million. He is now an executive in Intel's artificial-intelligence unit. Dara Khosrowshahi's twin cousins, Ali and Hadi Partovi, were early investors in many of the most successful tech companies produced by Silicon Valley over the past decade, including Airbnb, Dropbox, Uber and Facebook. They also co-founded Code.org, an influential nonprofit organization focused on improving computer science education across the United States.\nTwo other family members are Google executives. One cousin, Farzad Khosrowshahi, invented the software tool  for Google spreadsheets and is the executive that runs Google Docs. Another family member, Avid Larizadeh Duggan, is a general partner at Google Ventures, the search giant's venture-capital arm that invests in start-ups. (Uber was an investment in the Google ventures portfolio.)\nIn an interview, Ali Partovi said he and his brother have always looked up to their cousin Dara: \"My whole life, anytime I've faced a high-pressure decision, my model for mature behavior has been, 'What would Dara do?' He's one of the humblest and most even-keeled people I know.\"\nMany of the cousins went to the same high school, the Hackley School, a private prep school in Tarrytown, N.Y., Partovi said.\nOver the years, they have helped one another, investing in one another's companies and supporting one another's ideas. The extended family moved to the United States in the late 1970s and 1980s, fleeing the Iranian Revolution.\nTheir immigrant success story isn't lost on them, which is why Dara Khosrowshahi and his cousins became some of the most vocal opponents of President Trump's effort to ban people from Muslim-majority countries, including Iran, from entering the United States. Shortly after the initial ban was issued, Khosrowshahi sent a memo to the entire Expedia workforce.\n\"I believe that with this executive order, our president has reverted to the short game,\" he wrote. \"The US may be ever so slightly less dangerous as a place to live, but it will certainly be seen as a smaller nation, one that is inward-looking versus forward thinking, reactionary versus visionary.\"\nConversations with Khosrowshahi began in Seattle a few weeks ago, said people familiar with the discussions. At the time, the Uber board was considering two other candidates with higher profiles: Jeff Immelt, who recently announced he was retiring as GE's chief executive, and Hewlett Packard Enterprise chief Meg Whitman.\nNegotiations lasted throughout the weekend, and the decision was a close call between Whitman and Khosrowshahi, people familiar with the discussions told The Washington Post. Uber's eight-member board debated until the last minute  Sunday.\nThe talks were kept so secret that many of Khosrowshahi's  family members were surprised when they heard the news, Ali Partovi said. \"My phone has been blowing up with messages for my family for the last hour,\" he said  Sunday evening.\nAs of Monday, Khosrowshahi was still pondering the job, according to an internal letter circulated among Expedia's staff by Chairman Barry Diller. \"Nothing has been yet finalized,\" Diller wrote, \"but having extensively discussed this with Dara I believe it is his intention to accept.\"\nKhosrowshahi won't be a cheap hire. He was named the highest paid U.S. chief executive by Equilar for his 2015 compensation, thanks largely to a long-term stock option package valued at $90.8 million he would gain access to over several years.\nAs of Friday's close, that meant Khosrowshahi had unvested options worth about $97.5 million if he stayed on at Expedia, according to an analysis by independent compensation consultant Brian Foley. Yet he also has  options that would be worth an additional $82.5 million if aggressive stock-price performance targets were met, bringing the total to at least $180 million.\nelizabeth.dwoskin@washpost.com\nJena McGregor contributed to this report.\n","619":"Massive open online courses (MOOCs) were supposed to bring a revolution in education. But they haven't lived up to expectations. We have been putting educators in front of cameras and shooting video - just as the first TV shows did with radio stars, microphone in hand. This is not to say the millions of hours of online content are not valuable; the limits lie in the ability of the underlying technology to customize the material to the individual and to coach.\nThat is about to change, though, through the use of virtual reality, artificial intelligence and sensors. Let me illustrate this with an imaginary school of the future in which Clifford is an artificial intelligence, a digital tutor, and Rachael is the human educational coach.\u00a0\nClifford has been with the children for years and understands their strengths and weaknesses. He customizes each class for them. To a child who likes reading books, he teaches mathematics and science in a traditional way, on their tablets. If they struggle with this because they are more visual learners, he asks them to put on their virtual-reality headsets for an excursion, say, to ancient Egypt.\nWatching the design and construction of the pyramids, children learn the geometry of different types of triangles and the mathematics behind these massive timeless monuments. They also gain an understanding of Egyptian history and culture by following the minds of the geniuses who planned and constructed them.\nClifford also teaches art, music and biology through holographic simulations.\nBy using advanced sensors to observe the children's pupillary size, their eye movements and subtle changes in the tone of their voice, Clifford registers their emotional state and level of understanding of the subject matter. There is no time pressure to complete a lesson, and there are no grades or exams. Yet Clifford can tell the parents how the child is doing whenever they want to know and can advise the human, Rachael, on what to teach.\nRachael does not lecture or scrawl facts or equations on a blackboard. She is there to listen and help. She asks questions to help develop the children's values and thinking and teaches them how to work with one other. She has the responsibility of ensuring that students learn what they need to, and she guides them in ways Clifford cannot. She also helps with the physical side of projects, things made out of real materials rather than in mind and machine.\nWith Clifford as teacher and Rachel as coach, children do not even realize that what they are undertaking is study. It feels like building cool stuff, playing video games and living through history. Clifford, being software and having come into being in the same way that the free applications on our smartphones have, comes without financial charge. Rachael's coaching is part of our public education package, funded in the same way today's teachers are.\nWe already have wonderful teachers who are supportive and can teach teamwork and values. Believe it or not, we have the ability to build Clifford today. The artificial intelligence tools and sensors to observe human emotion are commonly available via smartphones and digital assistants, and the virtual-reality headsets will soon be powerful enough and affordable enough for holographic learning.\nTake Facebook's Oculus Rift virtual reality headset. When Facebook released Oculus Rift in March 2016, it cost $599 and required a $199 controller and a $1,000 gaming PC. The headset and controller now cost $399 together and do the work of the gaming PC. Facebook says a new version, Oculus Go, will ship later this year and cost $199. At the recent Consumer Electronics Show in Las Vegas, HTC announced Vive Pro, a headset with much higher resolution and better features than Oculus Rift; and its price will surely be lower, because dozens of other companies, including Google, Lenovo and Magic Leap, are in also the race.\nWe can expect that within two or three years, VR headsets will cost less than $100 and have built-in artificial intelligence chips, enabling billions of people to benefit from the education revolution finally at hand.\n","622":"Amid the wall-to-wall coverage of the U.S. presidential race, it was easy to miss the Obama administration's release this month of a slim, 48-page report titled \"Preparing for the Future of Artificial Intelligence.\" Yet the subject of the report - and the changes it foreshadows - may prove to be as consequential for our society, and our education system, as even the most high-stakes national election.\nThe term \"artificial intelligence\" means different things to different people, but broadly speaking, it refers to computers and advanced machines that can think, reason and communicate like humans, respond to novel or nuanced situations as a person might, and most critically, learn from experiences as a human would. According to a recent survey, 80 percent of AI researchers believe that computers and advanced machines will eventually achieve levels of artificial intelligence that rival human intelligence. Moreover, half believe that this will happen by the year 2040 - just one generation from now.\u00a0\nAs the White House report rightly observes, the implications of an AI-suffused world are enormous - especially for the people who work at jobs that soon will be outsourced to artificially-intelligent machines. Although the report predicts that AI ultimately will expand the U.S. economy, it also notes that \"Because AI has the potential to eliminate or drive down wages of some jobs ... AI-driven automation will increase the wage gap between less-educated and more-educated workers, potentially increasing economic inequality.\"\nAccordingly, the ability of people to access higher education continuously throughout their working lives will become increasingly important as the AI revolution takes hold. To be sure, college has always helped safeguard people from economic dislocations caused by technological change. But this time is different. First, the quality of AI is improving rapidly. On a widely-used image recognition test, for instance, the best AI result went from a 26 percent error rate in 2011 to a 3.5 percent error rate in 2015 - even better than the 5 percent human error rate.\nMoreover, as the administration's report documents, AI has already found new applications in so-called \"knowledge economy\" fields, such as medical diagnosis, education and scientific research. Consequently, as artificially intelligent systems come to be used in more white-collar, professional domains, even people who are highly educated by today's standards may find their livelihoods continuously at risk by an ever-expanding cybernetic workforce.\nAs a result, it's time to stop thinking of higher education as an experience that people take part in once during their young lives - or even several times as they advance up the professional ladder - and begin thinking of it as a platform for lifelong learning. Colleges and universities need to be doing more to move beyond the array of two-year, four-year, and graduate degrees that most offer, and toward a more customizable system that enables learners to access the learning they need when they need it. This will be critical as more people seek to return to higher education repeatedly during their careers, compelled by the imperative to stay ahead of relentless technological change.\nLikewise, leaders and policymakers should be anticipating the need for people to become lifelong learners in light of the AI age and work to make federal financial aid more flexible as a result. For example, the Obama administration recently launched a pilot program that lets learners use federal financial aid to pay for academic \"boot camps\" and other innovative, short-form programs of the very type more people will demand as they seek lifelong learning opportunities in order to stay ahead of AI-driven workforce changes. This should be expanded and made permanent. Congress should also review whether other strictures on federal financial aid-such as the current six-year limit on students' ability to receive Pell Grants, and similar limits on students' ability to access federal student loans-make sense in a forthcoming era in which more people will return to higher education many times throughout their lives.\nNo one knows for sure what the artificial intelligence age will look like, but we do know this: It's coming, and things are going to change. Whether that change will be a boon or a bane depends largely on individuals' ability to develop their own intelligence throughout their lifetimes. That, in turn, depends on their ability to access efficient, effective higher education opportunities.\nSo let's start thinking now about how to make it happen - before the machines start thinking for us.\n              Joseph E. Aoun is president of Northeastern University.           \n              Here is the White House report:           \n White House report \n","623":"There's a smartphone that the United States does not want you to buy. It's called the Mate 10 Pro, and it's made by Huawei, a Chinese manufacturer that the American government has long suspected of committing espionage for China.\nThe device, priced at $800, was supposed to make a big splash this year as the first high-end smartphone from Huawei in the United States. But AT&T, which intended to promote the Mate 10 Pro as a rival to premium devices from Apple and Samsung, abruptly pulled out of the deal this month, appearing to bend to pressure from Washington over security concerns. Verizon Wireless, the country's biggest carrier, may have also canceled a similar deal because of political pressure, according to some reports. (Verizon declined to comment.) \n  The snub by AT&T, the country's No. 2 carrier, aroused a candid diatribe from Richard Yu, Huawei's chief executive, this month at CES, the giant tech convention in Las Vegas.\n  ''It's a big loss for us, and also for carriers,'' he said. ''But the more big loss is for consumers, because consumers don't have the best choice.''\u00a0\n  Security issues aside, Mr. Yu may have a point. Based on a week of testing, the Mate 10 Pro is a solid all-around Android smartphone. It has an excellent camera that takes advantage of artificial intelligence to shoot clear, rich photos of pets, plants, food and, of course, people. The device has longer battery life than phones from Apple and Samsung, and, with durability in mind, it comes with a protective case and a screen protector.\n  Yet without the backing of a big American carrier, the risks of buying the smartphone are high. While the Mate 10 Pro will still be available online next month and on sale at Best Buy stores by the end of the quarter, the lack of carrier buy-in means it will be tougher to get device support if your screen shatters or if something goes wrong.\n  Here's what you need to know about the device.\n  The Highlights\n  The signature feature of the Mate 10 Pro is the processor, which has a dedicated part of its silicon specifically designed for artificial intelligence.\n  This allows the phone to crunch algorithms and do things like automatically recognize an object so that the camera can be adjusted to focus quickly and let in the right amount of light. Huawei also says A.I. allows the phone to maximize its performance: Periodically, it will automatically do maintenance, like clearing out old system files that might otherwise slow down the phone.\n  The camera is notable as well. Huawei teamed up with Leica, a popular camera maker, to develop the phone's dual-lens setup. Like phones from Apple and Samsung, the Mate 10 Pro's camera can create a so-called bokeh effect, where the two cameras work together to show the picture's main subject in sharp focus while gently blurring the background.\n  Like other modern smartphones, the Mate 10 Pro is water and dust resistant. But it also has an extra-large battery that Huawei says will last longer than that in many other phones. That's partly because of its A.I. processor, which examines how the battery is being used and changes resource allocation to prolong its life.\n  The Mate 10 Pro also ships with a screen protector applied to its display, and inside the box there is a plastic protective case. These are thoughtful additions. The case absorbs the impact of drops, and the screen protector helps prevent scratches, which weaken the structural integrity of a display.\n  Pros and Cons\n  In my tests, the two best features of the Mate 10 Pro were the camera and battery. The least impressive was the display.\n  But let's start with the good stuff. In side-by-side comparisons with an iPhone X and Samsung's Galaxy S8+, the Mate 10 Pro came in second to Apple's offering in photo quality. All took nice photos, but the colors in the Galaxy S8+'s pictures looked oversaturated, and while the Mate 10 Pro's photos appeared rich and clear, the shadow details looked better on the iPhone X.\n  As for the bokeh effect, also known as portrait mode, the Mate 10 Pro excelled at separating the subject from the background compared with the Galaxy S8+, but I still preferred the iPhone X because it did a better job at lighting up a person's face.\n  There was one area where the Mate 10 Pro was the clear winner: the battery. In my tests browsing the web over a cellular connection, Huawei's phone had roughly two hours more juice than Samsung's Galaxy Note 8 and the iPhone X.\n  The display -- the biggest downside of the Mate 10 Pro -- had a lower resolution than the Note 8, the Galaxy S8+ and the iPhone X, meaning some graphics and text looked more pixelated. Over all, text appeared crisper and websites more vibrant on the iPhone X and Samsung Galaxy screens than they did on the Mate 10 Pro's display.\n  Bottom Line\n  The Mate 10 Pro is an impressive smartphone, but you probably aren't going to buy it even if you get your hands on it. The lower-resolution display is a major negative, as is the lack of carrier support.\n  Huawei said that to get technical support for the Mate 10 Pro, you can call its hotline, and for repairs, you can ship your device to a center in Texas. That's still not ideal compared with the ease of strolling into an Apple store or your carrier's nearest location.\n  Privacy and trust are also important. In 2012, the House Intelligence Committee concluded that Huawei and ZTE, another Chinese telecommunications company, were a national security threat because of their attempts to extract sensitive data from American companies. And in 2016, security researchers discovered preinstalled software on some Huawei and ZTE phones that included a back door that sent all of a device's text messages to China every 72 hours. That feature was not intended for American phones, according to the company that made the software. But American lawmakers have been wary of Huawei.\n  Most important, you will have to decide whether you trust Huawei. The onus is on you to carefully read Huawei's privacy policy and determine if you feel confident using this phone. In a statement, Huawei said that privacy and security were top priorities and that it complied with stringent privacy frameworks and regulations.\n  At CES, Huawei's Mr. Yu described how the company had previously overcome trust hurdles -- including at home in China, where Huawei's smartphones were initially distrusted by Chinese carriers because the company was a newcomer.\n  ''It was very hard,'' he said. ''But we won the trust of the Chinese carriers, we won the trust of the developing market and we also won the global carriers, all the European and Japanese carriers. Over the last 30 years, we've proven our quality.''\n  Brian X. Chen writes Tech Fix, a column about solving tech-related problems like sluggish Wi-Fi, poor smartphone battery life and the complexity of taking your smartphone abroad. What confuses you about your tech? Send your suggestions for future Tech Fix columns to brian.chen@nytimes.com or via Twitter to @bxchen.\n\n\n\n","624":"Here's a look at what's coming up this week.\nBANKING \n  R.B.S.'s shareholder lawsuit heads to court.\n  A trial is set to begin in London on Monday in a shareholder lawsuit against the Royal Bank of Scotland over a rights issue by the lender before its near collapse in 2008. The litigation is related to the bank's raising of 12 billion pounds, or about $15.5 billion, weeks before the British government rescued it. R.B.S., which remains 73 percent owned by the British government, received a \u00a345 billion bailout that year. After an agreement announced last month, R.B.S. has settled with shareholders representing 87 percent of the claims against it, based on value. Chad Bray\n  TECHNOLOGY\u00a0\n  Humans get another crack at beating a computer.\n  The venerable Chinese game of Go will give humans another shot to prevail over machines. AlphaGo, the Go-playing contender from the DeepMind artificial intelligence arm of Google's parent, Alphabet, will take on the world champion, Ke Jie, at a humans vs. computers competition in Wuzhen, China, beginning on Tuesday. AlphaGo's defeat of a top Go player in South Korea last year was considered a major victory for artificial intelligence. Carlos Tejada\n  CENTRAL BANKING\n  Investors anticipate a benchmark interest rate increase.\n  The Federal Reserve on Wednesday will release the minutes of its most recent meeting, in early May. Surprises are unlikely. Investors expect the Fed to raise its benchmark interest rate at its next meeting in June, partly because the Fed did nothing to diminish those expectations at the May meeting. The Fed's statement after the meeting said officials expected the economy to rebound after a weak first quarter. And a few days later, the federal government announced that the unemployment rate had declined to 4.4 percent in April. Binyamin Appelbaum\n  HOUSING\n  Data on sales of previously owned homes in April is coming.\n  Also on Wednesday, the National Association of Realtors will report sales in April of previously owned homes. Sales of previously owned homes posted solid gains in March, as job growth and easing lending standards made it easier to buy a home. Economists expect the housing market to continue its steady recovery; however, a persistent scarcity of homes for sale will limit the pace of growth, and is likely to lead to a lot of disappointed home shoppers through the spring and summer. Conor Dougherty\n  ENERGY\n  OPEC leaders will consider extending production cuts.\n  OPEC ministers are scheduled to meet in Vienna on Thursday amid skepticism about whether the oil output cuts agreed to late last year are working. OPEC is likely to go along with the recent agreement by Saudi Arabia, the cartel's de facto leader, and Russia to extend the trims nine more months, through March. But analysts say OPEC, which will most likely be joined in Vienna by officials from other producing countries, may also consider other measures to further prop up prices -- now about $49 per barrel for United States crude -- such as curbing exports to soak up the oil glut. Stanley Reed\n  BANKING\n  New guidelines for trading in foreign currency markets will be unveiled.\n  The Bank of England will unveil on Thursday a new global code for best practices for trading in the foreign currency markets. The code was developed after a series of investigations into potential manipulation of the currency markets. The inquiries have led to billions of dollars in penalties, guilty pleas and even criminal charges in the United States by some of the world's biggest banks, including Barclays, Citigroup, JPMorgan Chase and the Royal Bank of Scotland. Several traders have lost their jobs as a result. The code, which is a self-regulatory measure, was jointly developed by the industry and central bankers. Currency trading firms will voluntarily agree to abide by it. Chad Bray\n  ECONOMY\n  The Commerce Department will offer a snapshot of early 2017's economy.\n  On Friday, the Commerce Department will issue two reports outlining how the economy performed in early 2017. After an initial estimate of gross domestic product growth of 0.7 percent in the first quarter, statisticians will offer a revised number based on additional data. Wall Street economists are not expecting a big change, but there could be a slight upward adjustment in the annual rate to near 1 percent, factoring in slightly healthier business investment and government spending.\n  While the G.D.P. number covers January, February and March, new data on durable goods sales for April will count toward growth in the current second quarter. The headline figure is expected to show a drop in overall durable goods orders, led by a decline in volatile aircraft and defense orders. Demand for core durable goods like appliances is expected to have held up better, showing a slight rise of 0.6 percent. Nelson D. Schwartz\n  This is a more complete version of the story than the one that appeared in print.         \n\n\n\n","625":"Many people fear that the path of artificial intelligence will eventually lead to a standoff between humans and machines, with humans as the underdogs. Confrontation looms in the forecasts of futurists and in the narratives of science fiction movies such as \"The Matrix,\" \"The Terminator\" and \"Westworld.\" But there's another way our demise could go down. We could begin wondering what makes people so special, anyway, and willingly give up the title of supreme species - or even the preservation of humanity altogether. This is the path explored by historian Yuval Noah Harari in his new book, \"Homo Deus.\" There's no need for a Terminator to come after us when, instead of fighting the network in the sky, we assimilate into it. \nAt stake is the religion of humanism. Whereas theists worship gods, humanists worship humans. Harari, whose previous book, \"Sapiens: A Brief History of Humankind,\" foreshadows this one, defines religion as any system of thought that sees certain values as having legitimacy independent of people. \"Thou shalt not kill\" derives its force from God, not from the mortal Moses. Similarly, humanists believe in \"human rights\" as things earned automatically from the universe, whatever anyone else says. The right not to be tortured or enslaved exists outside human convention. (Philosophers call this bit of magical thinking moral realism.) \u00a0\nWe may take for granted the right not to be tortured or enslaved - or various other humanist doctrines, such as the idea that we're all inherently valuable individuals with the free will to express our authentic selves -  but we have not always done so. People were seen as property even well after that bit about \"life, liberty and the pursuit of happiness\" was inked to parchment. As Harari argues, we've lived with alternatives to humanism, and we can again. And ironically, he writes, \"the rise of humanism also contains the seeds of its downfall.\" \nThat's kind of a fudge, one of a few in the book. It's not the humanist revolution per se that planted those poison seeds. It's more the (somewhat symbiotic) scientific revolution. You don't need universal rights to study electricity and invent computers. Or to apply our inventions toward the evergreen pursuits of health, happiness and control over nature (or as Harari calls them, \"immortality, bliss and divinity\"). Nevertheless, scientific and technological progress might eventually undermine the humanist ethos.\nOn the scientific front, research is pushing back on the idea of free will (as philosophers have for ages). The more we can explain human behavior with neuroscience and psychology, the less room there is for some magical human soul. \nMeanwhile, artificial intelligence is rendering us useless, taking the jobs of taxi drivers, factory workers, stock traders, lawyers, teachers, doctors and \"Jeopardy!\" contestants. And, Harari argues, liberal humanism rose on the back of human usefulness. It advanced not on moral grounds but on economic and military grounds. Countries such as France offered dignity to all in exchange for service to the nation. \"Is it a coincidence,\" Harari asks, \"that universal rights were proclaimed at the precise historical juncture when universal conscription was decreed?\" But with robots making and killing things better than we can, who needs people? Intelligence will matter more than consciousness. \"What's so sacred about useless bums who pass their days devouring artificial experiences\" in virtual reality? \nEven if the human species does continue to serve the system meaningfully, we might not matter as individuals. Harari suggests that algorithms might get to know us better than we know ourselves. As they collect data on our Web searches, exercise routines and much more, they'll be able to tell us whom we should date and how we should vote. We may happily take their advice, literally ceding democracy to databases. Once our authentic, enigmatic, indivisible selves are exposed as mere predictable computations - not just by philosophers and scientists but by our every interaction with the world - the fiction of free will might finally unravel. (Personally, I'm not sure our brains will allow this.) We'll enlist as mere specialized processors in the global cyborganic network. \nHarari presents three possible futures. In one, humans are expendable. In a second, the elite upgrade themselves, becoming essentially another species that sees everyone else as expendable. In a third, we join the hive mind, worshiping data over individuals (or God). \"Connecting to the system becomes the source of all meaning,\" he writes. In any case, he says convincingly, \"the most interesting place in the world from a religious perspective is not the Islamic State or the Bible Belt, but Silicon Valley.\" \nI enjoyed reading about these topics not from another futurist but from  a historian, contextualizing our current ways of thinking amid humanity's long march - especially a historian with Harari's ability to capsulize big ideas memorably and mingle them with a light, dry humor. \nIn \"Homo Deus,\" Harari offers not just history lessons but a meta-history lesson. In school, history was my least favorite subject. I preferred science, which offered abstract laws useful for predicting new outcomes. History seemed a melange of happenstance and contingency retroactively cobbled into stories. If history's arcs were more Newtonian,  we'd be better at predicting elections. \nHarari points to an opposing goal of his field. He writes that \"studying history aims to loosen the grip of the past,\" showing that \"our present situation is neither natural nor eternal.\" In other words, it emphasizes happenstance. That's a useful tactic for the oppressed fighting the status quo. It's also a useful exercise for those who see the technological singularity as a given. We have options.\nIt's possible we'll choose to avoid our loss of values. On the other hand, it's possible we'll choose to accelerate it. Harari, a vegan who disputes humanity's reserved seat atop the great chain of being, briefly ponders this option: \"Maybe the collapse of humanism will also be beneficial.\" Indeed, don't we owe a chance to animals and androids, too?\n","626":"Some people are already used to having their personal information exposed in massive data breaches. But the rise of artificial intelligence and connected computers in everything from toasters to implanted\u00a0medical device\u00a0could dramatically raise the stakes of digital security.\u00a0\nIn fact, they topped a long list of \"global threats\" that Director of National Intelligence James Clapper unveiled Tuesday before the Senate Armed Services committee.\n\"The Internet of Things will connect tens of billions of new physical devices that could be exploited,\" Clapper wrote in his Senate testimony. \"Artificial intelligence will enable computers to make autonomous decisions\" that hackers could disrupt to cause chaos.\nFor the nation's\u00a0spy chief to place those threats so high on his list is a big deal, and it reflects how deeply concerned the intelligence community is about the potential pitfalls of these technologies. But Clapper also found a silver lining, writing in his assessment that the\u00a0technology can \"also create new opportunities for our own intelligence collectors.\"\nIn other words, you\u00a0can expect America's intelligence community\u00a0to use driverless cars, smart thermostats and automated networks for spying purposes, too. And indeed, in the written assessment Clapper notes that \"intelligence services\u00a0might use the IoT for identification, surveillance, monitoring, location tracking, and targeting for\u00a0recruitment, or to gain access to networks or user credentials,\" in the future.\nSome high-profile figures, such as Elon Musk and Stephen Hawking, fear that unchecked artificial intelligence could even lead to killer robots that escape human control.\nBut Clapper appears more concerned about non-sentient machines that can be manipulated by malicious actors. \"AI systems are susceptible to a range of disruptive and deceptive tactics that might be difficult to anticipate or quickly understand,\" Clapper said in his written testimony. \"Efforts to mislead or compromise automated systems might create or enable further opportunities to disrupt or damage critical infrastructure or national security networks.\"\nWhen it comes to connected devices, there's plenty of evidence they are less secure than most users would hope.\nBack in 2013, for example, the\u00a0Federal Trade Commission\u00a0cracked down on a company that sold web-connected cameras. The privacy watchdog\u00a0alleged that faulty software packaged with the devices, which were marketed as secure, left private video feeds exposed online, eventually\u00a0allowing\u00a0hackers to share links to live streams from 700 customers' homes online.\n             More alarming is the potential for people to be able exploit devices to cause lethal results, from medical equipment\u00a0to cars. Last year, the Food and Drug Administration\u00a0warned hospitals not to use one kind of\u00a0drug pump after researchers uncovered a flaw that\u00a0\"could allow an unauthorized user to control the device and change the dosage the pump delivers, which could lead to over- or under-infusion of critical patient therapies.\"\n","627":"When it comes to cloud computing, Google is in a very unfamiliar position: seriously behind.\nGoogle is chasing Amazon and Microsoft for control of the next generation of business technology, in enormous cloud-computing data centers. Cloud systems are cheap and flexible, and companies are quickly shifting their technologies for that environment. According to analysts at Gartner, the global cloud-computing business will be worth $67 billion by 2020, compared with $23 billion at the end of this year. \n  ''The world's biggest maker of computer servers is making machines just for these guys,'' said John Lovelock, a cloud analyst at Gartner. ''It's the nexus of things like big data, social networks and mobility, and the next big thing, which is artificial intelligence.''\u00a0\n  For Google, a loss in cloud computing would be a rare misstep for a company that revolutionized media with its advertising business, and then made the world's leading smartphone operating system.\n  A victory for Google, however, could change how we work, turning advanced computing into an everyday utility that we use to run factories, interact in virtual reality or read one another's emotions. Given Google's track record, it's worth considering the prospect.\n  But it will be an uphill climb.\n  Amazon Web Services, which began its cloud product a decade ago, remains the leader. The company took in $2.6 billion, 9 percent of Amazon's sales, in the first quarter of 2016. Profits from the service made up 56 percent of Amazon's operating income. Those numbers may well be higher when Amazon reports its second-quarter earnings on Thursday.\n  Microsoft styled itself a cloud company, too, and the company said last week that revenue from Azure, its cloud business, which was founded in 2010, rose 100 percent over the last year. Cloud technology also figures in crucial businesses like Office 365.\n  In contrast, Google Cloud Platform does not even figure in the earnings reports of Alphabet, Google's parent company. That has to sting, since the company owns perhaps the largest network of computers on the planet, spending close to $10 billion a year to handle services like search, Gmail and YouTube.\n  Google is moving rapidly to change things. Three announcements it made last week show how it hopes to gain ground on Amazon Web Services and Azure.\n  First, the company said it has used artificial intelligence to cut the power use in its data centers 15 percent, a huge decrease considering how efficient these data factories were already.\n  Power is probably the largest single cost for all three of the cloud companies. Google is almost certain to use its savings to reduce prices, much the way it won in search advertising by figuring out its competitors' costs, then undercutting them.\n  That ability to find energy efficiency may be a powerful tool to sell to others over Google Compute. Mustafa Suleyman, the head of applied artificial intelligence at the company's DeepMind subsidiary, said the techniques could be used at power plants, refineries and other big industrial systems.\n  ''This certainly gives Google an edge,'' he said. ''Other people focused on narrow problems. We've focused on the widest possible problem.''\n  Google also released for public use ways to transcribe and analyze recorded speech for things like meaning, emotional content and whether a speaker was happy or sad. An outside company that worked on the project -- Google would not say which one -- used it to analyze over two billion minutes of customer service calls. It works in 80 languages, Google said.\n  At the same time, the company moved its customers on the United States West Coast from a data center in Iowa to facilities in Oregon. Google's network, which it claims is larger than the internet, can send the equivalent of 375 hours of video a second.\n  A move like that only makes sense when a company wants to offer the kind of split-second performance needed for virtual reality or instantaneous customer interactions over networks of cellphones and sensors.\n  At a recent conference at Amazon Web Services for software developers, an executive from iRobot, which makes the Roomba vacuuming robot, talked about using the cloud to map homes and human behavior in houses with potentially ''hundreds'' of connected devices. Doing something like that would require instantaneous connections.\n  Can faster networks, lower prices and lots of artificial intelligence put Google ahead? Amazon's lead seems to give it an edge for at least the next couple of years, as its cloud branch has perfected a method of developing hundreds of new cloud features annually. Yet while the company appears to have some basic artificial intelligence features, called machine learning, it seems to have little in the way of speech recognition or translation.\n  Mr. Lovelock, the Gartner analyst, predicted that Google would offer businesses the insights it has gained from years of watching people online. ''Amazon views the customer as the person paying the bill, while Google believes the customer is the end user of a service,'' he said. And Microsoft is promoting itself as the company that has products customers already know and use.\n  ''Everyone has to play to the strengths the market already sees they have,'' Mr. Lovelock said.\n\n\n\n","628":"The good news, according to a\u00a0study released Wednesday, is that a majority of tech experts canvassed by the Pew Research Center\u00a0Internet Project\u00a0don't think\u00a0robots\u00a0are going to displace\u00a0too many jobs\u00a0by\u00a02025.\nThe bad news is that majority - 52 percent - is ever so slight.\nForty-eight percent of the nearly 1,900 industry\u00a0experts, Internet analysts and tech enthusiasts\u00a0queried by the\u00a0research organization imagine a more dystopian\u00a0future, one in which\u00a0robots and \"digital agents\" have displaced many jobs and where there are\u00a0\"vast increases in income inequality, masses of people who are effectively unemployable, and breakdowns in the social order,\" according to the report. \u00a0\n Yikes. \n\"There was a group of people who took the economic view - technology has been shifting and changing jobs since the industrial revolution, and there's no reason to think this will change with the new wave of advances,\" says Aaron Smith, a senior researcher at Pew and a co-author of the report. \"On the other hand, we saw people say: Maybe that's true, but this next wave of change will be hitting people in ways it hasn't hit them in the past.\"\nSmith added that experts in this second camp say\u00a0change will happen particularly quickly, making it hard for people to retrain and adjust, and leading\u00a0to even greater disparities between the economy's\u00a0winners and losers.\nThe new\u00a0report\u00a0is part of a Pew series marking the 25th anniversary of the World Wide Web. Rather than asking a random sample of the population, it canvassed 12,000\u00a0tech experts, Internet analysts and members of the public who closely follow technology trends about eight different questions. One of the open-ended questions, about the impact of robotics and artificial intelligence on the future of work, generated nearly 1,900 answers and formed the basis of the report.\nSome of\u00a0the tech experts, many of whom\u00a0Pew quoted in the report, were rosy about the impact\u00a0of artificial intelligence. Vint Cerf, the chief Internet evangelist at Google, argued that all these robots would need caretakers. \"Someone has to make and service all these advanced devices,\" the report quotes him as saying. Tiffany Shlain, a filmmaker and host of the AOL Series The Future Starts Here, responded that robots would help to take away drudge\u00a0work, \"thus allowing humans to use their intelligence in new ways, freeing us up from menial tasks.\"\u00a0And economist\u00a0Michael Kende\u00a0told Pew \"every wave of automation and computerization has increased productivity without\u00a0depressing employment, and there is no reason to think the same will not be true this time.\"\n Others see disruption, but nothing so dramatic right away. Jari Arkko, chair of the Internet Engineering Task Force, said \"there are only 12 years to 2025\" and that \"some of these technologies will take a long time to deploy in significant scale.\"  \n If that's not exactly heartwarming, others were\u00a0downright alarming in some of their statements. Jerry Michalski, founder of a think tank on the future of the economy, made\u00a0a reference to the Harry Potter\u00a0villain, telling\u00a0Pew \"automation is Voldemort: the terrifying force nobody is willing to name.\" Judith Donath, a fellow with Harvard University's Berkman Center for Internet & Society, foresaw a world of chronic unemployment, one where \"live, human salespeople, nurses, doctors, actors will be symbols of luxury, the silk of human interaction as opposed to the polyester of simulated human contact.\"  \n Her colleague Justin Reich responded that \"I'm not sure that jobs will disappear altogether, though that seems possible, but the jobs that are left will be lower paying and less secure than those that exist now. The middle is moving to the bottom.\" Stowe Boyd, who writes and advises\u00a0on the future of work, was\u00a0even more bleak: \"The central question of 2025 will be what are people for in a world that does not need their labor, and where only a minority are needed to guide the 'bot-based economy.'\"\u00a0\u00a0\nWhile robotics may sound like a problem for a distant generation,\u00a0many\u00a0companies are already focused on their impact in\u00a0the workforce,\u00a0Garry Mathiason\u00a0told me in an interview. Mathiason\u00a0co-chairs the\u00a0global employment law firm Littler Mendelson's\u00a0practice group on robotics. The problem, he says, is that they too are \"struggling with the same problem demonstrated by the Pew\u00a0study. They're getting hit by both sides and aren't sure which direction to turn.\" While some\u00a0clients of his assert that\u00a0technology is coming fast and they want to\u00a0outpace it, Mathiason says, others are less concerned and think it will be slower than the predictions.\nThe irony of that quandary, of course, is that business leaders\u00a0will play a big role in deciding\u00a0just how fast and furious the changes come. Smith, the Pew study's co-author, told me\u00a0that many of the tech experts who weighed in with responses mentioned\u00a0the huge role for business leaders, political officials and the educational system in pacing and navigating what lies ahead.\n\"The technology isn't going to drive these changes,\" Smith\u00a0says. \"It's the choices we make in the political system, in the business system, and as a society.\" According to him, the prevailing view\u00a0among the tech experts surveyed\u00a0was that \"we need to be proactive and thoughtful instead of reactive to the change.\"\n              Read also:           \n The greatest memo about work-life balance ever? \n              Like On Leadership? Follow us on Facebook and Twitter.           \n","632":"WASHINGTON -- Thousands of Google employees, including dozens of senior engineers, have signed a letter protesting the company's involvement in a Pentagon program that uses artificial intelligence to interpret video imagery and could be used to improve the targeting of drone strikes.\nThe letter, which is circulating inside Google and has garnered more than 3,100 signatures, reflects a culture clash between Silicon Valley and the federal government that is likely to intensify as cutting-edge artificial intelligence is increasingly employed for military purposes.\n(Read the text of the letter.)\n\"We believe that Google should not be in the business of war,\" says the letter, addressed to Sundar Pichai, the company's chief executive. It asks that Google pull out of Project Maven, a Pentagon pilot program, and announce a policy that it will not \"ever build warfare technology.\"\u00a0\nThat kind of idealistic stance, while certainly not shared by all Google employees, comes naturally to a company whose motto is \"Don't be evil,\" a phrase invoked in the protest letter. But it is distinctly foreign to Washington's massive defense industry and certainly to the Pentagon, where the defense secretary, Jim Mattis, has often said a central goal is to increase the \"lethality\" of the United States military.\nFrom its early days, Google has encouraged employees to speak out on issues involving the company. It provides internal message boards and social networks where workers challenge management and one another about the company's products and policies. Recently, the heated debate around Google's efforts to create a more diverse work force spilled out into the open.\nGoogle employees have circulated protest petitions on a range of issues, including Google Plus, the company's lagging competitor to Facebook, and Google's sponsorship of the Conservative Political Action Conference.\nEmployees raised questions about Google's involvement in Project Maven at a recent companywide meeting. At the time, Diane Greene, who leads Google's cloud infrastructure business, defended the deal and sought to reassure concerned employees. A company spokesman said most of the signatures on the protest letter had been collected before the company had an opportunity to explain the situation.\nThe company subsequently described its work on Project Maven as \"non-offensive\" in nature, though the Pentagon's video analysis is routinely used in counterinsurgency and counterterrorism operations, and Defense Department publications make clear that the project supports those operations. Both Google and the Pentagon said the company's products would not create an autonomous weapons system that could fire without a human operator, a much-debated possibility using artificial intelligence.\nBut improved analysis of drone video could be used to pick out human targets for strikes, while also better identifying civilians to reduce the accidental killing of innocent people.\nWithout referring directly to the letter to Mr. Pichai, Google said in a statement on Tuesday that \"any military use of machine learning naturally raises valid concerns.\" It added, \"We're actively engaged across the company in a comprehensive discussion of this important topic.\" The company called such exchanges \"hugely important and beneficial,\" though several Google employees familiar with the letter would speak of it only on the condition of anonymity, saying they were concerned about retaliation.\nThe statement said the company's part of Project Maven was \"specifically scoped to be for non-offensive purposes,\" though officials declined to make available the relevant contract language. The Defense Department said that because Google is a subcontractor on Project Maven to the prime contractor, ECS Federal, it could not provide either the amount or the language of Google's contract. ECS Federal did not respond to inquiries.\nGoogle said the Pentagon was using \"open-source object recognition software available to any Google Cloud customer\" and based on unclassified data. \"The technology is used to flag images for human review and is intended to save lives and save people from having to do highly tedious work,\" the company said.\nSome of Google's top executives have significant Pentagon connections. Eric Schmidt, former executive chairman of Google and still a member of the executive board of Alphabet, Google's parent company, serves on a Pentagon advisory body, the Defense Innovation Board, as does a Google vice president, Milo Medin.\nIn an interview in November, Mr. Schmidt acknowledged \"a general concern in the tech community of somehow the military-industrial complex using their stuff to kill people incorrectly, if you will.\" He said he served on the board in part \"to at least allow for communications to occur\" and suggested that the military would \"use this technology to help keep the country safe.\"\nAn uneasiness about military contracts among a small fraction of Google's more than 70,000 employees may not pose a major obstacle to the company's growth. But in the rarefied area of artificial intelligence research, Google is engaged in intense competition with other tech companies for the most talented people, so recruiters could be hampered if some candidates are put off by Google's defense connections.\nAs Google defends its contracts from internal dissent, its competitors have not been shy about publicizing their own work on defense projects. Amazon touts its image recognition work with the Department of Defense, and Microsoft has promoted the fact that its cloud technology won a contract to handle classified information for every branch of the military and defense agencies.\nThe current dispute, first reported by Gizmodo, is focused on Project Maven, which began last year as a pilot program to find ways to speed up the military application of the latest A.I. technology. It is expected to cost less than $70 million in its first year, according to a Pentagon spokeswoman. But the signers of the letter at Google clearly hope to discourage the company from entering into far larger Pentagon contracts as the defense applications of artificial intelligence grow.\nGoogle is widely expected to compete with other tech giants, including Amazon and Microsoft, for a multiyear, multibillion-dollar contract to provide cloud services to the Defense Department. John Gibson, the department's chief management officer, said last month that the Joint Enterprise Defense Infrastructure Cloud procurement program was in part designed to \"increase lethality and readiness,\" underscoring the difficulty of separating software, cloud and related services from the actual business of war.\nThe employees' protest letter to Mr. Pichai, which has been circulated on an internal communications system for several weeks, argues that embracing military work could backfire by alienating customers and potential recruits.\n\"This plan will irreparably damage Google's brand and its ability to compete for talent,\" the letter says. \"Amid growing fears of biased and weaponized AI, Google is already struggling to keep the public's trust.\" It suggests that Google risks being viewed as joining the ranks of big defense contractors like Raytheon, General Dynamics and the big-data firm Palantir.\n\"The argument that other firms, like Microsoft and Amazon, are also participating doesn't make this any less risky for Google,\" the letter says. \"Google's unique history, its motto Don't Be Evil, and its direct reach into the lives of billions of users set it apart.\"\nLike other onetime upstarts turned powerful Silicon Valley behemoths, Google is being forced to confront the idealism that guided the company in its early years. Facebook started with the lofty mission of connecting people all over the world, but it has recently come under fire for becoming a conduit for fake news and being used by Russia to influence the 2016 election and sow dissent among American voters.\nPaul Scharre, a former Pentagon official and author of \"Army of None,\" a forthcoming book on the use of artificial intelligence to build autonomous weapons, said the clash inside Google was inevitable, given the company's history and the booming demand for A.I. in the military.\n\"There's a strong libertarian ethos among tech folks, and a wariness about the government's use of technology,\" said Mr. Scharre, a senior fellow at the Center for a New American Security in Washington. \"Now A.I. is suddenly and quite quickly moving out of the research lab and into real life.\"\nScott Shane reported from Washington, and Daisuke Wakabayashi from San Francisco. Cecilia Kang contributed reporting from Washington. \nPHOTO: Thousands of Google employees have signed a letter to Sundar Pichai, the company&rsquo;s chief executive, protesting Google&rsquo;s role in a program that could be used to improve drone strike targeting. (PHOTOGRAPH BY Michael Short\/Bloomberg FOR THE NEW YORK TIMES)Related Articles\n\n","633":"Ryszard S. Michalski, a George Mason University professor whose research helped shape the field of machine learning, bringing computers closer to the realm of human thought, died Sept. 20 of cancer at his home in Fairfax County. He was 70.\nWhile working in his native Poland in the 1960s, Dr. Michalski devised an early computer system that could recognize handwriting. After coming to the United States in 1970, he expanded the field of machine learning, creating applications in which computers could execute a form of reasoning, drawing conclusions from information supplied to them.\u00a0\n\"He was a pioneer in this field,\" said James S. Trefil, a GMU physicist and writer.\nDr. Michalski's specialty of machine learning is similar to but distinct from artificial intelligence. The underlying purpose of much of his work was to use computers to recognize patterns that could ease the decision-making process in seemingly unrelated systems. His research has been applied to agriculture, medicine, the stock market, fraud protection and voice recognition systems, among other things.\nIn 1997, he developed an influential computational system he called the Learnable Evolution Model, in which a machine's problem-solving capability increases in a clearly directed process, rather than by random increments.\nPreviously, the evolution of a machine's computational ability proceeded in a slow, fitful manner similar to Darwinian evolution in biology. With Dr. Michalski's advances, machines could guide their capacity to compute by rejecting variables that slowed their ability to process information.\nDr. Michalski likened his Learnable Evolution Model to the genetic engineering of agricultural plants, in which elements can be added or withdrawn to produce an optimal crop.\nFor many years, Dr. Michalski directed GMU's Machine Learning and Inference Laboratory. He was a co-author of a multivolume textbook, \"Machine Learning: An Artificial Intelligence Approach,\" and was a co-author or editor of more than 15 other books. He wrote more than 500 technical papers.\nHe was a co-founder of Machine Learning journal and lectured around the world. He held visiting professorships at the Massachusetts Institute of Technology, Carnegie Mellon University and the University of Wisconsin as well as at universities in Europe.\n\"He was a scholar of international repute,\" said James E. Gentle, a GMU professor of computational statistics. \"He was one of the leading scholars at George Mason.\"\nSeveral months ago, Dr. Michalski became the founding director of George Mason's Center for Discovery Science and Health Informatics. The purpose of the center is to apply the theories of machine learning to medicine. Ultimately, it was hoped that a computer could use data about a patient to make a medical diagnosis.\n\"Most computing is computation,\" Gentle said. \"This is more like reasoning.\"\nDr. Michalski was born in the Polish town of Kalusz, which later became part of Russia, Germany and, finally, Ukraine. He received his undergraduate education at the Krakow and Warsaw universities of technology and a master's degree from the St. Petersburg State Polytechnic University in Russia. In 1969, he received a doctorate in computer science from the Silesian University of Technology in Poland.\nHe was a research scientist at the Polish Academy of Sciences in Warsaw before coming to the United States in 1970 to take a faculty position at the University of Illinois. (At that time, he changed his middle name from Stanislas to Spencer.) He came to George Mason, bringing his research group with him, in 1988.\nDr. Michalski was a member of the Polish Academy of Sciences and received his homeland's Order of Merit. From 1987 to 1990, an exhibition that he developed, \"Robots and Beyond: The Age of Intelligent Machines,\" toured museums throughout the United States.\nHe was well known in Washington's Polish-American community and was a talented piano player who sometimes performed at nursing homes. He also loved to play Polish and Russian songs on the accordion at parties. In his youth, he was a ski instructor in Poland.\n\"He was not just a scientist,\" Trefil said. \"He read widely. We went to the opera together. He went to all the plays at George Mason. He was the life of the party.\"\nSurvivors include his wife of 19 years, Elizabeth Marchut-Michalski of Fairfax County; his mother, Eugenia Michalski of Vienna, Austria; a brother; and a sister.\n","634":"HONG KONG -- When the United States Air Force wanted help making military robots more perceptive, it turned to a Boston-based artificial intelligence start-up called Neurala. But when Neurala needed money, it got little response from the American military.\nSo Neurala turned to China, landing an undisclosed sum from an investment firm backed by a state-run Chinese company. \n  Chinese firms have become significant investors in American start-ups working on cutting-edge technologies with potential military applications. The start-ups include companies that make rocket engines for spacecraft, sensors for autonomous navy ships, and printers that make flexible screens that could be used in fighter-plane cockpits. Many of the Chinese firms are owned by state-owned companies or have connections to Chinese leaders.\n  The deals are ringing alarm bells in Washington. According to a new white paper commissioned by the Department of Defense, Beijing is encouraging Chinese companies with close government ties to invest in American start-ups specializing in critical technologies like artificial intelligence and robots to advance China's military capacity as well as its economy.\u00a0\n  The white paper, which was distributed to the senior levels of the Trump administration this week, concludes that United States government controls that are supposed to protect potentially critical technologies are falling short, according to three people knowledgeable about its contents, who spoke on the condition of anonymity.\n  ''What drives a lot of the concern is that China is a military competitor,'' said James Lewis, a senior fellow at the Center for Strategic and International Studies, who is familiar with the report. ''How do you deal with a military competitor playing in your most innovative market?''\n  The Chinese deals can pose a number of issues. Investors could push start-ups to strike partnerships or make licensing or hiring decisions that could expose intellectual property. They can also get an inside glimpse of how technology is being developed and could have access to a start-up's offices or computers.\n  Trump administration officials and lawmakers are raising broad questions about China's economic relationship with the United States. While the report was commissioned before President Trump took office, some Republicans have called for tighter regulation of foreign takeovers by giving a broader mandate to the Committee on Foreign Investment in the United States. Known as Cfius, the committee reviews foreign takeovers of American companies, but critics say that its scope does not include smaller deals and that it has other weak spots.\n  Ashton B. Carter, former secretary of defense under President Barack Obama, had tapped Mike A. Brown, the former chief executive of Symantec, the cybersecurity firm, to lead the inquiry into the Chinese investments, according to two of the people aware of the white paper's contents.\n  A spokesman for the Department of Defense said it ''will not discuss the details or components of draft internal working documents.''\n  The size and breadth of the deals are not clear because start-ups and their backers are not obligated to disclose them. Over all, China has been increasingly active in the American start-up world, participating in investment rounds worth $9.9 billion in 2015, according to data from the research firm CB Insights, more than four times the level the year before.\n  Neither the high-tech start-ups nor their Chinese investors have been accused of wrongdoing, and experts said much of the activity could be innocent. Chinese investors have money and are looking for returns, while the Chinese government has pushed investment in ways to clean up China's skies, upgrade its industrial capacity and unclog its snarled highways. Proponents of the deals said American limits on technology exports would still apply to American start-ups with Chinese backers.\n  But the fund flows fit China's pattern of using state-guided investment to help its industrial policy and enhance its technology holdings, as it has recently done with semiconductors. China has also carried out efforts to steal military-related technology.\n  Still, some start-ups -- especially those making hardware rather than money-drawing mobile apps like Snapchat -- said Chinese money was sometimes the only available funding. But even a company struggling for money can ultimately come up with a big breakthrough.\n  Chinese investors have a bigger appetite for risk and a willingness to do deals fast, said Neurala's chief executive, Max Versace.\n  To demonstrate his software's capabilities to the Air Force, Mr. Versace said, Neurala used its software on a ground drone from Best Buy to make it recognize and follow around the service's secretary, then Deborah Lee James, during a meeting.\n  ''We were told by the secretary of the Air Force, 'Your tech is awesome, we should put it everywhere,''' he said. ''No one followed up.''\n  Neurala finally took a minority investment from a Chinese fund called Haiyin Capital as part of a $1.2 million round, Mr. Versace said. He did not disclose the size of Haiyin Capital's commitment. Haiyin Capital is backed by a state-run Chinese company, Everbright Group, according to a statement from one of its subsidiaries.\n  American military officials have ''figured out a very good way to give $10 billion to Raytheon,'' he said. ''But to give a start-up $1 million to develop a proof of concept? That's still very, very hard.''\n  Late last year, a research firm called Defense Group Inc. argued in a report prepared for Congress that the Neurala investment could give China access to the company's underlying technologies. It also said the deal could create enough uncertainty that American officials would steer clear of Neurala's technology, effectively wasting any American money that had gone into the firm.\n  Mr. Versace of Neurala said the company took pains to ensure that the Chinese investor had no access to its source code or other important technological information.\n  To address concerns that it was not tapping innovations from start-ups, the Pentagon in 2015 set up a group called Defense Innovation Unit Experimental to enable investments into promising new companies. While at first it struggled, in 2016 it helped carry off a barrage of deals. The unit also prepared the white paper.\n  In May 2015, Haiyin Capital also invested an amount it did not disclose in XCOR Aerospace, a Mojave, Calif., commercial space-travel company that makes spacecraft and engines and has worked with NASA. XCOR did not respond to requests for comment.\n  In an interview in Chinese media, Haiyin Capital's founder, Yuquan Wang, said that part of its goal is to build Chinese industrial capabilities and that it can be hard to get space technology into China because of American export controls.\n  About the fund's investments, Mr. Wang said, ''We strive to get a portion of research and development moved back to China so that we can avoid China being only a low-end manufacturer.'' Haiyin Capital did not respond to a request for comment.\n  Quanergy, a company that works on the light-detecting sensors used in driverless cars, raised financing last summer that included funds from the partly state-backed Chinese venture fund GP Capital. A few days later, Quanergy purchased people-tracking software from Raytheon for an undisclosed amount. Alongside a wide array of commercial technology, it makes sensors for military driverless vehicles and a security system billed as ''the most complete and intelligent 3-D perimeter fencing and intrusion-detection system.''\n  Quanergy did not respond to requests for comment. Its investors also include foreign automakers and South Korea's Samsung.\n  Chinese investors have also made a push in another industry, flexible electronics. The technology, which the National Research Council has said is a priority for the American military, can help make electronics lighter and easier to attach to anything from a uniform to an airplane.\n  In 2016, a Silicon Valley start-up called Kateeva that makes machines that print flexible screens raised $88 million from a group of Chinese investors. Three took board seats, including Redview Capital, a spinoff of a firm run by the former Chinese premier Wen Jiabao's son, Wen Yunsong.\n  Kateeva's chief executive, Alain Harrus, said that while investors in Silicon Valley had begun looking more at hardware companies, raising big rounds for capital-intensive technology can be tough. Kateeva ultimately raised money where its customers were, in China and South Korea. Mr. Harrus said he believed more should be done in America to figure out the best way to nurture and fund core next-generation technologies.\n  Ken Wilcox, chairman emeritus of Silicon Valley Bank, said in the past six months he had been approached by three different Chinese state-owned enterprises about being their agent in Northern California to buy technology, though he declined.\n  ''In all three cases they said they had a mandate from Beijing, and they had no idea what they wanted to buy,'' he said. ''It was just any and all tech.''\n  Follow Paul Mozur and Jane Perlez on Twitter @paulmozur and @JanePerlez.\n\n\n\n","635":"Will robots destroy jobs, or create them?\nThe good news, according to a study released this week, is that a majority of tech experts canvassed by the Pew Research Center Internet Project don't think robots are going to displace too many jobs by 2025.\nThe bad news is that this majority - 52 percent - is ever so slight.\nForty-eight percent of the nearly 1,900 industry experts, Internet analysts and tech enthusiasts queried by the research organization imagine a more dystopian future, one in which robots and \"digital agents\" have displaced many jobs and where there are \"vast increases in income inequality, masses of people who are effectively unemployable, and breakdowns in the social order,\" according to the report. \u00a0\n Yikes. \n\"There was a group of people who took the economic view - technology has been shifting and changing jobs since the Industrial Revolution, and there's no reason to think this will change with the new wave of advances,\" says Aaron Smith, a senior researcher at Pew and a co-author of the report. \"On the other hand, we saw people say: Maybe that's true, but this next wave of change will be hitting people in ways it hasn't hit them in the past.\"\nSmith added that experts in this second camp say change will happen particularly quickly, making it hard for people to retrain and adjust, leading to even greater disparities  among the economy's winners and losers.\nThe new report is part of a Pew series marking the 25th anniversary of the World Wide Web. Rather than asking a random sample of the population, it canvassed 12,000 tech experts, Internet analysts and members of the public who closely follow technology trends about eight different questions. One of the open-ended questions, about the impact of robotics and artificial intelligence on the future of work, generated nearly 1,900 answers and formed the basis of the report.\nSome of the tech experts, many of whom Pew quoted in the report, were hopeful about the impact of artificial intelligence. Vint Cerf, the chief Internet evangelist at Google, argued that all these robots would need caretakers. \"Someone has to make and service all these advanced devices,\" the report quotes him as saying. Tiffany Shlain, a filmmaker and host of the AOL Series \"The Future Starts Here,\" responded that robots would help to take away drudge work, \"thus allowing humans to use their intelligence in new ways, freeing us up from menial tasks.\" Economist Michael Kende told Pew that \"every wave of automation and computerization has increased productivity without depressing employment, and there is no reason to think the same will not be true this time.\"\n Others see disruption, but nothing so dramatic right away. Jari Arkko, chair of the Internet Engineering Task Force, said \"there are only 12 years to 2025\" and that \"some of these technologies will take a long time to deploy in significant scale.\"  \n If that's not exactly heartwarming, others were downright alarming in some of their statements. Jerry Michalski, founder of a think tank on the future of the economy, made a reference to the \"Harry Potter\" villain, telling Pew, \"automation is Voldemort: The terrifying force nobody is willing to name.\" Judith Donath, a fellow with Harvard University's Berkman Center for Internet and Society, foresaw a world of chronic unemployment, one where \"live, human salespeople, nurses, doctors, actors will be symbols of luxury, the silk of human interaction as opposed to the polyester of simulated human contact.\"  \nHer colleague Justin Reich responded that \"I'm not sure that jobs will disappear altogether, though that seems possible, but the jobs that are left will be lower paying and less secure than those that exist now. The middle is moving to the bottom.\"\nWhile robotics may sound like a problem for a distant generation, many companies are already focused on their impact in the workforce, Garry Mathiason said in an interview. Mathiason co-chairs the global employment law firm Littler Mendelson's practice group on robotics. The problem, he says, is that they, too, are \"struggling with the same problem demonstrated by the Pew study. They're getting hit by both sides and aren't sure which direction to turn.\" While some clients of his assert that technology is coming fast and they want to outpace it, Mathiason says, others are less concerned and think it will be slower than the predictions.\njena.mcgregor@washpost.com\n","636":"The Pentagon didn't know it, but it had, until recently, a unique bond with Stuart, Va. Specifically, my little rural community near the North Carolina border and the Pentagon shared a telephone line and a telephone prefix, 694.\n\nFor many years, the people in Stuart got calls meant for the Pentagon. We'd politely say, \"Sorry wrong number,\" and tell callers that the area code for The Pentagon was 202, not 703. But we felt rather pleased with our personal link to that important federal building.\u00a0\nOne time the person at the other end of my line -- a man in Australia -- told me he was trying to get the U.S. Army Artificial Intelligence Center. He said he wanted \"artificial intelligence, because there seemed to be a shortage of natural intelligence.\" We laughed.\nBut this is the month that I stopped laughing. The phone was ringing so much, I couldn't get a thing done. And to top it off, callers argued with me when I told them that the area code for the Pentagon was 202. One man calling from California claimed that the Pentagon had changed its area code to 703. I said that couldn't be but then later thought about the number of calls I'd been receiving for the Pentagon.\nI called a friend, and she told me she had been getting a lot of odd calls too. One day, she said, she had come home to nine recorded messages. One was from the Korean Embassy, another from a general and others from people with heavy accents. She said the answering machine tape was so special that she had decided to save it.\nI investigated further. The Patrick County Administration Building and the county building inspector's office reported that they also had been receiving more than their normal share of Pentagon calls.\nThe mystery was cleared up a few days later when I received a call from F. G. Seaburger of Texas Instruments in Austin. He was trying to reach the Artificial Intelligence Center too. I told him of the problem I was having with Pentagon calls, and he was kind enough to call me back with the story.\nThe Pentagon had indeed changed its area code from 202 to 703, he said. It also had changed its 694 prefix to 614. The Pentagon had abruptly cut its link to Stuart, but many telephone users obviously didn't know about the changes.\nIn an effort to direct telephone traffic, I wrote an article in our local paper, The Bull Mountain Bugle, about the new area code and prefix for Pentagon calls. And judging from the calls I've received from my neighbors, I wasn't the only one in Stuart playing operator.\nI assume that the Pentagon is now aware of the relationship it has had with the folks in Stuart. And I hope it realizes that they also serve who only answer phones.  -- Jackie Love\n","637":"Joseph Weizenbaum, whose famed conversational computer program, Eliza, foreshadowed the potential of artificial intelligence, but who grew skeptical about the potential for technology to improve the human condition, died on March 5 in Groben, Germany. He was 85.\n  The cause was complications of cancer, said his daughter Sharon Weizenbaum.\n  Eliza, written while Mr. Weizenbaum was a professor at the Massachusetts Institute of Technology in 1964 and 1965 and named after Eliza Doolittle, who learned proper English in ''Pygmalion'' and ''My Fair Lady,'' was a groundbreaking experiment in the study of human interaction with machines. \u00a0\n  The program made it possible for a person typing in plain English at a computer terminal to interact with a machine in a semblance of a normal conversation. To dispense with the need for a large real-world database of information, the software parodied the part of a Rogerian therapist, frequently reframing a client's statements as questions. \n  In fact, the responsiveness of the conversation was an illusion, because Eliza was programmed simply to respond to certain key words and phrases. That would lead to wild non sequiturs and bizarre detours, but Mr. Weizenbaum later said that he was stunned to discover that his students and others became deeply engrossed in conversations with the program, occasionally revealing intimate personal details.\n  ''It was amazing the extent that people did not understand they were talking to a computer,'' said Robert Fano, emeritus professor of electrical engineering and computer science at M.I.T. In the wake of the creation of Eliza, which was described in a technical paper in January 1966, a group of M.I.T. scientists, including Claude Shannon, a pioneer in the field of cybernetics, met in Concord, Mass., to discuss the social implications of the phenomenon, Mr. Fano said.\n  The seductiveness of the conversations alarmed Mr. Weizenbaum, who came to believe that an obsessive reliance on technology was indicative of a moral failing in society, an observation rooted in his experiences as a child growing up in Nazi Germany. \n  In 1976, he sketched out a humanist critique of computer technology in his book ''Computer Power and Human Reason: From Judgment to Calculation.'' The book did not argue against the possibility of artificial intelligence but rather was a passionate criticism of systems that substituted automated decision-making for the human mind. In the book, he argued that computing served as a conservative force in society by propping up bureaucracies as well as by redefining the world in a reductionist sense, by restricting the potential of human relationships.\n  ''He raised questions about what kinds of relationships we want to have with machines very early,'' said Sherry Turkle, a professor in the program in science, technology and society at M.I.T. who taught courses with Mr. Weizenbaum on the social implications of technology. \n  Mr. Weizenbaum also believed that there were transcendent qualities in the human experience that could not be duplicated in interactions with machines. He described it in his book as ''the wordless glance that a father and mother share over the bed of their sleeping child,'' Ms. Turkle said.\n  The book drove a wedge between Mr. Weizenbaum and other members of the artificial intelligence research community. In his later years he said he came to take pride in his self-described status as a ''heretic,'' estranged from the insular community of elite computer researchers. \n  Joseph Weizenbaum was born on Jan. 8, 1923, in Berlin. He was the second son of Jechiel Weizenbaum, a furrier, and his wife, Henrietta. The family was forced to leave Berlin in 1935 when the Nazis enacted anti-Semitic legislation, and they emigrated the next year from Bremen, Germany, to the United States.\n  He began studies in mathematics at Wayne State University in Detroit in 1941, but left the next year to join the Army Air Corps, in which he served as a meteorologist. After the war he returned to complete his studies at the mathematics department, where he worked on the development and programming of the first large computers.\n  In 1952, he went into industry, working on an early General Electric computer development project for the Bank of America. In 1962, he was invited to become a visiting professor at M.I.T. and in 1970 became a professor of computer science at the school. \n  Attracted by his childhood experiences and the German language, Mr. Weizenbaum decided to return to Germany in 1996. His social criticism of computing technology was warmly received by a younger generation there. Much honored in German, he spoke frequently on the political and social consequences of technology.\n  His marriage to Ruth Manes Weizenbaum ended in divorce. Besides his daughter Sharon, of Amherst, Mass., he is survived by three other daughters: Miriam, of Providence, R.I.; Naomi, of Groben; and Pm, of Seattle. \n","638":"Monday\nThis slumbering sun-baked city will get a shot of adrenalin when thousands of people from around the nation gather late in the week to mark the anniversary of the civil rights march on Washington 20 years ago led by the Rev. Dr. Martin Luther King Jr. There won't be many national politicians on hand to witness the event. President Reagan is on the West Coast, relaxing at his ranch between a series of speeches in that part of the country, and Congress is in recess until Sept. 10.\u00a0\nEconomic Survey: The National Association of Business Economists releases a quarterly survey on business conditions and the economic outlook. 10 A.M., 1575 I Street N.W.\nArtificial I.Q.'s: The American Association for Artificial Intelligence, a scientific organization, opens a five- day conference to examine ways in which the computer can produce ''intelligent'' machines. 9 A.M., Washington Hilton.\nMilitary Protest: The D.C. Committee in Solidarity with the People of El Salvador stages a civil disobedience demonstration protesting American military maneuvers in Central America. 9 A.M., Fort McNair, P Street S.W.\u00a0Tuesday\nAgriculture Secretary John R. Block flies to Moscow for a ceremony Thursday at which he is to sign the new five-year grain trade agreement under which the Russians are obligated to buy a minimum of nine million metric tons of American corn, wheat and soy beans each year.\nLiving Costs: The Labor Department's Consumer Price Index for July is expected to show that inflation is still dwindling.\nClassrooms: The Carnegie Foundation for the Advancement of Teaching reports on its state-by-state analysis on the condition of teaching. 10 A.M., National Press Club.\u00a0Wednesday\nSecretary of Education T. H. Bell discusses ''The Pursuit of Excellence'' for the nation's schools. 12:30 P.M., National Press Club.\nAIDS Conference: The Surgeon General, Everett Koop, meets with doctors from Washington and New York to discuss efforts to control acquired immune deficiency syndrome.\u00a0Thursday\nSenator Gary Hart, the Colorado Democrat seeking the Democratic Presidential nomination, addresses the Southern Christian Leadership Conference's 26th annual convention. 2 P.M., New Bethel Baptist Church, 1739 Ninth Street N.W.\u00a0Friday\nThis is Women's Equality Day, by Presidential proclamation, marking the 63rd anniversary of approval of voting rights for women. It is also known as Women's Suffrage Day and Women's Liberation Day. The National Woman's Party holds a garden party to mark the anniversary. 5:30 P.M., Sewall-Belmont House, 144 Constitution Avenue N.E.\nNew Voters: Ariela Gross, a 17- year-old Presidential Scholar, is to open a voter registration drive for college-age people. 8 P.M., National Education Association, 16th Street near Massachusetts Avenue.\u00a0Saturday\nMore than 200,000 people are expected to assemble on the Mall for the 20th anniversary march on Washington. From 8 A.M. until the march begins at 10 A.M., there will be music, entertainment and speeches. The marchers are to move along Constitution and Independence avenues to the Lincoln Memorial, where there will be a rally at 1 P.M.\n","639":"David Lodge regards his fiction as \"a kind of DNA of the imagination.\" As he said in a recent interview, \"There must be a signature in everything you do, a family resemblance.\" He was in his London pied-a-terre, having come from his home in Birmingham, England. \"The same preoccupations come up in whatever form you use, but I do like to vary the form,\" he added. For him, each novel has to have a new playing field and angle of vision.\n     In his comedies of manners (\"Changing Places,\" \"Small World,\" \"Nice Work\"), Mr. Lodge has proven himself to be a one-man master class of novelists, often writing about academics and writers and the philistines with whom they inevitably collide. \n He said that the older he got -- he is now 66 -- \"the more you become aware of the structure of your imagination.\"\u00a0\n\"I tend to think in binary oppositions,\" he said, a quality exemplified by \"Nice Work.\" In that book Robyn Penrose, a post-structuralist critic and teacher, encounters a vulgarian titan of industry. Despite their differences, she and he eventually fall in love.\nWith his new novel, \"Thinks . . .\" (Viking), he broadens his syllabus to include science as well as art. In it he raises questions about human consciousness and artificial intelligence, while also keeping one foot in academia, the territory he knows so well. The polarities are represented by Ralph Messenger, a cognitive scientist and lady-killer, and Helen Reed, a novelist and teacher of creative writing. Although Mr. Lodge denies that he has any real-life prototypes for his characters, imagine a technocratic Ted Hughes in a secret romance with a newcomer to the faculty of a provincial university.\nThis time Mr. Lodge's form is a computerized variation on the epistolary. In alternating chapters, the principal characters offer accounts of their lives and loves, with Ralph recording his rambling monologues on voice-recognition software and Helen typing hers neatly on a laptop. Crosscut, the chapters are like the text in those balloons over the heads of characters in comic strips, the ones relating inner thoughts rather than public discourse. As Ralph says, \"We never know for certain what another person is really thinking,\" unless, of course, a novelist like Mr. Lodge is telling the story. Teasingly, Ralph suggests that he and Helen exchange diaries. Naturally, the reticent Helen refuses.\nAs Mr. Lodge probes the couple's cognitive conflicts -- his belief in the power of science, her faith in the power of the mind -- the novel becomes a tantalizing philosophical debate. \"Perhaps the most ambitious novel of the author's sunny career,\" Joyce Carol Oates said in a review in The Times Literary Supplement. The word sunny was carefully chosen. As with Mr. Lodge's other books, \"Thinks . . .\" is on one level a droll, irreverent exploration of sexual and educational shenanigans on a university campus.\nThe idea for the novel came after Mr. Lodge read an article by John Cornwell about two scholarly books that demonstrated that scientists were becoming very interested in consciousness, a subject previously thought unsuitable for their investigation.\nMr. Lodge's primary adviser was Aaron Sloman, professor of artificial intelligence and cognitive science at the University of Birmingham, where Mr. Lodge taught for many years until his retirement. His previous novels about academia took place at the fictitious Rummidge University, a stand-in for Birmingham. After he assured Mr. Sloman that the new book would not be set at Rummidge but at the equally fictitious University of Gloucester, this professor agreed to cooperate as his \"chief mole in the world of artificial intelligence,\" Mr. Lodge said. He was quick to add that Mr. Sloman was happily married and \"the ultimate antithesis of Ralph as a human being.\"\nImmersing himself in science, Mr. Lodge went to conferences on consciousness, where he noted a striking difference between them and the literary ones he spoofs in his novels. He said that \"every speaker at a science conference has a visual display of some kind, typically a set of slides,\" and generally improvises his talk. In contrast, academics and humanists write papers and read them, often boringly, without any visual accompaniment. Academics are also more interested in partying, as evidenced in other Lodge novels.\nIn what he calls \"a shift toward the dialogic,\" Mr. Lodge was able to scrutinize two cultures. Although, as a novelist, his natural sympathies are with Helen, he can also see validity in Ralph's stance. He said he did not write the book \"as a polemic against either artificial intelligence or neurobiology.\"\nTo keep the match even, Helen makes many insightful remarks, including her observation that because of the specialized knowledge contained in a computer, it could be regarded as the equivalent of the idiot savant. Illustrating the point, Mr. Lodge quoted one unnamed scientist: \"We can design a robot that will be a champion chess player, but we can't design one that will come in out of the rain.\"\nPartly because he was untrained in science, Mr. Lodge found the book difficult to write. It is his 11th novel and the first since \"Therapy\" in 1995, although between the two he wrote the play \"Home Truths\" and later turned it into a novella. He also wrote screenplays of his last two novels, as yet unproduced.\nIn recent years his novels have become popular in countries other than Britain. France, in particular, has a great affinity for his work. Because the title of the new book would be meaningless in French, he has been trying to think of an alternative. He realized that the French word for consciousness is the same as its word for conscience: conscience. The French title may become \"Matters of Conscience.\" Savoring the double meaning, he said, \"That's very much what the novel is about.\" \nOccasionally, Mr. Lodge still writes criticism, but fiction is his major preoccupation. \"Though I play games with novels,\" he said, \"basically I'm in the English realist tradition. In order to create characters, I have to imagine histories for them, which are consistent eternally and also with real history.\" Sometimes the same characters have appeared in several books. Robyn Penrose from \"Nice Work,\" for example, has a cameo in \"Thinks . . .\"\nMr. Lodge always tries to find \"a precursor text,\" a book or myth that can give thematic heft to a novel. With \"Small World,\" it was King Arthur and the search for the Holy Grail, and with \"Therapy,\" it was Kierkegaard's work. The inspiration for \"Thinks . . .\" came from Henry James, particularly \"The Wings of the Dove\" and \"The Golden Bowl.\" \"Both of them are about deception, sexual deception,\" he said. In \"Thinks . . .,\" he explained, \"infidelity is the main motor of the narrative.\"\nTo underscore the connection, Helen is an expert on James. Because of James's authorial control of his characters, she considers him to be a classic example of how a novelist represents consciousness. Ralph cavalierly dismisses that approach as affectation. Mr. Lodge artfully strews his book with literary references, parodying James as well as Samuel Beckett, Salman Rushdie, Martin Amis and Fay Weldon -- and challenging the reader's gamesmanship.\nSuddenly the buzzer sounded on his apartment intercom. It was a photographer coming to take his picture. Asked what he was thinking about at that moment, Mr. Lodge told a story: \"There's a famous picture of James Joyce being photographed. He's looking at the camera. Somebody said to him, 'What were you thinking when that picture was taken?' He said, 'I was thinking, would this photographer lend me five shillings?' \"\n","640":"Johnny Depp, who's built a brilliant career despite many of his lamentable film choices, may not be the first actor you think of to play a genius -- much less humanity's destroyer or savior. But he's weirdly perfect in ''Transcendence,'' an inelegant, no doubt implausible (maybe not) science-fiction film about a futurist whose consciousness is uploaded onto the Internet. There, he (or a mysterious vestige of his being) expands like the universe, growing larger and mutating into a being who is godlike and yet far from divine, sort of like a star at the apex of his popularity.\u00a0\nDirected by Wally Pfister, a cinematographer making his feature directing debut, ''Transcendence'' is a dark, lurchingly entertaining pastiche of age-old worries, future-shock jolts, hot-button topics and old-fashioned genre thrills. It was written by Jack Paglen, who, while researching, probably thumbed through Mary Shelley's ''Frankenstein; or, the Modern Prometheus,'' along with some Isaac Asimov and William Gibson. The scientist here is Will Caster (Mr. Depp), whose work in artificial intelligence has landed him on the cover of Wired magazine. (Even in a brave new world, old media remains useful shorthand.) With his wife, Evelyn (Rebecca Hall), a computer scientist, Will has created PINN, one of those supermachines with sleek surfaces, blinking lights and pulsating menace.\nThe twist in ''Transcendence'' is that the scientist becomes the monster of his own creation or so it seems. Shortly after the film opens, Will is shot by an extremist during a series of coordinated attacks on high-tech targets. He survives the initial assault only to succumb to the radiation that laced the bullet.\nAs he lies dying, Evelyn, in one of those eureka moments that implies her brain is as infinite as her husband's, initially uploads Will's consciousness into PINN, a clever bit of business she manages with their friend Max Waters (a solid Paul Bettany), a neurobiologist. One minute Will looks like a tortured lab monkey; the next, he's the ghost in everybody's machine. Well, it sounds plausible or at least Mr. Pfister moves fast enough that you don't have time to puzzle over niceties like logic.\nLike some other notable machine-based intelligences -- including the ship's computer in the original ''Star Trek,'' CORA in the television series ''Battlestar Galactica'' and Samantha in Spike Jonze's recent film ''Her'' -- PINN has a female voice. It may be that the enduringly creepy legacy of HAL 9000, the mutinous computer from ''2001: A Space Odyssey,'' has made it tougher for male voices to fill in these disembodied characters. Whatever the case, while artificial intelligences with any kind of human voice seem at once familiar and uncanny, further blurring the line between makers and their machines, those with female voices also suggest another creation story, that of Adam (the origin) and Eve (the product). ''Transcendence'' plays with this idea, and it's not for nothing that its heroine is named Evelyn.\nOnce Will's consciousness is uploaded, his voice supplants PINN's. In its initial poetic fragmentation, the voice emanating from Evelyn's reconfigured supercomputer sounds amusingly like Marlon Brando's tape-recorded ramblings as Colonel Kurtz in ''Apocalypse Now.'' (Mr. Depp and Brando were friends.) After some throat clearing, though, Will starts to sound like himself and the movie gets its crazy on.\nHis image pops up on screen, like some holographic specter, a kind of rebirth that thrills Evelyn and freaks out most everyone else, including, naturally enough, the extremists. Implausibly led by a scowling half-pint, Bree (Kate Mara), they try to stop Will and Evelyn, in a chase that also pulls in other scientists and government agents played by the likes of Morgan Freeman and Cillian Murphy.\nMr. Pfister, shooting on film and working with the cinematographer Jess Hall, gives ''Transcendence'' the dark, gleaming surfaces that gloss up a lot of contemporary thrillers and which can't help evoking Christopher Nolan's work. Mr. Pfister has been the director of photography on most of Mr. Nolan's films, including the ''Dark Knight'' trilogy, and Mr. Nolan has given ''Transcendence'' his blessing by signing on as an executive producer. So it's no surprise that the depthless blacks and glowing whites in ''Transcendence'' and Mr. Pfister's use of negative space suggest Mr. Nolan's influence, notably in the high-tech complex where Evelyn and Will set up a compound. When Evelyn walks down one of its luminous white halls, she looks as if she's headed right for one of Bruce Wayne's lairs.\nThese visual echoes don't hurt ''Transcendence,'' and they soon recede amid the escalating narrative noise. Mr. Pfister handles the predictable third-act action booms adequately -- it must be contractual that every director who makes a thriller these days must blow his sets to smithereens -- but he's better when the story scales down.\nWhen Evelyn appears in that white hallway, Mr. Pfister is showing off the production design, but he's also bringing you close to a woman who, as Will's power expands, is becoming progressively more isolated. One of those actresses who always seem smart even in dumb roles, Ms. Hall is very sympathetic as a woman in love and then in fear who, scene by scene and with palpable tenderness, takes over the film as Will gobbles up the world.\nTo an extent, ''Transcendence'' can be filed alongside other movies about fanaticism that have emerged since Sept. 11. Yet, for all its topical gloss and technobabble, it also draws from older, familiar preoccupations about the nature of being, which, along with Mr. Pfister's eye and largely smooth handling of his actors, accounts for its modest pleasures.\nHowever predictable and ridiculous, the film raises the question of what -- as the machines rise -- makes us human and why, which certainly gives you more to chew on at the multiplex than is customary these days. In ''Frankenstein,'' the monster tells his creator, ''I ought to be thy Adam, but I am rather the fallen angel.'' This is the warning that pulses through many dystopian fictions and which here finds another beat.\n''Transcendence'' is rated PG-13. (Parents strongly cautioned.) Dystopian violence.\nTranscendence\nOpens on Friday.\nDirected by Wally Pfister; written by Jack Paglen; director of photography, Jess Hall; edited by David Rosenbloom; music by Mychael Danna; production design by Chris Seagers; costumes by George L. Little; produced by Andrew A. Kosove, Broderick Johnson, Kate Cohen, Marisa Polvino, Annie Marter, David Valdes and Aaron Ryder; released by Warner Bros. Pictures. Running time: 1 hour 59 minutes.\nWITH: Johnny Depp (Will Caster), Rebecca Hall (Evelyn Caster), Paul Bettany (Max Waters), Cillian Murphy (Agent Buchanan), Kate Mara (Bree), Clifton Collins, Jr. (Martin) and Morgan Freeman (Joseph Tagger).\n","641":"George  the robot is playing hide-and-seek with scientist Alan Schultz. George whirrs and hides behind a post until he's found. \n  Then a bit later, he hunts for and finds Schultz hiding. \n  If that sounds childish, consider that Schultz is working his way up to teaching the robot to play Capture the Flag. \n  What's so impressive about robots playing children's games? \n  For a robot to actually find a place to hide, and then hunt for its human playmate, is a new level of human interaction. The machine must take cues from people and behave accordingly. \u00a0\n  This is the beginning of a real robot revolution: giving robots some humanity. \n  \"Robots in the human environment, to me that's the final frontier,\" said Cynthia Breazeal, robotic life group director at the Massachusetts Institute of Technology. \"The human environment is as complex as it gets; it pushes the envelope.\" \n  Robotics is moving from software and gears operating remotely  --  Mars, the bottom of the ocean or assembly lines  --  to finally working with, beside and even on people. \n  \"Robots have to understand people as people,\" Breazeal said. \"Right now, the average robot understands people like a chair: It's something to go around.\" \n  The researchers who are injecting humanity into robotics are creating robots that can connect with humans in a more \"thoughtful\" way. They are building robot receptionists and robot physical therapists. They are finishing work on Huggable, a teddy bear robot line that will help monitor the mental and physical health of sick children for only a few thousand dollars apiece. Robots are coaxing autistic kids out of their shells. And there's a cute penguin robot, Mel, that makes eye contact with people and nods when they talk. \n  The places we will first see these robots are in the most human-oriented fields  --  those that require special care in dealing with the elderly, young and disabled. \n  That's why George's game is important. As a machine, George is not a breakthrough. He's an off-the-shelf robot reprogrammed at the Navy Center for Applied Research in Artificial Intelligence, which Schultz directs. \n  George moves with a bulky red wheel base and binoculars that gaze around the room below a computer screen with an animated face  --  complete with blinking blue eyes. What's different is the way he interacts with people. \n  \"George, go hide,\" Schultz orders the robot in a cluttered room at the naval research lab. George's \"head\" rotates around several times. Computer codes zip by on the monitor as George is thinking. \n  Finally, George announces in a mechanical, definitely non-human voice: \"I will hide now.\" \n  He ducks behind some boxes and declares: \"I made it to the goal.\" \n  Schultz finds George easily. George has a harder time spotting Schultz, but eventually succeeds. \n  For a child, this is nothing, but for a robot this should lead to a lot. \n  \"We have only scratched the surface,\" said Sebastian Thrun, the Stanford Artificial Intelligence Lab director who won the Defense Department's Grand Challenge for a self-driving robot car through the desert last year. He predicted that 10 years from now robots will roam the health-care system and that in our homes, multi-armed robots will be doing the cleaning. \"There will be a lot of personalized devices,\" he says. \n  That's a big switch. The latest commercial home robots  --  the $280 vacuuming iRobot Roomba, with more than 2 million of the disc-shaped devices sold, and its floor-cleaning cousins  --  are designed to work best when people leave the room. But the promise of robots for scientists is represented by Rosie, the vacuuming robot of \"The Jetsons\" cartoon series, who dutifully works as Jane blithely walks by. \n  \"If Rosie is going to be around and in your face, it would be good if the interaction is natural and easy,\" says Rod Brooks, director of MIT's artificial intelligence lab. \n  So after spending decades tinkering with wiring, some roboticists  --  a usually male and techno-geek-heavy field  --  did the unthinkable. They put aside their hardware and software, and studied how humans think, work together and communicate so they could apply that to robots. \n  The new field of human-robot interaction was born. Unlike the rest of robotics, many of its leaders are women. It has social scientists, language specialists, medical doctors and even ethicists who wonder if putting robots into places like nursing homes is the right thing to do. \n  That's a big change from 50 years ago, when the field of artificial intelligence was created at a forum at Dartmouth University.  \n The experts focused on puzzles and chess and skipped over concepts such as perception, a sense of where you are, what's around you and how to interact. \n  \"They all thought perception was easy  --  a 2-year-old could do that  --  but smart people play chess,\" said Brooks, co-founder of iRobot Corp. \"They all missed it and Hollywood missed it. The stuff a 2-year-old could do, that's the hard stuff.\" \n  One preschooler-type skill, the ability to take someone else's perspective, \"turned out to be a very important capability that we needed on our robots so that they could really work comfortably with humans,\" said Schultz. \n  Thus, Schultz hopes in the next year or so to have a robot that could, like an old-time movie detective working a case, tail a person walking through the naval research lab campus unseen. \n  Similarly, researchers are working on teaching language-reasoning  --  not just dumping a dictionary in the robot's database,  but gestures and eye contact so robots can understand the many ways people communicate. At NASA, astronauts are working with Schultz and a spacewalking prototype called Robonaut to make machines understand when an astronaut points to something and says \"there.\" \n  We as humans understand that, but getting robots to put those clues together is proving to be a big leap, he said. And then there are subtle clues that humans pick up without even knowing it, such as nods and eye contact. \n  Research scientist Candy Sidner at the Mitsubishi Electric Research Lab in Cambridge, Mass., found that people respond better to more animated robots  --  those that nod, move and point. So she developed Mel, a pointing, nodding penguin robot. You nod at Mel, Mel nods back. \n  \"It's absolutely very compelling. People tell me, 'I like Mel because he's really kind of cute,' \" Sidner said. \n  How should a robot look? There's debate on that. On one extreme are the stroke-therapy robots of MIT scientists Neville Hogan and Hermano Igo Krebs. Those look like exercise machines with video game screens. They guide the arms and legs of paralyzed stroke patients through physical therapy, and the patients don't even realize they are robots. \n  On the other end of the spectrum are David Hanson of Dallas and Osaka University's professor Hiroshi Ishiguro, whose robots look creepily human. Ishiguro's robot Geminoid looks just like Ishiguro. \n  Such uncanny resemblances have led roboticists to coin the term \"uncanny valley\" syndrome. It suggests that people respond better to robots the closer they resemble humans  --  up to a point. If the resemblance is too good, people \"are weirded out,\" Sidner said. At that point, acceptance plummets. That's why Sidner prefers her penguin robot. \n  Sherry Turkle at MIT worries about robots that seem too human. \n  \"We're cheap dates,\" she says. \"If an entity makes eye contact with you, if an entity reaches toward you in friendship, we believe there is somebody there. . . . But that doesn't mean that there is. That just means that our Darwinian buttons are being pushed.\" \n  Turkle, who directs the MIT Initiative on Technology and Self, fears people will be subconsciously tricked into giving robots more credit than they deserve. Her point is that when you are sick, hurt or elderly, \"you really do want a person,\" not a robot. \n  Unfortunately, there's a shortage of people working in nursing homes and caring for old people and the disabled, said Maja Mataric, director of the University of Southern California's Center for Robotics and Embedded Systems. The average stroke victim gets 39 minutes of active exercise a day when six hours a day are needed, she said, so robots can free up the few nurses for more nurturing activities. \n  Mataric adjusts her robots' personalities to fit the needs of stroke patients  --  nurturing buddy or goal-pushing coach. \n  And in the case of low-functioning autistic children, they actually seem to relate better to robots than humans, Mataric said. \"You'll see a child smile that has never smiled before. No one knows why it happens.\" \n","642":"Over the course of her dance company's 35 years, the choreographer Trisha Brown has worked with such independent-minded collaborators as the musician Laurie Anderson and the artist Robert Rauschenberg. On Saturday, Ms. Brown's newest creative partner demonstrated that it might have a mind of its own, too.\n In an innovative fusion of modern dance and high technology, Ms. Brown is collaborating with a computer on ''how long does the subject linger on the edge of the volume ,'' a 30-minute work for seven dancers and animated graphics. Ms. Brown has choreographed the dance. And the computer, driven by an artist-designed artificial-intelligence software program that responds instantly to the dancers' movements, draws graphics that are projected on a transparent screen in front of the stage.\u00a0\n  At the premiere, on Saturday in the Galvin Playhouse at Arizona State University, Ms. Brown, 68, discovered that a digital collaborator can be as unpredictable as a human one. The computer-generated graphics were bolder than they had been at the dress rehearsal the night before, even though the choreography was unchanged.\n After the premiere, Ms. Brown said she was thrilled by the surprise appearance of what she described as strange shapes and white blobs. The experience was like having ''a living set,'' she said. ''Of course,'' she added, ''we may not be able to get them back.''\n New York audiences will have an opportunity to watch for the white blobs when ''how long'' is presented in the Trisha Brown Dance Company's program tomorrow and Saturday in the Rose Theater at Lincoln Center. The performances are part of a four-evening observance of the company's 35th anniversary. Ms. Brown's collaborations with Mr. Rauschenberg will be featured tonight and Friday.\n At a panel discussion after the premiere, Ms. Brown asserted that ''how long'' was one of her best works. But in an earlier interview, she had acknowledged that the dance was not hers alone. ''I build a world when I make a choreography,'' she said, ''and I didn't have to do the whole job, because the graphics are dancing, too.''\n The computer was hardly Ms. Brown's sole collaborator. Arizona State, which commissioned ''how long'' with Lincoln Center, invited Ms. Brown to develop the dance to advance the school's motion-analysis research.The artists Marc Downie, Shelley Eshkar and Paul Kaiser created the work's visual imagery and the artificial-intelligence software behind it. Curtis Bahn, a composer at Rensselaer Polytechnic Institute, devised a pointillistic soundscape that reacts to the dancers' movements.\n The work's title, ''how long does the subject linger on the edge of the volume '' is a technical crew member's phrase, which Ms. Brown said she overheard at an early rehearsal. ''I thought, holy moly, what kind of poetry is this?'' she said, indirectly affirming that artists and engineers do not always speak the same language.\n But Ms. Brown's new work is the latest example of how choreographers have embraced technology, expanding dance's expressive possibilities with video projections, electronic sensors and motion-capture technology. Motion-capture technology's ability to create a digital, three-dimensional record of movements has inspired choreographers besides Ms. Brown. In Merce Cunningham's ''Biped'' (1999), his dancers performed with video projections of their motion-captured selves, which had been transformed by Mr. Eshkar and Mr. Kaiser into sparsely sketched figures.\n By adding artificial intelligence to the mix, the collaborators on ''how long'' hope to create what they call ''thinking images.'' By their reckoning, graphics that can respond in real time to the dancers can illustrate, interpret or provide counterpoint to the movements.\n Because they are lines and abstract shapes, the relationship is not always obvious. Still, Mr. Kaiser said, ''there is a deep rhythm connected to what's happening onstage, even if you can't see it.''\n Nor is the software likely to run amok and start singing ''A Bicycle Built for Two'' in the middle of a performance. Even with slight variations in dancers' movements and the software's capriciousness, Mr. Downie said, ''it will always be this work.''\n For Ms. Brown, who has based dances on mathematical formulas, entering the digital realm is not a conceptual stretch. She worked on her choreography for months before she saw the graphics.\n ''When I saw these enormous pictograms manifest for the first time, I thought they were a new form of art,'' she said. Ms. Brown, who is also a visual artist, said the graphics reflected her own style.\n But ''my formal, organized choreography got overrun by the expansive graphics,'' she said. Although the work is built on her core vocabulary, her dancers are covering more ground so they are not obscured by the graphics.\n A lot of technology is required to make ''how long'' go, starting with a motion-capture system. Four of the seven dancers wear reflective markers. An array of 16 infrared cameras captures their movements 100 times per second and transmits the data to a computer. The computer calculates each marker's position and creates a three-dimensional record of the action.\n The motion-capture information is sent to another computer, where software developed by Arizona State engineers starts to analyze it. Rather than determining whether a dancer is leaping or twisting, the software considers where the dancers are in relation to one another.\n This data is sent to Mr. Bahn's computer, where it triggers the music, and to Mr. Downie's, so his artificial-intelligence software can deliver the graphics within one-tenth of a second. \n As triangles, squares and rectangles drift across the front of the stage, their shapes might mirror a dancer's bent frame. Lines reach down and seem to touch the dancers.\n Although the images sometimes overwhelm the dance, the dancers are not distracted by them. Neal Beasley said he was more aware of the markers sewn onto his costume. Most of the graphics were above his line of sight, he said. As a result, he added, ''they're interacting with us as opposed to us interacting with them.''\n A new computer-enhanced 30-minute dance, ''22'' by Bill T. Jones, shared the stage with ''how long'' on Saturday. Thanassis Rikakis, director of the arts, media and engineering program at Arizona State, said it cost $1.8 million to develop the works over three years. One of the program's goals, he added, is to build scaled-down versions of the motion-analysis system so smaller dance troupes can afford them.\n Ms. Brown, for her part, said she would welcome the opportunity to work again with motion-driven graphics. Sounding like a computer scientist, she added, ''I want to know these objects better, so I can have a human relationship with them.''\n But Mr. Downie cautioned that motion-capture technology still has a long way to go before it can fully document all the nuances of human movement. Sounding like a choreographer, he said, ''The dancers are still there for a reason.''\n","643":"             How about that!\n             A year ago today, on April 1, 2015, as part of a project on prediction at the New York Universe - Center for Data Superiority (NYU-CDS), I gathered together a political scientist, a data-driven journalist, a traditional campaign reporter for a major newspaper and an artificial intelligence application created by a major international business machine company to discuss the state of the primary contests and to collect their predictions for the eventual nominees. I promised to only reveal these predictions a year later.\u00a0I also agreed to use\u00a0pseudonyms for all four participants and - with some regret - allowed them to choose their own pseudonyms.\nWhat follows is a lightly edited transcript of our discussion.\u00a0\n             Me:              Please let me thank you all for joining me today! So who do you think will be the Republican nominee and why?\n             Etan Gold (data-driven journalist): Polls offer the best opportunity for insight into the eventual winner. If you look at the most recent national Quinnipiac poll, you will see that Scott Walker (18\u00a0percent) and Jeb Bush (16\u00a0percent) are the only candidates polling in double digits. Of those in single digits, only Chris Christie and Mike Huckabee even reach 8\u00a0percent, so these are pretty serious leads for Walker and Bush. Of course, one poll can be flawed, but most recent numbers from Pew also show Walker, Bush, and Huckabee at the top of the pack.\n             Timesy McTimesface (traditional campaign journalist): You know, Rubio's campaign manager also managed Joni Ernst's stunning victory in Iowa in 2014. What I'm hearing from unidentified sources within that campaign is that Rubio has a real plan to win in place, and that the energy surrounding both the team and its supporters is phenomenal.\n             Professor Christmas (political scientist): A lot of prior research shows us that eventually the party is going to decide on the candidate that gives it the best chance to win in 2016.\u00a0 So you need to be asking, what does the party need to win in 2016?\u00a0 Outreach to Latinos and other minorities seems crucial. Someone who can attract the undecideds while still rallying the base is also important.\u00a0 Above all, though, the party is going to want to someone who is careful about what he or she says in public so that we don't see a repeat of Romney's \"47\u00a0percent\" comments.\n             Sherlock (artificial intelligence program): Trump.          \n             Etan Gold: Really, Sherlock? Donald Trump isn't even listed as an option in the Quinnipiac or Pew Polls, and only 1 percent of Republicans in the Quinnipiac poll reported they were considering voting for a candidate other than those listed in the poll (Bush, Carson, Christie, Cruz, Graham, Huckabee, Jindal, Kasich, Paul, Perry, Rubio, Santorum, Walker).\n             Timesy McTimesface:              I couldn't agree more.\u00a0 Trump has no campaign staff to speak of, no political organization and no real track record to show he could manage this sort of effort.\n             Professor Christmas: Look, Donald Trump is the last person the Republican Party elite would decide on. Seriously! #NeverTrump!\n             Sherlock (artificial intelligence): Trump.\n             Etan Gold: Oh -- maybe Sherlock thinks we are playing cards? Do you want to pick a suit for trump?\n             Timesy McTimesface: Or is presidential politics too complicated for your operating system, Sherlock? All systems not a Go?\n             Sherlock:              Trump.\n             Professor Christmas: Sherlock, do you really think Donald Trump can beat all of these experienced Republican candidates who have had years to interact with party elites? What's he going to do, Tweet his way to the nomination?\n             Sherlock: Trumped!\n             Me:              Apparently there's some sort of bug in the AI system and it's just stuck on Trump. I don't quite understand why, but no matter what you feed into it, it seems to just spit out Trump. Sorry about that! Didn't mean to subject you to all Trump all the time - can you imagine?\n             Me: So let's turn to the Democrats - anyone who can give Hillary Clinton a run for her money?\n             Gold: No.\n             McTimesface: No.\n             Professor Christmas: No.\n             Sherlock: A 75 year-old Jewish socialist from the whitest state in the country.\n             Me: Whoops!\u00a0 That thing really is broken.\u00a0 Well, thanks for your time, and looking forward to seeing how things turn out next year!\n","644":"In a dusty valley in southern Lebanon, ''Sgt. John Smith'' of the Special Forces scans the scene in front of him. Ahead is a village known as Talle. His immediate mission: to find out who the local headman is and make his way to that house. \n All discussions with the villagers will have to be conducted in Arabic, and Sergeant Smith must comport himself with the utmost awareness of local customs so as not to arouse hostility. If successful, he will be paving the way for the rest of his unit to begin reconstruction work in the village.\n Sergeant Smith is not a real soldier, but the leading character in a video game being developed at the University of Southern California's School of Engineering as a tool for teaching soldiers to speak Arabic. Both the game's environment and the characters who populate it have a high degree of realism, in an effort to simulate the kinds of situations troops will face in the Middle East. Talle is modeled on an actual Lebanese village, while the game's characters are driven by artificial-intelligence software that enables them to behave autonomously and react realistically to Sergeant Smith.\u00a0\n The Tactical Language Project, as it is called, is being developed at U.S.C.'s Center for Research in Technology for Education, in cooperation with the Special Operations Command. From July 12 to 16, real Special Forces soldiers at Fort Bragg in Northern California will test the game and put Sergeant Smith through his paces. \n The user plays Sergeant Smith, while the other characters are virtual constructs. Using a laptop, the user speaks for the sergeant, in Arabic, through a microphone headset and controls the character's actions by typing keyboard instructions.\n The project is part of a major initiative, financed by the Defense Advanced Research Projects Agency, or Darpa, to explore new ways of training troops by making use of the large installed base of existing technology, especially laptops.\n ''I'd like to be able to send something like this to every soldier stationed in a foreign country,'' said Dr. Ralph Chatham, the Darpa project manager. \n The philosophy is to deliver what Dr. Chatham calls ''tactical language,'' linguistic skills sufficient to the task at hand.\n Dr. Lewis Johnson, the director of the Center for Research in Technology for Education, or Carte, said, ''The basic assumption is that there's certain situations you need to face -- such as establishing a rapport with the people you meet and finding out where the headman lives -- and how do you cope effectively with those situations.''\n No one is going to be able to read Omar Khayyam after this training, but the agency hopes it will enable soldiers to navigate more easily and safely through the Arab world. In its current version, the game teaches Lebanese Arabic. The U.S.C. team is also working on an Iraqi Arabic version. Darpa hopes to have at least some preliminary version to the military by the fall, Dr. Chatham said. \n Dr. Johnson, a linguist and an artificial intelligence expert, noted that for English speakers, Arabic is a relatively difficult language, containing sounds that they find hard to distinguish. Moreover, Arabic dialects differ considerably by region. \n ''People who are taught literary Arabic typically have a lot of difficulty on the street,'' he said.\n But it is on the street that soldiers need to be most effective. One of the tools the Carte team has developed is a virtual tutor that uses artificial intelligence software to coach individual students through the minefield of pronunciation. To do this, the researchers have had to design speech recognition software tailored specifically for language learners. \n At the United States Military Academy, senior-level Arabic students who tested an early version last October were ''very enthusiastic'' about it, said Sherri Bellinger, director of West Point's Center for Technology Enhanced Language Learning. \n ''We've had a vision of learner language speech recognition for a long time, but until recently we didn't have the computer power to make this possible,'' Ms. Bellinger said.\n Communicating is not just about uttering the right words, Dr. Johnson said. It also involves a huge amount of nonverbal interaction. The Tactical Language Project was born of Darpa's realization that in addition to basic vocabulary soldiers in foreign countries also need to understand basic cultural and gestural cues. Dr. Chatham tells the story of a soldier in Afghanistan, soon after the start of war there: soldiers in his unit came to a village and realized that not only did they not understand a word being spoken, they could not interpret people's nonverbal cues. \n In tense situations like those induced by war, nonverbal messages may be just as important as words themselves. The Tactical Language Project game is intended to teach such skills. Users learn, for example, that when Sergeant Smith starts or finishes a conversation with an important person, he can cross his right hand over his heart and bow slightly, a common gesture of respect in the Arab world.\n As with speech, nonverbal communication is a two-way process, and here, said Dr. Hannes Vilhjalmsson, a Carte scientist, realism becomes a critical quality. A great deal of the team's research has been directed at getting the game's virtual characters, or ''agents,'' to behave in realistic ways. \n Dr. Vilhjalmsson pointed out a simple example: ''When you are talking to someone, you want them to be facing you.'' \n Humans take this for granted, but agents have to be taught it.\n Such intense levels of realism may sound like a luxury, but Dr. Chatham notes that research on memory formation suggests that people retain more information when they are in a heightened state of mental engagement with their surroundings. In order to make the game's characters as realistic as possible, each one is programmed with what the researchers call a belief system. Each character has its own individual set of beliefs about the world and about Sergeant Smith that will change in response to his actions, said Mei Si, a doctoral student in charge of coding this element. \n One of their most critical beliefs is their trust level, Ms. Si said. If Sergeant Smith behaves appropriately, he will gain the characters' trust and they will help him; if not, he is likely to cause suspicion. \n ''You don't have to be obnoxious,'' Dr. Vilhjalmsson said. ''Mainly, you just have to be impolite, or not seem to care about what you are saying.'' \n Dr. Johnson noted that one of the first things many users have to learn is simply to say thank you.\n ''Most video gamers are not used to saying thank you in the context of a game,'' he said.\n Developing so-called intelligent agents is currently a hot research topic and U.S.C.'s Information Sciences Institute, where Carte is based, is home to world leaders in this field. \n Two institute scientists, Dr. David Pynadath and Dr. Stacy Marsella, have developed a program called PsychSim to model individual and group behavior among agents. PsychSim is the software platform guiding the behavior of the Tactical Language characters. \n Another characteristic the agents possess is what Dr. Vilhjalmsson calls their ''arousal level.'' One way of understanding this, he said, is as a form of virtual anxiety: ''If their anxiety level gets too high, it will trigger them to act.''\n In a scene in a cafe, Sergeant Smith must try to find out who the village headman is. If he doesn't act properly, one of the cafe patrons will jump up and demand to know who he really is. If tensions escalate, the patron will eventually accuse the sergeant of being a C.I.A. agent.\n Intelligent agents have been used before in research environments, but this is the first time such sophisticated behavioral modeling has been put in a video game, Dr. Johnson said.\n In the second phase of the project, beginning late this year, two further languages will be included. One will probably be Dari, a major language in Afghanistan. Another under consideration is one of the Indonesian languages. Once the basic platform is designed, Dr. Chatham said, the team hopes to use it with many different languages and cultural contexts.\n ''We're spending a lot on developing this,'' Dr. Chatham said, noting that the cost is about $7.2 million. But the hope, he said, is that eventually such intelligent games could be used not just for teaching languages but ''for other kinds of memory-intensive training tasks.''\n","645":"They think Viv can really get people talking - and alter the tech landscape\n             SAN JOSE - In an ordinary conference room in this region of start-ups, a group of engineers sat down to order pizza in an entirely new way.\n\"Get me a pizza from Pizz'a Chicago near my office,\" one of the engineers said into his smartphone.\nIt was their first real test of Viv, the artificial-intelligence technology that the team had been quietly building for more than year. Everyone was a little nervous. Then, a text from Viv piped up: \"Would you like toppings with that?\"\nThe engineers, eight in all, started jumping in: \"Pepperoni.\" \"Half cheese.\" \"Caesar salad.\" Emboldened by the result, they peppered Viv with more commands: Add more toppings. Remove toppings. Change medium size to large.\u00a0\nAbout 40 minutes later - and after a few hiccups when Viv confused the office address - a Pizz'a Chicago driver showed up with four made-to-order pizzas.\nThe engineers erupted in cheers as the pizzas arrived. They had ordered pizza, from start to finish, without placing a single phone call and without doing a Google search - without any typing at all, actually. Moreover, they did it without downloading an app from Domino's or Grubhub.\nOf course, a pizza is just a pizza. But for Silicon Valley, a seemingly small change in consumer behavior or design can mean a tectonic shift in the commercial order, with ripple effects across an entire economy. Engineers here have long been animated by the quest to achieve the path of least friction - to use the parlance of the tech world - to the proverbial pizza.\nThe stealthy, four-year-old Viv is among the furthest along in an endeavor that many in Silicon Valley believe heralds that next big shift in computing - and digital commerce itself. Over the next five years, that transition will turn smartphones - and perhaps smart homes and cars and other devices - into virtual assistants with supercharged conversational capabilities, said Julie Ask, an expert in mobile commerce at Forrester.\nPowered by artificial intelligence and unprecedented volumes of data, they could become the portal through which billions of people connect to every service and business on the Internet. It's a world in which you can order a taxi, make a restaurant reservation and buy movie tickets in one long unbroken conversation - no more typing, searching or even clicking.\nViv, which will be publicly demonstrated for the first time at a major industry conference on Monday, is one of the most highly anticipated technologies expected to come out of a start-up this year. But Viv is by no means alone in this effort. The quest to define the next generation of artificial-intelligence technology has sparked an arms race among the five major tech giants: Apple, Google, Microsoft, Facebook and Amazon.com have all announced major investments in virtual-assistant software over the past year. \n                                                                                           Two of them - Google and Facebook - have made offers to buy Viv, according to people familiar with the matter. (Facebook chief executive Mark Zuckerberg is also an investor in Viv through the firm Iconiq Capital.) \nViv also has the ultimate pedigree in the elite universe of technologists who strive to build machines that can talk to people. Its creators, Dag Kittlaus and Adam Cheyer, were also co-founders of Siri, the app that became the first widely distributed virtual assistant when it was acquired by Apple in 2010. \n\"It's about taking the way that humans have naturally interacted with each other for thousands of years and applying that to the way they interact with services,\" said Kittlaus, Viv's chief executive. \"Everyone knows how to hold a conversation.\" \n          Moving past the apps          \nThe goal is not just to build great artificial intelligence. Companies see in this effort the opportunity to become the ultimate intermediary between businesses and their customers. \nSearch engines were among the first of these \"platforms,\" enabling Google to generate a fortune from organizing the vast array of Web pages for ordinary users. Then, with the rise of smartphones, came apps that pulled consumers out of desktop search into the mobile world. Apple and Google raced to become the gatekeepers of these smartphone programs by building app stores that take a cut of the profits.\nBut despite apps growing into a $50 billion business, consumer enthusiasm for most new apps is waning, according to ComScore and the analytics company App Annie.\n\"Little siloed chiclets, none of which speak to each other, living inside the walled gardens of rival app stores owned by Apple and Google,\" said John Battelle, a Web entrepreneur and the chairman of digital-ad company Sovrn Holdings.\nToo much data used up, too many passwords to remember, too many useless notifications, concluded Dan Grover, product manager at WeChat - the popular Chinese messaging platform that is helping make many apps irrelevant - in a recent blog post. \nMobile users now spend 80 percent of their time in just five apps, according to 2015 data from Forrester. \"It's just too inconvenient for consumers to hop in and out of so many apps,\" Ask said. \"So consumers are consolidating where they spend their time. There's now a much bigger bar to get over if you're going to build an app.\"\nChris Messina, developer-experience lead at Uber, one of the most highly valued apps on the market, said that \"apps will still have a place. But the landscape is going to get a lot broader.\"\nVirtual assistants offer an alternative. But the difficulty, stemming back to the early artificial-intelligence efforts in the 1960s, has always been understanding the nuances of how humans talk.\nMost virtual assistants today can understand a set of human questions. But those queries have to be stated in a precise way, and they trigger largely scripted responses. What distinguishes Viv is that it aims to mimic the spontaneity and knowledge base of a human assistant, said Oren Etzioni, chief executive of the Allen Institute for Artificial Intelligence in Seattle.\nBy working with data from movie-ticket vendors, Viv can understand the multitude of ways people can ask it to buy movie tickets. It can look up showtimes and, on its own, suggest entertainment alternatives from other vendors if the desired showing is sold out. And it can compare prices and then buy the tickets, along with making a restaurant reservation beforehand. If the user changes her mind, the assistant can take care of the cancellations and let her know it's done.\nGrubhub chief executive Matt Maloney said he rushed to sign up with Viv two years ago, impressed with the idea of allowing consumers to perform different activities without having to toggle between services. \"No one has been able to say, 'I want the movie ticket, and the bottle of wine, and some flowers on the side' - all in one breath,\" he said.\nAchieving that level of communication is a very high bar, Etzioni said. And no technologist has come close to achieving it. In a way, Viv's founders are among the staunchest adherents to the original Turing Test - the proposition, laid out by artificial-intelligence pioneer Alan Turing over half a century ago, that a machine has achieved intelligence if it can carry on a conversation that is indistinguishable from a human one. \n\"If it were anybody else, I'd say it was probably too ambitious,\" Etzioni said of the Viv team. \"If anybody has a shot at doing this, it's them.\"\n          Viv's vision was once Siri's          \nViv's 26-person team has been toiling away for longer than just about anyone else. The effort preceded Siri and goes back to 2003, when Cheyer led a 300-person team at SRI International - a nonprofit, government-funded research-and-development lab in Palo Alto, Calif. - working on a sprawling Defense Department project to create a next-generation personal assistant.\nKittlaus, an SRI colleague and former Motorola executive, persuaded Cheyer to build the technology into a mobile app after he saw the popularity of smartphones. (Kittlaus, who is Norwegian American, named the product Siri after a former co-worker - he liked that the Nordic word meant \"beautiful woman who leads you to victory.\")\nThough Siri is known for her conversational skills - which includes some dry wit and sass - there's a lot she and other virtual assistants can't do. Ask Siri to \"buy me a ticket for the Beyonc\u00e9 concert\" and she'll pull up a link to Ticketmaster's Web page. Ask her to reserve you a table at a restaurant near your house and she can pull up the time and date you requested, but you can't book the reservation unless you have the OpenTable app installed.\nThat wasn't how it was supposed to be, Kittlaus said. The original Siri wasn't supposed to be a clever AI chatbot. The goal was to reinvent mobile commerce itself. When it initially launched as an independent app in 2010, Siri could buy tickets, reserve tables and summon a taxi - all the while bypassing search pages and without a user having to open or download another app. She was able to siphon data from 42 Web services, including Yelp, StubHub, OpenTable and Google Maps.\nBut nearly all of the partnerships were dissolved after Apple took over. To build them, Kittlaus had essentially gone door-to-door to various tech companies asking for permission to connect to their stores of proprietary data. Kittlaus and Cheyer, who became close with Apple's Steve Jobs before his death in 2011, would not discuss what happened beyond this from Kittlaus: \"Steve had some ideas about the first version, and it wasn't necessarily aligned with all the things that we were doing.\" Kittlaus quietly left Apple in 2011. A third of the original Siri engineering team members, including Cheyer, eventually followed him and are now building Viv.\nViv \"is what they wanted Siri to become - an open system,\" said Bart Swanson, an adviser at the venture-capital firm Horizons Ventures and an investor in Viv, Siri and other artificial-intelligence technologies.\nToday, Viv has replicated its pizza experiment with about 50 partners. You can tell Viv to order a car and it will deliver your options, nearby, using data from Uber. Viv will order flowers using data from FTD. Viv will turn lights on and off via a home automation platform called Ivee. Other partners include SeatGuru, Zocdoc and Grubhub. Kittlaus is talking to television companies, car companies, media companies and makers of smart refrigerators in his quest to unite all of them into a single, unbroken conversation. The data from these services enables the Viv brain to seem \"intelligent.\" \nMaloney, the Grubhub chief, said he liked the idea of getting access to voice and conversational technology without having to build it himself. \nThe prospect of a new channel that would bypass the primary gatekeepers for apps -  Apple and Google's app stores - was also attractive. \"Right now, the main conduits to a consumer are owned by Google and Apple,\" he said. \"My job is to get my restaurants in front of people. This gives us a new route.\"\n          A race to find partners          \nThe landscape has changed dramatically since Kittlaus and Cheyer released Siri - even more so since they, along with a third co-founder, Chris Brigham, started building Viv. For example, Amazon, which last year released its conversational virtual assistant Alexa - a cylindrical device for the home - has opened its voice capabilities to third parties. You can now order an Uber car by talking aloud to Alexa in your home, and she can read you news, weather and traffic information. Alexa not only bypasses apps and Google - she bypasses the smartphone itself. (Amazon chief executive Jeffrey P. Bezos owns The Washington Post.)\nFacebook, meanwhile, is trying to turn its messaging app, Messenger, into a portal for businesses. At its annual developer conference last month, Facebook enabled a handful of companies such as Expedia and 1-800-Flowers.com to conduct basic customer service over chat on Messenger. Early reviews found the product to be cumbersome, but businesses see big possibilities. In an interview, Expedia chief executive Dara Khosrowshahi said chatbots and artificial intelligence have the ability to return online travel to the roots of the traditional agent, who knew the customers and their preferences. \nWith such promising technology and partnerships already emerging, the biggest challenge for Kittlaus and Cheyer will be to find a distribution model that gets Viv into the hands of as many people as possible - without compromising the vision. \nThe two faced a similar choice six years ago when Jobs offered to buy their little-known app and distribute it to millions of people. Jobs took them to his home in Palo Alto, and the group talked for three hours by the fireplace. They left his home convinced that they shared a vision. It didn't turn out quite that way.\nToday, Kittlaus and Cheyer find themselves in a similar position: Do they sell to a giant or go at it alone? \n\"Our goal is ubiquity,\" Kittlaus said. \"There's no way to predict where that goes except to say we'll pick the path that gets us there. Either way, we will finish the job.\"\nelizabeth.dwoskin@washpost.com\n","647":"Who is winning the race for jobs between robots and humans? Last year, two leading economists described a future in which humans come out ahead. But now they've declared a different winner: the robots.\nThe industry most affected by automation is manufacturing. For every robot per thousand workers, up to six workers lost their jobs and wages fell by as much as three-fourths of a percent, according to a new paper by the economists, Daron Acemoglu of M.I.T. and Pascual Restrepo of Boston University. It appears to be the first study to quantify large, direct, negative effects of robots. \n  The paper is all the more significant because the researchers, whose work is highly regarded in their field, had been more sanguine about the effect of technology on jobs. In a paper last year, they said it was likely that increased automation would create new, better jobs, so employment and wages would eventually return to their previous levels. Just as cranes replaced dockworkers but created related jobs for engineers and financiers, the theory goes, new technology has created new jobs for software developers and data analysts.\u00a0\n  But that paper was a conceptual exercise. The new one uses real-world data -- and suggests a more pessimistic future. The researchers said they were surprised to see very little employment increase in other occupations to offset the job losses in manufacturing. That increase could still happen, they said, but for now there are large numbers of people out of work, with no clear path forward -- especially blue-collar men without college degrees.\n  ''The conclusion is that even if overall employment and wages recover, there will be losers in the process, and it's going to take a very long time for these communities to recover,'' Mr. Acemoglu said.\n  ''If you've worked in Detroit for 10 years, you don't have the skills to go into health care,'' he said. ''The market economy is not going to create the jobs by itself for these workers who are bearing the brunt of the change.''\n  The paper's evidence of job displacement from technology contrasts with a comment from the Treasury secretary, Steve Mnuchin, who said at an Axios event last week that artificial intelligence's displacement of human jobs was ''not even on our radar screen,'' and ''50 to 100 more years'' away. (Not all robots use artificial intelligence, but a panel of experts -- polled by the M.I.T. Initiative on the Digital Economy in reaction to Mr. Mnuchin's comments -- expressed the same broad concern of major job displacement.)\n  The paper also helps explain a mystery that has been puzzling economists: why, if machines are replacing human workers, productivity hasn't been increasing. In manufacturing, productivity has been increasing more than elsewhere -- and now we see evidence of it in the employment data, too.\n  The study analyzed the effect of industrial robots in local labor markets in the United States. Robots are to blame for up to 670,000 lost manufacturing jobs between 1990 and 2007, it concluded, and that number will rise because industrial robots are expected to quadruple.\n  The paper adds to the evidence that automation, more than other factors like trade and offshoring that President Trump campaigned on, has been the bigger long-term threat to blue-collar jobs. The researchers said the findings -- ''large and robust negative effects of robots on employment and wages'' -- remained strong even after controlling for imports, offshoring, software that displaces jobs, worker demographics and the type of industry.\n  Robots affected both men's and women's jobs, the researchers found, but the effect on male employment was up to twice as big. The data doesn't explain why, but Mr. Acemoglu had a guess: Women are more willing than men to take a pay cut to work in a lower-status field.\n  The economists looked at the effect of robots on local economies and also more broadly. In an isolated area, each robot per thousand workers decreased employment by 6.2 workers and wages by 0.7 percent. But nationally, the effects were smaller, because jobs were created in other places.\n  Take Detroit, home to automakers, the biggest users of industrial robots. Employment was greatly affected. If automakers can charge less for cars because they employ fewer people, employment might increase elsewhere in the country, like at steel makers or taxi operators. Meanwhile, the people in Detroit will probably spend less at stores. Including these factors, each robot per thousand workers decreased employment by three workers and wages by 0.25 percent.\n  The findings fuel the debate about whether technology will help people do their jobs more efficiently and create new ones, as it has in the past, or eventually displace humans.\n  David Autor, a collaborator of Mr. Acemoglu's at M.I.T., has argued that machines will complement instead of replace humans, and cannot replicate human traits like common sense and empathy. ''I don't think that this paper is the last word on its subject, but it's an exceedingly carefully constructed and thought-provoking first word,'' he said.\n  Mr. Restrepo said the problem might be that the new jobs created by technology are not in the places that are losing jobs, like the Rust Belt. ''I still believe there will be jobs in the years to come, though probably not as many as we have today,'' he said. ''But the data have made me worried about the communities directly exposed to robots.''\n  In addition to cars, industrial robots are used most in the manufacturing of electronics, metal products, plastics and chemicals. They do not require humans to operate, and do various tasks like welding, painting and packaging. From 1993 to 2007, the United States added one new industrial robot for every thousand workers -- mostly in the Midwest, South and East -- and Western Europe added 1.6.\n  The study, a National Bureau of Economic Research working paper published Monday, used data on the number of robots from the International Federation of Robotics (there is no consistent data on the monetary value of the robots in use.) It analyzed the effect of robots on employment and wages in commuting zones, a way to measure local economies.\n  The next question is whether the coming wave of technologies -- like machine learning, drones and driverless cars -- will have similar effects, but on many more people.\n  The Upshot provides news, analysis and graphics about politics, policy and everyday life. Follow us on Facebook and Twitter. Sign up for our newsletter.\n\n\n\n","648":"''Person of Interest'' has no superheroes, no zombies, no ninjas, no direwolves. But as much as any show on television, it makes the high-low connection that's at the heart of genre and comic-book storytelling.\u00a0\nThe obvious appeal of this CBS series, which returns for an abbreviated fifth and final season on Tuesday, is pulpy and visceral. Its central cast includes three wisecracking assassins with good looks and hearts of gold, and the spiraling, frantic story lines allow them and their adversaries to run up prodigious body counts. (Secondary characters on ''Person of Interest'' almost always come with expiration dates.) \n  But the show's premise, combining fear of sentient computers with fear of unfettered post-9\/11 government surveillance, has been sufficiently sturdy and coherent to provide ballast for the comic-book action. It makes you think, or think you're thinking, while you enjoy the gunplay and slightly dopey humor, and it makes you care more than you expect to about the fates of the show's heroic, heavily outnumbered band of hackers, black-ops agents and cops.\n  In the new season the series pushes that conceptual framework hard. Finch (Michael Emerson), the owlish genius who created the crime-predicting artificial intelligence program called the Machine, wonders whether it's time to pull the plug: Is the existence of a program that could exterminate us ''an existential risk the world can't afford''? (Two programs, actually, given that the Machine's main antagonist since Season 3 has been another artificial intelligence, named Samaritan.) The answer, as fans of Finch will immediately predict, lies in the Machine's close connection to its benevolent maker.\n  At the end of Season 4 (a full year ago), the writers killed off or sidelined most of the adversaries of Finch's group, leaving the corporate-backed Samaritan and its shadowy minders as the only significant enemy. In the early going of Season 5 (four episodes were available to critics) the show spins its wheels a little.\n  One of the clever things the show's creator, Jonathan Nolan, did was to encapsulate the creative tension between self-contained, crime-solving episodes (a CBS hallmark) and a long-range conspiracy story by having the Machine spit out ''numbers'' that led the heroes to individual, weekly cases. With 13 episodes to go, the producers appear to be leery both of spending time on cases and of setting up their climactic showdown too soon, and the plots have a neither-here-nor-there quality. In the fourth week, however, the reintroduction of some favorite characters begins to bring the story into focus.\n  The most crucial element of the show's success remains the same: Mr. Emerson's tremendously appealing, oddly moving performance as Finch, the programming genius who is uncomfortable in every situation but always rises to the occasion. As the new season opens Mr. Emerson has to play a number of very emotional scenes with a computer monitor, carrying on passionate conversations with the blinking cursor. Not one moment of this will tempt you to laugh (in fact, you might have to blink back a tear), and that's a bit of magic as impressive as any string of computer code.\n\n\n\n","649":"FOR more than a quarter-century, Steven Spielberg has been at the pinnacle of American filmmaking, the most famous, influential and consistently successful director in Hollywood. Beginning as a shaggy-haired wunderkind in the early 1970's, with a cadre of similar film school dreamers, he revolutionized movie-making and revitalized the classic Hollywood genres. He grew into a director who alternated between exploiting the visceral pleasures of pure, hyper-charged cinema in films like \"Jurassic Park\" and the \"Indiana Jones\" series and one who exploded the boundaries of the mainstream with bolder fare like \"Schindler's List\" and \"Saving Private Ryan.\"\n     Now, after a self-imposed three-year hiatus, he is back -- having taken the baton from one of his heroes, Stanley Kubrick, on \"A.I. Artificial Intelligence,\" a project that Kubrick initiated in the mid-80's, that the two directors briefly worked on together in the early 90's and that Mr. Spielberg inherited when Kubrick died in 1999. What is most surprising about the result is that while it plies the same territory as some of Mr. Spielberg's work of two decades ago, a world of benevolent science fiction as seen through the sensibilities of a child, this time he has married it to the darker, emotional currents of his later, adult films. No doubt partly under the influence of his long collaboration with Kubrick on this project, his latest answers are not so simple. \u00a0\n In \"A.I.,\" death is real, the world is cruel, innocents suffer wounds that can never fully heal, fairy tales come with their edges intact and magical resurrections are possible only at a bitter and tragic cost. And \"Minority Report,\" another science-fiction film he is about to finish shooting, may well carry him further in that darker direction.\n\"I have made a lot of the films that I wanted to make over the last 25 years,\" Mr. Spielberg said. \"But the thing that I've allowed myself is to let time change my mind about the kind of films I always thought I wanted to make. I've left myself open to how my tastes have changed with my ever-changing life. And with such a large family, my tastes have changed. I am particularly, at this point in my life, somewhat interested in going back a little bit to my past, to tell the kinds of stories I'd like my kids to see. You know, between 'Schindler' and 'Ryan,' I had a lot of complaints from my kids that I'm not making movies for them anymore.\"\n\"A.I.\" may be an answer to that plea, but if so, it is strong medicine. On a surface level it resembles earlier Spielberg films. It is difficult, for instance, to resist comparisons to another film with initials for a title, \"E.T.\" (1982), since both films have a child protagonist and take place in a world of science-fiction, fantasy and wonderment. But it is the chasm between the two films that is most striking. Imagine if the scientists in \"E.T.\" had actually captured the alien and dissected him, or that there was no miraculous recovery from his mysterious ailment, and you will not be far off. In \"A.I.,\" Mr. Spielberg is clearly trying to push the audience, even a younger audience, into confronting harder truths and deeper emotions.\n\"A.I.\" is set in the distant future, when global warming has flooded coastal cities and the remnants of society are waited upon by convincingly human robots who provide such services as housecleaning, child care and, with astonishing explicitness for a Spielberg movie, sexual satisfaction. (In a supporting role, Jude Law plays a robot called Gigolo Joe, who is seen meeting lonely women in seedy hotel rooms and seducing them with boasts of his sexual prowess.) It is about what happens when scientists build the first child robot capable of love, to ease the loneliness of childless parents. What might happen, Mr. Spielberg asks, if this loving, synthetic creature, played in the film by Haley Joel Osment, is not loved in return?\n\"I think 'A.I.' is a film for my own kids, approached entirely differently from my earlier films and requiring a lot more tolerance on their part, to get through some of the stuff that might be a little beyond their reach, to be able to understand some of the things that come later and will be more viscerally interesting to them,\" Mr. Spielberg said.\nIt is not, he said, a film that he would have dared to make two decades ago.\n\"I would have played it safer than I think I did with 'A.I,' \" he said. \"I don't think 'A.I.' is a safe story, at all. I think it is a solid story, and I think it is an emotional story, but it is also a divergent story that takes you in several directions at the same time and then asks you to bring it all back at the end of a very emotional odyssey to understand what you have seen.\"\nMR. SPIELBERG, 53, was working his way through a pair of Subway turkey sandwiches on a round wooden table set up outside his trailer on the 20th Century Fox lot. He had only just completed the final dubbing and color-correcting on \"A.I.,\" which was to have its first pre-release screenings in the next few days, and was within two weeks of finishing a follow-up film called \"Minority Report.\" He said he had read an article about people who lost weight on a diet of Subway sandwiches. \"I thought if I ate them at just one meal a day, I'd lose the last four pounds that I need to get rid of,\" the rail-thin director said with a grin.\nThe release of Mr. Spielberg's first film since \"Saving Private Ryan,\" which came out during the summer of 1998, seemed a fitting occasion to talk to him about where he is these days, both as an artist and as a kind of avatar for the state of mainstream filmmaking. It would also provide an opportunity to assess his views on the quarter century of filmmaking that he has dominated -- and on the lasting achievements, if any, of the generation of directors who came of age with him during the fertile 70's.\n\"I don't remember any of us, back then, saying to each other, 'O.K., let's swarm the gates and take over Hollywood,' \" Mr. Spielberg said. \"I was much more selfish than that. When I was first starting out, I was more concerned about getting work, about being able to tell my own stories. If I worked in the mainstream, it was because I felt that was the more logical path for me than going in search of independent East Coast financing, or something. Remember, I was reared in the mainstream. I was hanging out at Universal Studios beginning with my last year in high school and through the first three years of college. So I never saw myself as saving Hollywood. I, frankly, didn't know if it needed saving. I was more concentrated on getting somebody to say yes to the movies I wanted to make.\"\nAs a result, Mr. Spielberg said, he has a myopic view of the so-called golden age of 70's filmmaking. \"I kind of leave it to the guys who write the book about our generation to inform me of what the heck we were doing back then,\" he said. \"And at the same time, a lot of the books that have been written about that period, I disagree with, because I don't remember it happening that way. I just remember that there were a handful of watershed films in the late 60's, beginning with Francis Coppola's 'You're a Big Boy Now,' and then Dennis Hopper's 'Easy Rider' and George Lucas's 'THX 1138,' that led me to believe that there was hope I'd get a job. All of a sudden, having long hair wasn't a liability. You got the feeling that the word went down to the guards at the studio gates that if any of those long-haired hippie kids try to get into the studio, they were to start letting them in. I think we can do well with them. I think we can make a lot of money with those guys.\"\nWhile it may seem, in retrospect, that the generation that brought filmmakers like Mr. Spielberg, Mr. Coppola, Mr. Lucas, Brian De Palma, Martin Scorsese, Terrence Malick and others to the forefront was almost a conspiracy to seize the mainstream, that was not the case, Mr. Spielberg said.\nFor one thing, Mr. Spielberg said, he and the other young filmmakers of the 70's were quite different from one another. \"When I was making 'Jaws,' Marty knew that that was something he would never want to make. And when he was making 'Taxi Driver,' I knew -- at that time, anyway -- that it was a genre I never wanted to be involved in, because it was too violent and too realistic and too yucky. Yet I thought 'Taxi Driver' was one of the great films when it was all put together.\"\nNor, Mr. Spielberg said, does he ever consider the wider cultural ramifications of the films he chooses to make. While he certainly hoped that \"Schindler's List\" would heighten awareness of the Holocaust, that was not the reason he made the movie. And while it seems clear that \"Saving Private Ryan\" sounded one of the loudest fanfares for the Greatest Generation cycle that the culture is now going through, he was not playing at social reform when he made the film.\n\"When I make a picture, I'm not trying to change the culture,\" he said. \"I'm not trying to change the way things are. I'm trying to tell a story. I know it sounds simplistic, but I'm simply trying to get a story down on film that makes sense and is sometimes satisfying and is sometimes controversial. But I am never trying to say that I am doing this because I can't wait to see what the repercussions are going to be socially.\"\nHe is essentially, he said, too unsure of himself, even after all of his successes, to plot in such grand terms.\n\"I'm the fraidy-cat who makes a picture and immediately assumes that nobody is going to show up the first day, and it will be reviled around the world,\" Mr. Spielberg said. \"That's the way I've been on every single project. Every one. When it doesn't turn out that way, I'm relieved. Relief is the largest reaction I have to a film that's well received and opens well. I don't celebrate. I don't have victory parties. I simply feel relief.\"\nMr. Spielberg leaned back and let the sun play on his face. His little sanctuary was in an alcove formed by a locked entrance to one of the sound stages where he has been shooting \"Minority Report.\" \nMr. Spielberg has always been secretive about his projects and the current one, \"Minority Report,\" is no exception. But as it is based on a science-fiction story by the frequently cynical Philip K. Dick about a future society in which federal agents are allowed to kill potential murderers before they can commit their crime, a kind of preemptive death penalty, the indications are that Mr. Spielberg will once again be exploring a new-found ominousness. The movie is set for release next June.\nMr. Spielberg's sanctuary was blocked from the view of passersby by a line of trailers where he and the film's stars -- Tom Cruise, Samantha Morton, Colin Farrell -- had their dressing rooms and resting areas. Only through a narrow gap could those walking past peer inside, and at one point another cast member, the Swedish actor Max von Sydow, wandered by, paused, peered in and then moved on.\n\"There have always been films of mine that people didn't understand in the way that I intended,\" Mr. Spielberg said. \"But I don't ever talk about that or go into detail, because I don't want to sound like I'm bellyaching.\"\nHe is not referring, Mr. Spielberg hastened to add, to either \"Schindler's List\" or \"Saving Private Ryan.\" What then? Perhaps his sentimental adaptation of \"The Color Purple\" in 1985?\nMr. Spielberg sighed. \"Well, 'Color Purple' is a good example,\" he said. \"That was a film where there was more ink shed on the comparison with the Alice Walker novel than there was on the merits or problems with the film. In fact, the film was not analyzed. Instead, a mirror of Alice Walker was held up to it.\"\nIt taught him a lesson, he said, one that came to mind last year when he was considering which project to choose to end his three-year hiatus, and briefly thought of directing the adaptation of \"Harry Potter and the Sorcerer's Stone\" for Warner Brothers.\n\"Every kid in the world will be waiting to see how close 'Harry Potter' will be to the books,\" he said. \"And if they make it religiously and everybody gives it up to the book, there will be tremendous satisfaction.\" But this leaves little for a creative artist to do who wants to tell stories in his own way, free of the tyranny of some sacred text.\nAlthough Mr. Spielberg was reluctant to assess the impact that his generation of filmmakers has had, he has very definite ideas of where movies have come since the 70's -- and what have been the most important technical and creative innovations that have changed them.\n\"To me, it's sound,\" Mr. Spielberg said. \"To me, the biggest breakthrough in the last 50 years, certainly since Cinerama in the 50's, is the sound. When I think of the sonic experience of the first time I heard Dolby stereo demonstrated at the American Film Institute years and years ago, it was then that I realized that we are making sound pictures. We could do a lot with sound, we could even tell a story with sound. And when we brought the best of sound together with the best of visuals in the 80's and 90's, you were able to show audiences so much more of the movie than they otherwise would have experienced.\"\nEven though his \"Close Encounters of the Third Kind\" (1977) was going to be released only on a handful of 70-millimeter prints, Mr. Spielberg said he spent 17 weeks on the sound for it, almost as long as he spent shooting the film. Most of the country, at the time, saw the film with a monaural soundtrack. Only later, with videocassettes and now, with the release of the DVD of the film, can most people hear what he created. And next year, he said, he intends to release a DVD version of \"E.T.\" that will include, for the first time, a Digital Theater Sound, or DTS soundtrack. The emotional experience of seeing it with that enhanced sound, Mr. Spielberg said, will be qualitatively different.\n\"You have to be careful, though, that you're not using sound in a way that draws attention away from the screen,\" he said. \"That's one of my pet peeves. Sometimes, a movie's sound design pulls your attention to a part of the theater that is beyond the screen, to the left or right or, worse yet, behind. That's just using the technology to show off.\"\nIndeed, another less-welcome trend in recent decades has been what he considers the improper use of technological breakthroughs -- like sound, or like computer effects -- to bludgeon the audience or otherwise distort the stories being told. \n\"I remember, back in the 70's, they had something called Sensurround,\" he said. \"Basically, they were speakers about the size of juke boxes. When they were first used, in the movie 'Earthquake,' they shook the theaters so hard you thought the rococo was going to come down around your ears.\"\nToo often, he said, the first wave of filmmakers given the use of new high-tech gizmos will overuse them, like children playing with a new toy. \"These are all tools to be folded in,\" Mr. Spielberg said. \"They are not the main ingredient. They are accent tools.\"\nAnother change in the last couple of decades has been the accelerated pace of movie storytelling, especially in adventures aimed at young audiences. It was Mr. Spielberg and his producer, Mr. Lucas, who decided when making \"Raiders of the Lost Ark\" (1981) that they would cut the action sequences into pieces as short as they could while still remaining coherent -- and then chop out one extra frame from each snippet.\nHE believed then, he said, and he believes now that younger audiences can assimilate information much more rapidly than people in their 40's or older. The advent of video games, channel surfing and the Internet have created a generation of people who are able to understand and act upon information more rapidly than previous generations. Action sequences in some films that seem dizzyingly fast to older viewers make perfect sense to younger ones and, in fact, better mirror the way they perceive the world.\nThis has led to a kind of film, especially during the summer months, that is designed to be a sensory experience almost physical in its pace and vividness and impact, the so-called \"roller-coaster movie,\" of which even Mr. Spielberg has made a few (the \"Indiana Jones\" and \"Jurassic Park\" series). \n\"I happen to like roller-coaster movies,\" he said. \"There's nothing wrong with being a roller-coaster movie, if that's all you're trying to be. Sometimes, though, it almost seems like the movie is on speed, if you will. It takes you through things so quickly that, like a roller-coaster, all you ever see to both sides is blur. You look ahead to the next twist and the next turn and the next tunnel, but when you come out of it, you feel like you've been in a kind of tunnel-vision experience. It's like you didn't have time to savor the experience as it was happening.\"\nMr. Spielberg's evolution from his more innocent past to a grimmer future was aided immeasurably by his collaboration, actual and posthumous, with Stanley Kubrick. Although Kubrick came from the previous generation of filmmakers, he and Mr. Spielberg were friends for the last 18 years of Kubrick's life, Mr. Spielberg said. They were simply drawn together by a natural curiosity to get to know each nother, although their relationship deepened into a very close if often long-distance one (Kubrick never strayed far from his London home). They kept up an almost constant correspondence and telephone relationship, peppered with occasional face-to-face visits, right up until Kubrick's death.\nMr. Spielberg said he considered Kubrick the supreme film artist of the last several decades. Indeed, when he agreed to pay for a new, high-tech sound stage at the University of Southern California film school earlier this year, Mr. Spielberg insisted that it be named for Kubrick (George Lucas endowed an adjacent stage, which he named for one of his heroes, the Japanese director Akira Kurosawa; students at the school refer to the stages as K1 and K2). \n\"Kubrick couldn't have made 'A.I.' in the 80's, when he first told me about the story,\" Mr. Spielberg said. \"He sent me his first treatment then, and I said, 'How are you going to do some of this stuff?' And he said, 'I don't know if we can yet. But we will be able to, soon.' Now, he's right, anything is possible. With the computer, you can do anything, you can show anything. The only limit is your own imagination.\"\nIn 1994, Mr. Spielberg said, Kubrick asked him to consider directing \"A.I.,\" based on a 90-page treatment Kubrick had written with Ian Watson, based on a short story by Brian Aldiss.\n\"I agreed to do it and knew that the next step was that we would write the screenplay together,\" Mr. Spielberg said. \"Stanley wanted a screenplay written in England, which meant Stanley would have about 99 percent control of the screenplay and I would have been happy to shoot whatever Stanley gave me. And I would have, until I realized this was an idea and a subject that really was something Stanley needed to see through to the end. So diplomatically and very carefully, I extracted myself from the project about two months after he got me to say yes to directing it.\"\nNot until later, after Kubrick had died in 1999, did Mr. Spielberg entertain the idea of taking that 90-page treatment and writing his own screenplay, one that he would direct, in part, as a tribute to Kubrick and that would be released in 2001, the year Kubrick had made famous.\n\"I didn't pass it on to another writer because I thought all the work that Stanley had done to download the story into me in 1994, all the things he wanted to express with this story, would not stand the test of time and would lose generations of emotional power if I tried to pass them on to yet another writer,\" Mr. Spielberg said. \"I felt it was much better, win or lose, to attempt to write the screenplay myself.\"\nMr. Spielberg was also attracted to the idea of making a more complex and emotionally rich film for children, especially his own. He has seven children. The first, Max, was born in 1985, during his marriage to the actress Amy Irving, which ended in divorce in 1989. The youngest, Destry, 4, was born to his current wife of 10 years, the actress Kate Capshaw.\n\"A.I.\" marks the first screenplay to bear Mr. Spielberg's name since \"Poltergeist\" in 1982, and the first he has also directed since \"Close Encounters\" five years earlier.\nAn assistant wearing a headset and microphone appeared suddenly and said that Mr. Spielberg was needed. The crew and the actors were ready to begin the afternoon's shooting. \"Is Tom there?\" he asked. The assistant nodded, and Mr. Spielberg rose. After finishing \"Minority Report\" on July 3, he said, he will take a little time off, though perhaps he will begin shooting another film before the year is out. He needs a rest, but he still feels like working.\nWhat will he make? \"I have three irons in the fire,\" he said. \nThe first is his long-delayed adaptation of the Arthur Golden novel, \"Memoirs of a Geisha,\" which Mr. Spielberg said he would still probably direct. He is also considering a screen adaptation of Daniel Wallace's bittersweet novel about a father and son called \"Big Fish.\" And Mr. Spielberg has bought the rights to a forthcoming book by Doris Kearns Goodwin about the last six years in Abraham Lincoln's life.\nFirst, though, he is sweating out the release of \"A.I.\" on Friday -- convinced, as always, that it will be a disaster. Does he worry, in his secret heart, that if it fails he won't get another turn at bat?\n\"Oh,\" Mr. Spielberg said, smiling. \"I know that I've reached a point where I'll get another turn at bat. I know I can strike out a whole bunch of times in a row and I'll always get another turn at bat. That's not really what worries me. It's not communicating the ideas. It bothers me when people don't get it. Even more than people not going, it's not getting it, that's what I'm afraid of.\"\n","651":"The article \"Dairy farm meets tech revolution\" [Economy & Business, April 6] did not address the reality for farmworkers. In recent years, dairy farms have consolidated into large operations. About one-half of dairies have 1,000 or more milking cows. These larger farms, even when using robots and artificial intelligence, need labor, and they increasingly hire immigrants. \u00a0\nConditions for workers on many dairy farms are poor and often dangerous. In 2014 there were 49 reported fatalities in dairy cattle and milk production. Occupational death and injury rates in agriculture, livestock production and dairies are disproportionately high compared with other sectors. Yet federal employment laws often don't apply to dairy farms. Even large farms are exempt from paying overtime and thus have little downside to requiring 70, 80 or 90 hours in a workweek, which can be dangerous and interfere with family life. Most federal occupational safety standards don't apply to farms, and the few that do cannot be enforced in agriculture unless a farm operates a labor housing camp or employs at least 11 workers. \nDairy workers have been pressing for changes in labor practices and employment laws, as well as seeking collaborations with the dairy industry to improve wages and working conditions. Robots and artificial intelligence can reduce the need for labor and improve efficiency on dairy farms, but there is still a need for improvements when it comes to dairy farmworkers.\nBruce Goldstein, Washington\nThe writer is president of Farmworker Justice.\n","652":"This week's challenges were suggested by the Japanese math teacher and puzzle creator, Tetsuya Miyamoto, inventor of the popular KenKen math and logic puzzle. KenKen (\"wisdom squared\" in Japanese) was developed by Mr. Miyamoto in 2004 as an example of his educational philosophy \"The Art of Teaching Without Teaching.\" I asked him about this philosophy and received this response by email:\n Children will improve their \"power to think\" through trial and error, working entirely by themselves as best they can using their full brain without help from others. Giving hints, particularly in solving math problems, actually deprives children of the chance to think by themselves. \u00a0\n If a person runs one mile, the person may not think that he\/she is able to run a full marathon. But if the person continues to run one mile today, five miles next week, 10 miles next month, 15 miles next year, then the person can visualize themselves running a full marathon.  \n The same applies to solving KenKen puzzles - starting with a 3 x 3 puzzle, and gradually increasing their levels to be able to solve an 8 x 8 will give them an idea of being able to solve 9 x 9 puzzles while building a spirit of challenge, a feeling of accomplishment and self-confidence. \n Speed is not important to solving KenKen. Continuing to think to find the solution all by themselves no matter how long it takes is important. That gives a general idea in life how to achieve higher hurdles step by step. \nIf you're not familiar with KenKen, check out this short introduction by Will Shortz.\nKenKen is a regular feature in The New York Times: You can get a daily dose here. KenKen puzzles like these are generated by the Kenerator, a high-level artificial intelligence program created in collaboration with the chess master and artificial intelligence researcher David Levy.\nOur Numberplay challenges this week are a bit different in that they were created by hand by Mr. Miyamoto. I asked Mr. Miyamoto about the difference between a Kenerated puzzle and one he makes himself. He replied:\n Handmade ones always have a story to solve and can have unique shape cages. Even if a computer can write a great essay, a computer cannot write Shakespeare. Same thing for puzzles. \nLet's give these man-made KenKen puzzles a try. The first below is a standard 6 x 6 with operations. The next is an 8 x 8 that uses multiplication and division only. (If a cage has two squares, the operator could be either multiplication or division - you have to figure it out - and will be multiplication otherwise.) The final is a 9 x 9 that uses addition and subtraction only. (If a cage has two squares, the operator could be either addition or subtraction, and will be addition otherwise.) Enjoy!\nThat concludes this week's challenge. As always, once you're able to read comments for this post, use Gary Hewitt's  to correctly view formulas and graphics. And send your favorite puzzles to gary.antonick@NYTimes.com\nKenKen is a registered trademark of Nextoy L.L.C. Puzzle content copyright 2014 Tetsuya Miyamoto \/ KenKen Puzzle L.L.C. All rights reserved.\n Solution \nCheck back here on Friday for the solutions.\n\n","653":"Imagine taking a college exam, and, instead of handing in a blue book and getting a grade from a professor a few weeks later, clicking the ''send'' button when you are done and receiving a grade back instantly, your essay scored by a software program.\nAnd then, instead of being done with that exam, imagine that the system would immediately let you rewrite the test to try to improve your grade.\nEdX, the nonprofit enterprise founded by Harvard and the Massachusetts Institute of Technology to offer courses on the Internet, has just introduced such a system and will make its automated software available free on the Web to any institution that wants to use it. The software uses artificial intelligence to grade student essays and short written answers, freeing professors for other tasks.\nThe new service will bring the educational consortium into a growing conflict over the role of automation in education. Although automated grading systems for multiple-choice and true-false tests are now widespread, the use of artificial intelligence technology to grade essay answers has not yet received widespread endorsement by educators and has many critics.\u00a0\nAnant Agarwal, an electrical engineer who is president of EdX, predicted that the instant-grading software would be a useful pedagogical tool, enabling students to take tests and write essays over and over and improve the quality of their answers. He said the technology would offer distinct advantages over the traditional classroom system, where students often wait days or weeks for grades.\n''There is a huge value in learning with instant feedback,'' Dr. Agarwal said. ''Students are telling us they learn much better with instant feedback.''\nBut skeptics say the automated system is no match for live teachers. One longtime critic, Les Perelman, has drawn national attention several times for putting together nonsense essays that have fooled software grading programs into giving high marks. He has also been highly critical of studies that purport to show that the software compares well to human graders.\n''My first and greatest objection to the research is that they did not have any valid statistical test comparing the software directly to human graders,'' said Mr. Perelman, a retired director of writing and a current researcher at M.I.T.\nHe is among a group of educators who last month began circulating a petition opposing automated assessment software. The group, which calls itself Professionals Against Machine Scoring of Student Essays in High-Stakes Assessment, has collected nearly 2,000 signatures, including some from luminaries like Noam Chomsky.\n''Let's face the realities of automatic essay scoring,'' the group's statement reads in part. ''Computers cannot 'read.' They cannot measure the essentials of effective written communication: accuracy, reasoning, adequacy of evidence, good sense, ethical stance, convincing argument, meaningful organization, clarity, and veracity, among others.''\nBut EdX expects its software to be adopted widely by schools and universities. EdX offers free online classes from Harvard, M.I.T. and the University of California, Berkeley; this fall, it will add classes from Wellesley, Georgetown and the University of Texas. In all, 12 universities participate in EdX, which offers certificates for course completion and has said that it plans to continue to expand next year, including adding international schools.\nThe EdX assessment tool requires human teachers, or graders, to first grade 100 essays or essay questions. The system then uses a variety of machine-learning techniques to train itself to be able to grade any number of essays or answers automatically and almost instantaneously.\nThe software will assign a grade depending on the scoring system created by the teacher, whether it is a letter grade or numerical rank. It will also provide general feedback, like telling a student whether an answer was on topic or not.\nDr. Agarwal said he believed that the software was nearing the capability of human grading.\n''This is machine learning and there is a long way to go, but it's good enough and the upside is huge,'' he said. ''We found that the quality of the grading is similar to the variation you find from instructor to instructor.''\nEdX is not the first to use automated assessment technology, which dates to early mainframe computers in the 1960s. There is now a range of companies offering commercial programs to grade written test answers, and four states -- Louisiana, North Dakota, Utah and West Virginia -- are using some form of the technology in secondary schools. A fifth, Indiana, has experimented with it. In some cases the software is used as a ''second reader,'' to check the reliability of the human graders.\nBut the growing influence of the EdX consortium to set standards is likely to give the technology a boost. On Tuesday, Stanford announced that it would work with EdX to develop a joint educational system that will incorporate the automated assessment technology.\nTwo start-ups, Coursera and Udacity, recently founded by Stanford faculty members to create ''massive open online courses,'' or MOOCs, are also committed to automated assessment systems because of the value of instant feedback.\n''It allows students to get immediate feedback on their work, so that learning turns into a game, with students naturally gravitating toward resubmitting the work until they get it right,'' said Daphne Koller, a computer scientist and a founder of Coursera.\nLast year the Hewlett Foundation, a grant-making organization set up by one of the Hewlett-Packard founders and his wife, sponsored two $100,000 prizes aimed at improving software that grades essays and short answers. More than 150 teams entered each category. A winner of one of the Hewlett contests, Vik Paruchuri, was hired by EdX to help design its assessment software.\n''One of our focuses is to help kids learn how to think critically,'' said Victor Vuchic, a program officer at the Hewlett Foundation. ''It's probably impossible to do that with multiple-choice tests. The challenge is that this requires human graders, and so they cost a lot more and they take a lot more time.''\nMark D. Shermis, a professor at the University of Akron in Ohio, supervised the Hewlett Foundation's contest on automated essay scoring and wrote a paper about the experiment. In his view, the technology -- though imperfect -- has a place in educational settings.\nWith increasingly large classes, it is impossible for most teachers to give students meaningful feedback on writing assignments, he said. Plus, he noted, critics of the technology have tended to come from the nation's best universities, where the level of pedagogy is much better than at most schools.\n''Often they come from very prestigious institutions where, in fact, they do a much better job of providing feedback than a machine ever could,'' Dr. Shermis said. ''There seems to be a lack of appreciation of what is actually going on in the real world.''\n","655":"Self-driving cars. They're the future of transportation - and they're getting smarter all the time. Thanks to advances in software and artificial intelligence, these machines are now able to distinguish between cars and cyclists, or between pedestrians and your pet. Many can now \"see\" just like you can, picking out objects and obstacles approaching ahead. All that tech could eventually save lives, helping to prevent the 95 percent of car accidents that safety regulators estimate are caused by human error each year.\nBut none of this would be possible without a piece of hardware many of us take for granted in our own home computers. It's a technology that traces back to the earliest days of modern personal computing, one that people tend to associate more with \"World of Warcraft\" than newfangled widgets on wheels.\u00a0\nWe're talking about the graphics processor.\nIn mainstream PCs, the graphics processor - often found on a graphics card - is what allows computers to draw all those pixels and polygons that make up today's photorealistic video games. But as these processors have grown ever more powerful, engineers have discovered their utility in all sorts of nongaming applications. Graphics processing units - or GPUs - have transcended their origins to become entire computers in their own right.\n\"[The GPU] is now powering everything from games to the visual effects you see in Hollywood films,\" said Danny Shapiro, the senior director of automotive at Nvidia, which accounts for roughly 75 percent of the $7.8 billion market for GPUs. GPUs, said Shapiro, are central to \"professional graphics, for automakers that are designing cars, to doctors and researchers that are searching for cures for cancer and using medical imaging techniques.\"\nIt's a sign of how big the GPU business has grown that some 200 other companies work with Nvidia's automotive unit alone. GPUs are even part of the brains behind artificial intelligence, appearing in technologies like the Amazon Echo, which converts natural human speech into data that machines can understand.\n(Amazon chief executive Jeffrey P. Bezos also owns The Washington Post.)\n\"The combination of GPUs and a CPU are now available that can accelerate analytics, deep learning, high-performance computing, and scientific simulations,\" Chris Niven, research director for oil and gas issues at the research firm IDC, told ZDNet last month.\nTo understand why GPUs have become so prevalent in next-generation technologies, we have to talk about how they work.\nTraditionally, the brain in most PCs has been the CPU, or the central processing unit. These chips are made by companies such as Intel. Apple has also been making its own, proprietary chips for the iPad and iPhone. The distinguishing feature of this technology is that it's designed to run calculations serially, one after another, very quickly. The rise of dual- and quad-core CPUs have expanded their capabilities, allowing for more computations to occur simultaneously.\nThese chips are still ideal for machines that only need to run a few processes at the same time. But when it comes to technology like self-driving cars, where the computers are constantly receiving and digesting information, multitasking becomes that much more important. And that's where GPUs excel.\nComputer researchers began to discover the potential behind GPUs as far back as the late 1990s, when the market was awash with dozens of competing chip makers. Their products found their way into desktop PCs and gaming consoles like the Sega Dreamcast and Xbox, enabling consumers to experience groundbreaking titles like \"Half-Life,\" \"Quake\" and \"Halo.\" By simultaneously and efficiently controlling the generation of shapes on a screen, GPUs helped bring first vector graphics, and then individual pixels, to life.\nBy the early 2000s, GPUs were being pit directly against CPUs in computing tests, with some results showing enormous promise for graphics processors.\n\"Researchers at universities realized that, 'Hey, here is this low-cost processor that we can apply to scientific and mathematical applications and get some acceleration for cheap,'\" said Jon Peddie, president of Jon Peddie Research, an industry analysis firm.\nOne paper in 2002 found that compared to CPUs, \"the graphics hardware allows us to establish a high-speed custom data processing pipeline. Once the pipeline is set up, data can be streamed through with devastating efficiency.\"\nThe best GPUs on the market today come with as many as 5,000 cores, said Peddie, not just two or four or eight as with CPUs. While CPUs can process smaller amounts of information very quickly, the advantage of GPUs has to do with scale - processing lots of information at the same time.\nThis is why self-driving cars find GPUs so useful. Through the use of optical cameras, laser and radar sensors, cars look at their surroundings by taking many measurements per second.\n\"It's 30 pictures every second,\" Shapiro said. \"Each picture, a single frame, is made up of pixels. Each of these pixels or dots is a numerical value that says, 'What is the color of the light there?' It's just a bunch of numbers.\"\nGPUs like the ones found in self-driving cars are designed to crunch those numbers and figure out that some of those pixels represent an obstacle, whereas other pixels are lane markings and still others are traffic lights. While GPUs weren't originally invented for those purposes, car engineers began taking advantage of the technology's parallel computing powers about six or seven years ago, according to Peddie.\n\"The original use of GPUs in an automobile was for the instrument panel in the entertainment system,\" he said. \"It's only been recently that people have been saying, 'Hey, we can do this, or that!'\"\nAs GPUs become even more powerful and gain even more features, you can expect them to crop up in even more places. Within automobiles alone, many stand-alone processors that used to handle just one function - such as the anti-lock brakes or the power windows - will all someday be routed through a single processor, the GPU, said Shapiro. And we'll see cars work increasingly like Tesla's automobiles, where you might customize your vehicle by picking and choosing different software packages to suit your driving style.\n\"You can almost have in-app purchases to add new features that weren't there when you bought it,\" he said.\nFor gamers who've grown accustomed to buying expansion packs to their software - also known as downloadable content, or DLC - this idea might sound very familiar.\n","656":"As the Voyager 2 spacecraft sails deeper into space and nears the planet Neptune, it is being monitored by a computer system that was built using a software development tool produced by Software Architecture and Engineering Inc. of Arlington.\nSoftware A&E's Knowledge Engineering System (KES) was used by engineers at the Jet Propulsion Laboratory of Pasadena, Calif., which is managing the Voyager project for the National Aeronautics and Space Administration, to develop a system that automates the monitoring of the spacecraft.\nVoyager 2, which was launched 12 years ago, has traveled 4.5 billion miles and is expected to rendezvous with Neptune about midnight Thursday.\u00a0\nThe system uses artificial intelligence to help ground-control workers make decisions that human controllers previously would have had to make on their own.\nSoftware A&E, a software engineering and systems consulting firm, was founded in 1978 by Joseph M. Fox, a former president of International Business Machines Corp.'s federal systems division. The firm develops software tools and techniques for government agencies and companies to improve productivity. The company has 70 employees and had revenues of $ 6.2 million last year.\nSoftware A&E's Knowledge Engineering System allowed the Jet Propulsion Laboratory to combine its knowledge of the Voyager 2 mission and spacecraft with artificial intelligence techniques developed by the Arlington consulting firm. The software development tool, which can be run on personal computers, was initially released in 1983 and has been frequently updated.\n\"KES made our prototyping efforts go much more quickly,\" said Ursula M. Schwuttke, a member of the technical staff of the Jet Propulsion Laboratory's flight command and data management systems section. \"It eliminated the need for us to develop our own AI [artificial intelligence] environment and allowed us to concentrate on the knowledge base.\"\nThe resulting \"expert system\" allows the Jet Propulsion Laboratory's mission operations personnel to collect data transmitted by Voyager 2, analyze it and make recommendations to correct any discrepancies. The system cannot transmit corrections on its own -- engineers still must make the final decisions and implement the changes.\nBefore the system was developed, engineers sat at a terminal and compared the incoming data line by line with a computer printout of data predictions generated by computer simulators. If they matched, the spacecraft was operating as expected; if not, there could be a problem.\n\"It basically does the same thing the human operator did,\" Schwuttke said, \"but we think it's a tool for enhancing efficiency and accuracy instead of replacing the human operator completely.\"\nRicki Kleist, Software A&E's KES marketing manager, said the software package, which sells for $ 4,000, gives users the ability to collect and process large amounts of data and to make the information useful.\n\"There are new ways of building software and KES is an example of that,\" she said. \"It allows you to more easily capture and maintain decision-making information ... Customs uses KES to analyze bank transaction reports to uncover drug-money laundering operations.\"\nBut Kleist said that only recently has the business community begun to embrace expert systems technology.\n\"For a long time, the only place you read about it was in artificial intelligence publications,\" she said, \"but today, because of the real-world applications, the business community is beginning to see the value of this technology.\"\n","657":"The San Francisco techno-elite and local activists have not been on good terms in recent weeks, but it may be wise to tread carefully when writing about the latest tensions. In a letter published in The Wall Street Journal on Saturday, Tom Perkins, a founder of the venture capital firm Kleiner Perkins Caufield & Byers, wondered if \"progressive radicalism\" indicated the possibility of a new Kristallnacht.\n\"Writing from the epicenter of progressive thought, San Francisco, I would call attention to the parallels of fascist Nazi Germany to its war on its 'one percent,' namely its Jews, to the progressive war on the American one percent, namely the 'rich,'\" Mr. Perkins wrote. \"This is a very dangerous drift in our American thinking. Kristallnacht was unthinkable in 1930; is its descendent 'progressive' radicalism unthinkable now?\"\nOn Saturday afternoon, Kleiner Perkins distanced itself from its founder, posting on Twitter, \"Tom Perkins has not been involved in KPCB in years. We were shocked by his views expressed today in the WSJ and do not agree.\" Many others also expressed outrage and confusion over Mr. Perkins's remarks, including those in the venture capital industry. \"I wish to express my extreme displeasure with Tom Perkins,\" Marc Andreessen, a co-founder of the venture capital firm Andreessen Horowitz, said on Twitter on Sunday.\u00a0\n A MOVE TO STIFLE PAYDAY LENDERS \u00a0|\u00a0 For years, banks both large and small have been collecting hefty fees by allowing Internet payday lenders direct access to customers' bank accounts. Now, the Justice Department may be cracking down on the practice, which allows these online merchants to drain consumers' checking accounts, Jessica Silver-Greenberg writes in DealBook.\nUnder the Bank Secrecy Act, banks are required to maintain internal checks against suspicious activity by customers and the companies their customers do business with, but until now, banks have largely escaped scrutiny for their role in providing financial services to these payment processors. With the new initiative, \"Operation Choke Point,\" the Justice Department hopes to provide more rigorous oversight on shady Internet merchants.\nBut the move has set off an argument between payday lenders, who often provide loans at extremely high interest rates, and the government. The short-term lenders contend that, through banks, their loans allow borrowers access to cash they would otherwise not have from traditional banking services, while the agency claims that the banks are allowing online merchants to prey on desperate customers.\nPRAISE AND OUTRAGE OVER DIMON'S PAY INCREASE \u00a0|\u00a0 Despite the legal troubles that JPMorgan Chase has faced in the last year, the bank decided to award Jamie Dimon, its chairman and chief executive, with a 74 percent raise, Peter Eavis writes in DealBook. Mr. Dimon will receive $20 million - $1.5 million of base salary and $18.5 million of JPMorgan stock - for 2013, compared with the $11.5 million he earned in 2012.\nSome on Wall Street are praising the raise. \"I think he's worth more than that,\" said Warren E. Buffett, the chief executive of Berkshire Hathaway and a JPMorgan shareholder. \"Over all, I think the shareholders of JPMorgan and the American people should be happy that Jamie Dimon has been running the bank over this period.\"\nBut others are lamenting Mr. Dimon's pay increase. \"Jamie Dimon's bonus represents another whale of a fail for JPMorgan Chase,\" writes Antony Currie of Reuters Breakingviews. The pay decision, he writes, \"reveals an oversight failure as big as Mr. Dimon's of his chief investment office.\"\nON THE AGENDA \u00a0|\u00a0 Caterpillar releases fourth-quarter earnings before the bell. Apple releases earnings after the market closes. New home sales numbers come out at 10 a.m. The American Securitization Forum continues in Las Vegas. Steven R. Swartz, the president and chief executive of the Hearst Corporation, is on Bloomberg TV at 9:15 a.m.\nGOOGLE BUYS INTO ARTIFICIAL INTELLIGENCE FOR $400 MILLION \u00a0|\u00a0 When Google announced two weeks ago that it had purchased Nest Labs, which makes Internet-connected home devices, for $3.2 billion, the company was taking a step further into people's personal lives. Now, it seems, Google has entered into the realm of science fiction. According to ReCode, Google is acquiring the artificial intelligence company DeepMind for $400 million. DeepMind, which is based in London, has only a home page for its website, adding to the company's intrigue.\nWhile not much is known about DeepMind except that its \"first commercial applications are in simulations, e-commerce and games,\" according to its website, the venture appears to be Google's latest move to \"know everything.\" Sources told ReCode that Google's purchase was largely geared toward acquiring artificial intelligence talent.\nLIBERTY GLOBAL TO ACQUIRE ZIGGO \u00a0|\u00a0 Liberty Global, the media company controlled by the billionaire John C. Malone, announced on Monday that it was buying Ziggo, the largest cable provider in the Netherlands, in a deal that valued Ziggo at about $13.7 billion, Chad Bray writes in DealBook. The deal comes only four months after Ziggo rebuffed an acquisition by Liberty Global, calling that proposed deal \"inadequate.\"\nThe transaction represents a 22 percent premium to Ziggo's closing price the day before it announced that it had received a preliminary takeover offer from Liberty Global in October. With the deal, Michael T. Fries, the chief executive of Liberty Global, said the companies' combined operations would reach over 90 percent of Dutch households.\n\u00a0\u00a0|\u00a0 Contact: @melbournecoal | E-mail\nSign up for the DealBook Newsletter, delivered every morning and afternoon.\nMergers & Acquisitions \u00bb\nMartin Marietta Materials in Merger Talks With Texas Industries \u00a0|\u00a0 A deal for Texas Industries, a construction supplies company with a market value of more than $2 billion, could be announced early this week, people briefed on the matter said. DealBook \u00bb\nEuropean Confidence to Inspire Merger Activity \u00a0|\u00a0 Bankers said in Davos, Switzerland, that they expected the European Central Bank's stress tests to bolster the confidence of the largest European banks and reinvigorate domestic and cross-border merger and acquisition activity, Reuters reports. REUTERS\nConsolidation in Solar Industry Could Accelerate \u00a0|\u00a0 Jifan Gao, chairman and founder of Trina Solar, said in Davos, Switzerland, that he expected the pace of mergers in the solar manufacturing industry to increase in the next three years, Bloomberg News reports. BLOOMBERG NEWS\nGerman Software Maker to Consider Acquisitions \u00a0|\u00a0 Werner Brandt, the chief financial officer of the business software maker SAP, is quoted as saying that the company could begin looking at large acquisition targets, Reuters writes. REUTERS\nINVESTMENT BANKING \u00bb\nHSBC Apologizes After Cash Withdrawal Issue in Britain \u00a0|\u00a0 Some customers were prevented from making large cash withdrawals after they refused to present evidence about why they needed the money. DealBook \u00bb\nFor Banking Group, Australian Open Becomes a Marketing Opportunity in China \u00a0|\u00a0 The Australia and New Zealand Banking Group, known as ANZ, is making big bets to court China, including Chinese-language signs and displays it had at the Australian Open tournament. DealBook \u00bb\n Comptroller Proposes Banks Reveal High Rollers \u00a0|\u00a0 Thomas P. DiNapoli, the New York State comptroller, has put together a proposal that asks Wells Fargo to take an inventory of their risk-taking employees, Gretchen Morgenson writes in the Fair Game column in The New York Times. NEW YORK TIMES\nCitigroup Bankers to Receive Half Their Bonuses in Cash \u00a0|\u00a0 Citigroup announced it would pay at least half of the bonuses for its European investment bankers in cash, The Financial Times reports. The move highlights the continued generosity of United States banks compared with their European counterparts. FINANCIAL TIMES\nGoldman's Cautious Approach to Hiring the Well-Connected \u00a0|\u00a0 Goldman Sachs's chief executive, Lloyd C. Blankfein, said Friday that the bank had a procedure to ensure that hiring of relatives of powerful people did not cross a line into becoming a bribe. DealBook \u00bb\nGoldman May Restrict Employees' Use of Chat Services \u00a0|\u00a0 The firm is considering a new policy to prohibit employees from using peer-to-peer chat services provided by Bloomberg L.P., Yahoo and other third-party companies, a person close to the situation said. DealBook \u00bb\nPRIVATE EQUITY \u00bb\nWarburg Pincus Hires Former Mining Chief at ArcelorMittal \u00a0|\u00a0 As part of a push into mining investments, the private equity firm Warburg Pincus has hired a former top executive in ArcelorMittal's mining operations, Peter Kukielski. DealBook \u00bb\nGores Group to Take Control of Britain's Premier Foods \u00a0|\u00a0 Premier Foods will pass into the hands of the Gores Group, a private equity firm based in Los Angeles, in a deal that gives Premier an enterprise value of 87.5 million pounds, The Financial Times reports. FINANCIAL TIMES\nJPMorgan and K.K.R.-Backed Payment Processor Settle Dispute \u00a0|\u00a0 Frank Bisignano, who was once an ally of Jamie Dimon, the chairman and chief executive of JPMorgan Chase, agreed to pay millions of dollar to the bank to resolve a hiring dispute resulting from Mr. Bisignano's move to a payment processor backed by Kohlberg Kravis Roberts, The Wall Street Journal writes. WALL STREET JOURNAL\nHEDGE FUNDS \u00bb\nA Spike in Options Trading Before Herbalife's Stock Fell \u00a0|\u00a0 Trading volume in Herbalife put options - financial contracts that gain in value as a particular stock price declines - spiked late last week. Even for a volatile stock like Herbalife, it was enough to make some market experts take notice. DealBook \u00bb\nFormer Fraud Office Director May Take Witness Stand \u00a0|\u00a0 Richard Alderman, the former director of the Serious Fraud Office, may be called as a witness in next month's hearing related to charging Magnus Peterson, the founder of the $600 million hedge fund Weavering Capital, with fraud, forgery and false accounting, The Financial Times reports. FINANCIAL TIMES\nApple Obstinate Before Earnings \u00a0|\u00a0 Though Apple has so far ignored pressure from investors like Carl C. Icahn to return more cash to shareholders, it may \"eventually see for itself the merits of returning more to investors,\" Robert Cyran writes in Reuters Breakingviews. REUTERS\nI.P.O.\/OFFERINGS \u00bb\nJapanese Owner of Uniqlo Aims for Secondary Listing in Hong Kong \u00a0|\u00a0 Fast Retailing says it plans a secondary stock market listing in Hong Kong as a way to broaden its base of investors and customers in Asia and, specifically, in China. DealBook \u00bb\nVodafone Looks to Acquire ONO Before I.P.O. \u00a0|\u00a0 Vodafone, the British telecommunications giant, is planning an acquisition of Grupo Corporativo ONO, the Spanish cable operator, in advance of ONO's initial public offering, Bloomberg News reports, citing unidentified people familiar with the situation. BLOOMBERG NEWS\nA Dearth in Women-Led I.P.O.'s \u00a0|\u00a0 Only two out of 82 \"emerging growth\" companies that had initial public offerings in 2013 had female chief executives, The Wall Street Journal reports. Between 1996 and 2013, only 3 percent of companies that went public had women chief executives. WALL STREET JOURNAL\nAfter Ruling, Chinese Companies Not Likely to List in New York \u00a0|\u00a0 A six-month ban by the United States Securities and Exchange Commission on Chinese auditors affiliated with the Big Four accounting firms could result in Chinese companies choosing to list themselves in Hong Kong rather than New York, Bloomberg News reports. BLOOMBERG NEWS\nVENTURE CAPITAL \u00bb\nKlein Announces Move to Vox Media \u00a0|\u00a0 Ezra Klein, who created The Washington Post's Wonkblog, announced he was moving to Vox Media, an Internet media company with digital brands, The Verge reports. THE VERGE\n Klein Joins Other Big Names in Jump to Digital News \u00a0|\u00a0 The departure of Ezra Klein, the creator of The Washington Post's Wonkblog, is an example of how \"digital publishing offers its own thing, not an additional platform for established news companies,\" David Carr writes in his Media Equation column in The New York Times. NEW YORK TIMES\nUber Facing Legal Challenges \u00a0|\u00a0 Uber, the hot start-up whose software allows anyone with a smartphone to get a cab ride, is suddenly facing trouble, David Streitfeld writes in The New York Times. NEW YORK TIMES\nCambridge Tech Industry Falling Behind \u00a0|\u00a0 Despite Cambridge's established start-up pedigree, the city may be losing some of its luster to London's technology industry, the Bits blog reports. NEW YORK TIMES BITS\nWith Some Investing Help, Huffington Unveils a New International Venture \u00a0|\u00a0 Arianna Huffington and Nicolas Berggruen, a billionaire investor, announced the creation of WorldPost, a new website under the Huffington Post umbrella aimed at international issues. DealBook \u00bb\nLEGAL\/REGULATORY \u00bb\nThe Tale of the $8 Million 'Bargain' House in Greenwich \u00a0|\u00a0 In the always fraught market for over-the-top mansions in Greenwich, one man's pain can quickly become another man's gain. DealBook \u00bb\nDeutsche Bank Co-Chief Cleared in Libor Investigation \u00a0|\u00a0 An internal investigation found that Anshu Jain, the co-chief executive of Deutsche Bank, did not play a role in the bank's manipulation of global interest rates, Reuters reports. REUTERS\nPartner in a Prestigious Law Firm, and Bankrupt \u00a0|\u00a0 Gregory M. Owens is an extreme but vivid illustration of the economic factors roiling the legal profession, James B. Stewart writes in his Common Sense column in The New York Times. DealBook \u00bb\nBernanke Enters Final Week \u00a0|\u00a0 As Ben S. Bernanke begins his last week as chairman of the Federal Reserve, plans to continue the tapering of asset purchases should ease Janet L. Yellen's transition atop the Fed, The Financial Times writes. FINANCIAL TIMES\nStockholder Lawsuits Increase \u00a0|\u00a0 Stockholders filed 234 federal securities lawsuits in 2013, with settlements totaling $6.5 billion, The Wall Street Journal reports. The number of filings is the most since 2008. WALL STREET JOURNAL\nBanker Says Letting Banks Fail Is Essential to Rebuilding Public Trust \u00a0|\u00a0 \"Too big to fail\" is no longer needed in some parts of the world in the wake of banking reforms, said Urs Rohner, chairman of the Swiss bank Credit Suisse. But, he said, \"people have to be convinced there may be banks that fail.\" DealBook \u00bb\nSign up for the DealBook Newsletter, delivered every morning and afternoon.\n","662":"CAMP EDWARDS, Mass. -- The small drone, with its six whirring rotors, swept past the replica of a Middle Eastern village and closed in on a mosque-like structure, its camera scanning for targets.\nNo humans were remotely piloting the drone, which was nothing more than a machine that could be bought on Amazon. But armed with advanced artificial intelligence software, it had been transformed into a robot that could find and identify the half-dozen men carrying replicas of AK-47s around the village and pretending to be insurgents. \n  As the drone descended slightly, a purple rectangle flickered on a video feed that was being relayed to engineers monitoring the test. The drone had locked onto a man obscured in the shadows, a display of hunting prowess that offered an eerie preview of how the Pentagon plans to transform warfare.\n  Almost unnoticed outside defense circles, the Pentagon has put artificial intelligence at the center of its strategy to maintain the United States' position as the world's dominant military power. It is spending billions of dollars to develop what it calls autonomous and semiautonomous weapons and to build an arsenal stocked with the kind of weaponry that until now has existed only in Hollywood movies and science fiction, raising alarm among scientists and activists concerned by the implications of a robot arms race.\u00a0\n  The Defense Department is designing robotic fighter jets that would fly into combat alongside manned aircraft. It has tested missiles that can decide what to attack, and it has built ships that can hunt for enemy submarines, stalking those it finds over thousands of miles, without any help from humans.\n  ''If Stanley Kubrick directed 'Dr. Strangelove' again, it would be about the issue of autonomous weapons,'' said Michael Schrage, a research fellow at the Massachusetts Institute of Technology Sloan School of Management.\n  Defense officials say the weapons are needed for the United States to maintain its military edge over China, Russia and other rivals, who are also pouring money into similar research (as are allies, such as Britain and Israel). The Pentagon's latest budget outlined $18 billion to be spent over three years on technologies that included those needed for autonomous weapons.\n  ''China and Russia are developing battle networks that are as good as our own. They can see as far as ours can see; they can throw guided munitions as far as we can,'' said Robert O. Work, the deputy defense secretary, who has been a driving force for the development of autonomous weapons. ''What we want to do is just make sure that we would be able to win as quickly as we have been able to do in the past.''\n  Just as the Industrial Revolution spurred the creation of powerful and destructive machines like airplanes and tanks that diminished the role of individual soldiers, artificial intelligence technology is enabling the Pentagon to reorder the places of man and machine on the battlefield the same way it is transforming ordinary life with computers that can see, hear and speak and cars that can drive themselves.\n  The new weapons would offer speed and precision unmatched by any human while reducing the number -- and cost -- of soldiers and pilots exposed to potential death and dismemberment in battle. The challenge for the Pentagon is to ensure that the weapons are reliable partners for humans and not potential threats to them.\n  At the core of the strategic shift envisioned by the Pentagon is a concept that officials call centaur warfighting. Named for the half-man and half-horse in Greek mythology, the strategy emphasizes human control and autonomous weapons as ways to augment and magnify the creativity and problem-solving skills of soldiers, pilots and sailors, not replace them.\n  The weapons, in the Pentagon's vision, would be less like the Terminator and more like the comic-book superhero Iron Man, Mr. Work said in an interview.\n  ''There's so much fear out there about killer robots and Skynet,'' the murderous artificial intelligence network of the ''Terminator'' movies, Mr. Work said. ''That's not the way we envision it at all.''\n  When it comes to decisions over life and death, ''there will always be a man in the loop,'' he said.\n  Beyond the Pentagon, though, there is deep skepticism that such limits will remain in place once the technologies to create thinking weapons are perfected. Hundreds of scientists and experts warned in an open letter last year that developing even the dumbest of intelligent weapons risked setting off a global arms race. The result, the letter warned, would be fully independent robots that can kill, and are cheap and as readily available to rogue states and violent extremists as they are to great powers.\n  ''Autonomous weapons will become the Kalashnikovs of tomorrow,'' the letter said.\n  The Terminator Conundrum\n  The debate within the military is no longer about whether to build autonomous weapons but how much independence to give them. Gen. Paul J. Selva of the Air Force, the vice chairman of the Joint Chiefs of Staff, said recently that the United States was about a decade away from having the technology to build a fully independent robot that could decide on its own whom and when to kill, though it had no intention of building one.\n  Other countries were not far behind, and it was very likely that someone would eventually try to unleash ''something like a Terminator,'' General Selva said, invoking what seems to be a common reference in any discussion on autonomous weapons.\n  Yet American officials are only just beginning to contend with the implications of weapons that could someday operate independently, beyond the control of their developers. Inside the Pentagon, the quandary is known as the Terminator conundrum, and there is no consensus about whether the United States should seek international treaties to try to ban the creation of those weapons, or build its own to match those its enemies might create.\n  For now, though, the current state of the art is decidedly less frightening. Exhibit A: the small, unarmed drone tested this summer on Cape Cod.\n  It could not turn itself on and just fly off. It had to be told by humans where to go and what to look for. But once aloft, it decided on its own how to execute its orders.\n  The software powering the drone has been in development for about a year, and it was far from flawless during the day of trials. In one pass over the mosque, the drone struggled to decide whether a minaret was an architectural feature or an armed man, living up to its namesake, Bender, the bumbling robot in the animated television series ''Futurama.''\n  At other moments, though, the drone showed a spooky ability to discern soldier from civilian, and to fluidly shift course and move in on objects it could not quickly identify.\n  Armed with a variation of human and facial recognition software used by American intelligence agencies, the drone adroitly tracked moving cars and picked out enemies hiding along walls. It even correctly figured out that no threat was posed by a photographer who was crouching, camera raised to eye level and pointed at the drone, a situation that has confused human soldiers with fatal results.\n  The project is run by the Defense Advanced Research Projects Agency, known as Darpa, which is developing the software needed for machines that could work with small units of soldiers or Marines as scouts or in other roles.\n  Unlike the drones currently used by the military, all of which require someone at a remote control, ''this one doesn't,'' said Maj. Christopher Orlowski of the Army, a program manager at Darpa. ''It works with you. It's like having another head in the fight.''\n  It could also easily be armed. The tricky part is developing machines whose behavior is predictable enough that they can be safely deployed, yet flexible enough that they can handle fluid situations. Once that is mastered, telling it whom or what to shoot is easy; weapons programmed to hit only certain kinds of targets already exist.\n  Yet the behavioral technology, if successfully developed, is unlikely to remain solely in American hands. Technologies developed at Darpa do not typically remain secret, and many are now ubiquitous, powering everything from self-driving cars to the internet.\n  Chess Champions\n  Since the 1950s, United States military strategy has been based on overwhelming technological advantages. A superior nuclear arsenal provided the American edge in the early days of the Cold War, and guided munitions -- the so-called smart bombs of the late 20th century -- did the same in the conflict's final decade.\n  Those advantages have now evaporated, and of all the new technologies that have emerged in recent decades, such as genomics or miniaturization, ''the one thing that has the widest application to the widest number of D.O.D. missions is artificial intelligence and autonomy,'' Mr. Work said.\n  Today's software has its limits, though. Computers spot patterns far faster than any human can. But the ability to handle uncertainty and unpredictability remain uniquely human virtues, for now.\n  Bringing the two complementary skill sets together is the Pentagon's goal with centaur warfighting.\n  Mr. Work, 63, first proposed the concept when he led a Washington think tank, the Center for a New American Security. His inspiration, he said, was not found in typical sources of military strategy -- Sun Tzu or Clausewitz, for instance -- but in the work of Tyler Cowen, a blogger and economist at George Mason University.\n  In his 2013 book, ''Average Is Over,'' Mr. Cowen briefly mentioned how two average human chess players, working with three regular computers, were able to beat both human chess champions and chess-playing supercomputers.\n  It was a revelation for Mr. Work. You could ''use the tactical ingenuity of the computer to improve the strategic ingenuity of the human,'' he said.\n  Mr. Work believes a lesson learned in chess can be applied to the battlefield, and he envisions a military supercharged by artificial intelligence. Brilliant computers would transform ordinary commanders into master tacticians. American soldiers would effectively become superhuman, fighting alongside -- or even inside -- robots.\n  Of the $18 billion the Pentagon is spending on new technologies, $3 billion has been set aside specifically for ''human-machine combat teaming'' over the next five years. It is a relatively small sum by Pentagon standards -- its annual budget is more than $500 billion -- but still a significant bet on technologies and a strategic concept that have yet to be proved in battle.\n  At the same time, Pentagon officials say that the United States is unlikely to gain an absolute technological advantage over its competitors.\n  ''A lot of the A.I. and autonomy is happening in the commercial world, so all sorts of competitors are going to be able to use it in ways that surprise us,'' Mr. Work said.\n  The American advantage, he said, will ultimately come from a mix of technological prowess and the critical thinking and decision-making powers that the United States military prioritizes. The American military delegates significant decisions down its chain of command, in contrast to the more centralized Chinese and Russian armed forces, though that is changing.\n  ''We're pretty confident that we have an advantage as we start the competition,'' Mr. Work said. ''But how it goes over time, we're not going to make any assumptions.''\n  Experts outside the Pentagon are far less convinced that the United States will be able to maintain its dominance by using artificial intelligence. The defense industry no longer drives research the way it did during the Cold War, and the Pentagon does not have a monopoly on the cutting-edge machine-learning technologies coming from start-ups in Silicon Valley, and in Europe and Asia.\n  Unlike the technologies and material needed for nuclear weapons or guided missiles, artificial intelligence as powerful as what the Pentagon seeks to harness is already deeply woven into everyday life. Military technology is often years behind what can be picked up at Best Buy.\n  ''Let's be honest, American defense contractors can be really cutting edge on some things and really behind the curve on others,'' said Maj. Brian Healy, 38, an F-35 pilot. The F-35, America's newest and most technologically advanced fighter jet, is equipped with a voice command system that is good for changing channels on the radio, and not much else.\n  ''It would be great to get Apple or Google on board with some of the software development,'' he added.\n  Submarines and Civilians\n  Beyond the practical concerns, the pairing of increasingly capable automation with weapons has prompted an intensifying debate among legal scholars and ethicists. The questions are numerous, and the answers contentious: Can a machine be trusted with lethal force? Who is at fault if a robot attacks a hospital or a school? Is being killed by a machine a greater violation of human dignity than if the fatal blow is delivered by a human?\n  A Pentagon directive says that autonomous weapons must employ ''appropriate levels of human judgment.'' Scientists and human rights experts say the standard is far too broad and have urged that such weapons be subject to ''meaningful human control.''\n  But would any standard hold up if the United States was faced with an adversary of near or equal might that was using fully autonomous weapons? Peter Singer, a specialist on the future of war at New America, a think tank in Washington, suggested there was an instructive parallel in the history of submarine warfare.\n  Like autonomous weapons, submarines jumped from the pages of science fiction to reality. During World War I, Germany's use of submarines to sink civilian ships without first ensuring the safety of the crew and passengers was seen as barbaric. The practice quickly became known as unrestricted submarine warfare, and it helped draw the United States into the war.\n  After the war, the United States helped negotiate an international treaty that sought to ban unrestricted submarine warfare.\n  Then came the Japanese attack on Pearl Harbor on Dec. 7, 1941. That day, it took just six hours for the United States military to disregard decades of legal and ethical norms and order unrestricted submarine warfare against Japan. American submarines went on to devastate Japan's civilian merchant fleet during World War II, in a campaign that was later acknowledged to be tantamount to a war crime.\n  ''The point is, what happens once submarines are no longer a new technology, and we're losing?'' Mr. Singer said. He added: ''Think about robots, things we say we wouldn't do now, in a different kind of war.''\n\n\n\n","663":"Here's what to expect in the week ahead:\nTRADE \n  A final push begins in the talks over Nafta.\n  Negotiators from the United States, Canada and Mexico will gather again in Washington on Monday to begin what government officials and industry leaders hope is the final leg of negotiations over the North American Free Trade Agreement.\n  Although the countries continue to have significant differences of opinion about the deal, the Trump administration is pushing to have talks finalized soon to meet all the necessary deadlines to submit the deal for a vote in the current Republican-controlled Congress. Robert Lighthizer, the United States trade representative charged with negotiating the deal, said last week that he aimed to wrap up talks in the next week or two. Ana Swanson\u00a0\n  MEDIA\n  Disney reports earnings, but the big issue is Fox.\n  Disney is expected to report strong results for its fiscal second quarter on Tuesday. ESPN is sound (for the moment). The theme park business is going up, up, up. And Walt Disney Studios, which released ''Black Panther'' in the quarter, is the envy of Hollywood. Analysts expect per-share profit to increase about 12 percent, and revenue to climb about 6 percent.\n  But Disney's near-term performance is not what Wall Street cares about most right now -- not with Comcast positioning itself to upend Disney's pending $52.4 billion acquisition of 21st Century Fox assets. If Comcast does make a renewed push for Fox, is Disney willing to go to war? What happens if Fox slips out of Disney's grip? Does Disney have a Plan B?\n  Robert A. Iger, Disney's chief executive, will probably sidestep direct questions about the Fox acquisition on Disney's earnings conference call. But some tea leaves may emerge. Brooks Barnes\n  TECHNOLOGY\n  Google conference is expected heavily emphasize artificial intelligence.\n  Google is holding its annual conference for developers near its Mountain View, Calif., headquarters this week. The event open on Tuesday with a keynote speech by Sundar Pichai, Google's chief executive, who is expected to talk about its latest efforts around artificial intelligence -- a technology that the company believes will be instrumental to its future.\n  The conference comes as Google faces a growing backlash against its vast data collection practices in the wake of Facebook's scandal involving Cambridge Analytica. Dai Wakabayashi\n   Microsoft is expected to show off new initiatives at its Build conference.\n  One of the other centers of gravity in the technology industry, Microsoft, will hold its own technical conference for developers, Build, in Seattle through Wednesday.\n  There was a time no company was better at rallying developers to write software for its underlying technologies -- the Windows operating system, in Microsoft's case. But over the past decade, a lot of developer activity has shifted to mobile technologies from Apple and Google.\n  Build is expected to showcase a variety of Microsoft initiatives that have helped the company maintain its influence in the tech industry, including its Azure cloud computing service and tools for adding artificial intelligence to software. Nick Wingfield\n  ECONOMY\n  Bank of England seen as unlikely to increase rates.\n  The Bank of England, Britain's central bank, faces a tricky balancing act when its rate-setting committee meets on Thursday. With inflation still sharply above the bank's target, traders had been betting that policymakers would raise the benchmark interest rate.\n  But a recent run of poor economic data -- including disappointing growth figures and worse-than-anticipated results in the manufacturing sector -- has prompted traders to revise those expectations. Mark J. Carney, the bank's governor, has also played down the possibility of a rate increase. Prashant Rao \n  Inflation probably rose in April.\n  Friday's jobs report showed that the unemployment rate fell below 4 percent for the first time since 2000. That's good news for workers, but it may add to fears that the tightening labor market will lead to faster inflation. Data that is to be released by the Labor Department on Thursday could add more fuel to the fire.\n  Economists expect the report to show that consumer prices rose 0.3 percent in April and were up 2.5 percent from a year earlier, which would mark the fastest growth in more than a year. That acceleration would partly reflect higher oil prices, but inflation most likely picked up even setting aside volatile food and energy prices.\n  The Federal Reserve has already factored somewhat faster inflation into its forecasts, but an unexpected spike could force the central bank to raise interest rates more quickly than planned. Ben Casselman\n  This is a more complete version of the story than the one that appeared in print.         \n\n\n\n","665":"SAN FRANCISCO -- They are a dream of researchers but perhaps a nightmare for highly skilled computer programmers: artificially intelligent machines that can build other artificially intelligent machines.\nWith recent speeches in both Silicon Valley and China, Jeff Dean, one of Google's leading engineers, spotlighted a Google project called AutoML. ML is short for machine learning, referring to computer algorithms that can learn to perform particular tasks on their own by analyzing data. AutoML, in turn, is a machine-learning algorithm that learns to build other machine-learning algorithms. \n  With it, Google may soon find a way to create A.I. technology that can partly take the humans out of building the A.I. systems that many believe are the future of the technology industry.\n  The project is part of a much larger effort to bring the latest and greatest A.I. techniques to a wider collection of companies and software developers.\u00a0\n  The tech industry is promising everything from smartphone apps that can recognize faces to cars that can drive on their own. But by some estimates, only 10,000 people worldwide have the education, experience and talent needed to build the complex and sometimes mysterious mathematical algorithms that will drive this new breed of artificial intelligence.\n  The world's largest tech businesses, including Google, Facebook and Microsoft, sometimes pay millions of dollars a year to A.I. experts, effectively cornering the market for this hard-to-find talent. The shortage isn't going away anytime soon, just because mastering these skills takes years of work.\n  The industry is not willing to wait. Companies are developing all sorts of tools that will make it easier for any operation to build its own A.I. software, including things like image and speech recognition services and online chatbots.\n  ''We are following the same path that computer science has followed with every new type of technology,'' said Joseph Sirosh, a vice president at Microsoft, which recently unveiled a tool to help coders build deep neural networks, a type of computer algorithm that is driving much of the recent progress in the A.I. field. ''We are eliminating a lot of the heavy lifting.''\n  This is not altruism. Researchers like Mr. Dean believe that if more people and companies are working on artificial intelligence, it will propel their own research. At the same time, companies like Google, Amazon and Microsoft see serious money in the trend that Mr. Sirosh described. All of them are selling cloud-computing services that can help other businesses and developers build A.I.\n  ''There is real demand for this,'' said Matt Scott, a co-founder and the chief technical officer of Malong, a start-up in China that offers similar services. ''And the tools are not yet satisfying all the demand.''\n  This is most likely what Google has in mind for AutoML, as the company continues to hail the project's progress. Google's chief executive, Sundar Pichai, boasted about AutoML last month while unveiling a new Android smartphone.\n  Eventually, the Google project will help companies build systems with artificial intelligence even if they don't have extensive expertise, Mr. Dean said. Today, he estimated, no more than a few thousand companies have the right talent for building A.I., but many more have the necessary data.\n  ''We want to go from thousands of organizations solving machine learning problems to millions,'' he said.\n  Google is investing heavily in cloud-computing services -- services that help other businesses build and run software -- which it expects to be one of its primary economic engines in the years to come. And after snapping up such a large portion of the world's top A.I researchers, it has a means of jump-starting this engine.\n  Neural networks are rapidly accelerating the development of A.I. Rather than building an image-recognition service or a language translation app by hand, one line of code at a time, engineers can much more quickly build an algorithm that learns tasks on its own.\n  By analyzing the sounds in a vast collection of old technical support calls, for instance, a machine-learning algorithm can learn to recognize spoken words.\n  But building a neural network is not like building a website or some run-of-the-mill smartphone app. It requires significant math skills, extreme trial and error, and a fair amount of intuition. Jean-Fran\u00e7ois Gagn\u00e9, the chief executive of an independent machine-learning lab called Element AI, refers to the process as ''a new kind of computer programming.''\n  In building a neural network, researchers run dozens or even hundreds of experiments across a vast network of machines, testing how well an algorithm can learn a task like recognizing an image or translating from one language to another. Then they adjust particular parts of the algorithm over and over again, until they settle on something that works. Some call it a ''dark art,'' just because researchers find it difficult to explain why they make particular adjustments.\n  But with AutoML, Google is trying to automate this process. It is building algorithms that analyze the development of other algorithms, learning which methods are successful and which are not. Eventually, they learn to build more effective machine learning. Google said AutoML could now build algorithms that, in some cases, identified objects in photos more accurately than services built solely by human experts.\n  Barret Zoph, one of the Google researchers behind the project, believes that the same method will eventually work well for other tasks, like speech recognition or machine translation.\n  This is not always an easy thing to wrap your head around. But it is part of a significant trend in A.I. research. Experts call it ''learning to learn'' or ''meta-learning.''\n  Many believe such methods will significantly accelerate the progress of A.I. in both the online and physical worlds. At the University of California, Berkeley, researchers are building techniques that could allow robots to learn new tasks based on what they have learned in the past.\n  ''Computers are going to invent the algorithms for us, essentially,'' said a Berkeley professor, Pieter Abbeel. ''Algorithms invented by computers can solve many, many problems very quickly -- at least that is the hope.''\n  This is also a way of expanding the number of people and businesses that can build artificial intelligence. These methods will not replace A.I. researchers entirely. Experts, like those at Google, must still do much of the important design work. But the belief is that the work of a few experts can help many others build their own software.\n  Renato Negrinho, a researcher at Carnegie Mellon University who is exploring technology similar to AutoML, said this was not a reality today but should be in the years to come. ''It is just a matter of when,'' he said.\n\n\n\n","666":"The \"new Pittsburgh\" of booming banks, thriving hospitals, expanding universities and more than 600 advanced-technology companies bears little resemblance to the factories and mills that dominated the city's past.\nWalk into the headquarters of Carnegie Group, founded by four computer science professors at Carnegie-Mellon University to market \"artificial intelligence\" to manufacturers. Through \"AI,\" as it is known, computers are programmed to behave autonomously: diagnosing problems on a factory floor, prescribing repairs for faulty cars, even pinpointing human illness.\nCarnegie Group is developing artificial intelligence systems for Ford, Boeing, Digital Equipment Corp. and other manufacturers -- in the name of making them more productive and thus more competitive. Carnegie Group's \"knowledge engineers\" interviewed Ford's top mechanics, dissected their know-how and created an \"expert system\" -- putting the knowledge of an expert mechanic in a computer -- to guide repairs at dealerships. The system aims to cut warranty costs, according to Larry Geisel, former president of the firm, and to improve customer satisfaction.\u00a0\nThe decor at Carnegie Group's headquarters is Danish modern superimposed on a turn-of-the-century railroad freight depot overlooking the Monongahela River. Clock-punching has given way to flex time. And, in contrast to the factory work force, whose jobs are under siege, here it is the executives who fear the loss of employes -- scientists with multiple degrees whose expertise is coveted in the United States and abroad.\n\"We are a $ 16 million company with a $ 60 million research and development program,\" said Geisel, referring to the firm's open line to Carnegie-Mellon's computer science department, a pioneer in AI. \"After working here for a very short time, our employes know more than all but a handful of people in the world. These people get very valuable very quickly.\"\nThe company is growing rapidly, but American manufacturers are not the only users of its technology. Carnegie Group has established an arm in Japan, selling systems to Japanese manufacturers and training engineers from that country in artificial intelligence. Geisel, who recently left the firm to start another, acknowledged that this may look to some like aiding the adversary, since the United States has a techological lead over Japan in AI. But to him, it reflects a new economic world order.\n\"There are two problems: competitiveness and balance of trade,\" Geisel said. \"The solution is not to say, 'Nobody sell to Japan.' Whether we make Japanese firms more competitive is another matter. We are open for business. To the extent American firms aren't interested, somebody else . . .  is.\"\n\"In our marketplace, there's a major new release every year,\" said Glen F. Chatfield, president and cofounder of Duquesne Systems Inc., a fast-growing $ 29 million-a-year company that custom designs software to make IBM mainframe computers run more efficiently. \"No matter what the new product is, if we don't make it better, we're a sitting target. If you're constantly improving, you're a moving target. And it can come from anywhere, Australia, Germany, anywhere.\"\nAn important catalyst to the growth of high technology here is the presence of two major universities: Carnegie-Mellon, pioneer of computer science, robotics and artificial intelligence, and the University of Pittsburgh, a leader in biotechnology with a world-renowned medical center.\nMellon Bank has taken over data processing for 450 banks round the country, and in the last five years has hired 400 computer technicians to service its equipment. These jobs average about $ 23,000 a year plus benefits, according to George P. DiNardo, a Mellon executive vice president.\n\"My main goal was to get with a big company that wouldn't lay people off,\" said Cathie Williamson, 27, a recently hired Mellon computer technician whose father, a construction worker, has suffered repeated layoffs.\n","667":"Google researchers have created an algorithm that has a human-like ability to learn, marking a significant breakthrough in the field of artificial intelligence. In a paper published in Nature this week, the researchers demonstrated that the algorithm could master many\u00a0Atari video games better than humans, simply through playing the game and learning from experience.\u00a0\n\"We can go all the way from pixels to actions as we call it and actually it can work on a challenging task that even humans find difficult,\" said Demis Hassabis, one of the authors of the paper. \"We know now we're on the first rung of the ladder and it's a baby step, but I think it's an important one.\"\nThe researchers only provided\u00a0the general-purpose algorithm its score on each\u00a0game, and the visual feed of the game, leaving it to then figure out how to win. It dominated Video Pinball, Boxing and Breakout, but struggled with Montezuma's Revenge and Asteroids.\nThey began their work at DeepMind, a London start-up that was purchased by Google in January 2014. Since joining the company they're exploring ways to weave this intelligence into Google products.\nWith plenty of Atari video games under their belt, the researchers will now move on to more complicated games with 3D environments. Hassabis expects the algorithm to crack these games\u00a0within the next five years.\n\"Ultimately the idea is that if this algorithm can race a car in a racing game then also essentially with a few extra tweaks it should be able to drive a real car,\" Hassabis said. \"But that's again, even further away than that.\"\nGoogle's self-driving cars have driven hundreds of thousands of miles. But those miles are centered around its home in Mountain View, Calif., where the company has built extensive, painstaking maps. Data such as the height of traffic signals and the exact position of curbs is preloaded onto the car's computer. As the car drives it compares its pre-installed map to what its sensors are seeing.\nBuilding such maps for the entire country - not to mention the world - would be a mammoth undertaking. They would also need to be regularly updated.\nA more appealing solution for Google would be for the car to develop a level of intelligence that's so high, it wouldn't need those preloaded maps. It could simply scan roads in front of it, and teach itself how to drive anywhere.\nThe algorithm is designed to tackle any sequential decision-making problem. Hassabis sees applications far outside video games and self-driving cars.\n\"In the future I think what we're most psyched about is using this type of AI to help do science and help with things like climate science, disease, all these areas which have huge complexity in terms of the data that the human scientists are having to deal with,\" Hassabis said.\nAnother potential use case be might telling your phone to plan a trip to Europe, and it would book your hotels and flights.\nBut that's all a very long way away. For now Hassabis wants his algorithm to move another rung up the artificial intelligence ladder, and teach itself to master Starcraft and Civilization.\n","668":"EVER SINCE computers were first installed in offices and factories, workers have feared that they were in danger of being replaced. Now there are worries that computers aren't replacing humans fast enough.\nThat is the chief frustration facing companies that were hoping that the rise of ''expert systems'' - the software equivalent of a doctor, mechanic or stockbroker, among others - would vastly cut labor costs. But as many in the computer industry are increasingly conceding, it hasn't worked out that way. At best, such programs appear to be relieving workers of rote tasks and simple diagnoses. True expertise, it turns out, is a subtle phenomenon - and one that rarely can be replicated in the pre-programmed ''rules'' that enable a software to simulate the thinking of its creators.\n''In all the use of expert systems I have ever seen,'' said Bill Turpin, the manager of artificial-intelligence software at Texas Instrument's data systems group, ''I have never seen one yet that could totally replace somebody. And I guess there is a lesson in that.''\u00a0\u00a0A Demystification Process\n The lesson is one of many in the recent and rapid demystification of artificial intelligence - both for those who design computers and those who use them. Just a few years ago, the phrase ''expert system'' conjured up images of programs that could make computers reason, converse and learn like the humans whose knowledge they seek to replicate. More than a few struggling software companies nurtured that image: programs that could ''think,'' even in a limited way, were always just a few years away from becoming a commercial product.\nIndeed, a handful of such programs, with rudimentary reasoning skills, are now in use. The Ford Motor Company has a system that diagnoses problems with its factory robots, saving engineers the need to leaf through hundreds of pages of reference manuals to find and fix a problem. Wall Street investment houses are toying with systems that evaluate investing opportunities, and companies like Texas Instruments and General Motors are tinkering with systems that advise executives about the right time to buy large equipment or build a new manufacturing facility.\nBut almost without exception, such programs have proved useful to a rather limited audience - and sometimes they hardly seemed worth the investment. And a whole raft of efforts to simulate other experts' duties - from loan officers at banks to chemists at multinationals - have largely failed. Now the whole field is undergoing re-examination, and more than a few expert-system experts are questioning whether their techniques have been applied to the right problems.\n''In the future artificial intelligence should simulate easily replaceable people,'' said Roger C. Schank, a professor of computer science and psychology at Yale University who also heads a fledgling artificial-intelligence company, Cognitive Systems. ''For example, there are a lot of people in the customer-service area - bank tellers, travel agents, airline reservation clerks - who don't do their job very well.'' And they are the kind of people who, programmers hope, they will soon learn to simulate.\nThe first step in that process came with computers that contain almost no intelligence at all: the bank-teller machine. The machines follow a preset order of instructions, asking the user first for a bank card, then for a password, and then to select from a limited menu of choices - a withdrawal or a deposit, for example, or maybe an account balance.\nBut the true banking ''expert'' - replicating a human bank officer - could advise a customer how much to keep in savings and how much in checking to get the maximum interest. It might be able to give a quick evaluation of the customer's likelihood to qualify for a mortgage. Or it could sell stocks and insurance.\nActually writing such a program, computer experts note, would not be all that difficult. Most of the decisions made by banks are strictly ''rule-based,'' the industry's lingo for actions that follow a closely prescribed set of guidelines. And rules are the lifeblood of any expert system. Usually, they are expressed as a series of ''if-then'' statements: If the patient complains of stomachaches, then what has she eaten? If the robot has stopped operating, then check its power supply.\nBut rules, it is turning out, are not enough. What makes ''experts'' is experience, and experience has more to do with past examples than with strict rules.\n''A library of cases should be what characterizes the intelligence of a machine,'' contends Professor Schank, who like many in the field questions whether the current generation of expert systems has very much to do with the goal of creating artificial intelligence. ''This concept that rules are where it's at in simulated intelligence is just dead wrong.''\nProfessor Schank's image of the perfect expert system seems to have more in common with Dear Abby than with HAL, the all-knowing computer in the movie ''2001.'' A customer might ask ''Should I invest in this product?'' The computer wouldn't answer ''yes'' or ''no'' - what investment adviser gives a straight answer? - but rather would offer five cases in the history of corporate America that bear similarities to the one at hand, and instruct the customer to ''see which ones are relevant.''\nIn the course of a dialogue the machine could refine its analysis, drawing on more and more information supplied by the customer. And ideally, it would track the investment in the company in question, adding the results automatically to its data bank of knowledge.\nSuch Socratic exchanges are already taking place, in a limited way, in laboratory experiments. Airlines are toying with systems that tell potential customers not only about flights and fares, but also the history of on-time performance of a particular flight, or the likelihood of bad weather on a given week.\u00a0Computers Dial for Help\n The International Business Machines Corporation already uses a system in which its larger computers, when suffering some internal distress, can automatically dial a telephone number that puts it in touch with the company's service engineers. The engineers are usually given enough guidance about the problem on portable, hand-held terminals that they know which spare parts to bring along.\nBut so far, such systems seem to create as much work for humans as they eliminate. ''They basically act as repair manual, and tell you what buttons to push,'' Mr. Turpin said.\nAnd the prospects that such systems will be able to talk to computer neophytes any time soon still seem remote. The chief blockade is what computer experts call the ''natural language'' barrier, the ability of a computer to speak to the end-user about a complex topic - more complex than how much to withdraw from a bank account - without using a human ''translator.''\u00a0An Airline Problem\nThat problem still bedevils the airline industry, which has yet to find a convenient way to have computers convey a vast amount of flight options to users without the intercession of a travel agent or reservations clerk. ''If you need someone to interpret the results of what the computer says,'' said one frustrated airline executive recently, ''then you have defeated the purpose of the machine: to save the labor of doing the job.''\nInstead, expert systems are turning out to be more a tool than a solution. They do what computers do best: keep track of a mass of data, and organize it in a reasonably digestible fashion. But oftentimes, that means the expert system is really just a good indexing system, allowing access to data but giving the user precious little idea about what to do with it. And as long as that is the case, most experts agree, it will threaten fewer jobs than it will create.\u00a0FASTEST-GROWING OCCUPATIONS, 1986-2000\u00a0(Numbers in thousands) \n","669":"THE SECOND SELF: Computers and the\nHuman Spirit. By Sherry Turkle. 362\npages. Simon & Schuster. $17.95.  There are dozens of remarkable vignettes to consider in Sherry Turkle's ''Second Self: Computers and the Human Spirit,'' and all they seem at first to share is their evidence of the surprisingly different ways that various people respond to computers. For instance, Jenny - a 6-year-old caught up in a debate among her playmates over whether an electronic toy that seems to cheat can be considered ''alive'' - announces disdainfully, ''To cheat you have to know you are cheating. Knowing is part of cheating.''\u00a0\nTanya, a sixth-grader who is failing all her subjects despite her sophisticated sense of style and her passionate interest in language, learns to write almost immediately upon being introduced to a computer. It turns out that she simply found her childish handwriting too shameful. Gerald, an editor, buys a computer to do word processing, but finds himself good enough at programming to enter a world he had thought forever barred to him. Frank, a computer-science major at the Massachusetts Institute of Technology and a devout Roman Catholic, believes that if God permits artificial intelligence to be achieved, a ''soul'' may well enter the machine and use it ''as its 'receptacle' in this universe.''\nBut for all the fascinating results of these and other surveys undertaken by Professor Turkle, there are even better reasons why ''The Second Self'' is more than just another routine study of the cultural impact of the computer revolution and its prospects for eventually fabricating intelligence. These have to do with the author's approach to her subject, which combines rigorous academic discipline with a sympathetic, even playful, imagination.\nSherry Turkle holds a joint doctorate in sociology and psychology from Harvard, and is an associate professor in M.I.T.'s Program in Science, Technology and Society. To write ''The Second Self,'' she spent six years studying and interviewing everyone from children undergoing their first encounters with computers, to lonely adolescents fixated on video games, to those members of M.I.T.'s community who are most obsessively involved with advancing the leading edge of computer technology.\n\nBecause she proceeds so methodically - viewing her youngsters according to Jean Piaget's model of childhood development, and judging her adults in the light of the current philosophical issues surrounding artificial intelligence - she is able to highlight the significant attributes of her disparate subjects. Not all that different are the self-styled misfits dreaming up variations of Pac-Man and Space Invaders, and the hackers and philosophers meditating on the patterns of Douglas Hofstadter's influential study, ''G\"odel, Escher, Bach: An Eternal Golden Braid.''\nWhile the theorists squabble over the relative pertinence of Lovelace patterns and the phenomenon of emergence or whether computers can ever be designed to be as intelligent as human beings, the children scratch their heads and wonder if the tic-tac-toe machine knows that it is cheating. It is left to Jenny to point out that if the thinking process is intelligent, then whatever is undergoing the process must also be intelligent. That, after all, remains the gist of the famous Turing test of artificial intelligence, which proposed that if, in a fair test, a reasonable person could not distinguish between the thinking of a person and a machine, then there is no difference between human intelligence and artificial intelligence.\nPerhaps even more significant, Professor Turkle comes to the subject of computers after having written ''Psychoanalytic Politics,'' a study, as she describes it, ''of how France, a country traditionally resistant to psychoanalytic ideas,'' was ''swept by an 'infatuation with Freud' in the late 1960's.'' Considering this background, we should not be surprised that she sees both Freudian theory and computers as popular metaphors for human behavior, with the former now giving way to the latter in the public mind.\n\nIt is apt, she believes, that the computational metaphor should be supplanting the psychoanalytic one, with people increasingly talking about themselves as needing ''reprogramming'' instead of attributing their behavior to ''unconscious drives,'' as they once were inclined to do. For if psychoanalysis developed in an age when hysteria caused by sexual trauma was the dominant cultural neurosis, then the computer has come along at a time when our prevailing complaint is loneliness accompanied by fear of intimacy. It is precisely to this neurotic condition that computers, with their ambiguous companionship, are so ideally suited.\nIs this a good thing or a danger? As her many illustrations show, it is not necessarily either. Some people are liberated by their contact; others get stuck. ''Computers are not good or bad; they are powerful,'' Professor Turkle concludes, having earned by the strength of her examples a right to such understatement.\nAnd having presented with extraordinary clarity both sides of the debate on the possibility of achieving artificial intelligence, she ends up her brilliant and challenging discussion on a note of neutrality. What is significant now, she believes, is that a fascination with our machine-like attributes that has come in the wake of the computer revolution has taken the place of our fixation on sexuality that followed the Freudian revolution. ''One thing is certain,'' she concludes, ''the riddle of mind, long a topic for philosophers, has taken on new urgency. Under pressure from the computer, the question of mind in relation to machine is becoming a central cultural preoccupation. It is becoming for us what sex was to the Victorians - threat and obsession, taboo and fascination.''\n","670":"The borough of Queens inspires certain associations for most New Yorkers. There are the airports, the highways and the row houses. There is Archie Bunker and the Mets.\nAnd in a narrow room above the Honey Bunch Childcare center in Woodside, there is Joseph Weintraub, wildly pursuing the outer edges of human thought.\u00a0\n Last week a group of judges gathered in Boston to determine whether a computer program could possibly be mistaken for the human mind. Ten people who wouldn't know a byte from a bark typed questions into terminals and tried from the responses to guess whether they were conversing with a human or a machine.\nThe winner of the $1,500 first prize was a whimsical program called PC Therapist III, a personal triumph in the unusually complex world of artificial intelligence. It was designed not at the Pentagon, in Silicon Valley or in the intellectual citadels of Cambridge, Mass., but by Mr. Weintraub in Woodside, just 200 yards from an entrance ramp to the Brooklyn- Queens Expressway.\u00a0Celebrity Blooms\n \"I'm still in shock,\" said Mr. Weintraub, a self-confessed bleary-eyed computer nebbish, as he sat in the study where he spends most of his waking hours. \"Even when the tournament was under way it never occurred to me I would win it.\"\nLife may never be the same for the portly, fiftyish Mr. Weintraub, who works by day as a computer programmer in Manhattan. Japanese television will visit him soon and so will a film crew from Russia. He has already received a flood of orders for the program -- an engaging amateur psychiatrist that draws on Mr. Weintraub's studies in psychology at City College.\nThe contest, first suggested decades ago by the British mathematician Alan M. Turing, generated intense interest among those who try to create programs that permit computers to think. Long before the renegade computer HAL became the cult hero of \"2001: A Space Odyssey,\" people struggled desperately with the question of whether computers could have brains.\nTuring's view was simple. If a computer could have a conversation that fooled people into thinking it was a person, then it could probably think -- at least in a conventional way. Last week, 5 of the 10 judges thought PC Therapist III was a person. For home practitioners of artificial intelligence, it was a signal achievement.\nBut on the quiet streets of Woodside, where there are plenty of auto part stores and family restaurants but no computer warehouses or capuccino outlets, the news of Mr. Weintraub's victory never made a ripple.\n\"If it's artificial how good could it be?\" said the proprietor of Guido's Fine Foods, on Queens Boulevard, when informed that one of his neighbors had created a winning computer program that employed artificial intelligence to think. \"We don't sell artificial things if we can help it.\" At nearby Beer Boys Discount Beverage Center, the manager, Alan Kalfin, was equally surprised, but much more excited by the news.\n\"That's great,\" said Mr. Kalfin. \"I don't think I've met him but he's probably a customer.\"\u00a03 Computers, No Waiting\n Mr. Weintraub's choice of residence, however, says more about the modern world of computing, where highways are made of telephone wire and technical advances are relayed throughout the world on networks and bulletin boards, than it does about Queens.\nThere are three computers in his dusty, poorly lighted study on the third floor of the row house at 46-16 65th Place. A quick look at them can help explain why Queens has become as comfortable a place to create certain technologies as Carnegie-Mellon University. Mr. Weintraub's oldest computer is also his most expensive and least useful, an original PC purchased a decade ago from 47th Street Photo for more than $4,000. His next machine, an AT-style computer, was many times faster and cost him half as much.\nThe computer he uses most often, however, purchased two years ago from a mail-order house, can process information thousands of times faster than any original PC, and it cost less than $2,500 in today's volcanic market.\n\"Your basic home machine right now is capable of computations that would have been at the forefront of human knowledge 10 years ago,\" said Chris Langton, a staff scientist in the complex systems group of the theoretical division at Los Alamos National Laboratory, which has long been a leader in exploring artificial intelligence. \"We call it garage band science. Personal computers today are reminiscent of the 60's when high school kids could first afford electric guitars and amplifiers. Within the last five years computers have crossed a threshold so that people can do incredible science at home.\"\u00a0Stock Phrases\n Mr. Weintraub's program -- available from him by mail -- is essentially a huge bundle of phrases, some stock, others not, that when stitched together do a fair imitation of human conversation. Although the Therapist frequently employs the type of psychological interrogatories New Yorkers can pay $100 to hear (\"Does that interest you?,\" \"How does that make you feel?\" and \"Can you elaborate on that?,\" are standards) Mr. Weintraub makes no claims for its abilities.\n\"It's fun,\" he says. \"It's addictive. It's not a professional.\"\nIt certainly is addictive, although its tart responses to heartfelt questions are not for everyone. The Therapist's bon mots are often reminiscent of a good fortune cookie, although fortune cookies don't usually ask such questions as (\"Were you always so sick, sick, sick?\") in response to a query about whether baseball is a usefull hobby.\nNor do the creators of most fortune cookies often end discussions of the ethics of morality in the modern world in the following way: \"What is moral is what you feel good after, and what is immoral is what you feel bad after.\"\n\"I said it was whimisical,\" Mr. Weintraub, noted sheepishly. \"I never said it was nice.\"\n","671":"SAN FRANCISCO -- There are plenty of unanswered questions about how self-driving cars would function in the real world, like understanding local driving customs and handing controls back to a human in an emergency.\u00a0\nNow a start-up called Drive.ai, based in Mountain View, Calif., is trying to address how an autonomous car would communicate with other drivers and pedestrians. The company is emphasizing what is known in the artificial intelligence field as ''human-machine interaction'' as a key to confusing road situations. \n  How does a robot, for example, tell everyone what it plans to do in intersections when human drivers and people in crosswalks go through an informal ballet to decide who will go first and who will yield?\n  ''Most people's first interaction with self-driving cars will not be as a rider, but more likely as a pedestrian crossing the street,'' said Carol Reiley, the co-founder and president of Drive.ai. ''I think it is so important for everyone to trust this type of technology.''\n  The start-up gained some attention earlier this year when it received a license from the State of California to test driverless cars on the road. But Tuesday was the first time its executives outlined, at least in broad terms, what they planned to do. They would not discuss the company's investors.\n  The Drive.ai cars won't speak with pedestrians and bicyclists. But they will try to communicate with visual displays that go beyond today's turn signals, perhaps with bannerlike text and easily identifiable sounds, company officials said.\n  The company, populated by graduate students and researchers from the Stanford Artificial Intelligence Laboratory, is entering a crowded field in the race to self-driving vehicles. There are about 20 self-driving car projects in Silicon Valley and more than four dozen around the country.\n  Unlike many of the efforts, however, Drive.ai will not attempt to build cars. Instead, it plans to retrofit commercial fleets for tasks like parcel delivery and taxi services.\n  The company is leaning on a technology called deep learning, a machine-learning technique that has gained wide popularity among Silicon Valley firms. It is used for a variety of tasks, like understanding human speech and improving the ability to recognize objects in computer vision systems.\n  An Israeli firm, Mobileye, is the dominant supplier of vision technology to the automotive industry, but Silicon Valley companies like Nvidia are also starting to compete for that business.\n  The self-driving cars of the future will need to be transparent about what their intentions are, how they make decisions and what they see, said Ms. Reiley, who is a roboticist with a background in designing underwater robotics and medical systems. They will need to communicate clearly both with the world around them as well as with their passengers.\n  ''There's the left brain in which a lot of discussion has taken place, what algorithms and what sensors, the logical side,'' she said. ''A lot of the discussion around self-driving cars has no human component, which is really weird because this is the first time a robotic system is going out in the world and interacting with people.''\n\n\n\n","672":"Pushing ahead in the decades-long effort to get computers to understand human speech, Google researchers have added sophisticated voice recognition technology to the company's search software for the AppleiPhone.\n  Users of the free application, which Apple is expected to make available as soon as Friday through its iTunes store, can place the phone to their ear and ask virtually any question, like ''Where's the nearest Starbucks?'' or ''How tall is Mount Everest?'' The sound is converted to a digital file and  sent to Google's servers, which try to determine the words spoken and pass them along to the Google search engine.\n  The search results, which may be displayed in just seconds on a fast wireless network, will at times include local information, taking advantage of iPhone features that let it determine its location.\u00a0\n  The ability to recognize just about any phrase from any person has long been the supreme goal of artificial intelligence researchers looking for ways to make man-machine interactions more natural. Systems that can do this have recently started making their way into commercial products. \n  Both Yahoo and Microsoft already offer voice services for cellphones. The Microsoft Tellme service returns information in specific categories like directions, maps and movies. Yahoo's oneSearch with Voice is more flexible but does not appear to be as accurate as Google's offering. The Google system is far from perfect, and it can return queries that appear as gibberish.    Google executives declined to estimate how often the service gets it right, but they said they believed it was easily accurate enough to be useful to people who wanted to avoid tapping out their queries on the iPhone's touch-screen keyboard.\n  The service can be used to get restaurant recommendations and driving directions, look up contacts in the iPhone's address book or just settle arguments in bars. The query ''What is the best pizza restaurant in Noe Valley?'' returns a list of three restaurants in that San Francisco neighborhood, each with starred reviews  from Google users and links to click for phone numbers and directions.\n  Raj Reddy, an artificial intelligence researcher at Carnegie Mellon University who has done pioneering work in voice recognition, said Google's advantage in this field was the ability to store and analyze vast amounts of data. ''Whatever they introduce now, it will greatly increase in accuracy in three or six months,'' he said.\n  ''It's important to understand that machine recognition will never be perfect,'' Mr. Reddy added. ''The question is, How close can they come to human performance?'' For Google the technology is critical to its next assault on the world of advertising. Google executives said location-based queries would make it possible to charge higher rates for advertisements from nearby businesses, for example, although it is not selling such ads now. \n  As with other Google products the service is freely available to consumers, and the company plans to eventually make it available for phones other than the iPhone.\n  ''We are dramatically increasing value to the advertiser through location and voice,'' said Vic Gundotra, a former Microsoft executive who now heads Google's mobile businesses.\n  Google is by no means the only company working toward more advanced speech recognition capabilities. So-called voice response technology is now routinely used in telephone answering systems and in other consumer services and products. These systems, however, often have trouble with the complexities of free-form language and usually offer only a limited range of responses to queries.\n  Several weeks ago Adobe added voice recognition technology developed by Autonomy, a British firm, to its Creative Suite software, allowing it to generate transcripts of video and audio recordings with a high degree of accuracy.\n  Mr. Gundotra said Google had been tackling the twin problems of entering and retrieving information with hand-held wireless devices.\n  ''Solving those two problems in a world-class way is our goal,'' he said.\n  The new iPhone search capability is not the first speech offering from Google. In March, it announced that GOOG-411, an experimental directory information service, had turned into a real product. The service allows users to ask for business phone and address information. The company said it had built on its experience and the data it collected through GOOG-411 in developing the iPhone service.\n  The new service is an example of the way Google tries to blend basic computer science research with product engineering. The company has hired many of the best speech recognition researchers in the world and now has teams working on different aspects of the problem in New York, London and its headquarters in Mountain View, Calif.\n  An intriguing part of the overall design of the service was contributed by a Google researcher in London, who found a way to use the iPhone accelerometer -- the device that senses how the phone is held --  to set the software to ''listen'' mode when the phone is raised to the user's ear. \n  Google researchers said that another of its advantages over competitors was the billions of queries its users have made over the years.\n  ''One thing that has changed is the amount of computation and the amount of data that is available,'' said Mike Cohen, a speech research who was co-founder of Nuance Communications before coming to Google.\n  Past queries can be used to build a statistical model of the way words are frequently strung together, Mr. Cohen said. This is just one of the components of the speech recognition system, which also includes a sound analysis model and a mechanism for linking the basic components of language to actual words.\n  Google recently published a technical paper on building large models for machine translation of language. The researchers wrote that they had trained the system on two trillion ''tokens,'' or words.  \n","673":"Everyone has heard the old anecdote about the frog in a pot of water. If the temperature is raised slowly, the frog won't react, eventually allowing itself to get boiled. That's where we're heading as a country when it comes to technological advances and the threat they pose to millions of jobs.\nSeemingly every day there are new stories in the media about artificial intelligence, data and robotics -- and the jobs they threaten in retail, transportation, carrier transport and even the legal profession. Yet no one is jumping out of the pot.\u00a0\nLet's be clear: This is not science fiction. In just the past few days, there have been articles on Amazon's automation ambitions, described by the New York Times\u00a0as \"putting traditional retail jobs in jeopardy,\" and on the legal profession bracing for technology taking over\u00a0some tasks once handled by lawyers. (Jeffrey P. Bezos is the owner of The Washington Post and founder and chief executive of Amazon.com.)\nAs reported in Recode, a new study by the research firm PwC found that nearly 4 out of 10 jobs in the United States could be \"vulnerable to replacement by robots in the next fifteen years.\" Many of those will be truckers, among the most common jobs\u00a0in states across the country.\nYet when President Trump hosted truck drivers at the White House last week, he dedicated his remarks to the threat of health care without uttering a word about the advanced driverless semi fleets that will soon replace them. His Treasury Secretary Steven Mnuchin shockingly said in an interview last week that we're \"50 to 100 years\" away from artificial intelligence threatening jobs.\n[             We're so unprepared for the robot apocalypse          ]\nIt's easy for sensationalist headlines about A.I. to dominate, like those about Elon Musk's warning that it poses an existential threat. Yet the attention of people such as Musk, Bill Gates and Stephen Hawking should be a signal to Trump and Mnuchin that A.I. and related robotics and automation are moving at a far faster clip than they are acknowledging. It should be on the administration's radar screen, and they should be jumping out of the boiling water.\nSolutions won't come easy. Already some experts suggest a Universal Basic Income\u00a0will be necessary to offset the job losses. We also have to transition our workforce. Educational institutions such as Miami-Dade College and Harvard University have introduced advanced programming courses that take students from zero to six programming languages on a fast track. More needs to be done. This should be the most innovative decade in human history, and it has to be if we're going to avoid a Mad Max dystopia in favor of a Star Trek future.\nOf course, there are those who say similar warnings were raised as technology revolutionized agriculture and other industries along the way. They might argue that then, as now, those advances led to more jobs. We would all welcome that and the potential these changes will represent for improving lives.\n[             These 6 new technology rules will govern our future          ]\nTechnological advances could greatly reduce the cost of living, make housing more affordable and solve some of the biggest challenges whether in energy or long-term care, an issue painfully familiar to so many families. It may also help improve quality of life in the long term, as men and women gain greater flexibility to spend time with loved ones rather than dedicating 40 or more hours a week to working and so many others commuting.\nIn the near-term, however, the job losses that are possible could inflict tremendous economic pain. We are far from where we need to be. That will continue to be the case until policymakers, educators and innovators come together to address the reality before us. We won't solve this overnight, but we can't afford to wait until it's too late.\n              Read more from The Washington Post's Innovation section.\u00a0           \n","674":"The World Economic Forum kicked off Tuesday, and the theme of this year's gathering of the world's leaders, celebrities, billionaires and the merely wealthy will be what it calls the \"Fourth Industrial Revolution.\" That's its term for the accelerating pace of technological changes, especially those that are \"blurring the lines between the physical, digital and biological spheres\" - the combination of things such as artificial intelligence, robotics, nanotechnology and 3-D printing.\nTo go with that theme, the WEF has released research looking at the effect all that change will have on jobs. It projects that by 2020, 7.1 million jobs are expected to be lost vs. just 2 million gained. \"Davos Robot Eclipses Davos Man as Gloom Descends on World Elite,\" Bloomberg wrote in covering the news of this year's theme, which will be the topic of 20 sessions over the four-day conference. The WEF said in a statement while releasing the report: \"These predictions are likely to be relatively conservative and leave no room for complacency.\"\u00a0\nCheery.\nThe dreary news is hardly unfamiliar, however, as the pending wave of tech disruption prompts study after study on the massive impact robotics and artificial intelligence are expected to have on jobs. Oxford University researchers famously found that an estimated 47 percent of U.S. jobs could be taken by robots in the next two decades. Nearly half of industry experts and tech enthusiasts, according to a Pew Research Center study, believe robots will displace many jobs and lead to \"vast increases in income inequality, masses of people who are effectively unemployable, and breakdowns in the social order.\"\nAnd even among those who think the immediate number of jobs that are actually lost will be small, the disruption to how individual jobs are done could be enormous, with positions all the way up to the chief executive experiencing change, according to a recent McKinsey report.\nBut what's interesting about the WEF's latest round of gloomy numbers is that it also shows what a potentially outsize impact these changes could have on women's jobs, in particular. A whopping two-thirds of those 5 million lost jobs are expected to come from office and administrative roles, where women are especially well-represented. While the traditionally male-dominated manufacturing and production industries are also expected to be sharply hit, resulting in a similar overall burden in job losses on men and women, the gender gap in jobs could still worsen, the WEF reports.\nHere's why. Women already make up a smaller share of the workforce, for one. The report states that in absolute terms, men will face about three jobs lost for every job gained, whereas women will face more than five jobs lost for each job added.\nCombine that with how many new jobs are in the traditionally male-dominated science, tech and engineering fields, and the prediction is even more troubling. If current gender-gap ratios persist in the STEM fields between 2015 and 2020, there's projected to be nearly one new STEM job for every four jobs lost for men - but only one new STEM job for every 20 jobs lost for women, the WEF reports.\nIn its \"Future of Jobs\" report, the WEF finds that things such as advanced robotics, self-driving cars, artificial intelligence and \"machine learning\" will affect industries and business models as soon as 2018.\nFor those without a tech background, however, there is some good news in the report. Yes, it projects that more than a third of the skills that will be most desirable by 2020 aren't even considered important today, illustrating the rapid pace of change the world is confronting.\nBut it also says the world will need people to lead. Management jobs have one of the most positive net-employment outlooks in the report. And social skills such as persuasion and emotional intelligence are expected to be in even higher demand across industries than more limited technical skills. So are creativity, active listening and critical thinking.\nThe future, in other words, may sound a little gloomy. But maybe the workplace will be a little nicer place to be, too.\njena.mcgregor@washpost.com\n","675":"The International CES, the annual consumer electronics show, is going on in Las Vegas, and, as the Times technology columnist writes, \"lots of potentially groundbreaking ideas are just around the corner\" - even if they're not quite there yet.\nRead about some of the ideas in development, then tell us, what new tech ideas does the world need? What problems could technology solve? How could it make our lives better?\u00a0\nIn \"On Display at CES, Tech Ideas in Their Awkward Adolescence,\" Farhad Manjoo  writes:\nIt's a clich\u00e9 for journalists to whine about International CES, the annual consumer electronics show that brings gadget-hounds and billionaires like a nerdy plague upon Las Vegas this week. As many have complained - yours truly included - CES long ago devolved into a noisy parade of puffed-up announcements that usually amount to nothing.\nBut if news from CES feels especially desultory this year, it might not be the show that's at fault. Instead, blame the tech cycle. We're at a weird moment in the industry: The best new stuff is not all that cool, and the coolest stuff isn't quite ready.\nIt's not that today's tech is terrible - lots of potentially groundbreaking ideas are just around the corner. In product labs across the world, engineers and designers are working to turn a collection of annoying buzzwords - artificial intelligence, virtual reality, wearables, the \"Internet of things,\" autonomous cars and drones - into products that we find irresistible. They are all pushing the limits of what machines can currently accomplish, and working to come up with business models that customers will stomach. In time, after years of experimentation and incremental advances, many of these technologies might become mainstays.\n... Over the next couple of CESes, there's a good chance we will see a lot of devices that will feel not quite ready. Virtual reality will underwhelm, artificial intelligence will feel annoyingly unintelligent, and cars and drones that navigate themselves will seem safer when parked. There will be wearables you won't want to wear and home devices that will make you want to buy a new place.\n\"It will eventually cross that hump and become mature, but it's not there right now,\" Mr. Bajarin said.\nIt's unsurprising that early versions of new tech will be a bit flawed. But what's unusual about the current moment is how many new, interdependent technologies are coming about at the same time. We're in the middle of a grand cycle in technology - the revolution pushed by smartphones, social networks and ubiquitous online servers (that is, the cloud everyone talks about). Over the last decade, these three technologies have altered wide swaths of the business world, destroying and creating hundreds of billions of dollars in wealth in sectors as varied as media and transportation.\n... All this could be grand: A few years from now, if you believe the boosters in Vegas this week, many of us will be shuttling around in self-driving cars, regularly plugging into reality-defying virtual worlds, and adorning ourselves with smart jewelry that will seamlessly integrate our bodies with the Internet. Our homes will be connected, our drones will be autonomous and our phones will sizzle with artificial intelligence.\n Students: Read the entire article, then tell us ...\n- Which of the ideas mentioned in the article are most interesting to you? Virtual reality? Drones? Self-driving cars? Smart wearables that keep you connected? Why?\n- What future uses for some of the technologies currently being developed would you like to see? For example, how could they help solve the world's problems, or just make our lives easier, more fun or more interesting?\n- What are some tech devices or innovations you wish existed? What are some problems, large or small, you wish tech could solve? Brainstorm a list, alone or with others, by thinking about as many aspects of life as you can - from broad fields like education, medicine, entertainment, sports, fashion, politics and economics to your personal needs in areas like relationships, spirituality, hobbies or interests. Post as many as you can here.\n Students 13 and older are invited to comment below. Please use only your first name. For privacy policy reasons, we will not publish student comments that include a last name.\n","678":"Famed scientist warns that we have only a century to leave Earth\nIn November, Stephen Hawking gave humanity what we thought was an intimidating deadline for finding a new planet to call home: 1,000 years.\u00a0\nTen centuries is a blip in the grand arc of the universe, but in human terms it was the apocalyptic equivalent of getting a few weeks' notice before our collective landlord kicks us to the curb.\nNow Hawking, the renowned theoretical physicist turned apocalypse warning system, is back with a revised deadline. In a BBC documentary that debuts this summer called \"Expedition New Earth,\" Hawking claims that we should gather our belongings and get out - not in 1,000 years, but in the next century or so.\n\"Professor Stephen Hawking thinks the human species will have to populate a new planet within 100 years if it is to survive,\" according to a BBC statement. \"With climate change, overdue asteroid strikes, epidemics and population growth, our own planet is increasingly precarious.\"\nThe BBC program gives Hawking a chance to wade into the evolving science and technology that may become crucial if humans hatch a plan to escape Earth and find a way to survive on another planet - from questions about biology and astronomy to rocket technology and human hibernation, the BBC says.\nIn recent months, Hawking has been explicit about humanity's need to find a \"Planet B.\" In the past, he has also called for humans to colonize the moon and find a way to settle Mars - a locale he referred to as \"the obvious next target\" in 2008, according to New Scientist.\nRemaining on Earth any longer, Hawking claims, places humanity at great risk of encountering another mass extinction.\n\"We must ... continue to go into space for the future of humanity,\" the 74-year-old Cambridge professor said during a November speech at Oxford University Union, according to Britain's Daily Express. \"I don't think we will survive another 1,000 years without escaping beyond our fragile planet.\"\nHawking told the audience that Earth's cataclysmic end may be hastened by humankind, which will continue to devour the planet's resources at unsustainable rates.\nSome of Hawking's most explicit warnings have revolved around the potential threat posed by artificial intelligence. That means - in Hawking's analysis - humanity's daunting challenge is twofold: develop the technology that will enable us to leave the planet and start a colony elsewhere, while avoiding the frightening perils that may be unleashed by said technology.\nWhen it comes to discussing that threat, Hawking is blunt.\n\"I think the development of full artificial intelligence could spell the end of the human race,\" Hawking told the BBC in a 2014 interview.\nDespite its current usefulness, he cautioned, further developing AI could prove a fatal mistake.\n\"Once humans develop artificial intelligence, it will take off on its own and redesign itself at an ever-increasing rate,\" Hawking warned in recent months. \"Humans, who are limited by slow biological evolution, couldn't compete and would be superseded.\"\npeter.holley@washpost.com\n","679":"SAN FRANCISCO -- Google unveiled an initiative on Thursday to help train Americans for jobs in technology and committed to donating $1 billion over the next five years to nonprofits in education and professional training.\nThe new program, Grow With Google, will create an online destination for job seekers to get training and professional certificates and for businesses to improve their web services. The company's goal, executives said, is to allow anyone with an internet connection to become proficient with technology and prepare for a job in areas like information technology support and app development. \n  Google detailed its job training program as Silicon Valley faces increasing criticism over what some say is an unchecked influence over business and society. Google is also among a number of companies facing scrutiny over the role their internet services played in Russian interference in the 2016 presidential election.\u00a0\n  Sundar Pichai, Google's chief executive, unveiled the initiative during a speech on Thursday in the company's Pittsburgh office, far from the search giant's Silicon Valley headquarters. He noted the city's transformation from an industrial manufacturing center for steel to a hub of robotics and artificial intelligence engineering.\n  ''We understand there's uncertainty and even concern about the pace of technological change, but we know that technology will be an engine of America's growth for years to come,'' Mr. Pichai said. ''The nature of work is fundamentally changing, and that is shifting the link between education, training and opportunity.''\n  Google plans to donate $1 billion to nonprofits through its charitable arm, Google.org, with the aim of addressing the gap between the skills required by modern companies and the skills that are taught in schools. Google said it was donating $10 million to Goodwill Industries, for example, for digital job training programs. Company employees also will volunteer one million hours at those nonprofits.\n  Much like a political campaign, Google will go on the road to spread the message about its new program, it said. In the coming months, company officials will make stops in Indianapolis; Oklahoma City; Lansing, Mich.; and Savannah, Ga.\n  Google is not the only big tech company that has gone on a charm offensive in recent months. Under fire from President Trump for producing most of its devices in China, Apple announced in May that it was creating a $1 billion fund to invest in advanced manufacturing in the United States. Amazon, another frequent target of Mr. Trump, said in January that it was planning to hire 100,000 new employees over the next 18 months.\n  And Sheryl Sandberg, Facebook's high-profile chief operating officer, made the rounds on Capitol Hill this week, offering explanations to congressional leaders about her company's role in last year's election.\n  Google has long been a leader in research on artificial intelligence, and Mr. Pichai has made it the centerpiece of his company's plans. Google and its parent company, Alphabet, are leaning on A.I. technology in all manner of products, from new smartphones to self-driving cars. But with that automation comes disruption and concern that breakthroughs may upend entire industries and eliminate millions of jobs, particularly in trucking and transportation.\n  Pittsburgh is one traditional manufacturing city, however, that could gain from advances in artificial intelligence. Researchers at Carnegie Mellon University in the city have been on the cutting edge of work on autonomous vehicles, and other big tech outfits like Uber and Amazon also have offices there.\n  Google has been put under a harsh light over the last year, an unaccustomed spot for a company that has long been one of the tech industry's most admired outfits and is still considered one of the best places in the world to work.\n  The European Union levied the largest antitrust fine in history against Google for unfairly favoring its own services over those of its rivals. A group of former employees sued the company, accusing it of paying women less than men. It also suffered an exodus of advertisers from its video platform, YouTube, after evidence that ads appeared next to extremist videos.\n  More recently, it has been mired in the spreading investigation of Russian interference in the election. Company representatives are expected to speak at House and Senate Intelligence Committee hearings on Nov. 1, along with Facebook and Twitter.\n\n\n\n","681":"A fledgling science that creates software whose behavior mimics that of living creatures is beginning to provide scientists with a new class of software tools with significant commercial potential.\nThe science is known as artificial life. Its practitioners have produced computer programs that can actually evolve into more powerful programs through their own interaction and merging to create a new generation - a Darwinian process similar to that of biological organisms.\nThrough the computerized equivalent of the survival of the fittest, a computer runs thousands of programs simultaneously and a specially tailored master program selects those that most efficiently accomplish a given task. The programs that do well are then combined to produce a next generation that is even better in accomplishing the task. Such evolution could produce software more reliable than that designed by human programmers, who cannot anticipate all the potential ways in which their software can fail.\u00a0\u00a0'Like Playing God'\n''The dream is to 'evolve' programs that do things that we want,'' said Danny Hillis, a computer scientist who is co-founder of Thinking Machines Inc., a Cambridge, Mass., maker of a parallel supercomputer. ''Ultimately, it will be like playing God,'' he said of the possibility of creating a computerized universe for software evolution in order to breed useful programs. ''It's not quite useful yet, but there's reason to believe that it's just about to be.''\nAs one example of such technology, Thinking Machines has created, in only its first attempt with artificial life techniques, a program for sorting a list of numbers that comes close to the most efficient human-crafted program for that purpose.\nArtificial life techniques are also being used to simulate the behavior and evolution of genes and of the organisms they make up. Instead of programming each step the software should take, the artificial life approach creates basic rules, allowing small, interacting software modules to exhibit the kind of complex and unexpected behavior that organisms do. In this way, scientists at the University of California at Los Angeles have developed a program that mimics the behavior of mosquitoes to determine growth rates and therefore precisely where and how much insecticide must be applied.\nOther software products are being designed based on the behavior of groups of organisms, particularly the tendency of their interactions to produce complex behavior. For example, Borland International, a Scotts Valley, Calif., software publisher has recently redesigned three of its most popular programs to allow the interactions of small software modules to control the programs.\u00a0Studying Basic Behavior\nThe science of artificial life resembles that of artificial intelligence, a commercial and scientific discipline that first flourished in the 1960's, in its attempt to mimic or simulate natural processes. But where artificial intelligence focuses on simulating human qualities like vision and speech, work on artificial life is taking on a much broader challenge: the basic behavior of all organisms.\nArtificial intelligence research has already spawned robots, vision- and voice-recognition systems, and ''expert systems'' of software that apply the knowledge of human experts.\nBut that field has in some ways been disappointing. Many problems have proved much more challenging than originally thought.\n''There is a lot of frustration in the lack of progress in artificial intelligence during the last 40 years,'' said J. Doyne Farmer, an artificial life researcher at the Santa Fe Institute, which studies complex organizations and biological and physical processes. ''They missed something along the way. We'd like to go back to fundamentals.''\u00a0Hunting for Improvement\nMr. Hillis at Thinking Machines has developed a striking example of the potential power of artificial life research. His company's computer, known as a Connection Machine, is based on as many as 64,000 individual processors that can simultaneously test different programs for sorting lists of numbers, combine them with other programs and evolve thousands of generations of programs in a relatively short time.\nMr. Hillis has added to his simulation some software with a role equivalent to a biological parasite's. It wipes out any program that is so competent that it comes to dominate the system. This forces a search for new programs that are even better.\nMr. Hillis said his approach to software design could offer an avenue for more rugged testing of advanced programs that are so complex that their designers cannot fully comprehend all of their functions.\nHe also says that artificial life methods suggest a novel solution to the problem of computer security. An operating system - a computer's basic control program - could be evolved in a manner similar to the way he derived his sorting program. Such an approach would insure diversity and limit the spread of computer viruses that now prey on extremely homogeneous computer environments.\nCreating more powerful software through evolution rather than by designing it ''is a radical approach to programming,'' said Esther Dyson, a computer industry analyst and publisher of Release 1.0. ''Just as you now find artificial intelligence techniques infiltrating regular programming, in the future you will find artificial life techniques infiltrating the design of operating systems and computer networks.''\u00a0Already in Practical Use\nArtificial life techniques are already being translated into practical use. At UCLA, a biologist, Charles Taylor, and a computer scientist, David Jefferson, have developed a simulation program that runs on a Connection Machine to assist mosquito abatement programs in Orange County and Alameda County in California. County officials are using the program to help them apply insecticides more efficiently.\n''By the use of our software Alameda County has reduced the number of treatment sites to 3,000 from 20,000,'' Dr. Taylor said. Minimizing the levels of insecticide applied is crucial because mosquitoes quickly build immunity, he said.\nMoreover, techniques being explored by artificial life researchers are already starting to find their way into commercial software. For example, Borland has redesigned three of its most popular programs so that they are composed of tiny modular ''capsules'' of software instructions. Borland's Reflex and Paradox, data base management programs, and Quattro, a spreadsheet, use the new technology to run more quickly within the limited confines of the memory available on an I.B.M. Personal Computer.\nA set of rules governing priority moves the small capsules from the disk to the computer's memory as they are needed and then discards them as space is needed for data. The advantage of the system is that it permits the program to run faster in a limited amount of memory available to it.\n","682":"In what is shaping up as an academic Battle of the Titans -- one that offers vast new learning opportunities for students around the world -- Harvard and the Massachusetts Institute of Technology on Wednesday announced a new nonprofit partnership, known as edX, to offer free online courses from both universities.\nHarvard's involvement follows M.I.T.'s announcement in December that it was starting an open online learning project, MITx. Its first course, Circuits and Electronics, began in March, enrolling about 120,000 students, some 10,000 of whom made it through the recent midterm exam. Those who complete the course will get a certificate of mastery and a grade, but no official credit. Similarly, edX courses will offer a certificate but not credit.\u00a0\nBut Harvard and M.I.T. have a rival -- they are not the only elite universities planning to offer free massively open online courses, or MOOCs, as they are known. This month, Stanford, Princeton, the University of Pennsylvania and the University of Michigan announced their partnership with a new commercial company, Coursera, with $16 million in venture capital.\nMeanwhile, Sebastian Thrun, the Stanford professor who made headlines last fall when 160,000 students signed up for his Artificial Intelligence course, has attracted more than 200,000 students to the six courses offered at his new company, Udacity.\nThe technology for online education, with video lesson segments, embedded quizzes, immediate feedback and student-paced learning, is evolving so quickly that those in the new ventures say the offerings are still experimental.\n''My guess is that what we end up doing five years from now will look very different from what we do now,'' said Provost Alan M. Garber of Harvard, who will be in charge of the university's involvement.\nEdX, which is expected to offer its first five courses this fall, will be overseen by a nonprofit organization governed equally by the two universities, each of which has committed $30 million to the project. The first president of edX will be Anant Agarwal, director of M.I.T.'s Computer Science and Artificial Intelligence Laboratory, who has led the development of the MITx platform. At Harvard, Dr. Garber will direct the effort, with Michael D. Smith, dean of the faculty of arts and sciences, working with faculty members to develop and deliver courses. Eventually, they said, other universities will join them in offering courses on the platform.\nM.I.T. and Harvard officials said they would use the new online platform not just to build a global community of online learners, but also to research teaching methods and technologies.\nEducation experts say that while the new online classes offer opportunities for students and researchers, they pose some threat to low-ranked colleges.\n''Projects like this can impact lives around the world, for the next billion students from China and India,'' said George Siemens, a MOOC pioneer who teaches at Athabasca University, a publicly supported online Canadian university. ''But if I were president of a mid-tier university, I would be looking over my shoulder very nervously right now, because if a leading university offers a free circuits course, it becomes a real question whether other universities need to develop a circuits course.''\nThe edX project will include not only engineering courses, in which computer grading is relatively simple, but also humanities courses, in which essays might be graded through crowd-sourcing, or assessed with natural-language software. Coursera will also offer free humanities courses in which grading will be done by peers.\nIn some ways, the new partnerships reprise the failed online education ventures of a decade ago. Columbia University introduced Fathom, a 2001 commercial venture that involved the University of Chicago, the University of Michigan and others. It lost money and folded in 2003. Yale, Princeton and Stanford collaborated on AllLearn, a nonprofit effort that collapsed in 2006.\nMany education experts are more hopeful about the new enterprises.\n''Online education is here to stay, and it's only going to get better,'' said Lawrence S. Bacow, a past president of Tufts who is a member of the Harvard Corporation. Dr. Bacow, co-author of a new report on online learning, said it remained unclear how traditional universities would integrate the new technologies.\n''What faculty don't want to do is just take something off the shelf that's somebody else's and teach it, any more than they would take a textbook, start on Page 1, and end with the last chapter,'' he said. ''What's still missing is an online platform that gives faculty the capacity to customize the content of their own highly interactive courses.''\n","683":"Everything's coming up robots.\n Whether they're the sylphlike humanoids of \" Ex Machina ,\" the disembodied voice of \" Her \" or just   uncomfortably ethical self-driving cars  , the advent of the intelligent machines has never seemed closer. It remains to be seen, however, whether their arrival will bode well or ill. \n The term \"artificial intelligence\" was coined in 1955 by the computer scientist John McCarthy, who defined it as the science of making intelligent machines. Today, the term has shifted slightly to also describe the kinds of intelligence exhibited by machines or software. Currently, our most advanced forms of AI are of \"narrow intelligence\" - geared toward solving particular tasks, whether playing Jeopardy or   searching the Internet  . \u00a0\n The next step, however, will be toward a \"general intelligence,\" sometimes referred to as human-level or strong AI. This would be a machine with intellectual capability equivalent to that of a person - one capable of interacting with its environment, learning and eventually making its own decisions. \n From there, based on exponential computing principles such as Moore's law, the assumption is that such an intelligence could quite easily iterate on itself - applying technology to improve its own intelligence, creating a positive feedback loop that would very quickly lead to   super  intelligence - described by Oxford philosopher and   leading AI thinker Nick Bostrom   as \"an intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.\" \n Computing optimists estimates say a true strong AI will emerge between 2029 and 2040, with almost all AI researchers predicting that the first machine with human-level intelligence will arrive at least before the century's end. Facebook has already developed  face recognition software,  and   speech-recognition technology   is reaching new heights.   Earlier this year  , Google announced that it had created an algorithm with a human-like ability to learn, a breakthrough in the field of AI. \n Although there is still vigorous debate as to how close we actually are to a superintelligent AI (or, indeed, if such a thing is even truly possible), the major worry is that we have no idea what such an intelligence might look like or do. Some, like futurist Ray Kurzweil, would suggest that a human-level or superintelligent AI could help us find solutions to global problems such as climate change, disease, or even our own mortality. Others,   like Elon Musk  , say that its existence would be an   existential risk   - an innovation which could lead to our extinction if, for example, our existence got in the way of the AI's pursuit of its goals, which at a certain point would be beyond our control. \n These questions may sound far-fetched, but within the last several years top scientists and industry leaders have warned of the catastrophes that could be unleashed if machines were granted the power to think. In an interview with the BBC   last year  , Stephen Hawking warned that \"the development of full artificial intelligence could spell the end of the human race.\" Bill Gates, Steve Wozniak, Noam Chomsky and others   have expressed grave concern   about the potential risks of artificial intelligence. \n As we continue our progress into an increasingly computerized world, should we be worried about the prospect of superintelligent machines? Should we be preparing? How best can we approach the risks of technology, and how can we predict what's coming next? \n                               Over the next few days, we'll hear from:                         \nPatrick Lin, director of the Ethics + Emerging Sciences Group\u00a0at California Polytechnic State University,\nDileep George, co-founder of Vicarious, an artificial intelligence\u00a0company, and a neuroscience researcher at Redwood Neuroscience Institute,\nFrancesca Rossi,\u00a0professor of computer science at the University of Padova and scientific advisor at the Future of Life Institute,\nNick Bostrom,\u00a0philosopher at the University of Oxford and\u00a0founding director of the Future of Humanity Institute,\nAri Schulman,\u00a0senior editor at\u00a0The New Atlantis,\nMurray Shanahan, robotics professor at Imperial College London.\n","684":"  There may be no reasoning with the boss in 30 years - because the chief executive could be a robot. \n  The tech disruption will reach into the C-suites at major companies over the next three decades, Alibaba founder Jack Ma is predicting. \u00a0\n  \"In 30 years, a robot will likely be on the cover of Time Magazine as the best CEO,\" according to Ma. \n  The billionaire investor is predicting a robot revolution because the machines are quicker and more rational than humans - and don't get bogged down with emotions. \n  Companies that fail to integrate cloud computing and artificial intelligence with their workforce and business models can expect to be hit the hardest, Ma said in a speech in China on Monday. \n  \"Social conflicts in the next three decades will have an impact on all sorts of industries and walks of life,\" Ma, chairman of Alibaba, the China-based e-commerce giant, said at an entrepreneurship conference in Zhengzhou. \n  Leaders who can't comprehend that should find young people in their companies to explain it to them, he said. \n  Ma added that, to prevent social upheaval, the world must change its approach to education and establish ways to work with robots to soften the blow of automation. \n  It was an unusually gloomy prediction from Ma, who typically likes to stay upbeat about the future as he builds Alibaba, the online powerhouse that sometimes is called the \"Amazon of China.\" \n  \"Fifteen years ago, I gave speeches 200 or 300 times reminding everyone the internet will impact all industries, but people didn't listen because I was a nobody,\" he said. \n  Nevertheless, Ma also sounded a hopeful note about the relationship between humans and robots in the long run - in contrast to Silicon Valley honchos like Elon Musk, who have warned that artificial intelligence poses a threat to humanity. \n  \"Machines will do what human beings are incapable of doing,\" Ma said. \"Machines will partner and cooperate with humans, rather than become mankind's biggest enemy.\" \n  rmorgan@nypost.com \n","685":"Stephen Hawking says he no longer feels welcome in the United States, now that President Trump is in power.\nThe renowned scientist and\u00a0theoretical physicist spoke Monday\u00a0on\u00a0ITV's \"Good Morning Britain\"\u00a0about his views on artificial intelligence, climate change and Trump - whom he\u00a0once called a\u00a0demagogue.\n\"The reaction to the election of Donald Trump may have been overdone, but it represents a definite\u00a0swing to a right-wing, more authoritarian approach,\" Hawking said about the\u00a0negative response to Trump's election.\u00a0\nOf the United States, he said: \"I have many friends and colleagues there, and it is still a place I like and admire in many ways. But I fear that I may not be welcome.\"\nLeading up to the U.S. presidential election,\u00a0Hawking was cheekily asked\u00a0on\u00a0\"Good Morning Britain\"\u00a0whether he could\u00a0explain Trump's\u00a0rise to presidential candidacy.\n\"I can't,\" Hawking said last May. \"He's a demagogue who seems to appeal to the lowest common denominator.\"\nAsked now about that statement, Hawking elaborated on\u00a0Trump's victory and his first two months in office.\n\"Trump was elected by people who felt disenfranchised by the governing elite in a revolt against globalization,\" Hawking said Monday. \"His priority will be to satisfy his electorate, who are neither liberal nor that well-informed. We have already seen this in the promise to build a wall along the Mexican border and the sanctioning of two oil pipelines and the appointment to\u00a0the Environmental Protection Agency of Scott Pruitt, a man who does not believe carbon dioxide causes climate change.\"\nIndeed, Hawking was\u00a0especially critical of\u00a0Pruitt's appointment.\nWhen the British journalist Piers Morgan then asked what he most wanted to say to Trump, Hawking said he would tell the president that Pruitt should be replaced as the EPA chief, adding that\u00a0\"climate change is one of the great dangers we face, and it's one we can prevent.\"\nHawking also expressed concerns about Trump's travel ban, calling it \"inefficient\" and explaining how it\u00a0should be done.\n\"His travel ban brands as Islamic State terrorists all citizens of six mainly Muslim countries, but not including America's allies such as Iraq, Saudi Arabia and Qatar, which allegedly help finance Islamic State,\" he said. \"This blanket ban is inefficient and prevents America recruiting skilled people from these countries. To be effective, it should be replaced by a more selective, intelligence-based approach.\n\"But again, I fear this may not happen, as Trump continues to appease his electorate.\"\nDuring the 11-minute interview,\u00a0Hawking was asked for his opinion on a wide range of topics, including the true meaning of life, gender equality and artificial intelligence,\u00a0a topic about which he has expressed grave concerns.\n\"Ever since the start of the industrial revolution, there have been fears of mass unemployment, as machines replaced humans,\" he said Monday. \"Instead, a demand for goods and services has risen in line with the increased capabilities. Whether this can continue indefinitely is an open question, but there is a greater danger from artificial intelligence if we allow it to become self-designing, for then it can improve itself rapidly, and we may lose control.\"\nHawking called himself a feminist, saying: \"I have always supported women's rights.\"\nMorgan brought up Britain's most powerful people - many of whom are women, including Queen\u00a0Elizabeth,\u00a0Prime Minister\u00a0Theresa May, politicians\u00a0Nicola Sturgeon and\u00a0Amber Rudd, and\u00a0Cressida Dick, commissioner of the Metropolitan Police Service.\nMorgan asked Hawking whether this was\u00a0scientific evidence of gender equality.\n\"It is not scientific proof of gender equality that is required, but general acceptance that women are at least the equals of men or better. This is coming,\" Hawking replied. \"If we factor in high-powered women in Europe as well, such as Angela Merkel, it seems we are witnessing a seismic shift for women to accede to high-level positions in politics and society.\u00a0But there may still be a gap between those women achieving high public status and those in the private sector. I welcome these signs of women's liberation.\"\nAnd he talked about his ambitions, saying he would be thrilled\u00a0to one day travel in space.\n\"I have already completed a zero-gravity flight, which allowed me to float weightless, but my ultimate ambition is to fly into space,\" Hawking\u00a0said. \"I thought no one would take me, but Richard Branson has offered me a seat on Virgin Galactic, and I said yes immediately. Since that day, I have never changed my mind.\"\nBut when asked perhaps the most obvious question - whether Hawking agrees with many others that he may indeed be the most intelligent person alive - he said,\u00a0\"I would never claim this.\"\n\"People who boast about their IQ are losers,\" he added.\n              Read more:           \n           What Stephen Hawking gets right and wrong about 'the most dangerous time for our planet'        \n           Stephen Hawking just gave humanity a due date for finding another planet        \n           Why Stephen Hawking believes the next 100 years may be humanity's toughest test        \n           Highlights from Stephen Hawking's Reddit AMA: 'Women' are the most intriguing 'mystery'        \n","687":"Some terrifying news out of Germany: A robot grabbed a worker and crushed him to death.\nVolkswagen said that a 22-year-old contractor at a plant in Baunatal, near Frankfurt, was setting up a stationary robot Monday when it grabbed him and crushed him against a metal plate,\u00a0according to the AP.\u00a0\n              [Military push for emergency robots worries skeptics about lethal uses]           \nThe robot is usually programmed by humans to perform discrete tasks in the assembly process, but this time, something went wrong. A second factory worker was in the area when the incident occurred and was unharmed.\nAccording to VW spokesman Heiko Hillwig, the incident can probably be blamed on human error, based on the company's initial investigation.\n\"It normally operates within a confined area at the plant, grabbing auto parts and manipulating them,\" Hillwig said.\n              [Should the world kill killer robots before it's too late?]           \nIf true, that would be reassuring, because this sort of scenario - a robot turning on its human overlords - ranks among the top fears of some of the world's smartest people.\nJust ask Bill Gates\u00a0and Stephen Hawking, who have been sounding the alarm about artificial intelligence. Just this week, an Elon Musk-backed group awarded nearly $7 million for research on the risks of A.I.\nIn this case, the robot was probably just doing what it was programmed to do by a human who made a mistake.\nAccording to the German news agency DPA, prosecutors are still considering whether to file charges -and if they do, against whom ... or what.\n              Read more:              \u00a0           \n How to punish robots when they inevitably turn against us \n Bill Gates on dangers of artificial intelligence: 'I don't understand why some people are not concerned' \n Elon Musk's nightmare: A Google robot army annihilating mankind \n Why you should root for a robot to take your doctor's job \n Apple co-founder on artificial intelligence: 'The future is scary and very bad for people' \n","688":"In January 1976, Ray Kurzweil introduced the Kurzweil Reading Machine, a breakthrough system that could photograph a book (with Kurzweil's flat-bed scanner), recognize the text (with Kurzweil's omnifont character-recognition technology) and speak the text (with Kurzweil's speech-synthesis software). Fifteen years later he struck gold again, this time a program that could turn natural speech into text. Today a descendant of that technology is Apple's voice-recognizing Siri. Clearly Kurzweil knows inventions: In 1999 President  Bill Clinton awarded him the National Medal of Technology.\nThese days, Kurzweil is better known as a futurist. Starting with his 1990 book, \"The Age of Intelligent Machines,\" he has delivered books and lectures explaining artificial intelligence, predicting the development of computers that are smarter than humans, and dispensing diet and health advice so his followers can live long enough to have their brains mapped and uploaded to some Great Computer in the Cloud. To quote the title of his  2005 book, our goal - realizable in 25 to 50 years -  should be to \"Live Long Enough to Live Forever.\"   \u00a0\nRealizing these predictions requires that science deliver a computer that can think. That's the premise that Kurzweil sets out to prove in his latest effort, \"How to Create a Mind.\" He argues that the brain's fundamental building block for intelligence has been discovered by neuroscientists, that the algorithm for intelligence has been observed in nature and independently invented by artificial intelligence researchers, and that the steady progress of Moore's Law will produce a computer fast enough to simulate an entire human brain by 2020. That wish is ultimately an appeal for a continuation of technological progress - humanity should create an intelligent machine unless something unforeseen stops us from doing so.\nKurzweil is at his best when he presents the reader with his \"thought experiments on thinking.\" For example, he asks you, the reader, to recite the alphabet. Next he suggests that you recite the alphabet backward. Most people can easily do the first but have a hard time with the second. This proves, he writes, that memories are stored as sequences of patterns that can be accessed only in the order in which they are remembered. Kurzweil presents similar experiments that he says establish that knowledge is stored in the brain as a series of hierarchical patterns, and that much of what we call \"thinking\" is really just pattern-matching and pattern-synthesizing. Of course, these simple thought experiments don't really prove anything, but they are entertaining. \nThe next two chapters present Kurzweil's misnamed \"Pattern Recognition Theory of Mind (PRTM)\" and delve into the anatomy of the human brain. PRTM is not a theory because it can't be tested. For example, Kurzweil argues that neuroplacticity, the ability of one part of the brain to take on the functions of another that's damaged, implies that different parts of the brain must use \"essentially the same algorithm\" to perform their computations. He then cites some recent neurological research to argue that this algorithm must run on some kind of neural \"module,\" which he says consists of about 100 neurons, and that there are roughly 300 million of these modules in each of our brains. That's too big a conceptual jump for many of Kurzweil's detractors, who say that the brain is likely to have many more secrets and algorithms than the ones Kurzweil describes. Over the next three decades we'll see who is right.\nLater chapters discuss scientists who are working to simulate a brain, briefly retell the history of computer science and present critiques of artificial intelligence from some of the field's greatest detractors. It's an eclectic collection, perhaps better suited to a dinner party or a TED talk than a scholarly effort; it's also a bit disorganized. The arguments about the nature of consciousness are interesting, although Kurzweil has presented many of them before. His recipe for creating a mind, then, is to build something that can learn and then give it stuff to learn. That, after all, is what parents do when they conceive and raise children. But this is not \"the secret of human thought\" that Kurzweil promises in the book's subtitle.\nSadly, Kurzweil's in-book autobiography, repeated mention of his company's products and snipes at his detractors come off as blatant self-promotion. This book would have benefited from a strong edit - perhaps in a few years there will be a program that Kurzweil trusts to critique his work. As it stands, much of the warmth and humanitarianism that are so evident in his talks are lost in this written volume. \n             bookworld@washpost.com          \nSimson Garfinkel\n writes and researches information technology. He is the author of 14 books, including \"Architects of the Information Society: Thirty-Five Years of the Laboratory for Computer Science at MIT.\"\nHOW TO CREATE A MIND\nThe Secret of Human Thought Revealed\nBy Ray Kurzweil\nViking. 336 pp. $27.95\n","689":"What's a 10-letter word for smarty pants?\nThis weekend the world may find out when computer technology again tries to best human brains, this time at the American Crossword Puzzle Tournament in Brooklyn.\nComputers can make mincemeat of chess masters and vanquish the champions of ''Jeopardy!'' But can the trophy go to a crossword-solving program, Dr. Fill -- a wordplay on filling in a crossword and the screen name of the talk show host Dr. Phil McGraw -- when it tests its algorithms against the wits of 600 of the nation's top crossword solvers?\nDOCTOR FILL was created by Matthew Ginsberg, 56, who holds a Ph.D. from Oxford, taught at Stanford and wrote a book on artificial intelligence. The program has already excelled in most simulations of 15 past tournaments, finishing on top three times. It can complete easier puzzles in a minute; even lightning-fast human solvers take about three minutes. Hard puzzles may take three minutes, about half as long as human whizzes.\u00a0\nBut humans and machines play the games very differently. Humans recognize patterns based on accumulated knowledge and experience, while computers make endless calculations to determine the most statistically probable answer.\n''We're at the point where the two approaches are about equal,'' said Peter Norvig, an artificial intelligence expert and Google's research director. ''But people have real experience. A computer has a shadow of that experience.''\nAlso, people tend to have a sense of humor. This helps.\nPuzzle constructors sometimes put in answers not found in the dictionary. For example, in a puzzle with the theme of rabbits, the answer to famous bank robbers might be BUNNY AND CLYDE, Dr. Ginsberg said, which requires a little imagination.\nOr take this clue from a 2010 puzzle in The Times: Apollo 11 and 12 (180 degrees). The answer is SNOISSIWNOOW, seemingly gibberish. A clever human could eventually figure out that those letters when rotated 180 degrees spell MOON MISSIONS.\nThis sort of thing requires imagination and creativity. Humans get the joke, while a literal-minded computer does not. ''Occasionally, Dr. Fill just doesn't get it,'' Dr. Ginsberg said. ''That's my nightmare.''\nWhatever Dr. Fill's final ranking at the Brooklyn matchup, which ends on Sunday, the program is an impressive achievement, experts say, and a sign of the times. In cerebral games, like chess, bridge, ''Jeopardy!'' and crossword puzzles, computers can now perform comparably to the top tier of humans -- sometimes a bit better, but also sometimes a bit worse.\nAt the tournament, players will get six puzzles to solve on Saturday, and one on Sunday -- progressively more difficult. Rankings are determined by accuracy and speed. The top three finishers enter a playoff with an eighth puzzle on Sunday afternoon, competing for the $5,000 prize. Game challenges are not just fun and games, but serious science that has opened the door to practical applications.\n''Games are a great motivator for artificial intelligence -- they push things forward,'' said David Ferrucci, the I.B.M. researcher who led the development of Watson, the ''Jeopardy!'' computer champion. ''But what really matters is where it is taking us.''\nWatson, for example, is being adapted for business uses, first in health care to assist doctors in making diagnoses.\nDr. Ginsberg's real job is chief executive of On Time Systems, in Eugene, Ore., whose software, used by the United States Air Force, helps in tasks like calculating the most efficient flight paths for aircraft. Some of the statistical techniques in this work are also handy, it turns out, for solving crossword puzzles.\nA typical puzzle might have 75 words, and up to 10,000 words in the dictionary with the same number of letters as each word in the space, down or across, for the answer. To narrow its choices, Dr. Fill taps a database of millions of answers and clues. If it spots a match, that is a sure thing.\nIf not, Dr. Fill calculates the 100 most probable answers, based on a number of factors, including how prevalent one of its millions of crossword-related words is in Google's directory of the Web.\nDr. Fill can fill in a puzzle in as little as five seconds, but then the program does fit and finish work.\nFor example, its initial best guess for a five-letter word across might be BEZEL, Dr. Ginsberg explained. The Z, though, might conflict with a higher-probability answer in a crossing word, going down, which would put W in that space. So Dr. Fill would change BEZEL to JEWEL.\nHow smart is Dr. Fill really?\n''On the easier puzzles, I think Dr. Fill will kill the field,'' said Will Shortz, the tournament director and the crossword puzzle editor for The Times, who has seen a demonstration of Dr. Ginsberg's program\nThe real hurdle for Dr. Fill, and perhaps its comeuppance, will come from the harder puzzles, especially those with the tricky themes or wordplay.\nDr. Fill was flummoxed by a puzzle from a previous tournament that had the theme of spoonerisms -- the switching of first letters in two words. So a clue might be heavy mist, and a logical answer would be LIGHT RAIN. But spoonerized, it becomes RIGHT LAIN.\nAn expert human solver, Mr. Shortz said, would ''slap your head and say, 'Oh, now I get it.''' Not so for Dr. Fill, a bundle of computer code on a notebook computer. ''It was totally adrift,'' lamented Dr. Ginsberg, whose hobby is constructing crossword puzzles, including more than two dozen published in The New York Times.\nDan Feyer, an ace solver who has won the last two tournaments, is betting that Mr. Shortz, who commissions and edits the puzzles, will include one with a quirky twist to try to stump the computer.\nMr. Shortz isn't saying. But he is handing out buttons to anyone who trounces the computer: ''I Beat Dr. Fill.'' And he is making sure that even if Dr. Fill wins, he will not taste all the fruits of victory. The machine is not eligible for the $5,000 prize.\n''The tournament is for humans,'' Mr. Shortz said.\n","691":"The future is obvious. It's the past that seems mysterious and alien.\nTelevision is beguiled by history, and not just chapter headings like World War II or the taming of the Wild West. Some of the more daring series right now explore recent epochs when cataclysm was muffled, and change rippled beneath the currents: the social combustion of the '60s on ''Mad Men,'' the demystification of sex on ''Masters of Sex,'' the Cold War on ''The Americans,'' the development of the personal computer on ''Halt and Catch Fire'' and, later this month, the building of the atom bomb on ''Manh(A)ttan'' (the project, not the borough).\nSo ''Extant,'' a sleek science-fiction series on CBS, starring Halle Berry as an astronaut named Molly Woods, seems almost old-fashioned. The show doesn't look backward to a younger, more confident age of space exploration, and it doesn't look forward to a post-apocalyptic wasteland.\u00a0\nMolly lives in a near future that seems a lot like the present 2.0: driverless cars, tablets, \u00fcber-recycling trash compactors and bathroom mirrors that turn into television screens with one tap. Clothes haven't changed, but hairstylists have lost their bearings: Molly sports a pixie cut with winged bangs that dip dangerously over one eye.\nThe most noticeable improvement is in artificial intelligence. Robots are so well engineered that they look and act like humans, and can even be programmed to behave as children. And that could be a blessing: There isn't a mother alive who hasn't at some point wished she could power down her toddler.\nBut fertility problems still exist, which is only one reason that Molly is gobsmacked to discover that despite her own inability to bear children, she has returned from a 13-month solo mission in space with an unexplainable ''Alien 3''-ish side effect: pregnancy.\nShe hasn't told her scientist husband, John (Goran Visnjic of ''E.R.''), who so yearned for a child that he designed a robot facsimile, Ethan (Pierce Gagnon), whom they raise as if he were a real boy. And Molly is afraid to tell her superiors at the multinational corporation that put her in space in the first place.\nThe creators deserve credit for trying to revive this kind of classic science-fiction mystery. It's hard to find new ways to reconfigure the Frankenstein myth. Movies like ''I, Robot,'' Steven Spielberg's ''A.I. Artificial Intelligence'' and television series like ''Battlestar Galactica'' have run the gamut of robots; machines were turning malevolent long before ''2001: A Space Odyssey.''\n''Extant'' is both suspenseful and quite silly, a paradox that may be explained by its provenance. The series is a collaboration between Mr. Spielberg's production company Amblin Television and CBS Television Studios, which previously teamed up to make ''Under the Dome,'' a CBS summer series that is now in a second, and full-on ridiculous, season.\n''Extant'' is more deft and sophisticated, and Halle Berry is a big star. But, as is the case with ''Under the Dome,'' the new series dilutes its own mystique with too many plodding plot devices and stock characters. CBS has made a lot of money by not overestimating its viewers: The promos on television and the Internet reveal pretty much everything important about the first episode, and that premiere ends with too few things left to the imagination.\nChildren can almost never be trusted on this kind of fantasy thriller, so it's not surprising that the series early on suggests that Molly's sweet-faced little boy has a bad seed mixed in with his ersatz snips and snails and puppy dogs' tails. (If scientists wanted to make a really lovable robot, they would make one like R2-D2 or the unnamed robot in ''Lost in Space.'' Robots should be robots. It's when they're designed like humans that inhumanity slips in.)\nThemes are heavily underlined. On ''Extant,'' scientists try to make robots as human as possible; greedy business tycoons dream of turning humans into robots. Conspiracies lurk in all the likely places, and, sure enough, someone is bound to warn that it's not safe to trust anyone.\nIt would be great if all those broad hints were misdirection, and things that are not as they seem proved to be genuinely different. But the premiere, while entertaining and expertly produced, doesn't hold out a lot of promise.\nToo often, the imagined unknown turns out to be more predictable than even the best remembered history.\nExtant\nCBS, Wednesday nights at 9, Eastern and Pacific times; 8, Central time.\nProduced by CBS Television Studios and Amblin Television. Steven Spielberg, Greg Walker, Mickey Fisher, Justin Falvey, Darryl Frank and Brooklyn Weaver, executive producers.\nWITH: Halle Berry (Molly Woods), Goran Visnjic (John Woods), Pierce Gagnon (Ethan Woods), Hiroyuki Sanada (Hideki Yasumoto), Michael O'Neill (Alan Sparks), Camryn Manheim (Sam Barton) and Grace Gummer (Julie Gelineau).\n","692":"Artificial intelligence will solve Facebook's most vexing problems, chief executive Mark Zuckerberg insists. He just can't say when, or how.\nZuckerberg referred to AI technology more than 30 times during ten hours of questioning from\u00a0congressional lawmakers Tuesday and Wednesday, saying that it\u00a0would one day be smart, sophisticated and eagle-eyed enough to fight against a vast\u00a0variety of platform-spoiling misbehavior, including fake news, hate speech, discriminatory ads and terrorist propaganda.\nOver the next\u00a0five to 10 years, he said, artificial intelligence would prove a champion for the world's largest social network in resolving its most pressing crises on a global scale - while also helping the company dodge pesky questions about censorship, fairness and human moderation.\n\"We started off in my dorm room with not a lot of resources and not having the AI technology to be able to proactively identify a lot of this stuff,\"\u00a0Zuckerberg\u00a0told the lawmakers, referring to Facebook's famous origin\u00a0story. Later in the hearing, he added\u00a0that \"over the long term, building AI tools is going to be the scalable way to identify and root out most of this harmful content.\"\u00a0\nBut Facebook's AI technology can't do any of those things well yet, and it's unclear when, if ever, it will be able to. Tech experts had a different opinion on why Zuckerberg spent so much time offering tributes to the much-hyped but largely unproven tech\u00a0advancement: The shapeless technology could help the company pawn off blame from the humans creating it.\n\"AI is Zuckerberg's MacGuffin,\" said James Grimmelmann, a law professor at Cornell Tech, using the film term for a mostly insignificant plot device that comes out of nowhere to move the story along. \"It won't solve Facebook's problems, but it will solve Zuckerberg's: getting someone else to take responsibility.\"\nTo many of us, AI\u00a0is the amorphous super-technology of science fiction, animating friendly computers and killer robots. Silicon Valley leaders often\u00a0promote that vision of AI by suggesting it will serve humanity as a quasi-magical force for good. But today's\u00a0artificial intelligence is being used in far more basic forms: driving cars, tracking cows and giving a voice to virtual assistants such as Siri and Alexa.\nFacebook uses AI in understated but important ways, such as for recognizing people's faces for tagging photos and using algorithms to decide placement of ads or News Feed posts to maximize users' clicks and attention. The company is also expanding its use, such as by scanning posts and suggesting resources when the AI\u00a0assesses\u00a0that a user is threatening suicide.\nFacebook has pointed to early AI successes in detecting problematic content. The company has said that advances in AI have helped it remove thousands of fake accounts and \"find suspicious behaviors,\" including during last year's special Senate race in Alabama, when AI helped spot political spammers from Macedonia, a hotbed of online fraud.\nFacebook, Zuckerberg said Tuesday, has also been \"very successful\" at deploying AI to police against terrorist propaganda. \"Today, as we sit here, 99 percent of the ISIS and al-Qaeda content that we take down on Facebook, our AI systems flag before any human sees it,\" he said. (Nonprofit groups such as the Counter Extremism Project have argued that Facebook has exaggerated its achievement and failed to crack down on well-known Islamist extremists.)\nZuckerberg said he was optimistic that Facebook's AI would, within five\u00a0to 10 years, be able to comprehend the \"linguistic nuances\" of content with enough accuracy to flag potential risks.\u00a0But those limited cases, experts said, were\u00a0helped by geography and required human moderators to make the final ruling. Real-world cases of violent videos, hate speech and dangerous content - the ones plaguing Facebook every day - are much more subtle, widespread and difficult to police.\n\"AI is extremely far off from being able to understand social context and nuance,\" Grimmelmann said. \"Even humans have a hard time distinguishing between hate speech and a parody of hate speech, and AI is way short of human capabilities.\"\nZuckerberg both understated the problem and overstated AI's abilities, experts said. For example, Facebook's AI has been deemed technically incapable of spotting discriminatory housing ads, which violate the federal Fair Housing Act and were a problem on the social network until the site changed its policy late last year.\nThe limitations of AI go much further than Facebook, experts say. No AI on the market is trained well enough to understand the social dimensions and verbal eccentricities of human speech, slang and dialect. (Anyone who uses Siri can attest to\u00a0that.)\nThe worst-kept secret to Silicon Valley's AI push, experts said, has been the tech industry's army of human moderators. Those often-low-wage contract workers spend their days screening posts for offensive or disturbing content, indirectly helping train the\u00a0artificial intelligence program\u00a0on what problems and patterns to look for.\nFacebook, Google and other tech giants are ramping up their content-moderation staffing, and Zuckerberg said his company aims to have more than 20,000 people working on security and content review by the end of the year. Today's AI, experts said, is still miles away from a responsible alternative to a human looking at a screen.\nFacebook's plan is \"continuing to grow the people who are doing review in these places with building AI tools, which - we're working as quickly as we can on that, but some of this stuff is just hard,\" Zuckerberg said. \"That, I think, is going to help us get to a better place on eliminating more of this harmful content.\"\nRobyn Caplan, a researcher at the think tank Data & Society, said Zuckerberg's optimism seemed to clash with the more pragmatic conversations she's had with representatives from platforms\u00a0similar to Facebook, who have stressed that AI can help flag questionable content \"but cannot be trusted to do removal.\"\nAt a major\u00a0conference\u00a0on content-moderation in\u00a0February at the Santa Clara University law school, where executives from Facebook, Google and Reddit detailed their operations, Caplan said nearly everyone agreed that human moderators were still the reigning gatekeepers for information on the Internet's most popular sites.\n\"AI can't understand the context of speech and, since most categories for problematic speech are poorly defined [by necessity], having humans determine context is not only necessary but desirable,\" she said.\nNot every senator on Tuesday was so welcoming to Zuckerberg's AI-as-savior talking point, including Sen. Patrick J. Leahy (D-Vt.), who questioned whether Facebook's AI was already failing by not stemming the spread of hate speech during the\u00a0crackdown on\u00a0Rohingya in Burma, also called Myanmar.\n\"You say you use AI to find this,\"\u00a0Leahy said, pointing to a poster\u00a0that showed Facebook posts calling for the murder of Muslim journalists. \"That threat went straight through your detection systems. It spread very quickly, and then it took attempt after attempt after attempt, and the involvement of civil society groups, to get you to remove it. Why couldn't it be removed within 24 hours?\"\nZuckerberg said the company was working on hiring more Burmese-language content reviewers and deleting the accounts of \"specific hate figures.\" \"What's happening in Myanmar is a terrible tragedy, and we need to do more,\" he said.\nBut tech experts said AI could also create new problems. The same bad actors behind viral hoaxes and fake accounts may be able to use AI to evade Facebook's filters,\u00a0make fake videos or spearhead targeted harassment campaigns, they said.\nAI also \"can't solve political problems; it can't make people agree,\" Grimmelmann said. \"What's 'fake' news depends on who you ask: Kicking the question over to AI just means hiding value judgments behind the AI.\"\nThe real issue, experts said, is that the problems plaguing Facebook may be battles that no one can truly win. Instead of acknowledging that grim fact, though, Zuckerberg has taken to gesturing vaguely at the future - and at a\u00a0version of\u00a0AI that could eventually save the day.\nAs Ryan Calo, an assistant professor at the University of Washington law school, tweeted Tuesday, \" 'AI will fix this' is the new 'the market will fix this.' \"\n","693":"More than a year after entering the White House, President Trump still has not selected his top science and technology adviser, leaving unfilled a critical policy post that guides the administration on issues as varied as artificial intelligence, climate change and cancer research.\nWhile the White House maintains that it is unconstrained in its work - and has staffed up to tackle such challenges as closing the country's Internet-access gaps - the vacancy still troubles policy experts, who believe that Trump would be best served by someone who could double as an emissary to the academic and engineering worlds.\u00a0\n\"Symbolically, it signals science and technology is at the table in the administration's policymaking,\" said Kumar Garg, who was an innovation policy aide under President Barack Obama. \"But also substantively, because the science adviser is the principal who gets invited to senior strategy meetings ... on critical topics like biosecurity, cybersecurity, how do we make sure America remains competitive against China and Russia in emerging technology.\"\nTechnically, the White House has two major science and technology posts. The first is the director of the Office of Science and Technology Policy, which has a broad remit that includes coordinating the country's vast federal research budget. The second is the assistant to the president on science and technology, a title that is supposed to afford its bearer access to the person occupying the Oval Office. Often, those roles are held by the same person, and under Obama, the responsibilities fell to John Holdren for eight years.\nSince Holdren's departure, the posts have remained vacant. But Trump's aides have actively sought new leadership for months. One of the candidates under consideration is Kelvin Droegemeier, an expert in extreme weather and meteorology from the University of Oklahoma, according to three people familiar with the president's thinking but not authorized to speak on the record. The people said others are in the running.\nThe White House and the OSTP declined to comment for this article. Reached this week by email, Droegemeier declined to comment.\nUnder Trump, the White House generally has operated with a much smaller complement of science and technology staffers than it did under Obama. The OSTP counts about 50 aides today, compared with the roughly 130 Obama had tapped before leaving office. Some of that is by design, as Trump has sought to scale back the size of government. But some of it is the result of the president's at-times icy relationship with liberal-leaning Silicon Valley, where top technology executives and engineers - experts who could have formed his policy ranks - have become some of his biggest public critics.\nAmong Trump's tech policy advisers is Michael Kratsios, a former aide to venture capitalist Peter Thiel who joined the administration in its early days to serve as deputy chief technology officer. The White House has retained other policy staffers recently who focus on artificial intelligence and education, and at the OSTP, they haveadvanced some early initiatives that have satisfied tech giants in Silicon Valley and beyond. That includes efforts to speed up the arrival of 5G, the next generation of ultrafast, wireless Internet service for smartphones and other devices.\nAt the same time, critics have bristled as the president has proposed multiple budgets that wouldslash federal programsthat seek to combat global warming, which Trumpdescribed in 2013 as a \"hoax.\"And Trump's decision last year to withdraw the United States from a worldwide carbon-reduction treaty, the Paris climate accord, drew outrage from hundreds of business leaders in technology and other industries.\nFor that reason, veterans of the White House maintain that Trump needs a full-time, senior expert with the same sort of access afforded to his top aides in defense and economics.\n\"I'm happy they are staffing up,\" said Kei Koizumi, an assistant director at the OSTP under Obama who is a visiting scholar at the American Association for the Advancement of Science.\nBut, Koizumi added: \"I remain concerned for them to be fully effective. There'sno substitute for leadership that can back their work and make sure they are listened to.\"\nIf the White House proceeds with Droegemeier, it would add to its ranks a highly experiencedmeteorologistwith government experience. Hehas served as Oklahoma'ssecretary of science and technology, and he aided the U.S. government's National Science Board under Presidents George W. Bush and Obama.\nPublicly, Droegemeier hasadvocated for sustained federal research spending, even as Trump at times has pursued cuts. Meanwhile, Droegemeier has come to the defense of at least one of Trump's current nominees,Rep. Jim Bridenstine(R-Okla.), whose views on climate change prompted Democrats to block his nomination to lead NASA.\nDroegemeier could be nominated to run the OSTP, or he could be appointed to serve solely as assistant to the president for science and technology. The latter post, if it is uncoupled from the OSTP, doesn't require Senate confirmation. In that case, Trump might be spared periodic, high-profile showdowns with his Democratic critics on Capitol Hill, who are itching to challenge him on global warming and related debates.\nPreviously, the White House considered turning the OSTP over to Jim O'Neill, a Silicon Valley investor who also has ties to Thiel, according to two of the three people who spoke off the record. O'Neill is no longer in the running, they said.\n","695":"Bahran Dean Ansari has a message for millions of personal computer users sick of telling their computers what to do with slashes, semicolons and random letters: Tell them in English.\nAnsari, 25, has come up with a software program for IBM PCs and compatibles called DOSTalk that allows users to type MS-DOS commands in simple English sentences. The software translates the English sentence into its equivalent in MS-DOS, then executes the command.\n\"It's going to eliminate a giant headache,\" said Ansari, who sells the software through his Arlington company, Sak Technologies Inc. \"This thing will pay for itself in a day if used by attorneys\" who find, move, copy, compare or erase dozens of case files during regular business hours, he said. But the program, which costs $ 89.95, can be used by any PC user who wants to simplify PC use.\u00a0\nTo sign off the computer system, for example, users can type in their choice of words, such as \"exit,\" \"end,\" \"stop,\" or \"goodbye.\"\nIf a user gets too creative, telling the software to \"blast away\" a file, say, instead of to \"erase\" it, the software asks that the request be rephrased, flashing the message, \"What should I do with the file?\" on the computer screen.\n\"If the request is outside of a reasonable conversation it will let the user know what part it doesn't understand,\" Ansari said.\nAnsari, an Iranian who came to the United States in 1978 and is earning a graduate degree in computer-aided design and artificial intelligence from George Washington University, raised about $ 125,000 in seed money from a few friends to start the company and now is looking for venture capital.\nSo far, he has sold about 1,000 of the software packages to law firms, schools and other customers. He also is negotiating with area defense firms about integrating his software into complex computer systems.\nAnsari has been developing and refining the software for about five years, and recently started marketing it. \"The technology that I use here, the fundamental research was conducted at the Massachusetts Institute of Technology's artificial intelligence lab and at the Yale linguistics lab,\" he said. \"I wanted to develop a natural language understanding ... and I straightened out a few stumbling blocks,\" he said. \"It's software engineering.\"\nThe ability of a computer to understand a natural human language is just one branch of the field of artificial intelligence, which also includes attempts to give computers learning capabilities and the capacity to distinguish objects.\nAnsari's product is one of several efforts that have reached the market recently to make personal computers easier to use. International Business Machine Corp.'s PS\/2 computer system, introduced a year ago as the replacement for the IBM PC line, is designed to use a new line of operating software, OS\/2, which also communicates with users in English and symbols rather than \"computerese.\" The OS\/2 system owes a great deal, philosophically, to the operating system Apple Computer Inc. designed for its Macintosh personal computer, which makes heavy use of simple graphic symbols for basic commands.\n\"All the things on the Apple Macintosh are done with pictures that are easy to understand,\" said Richard A. Shaffer, a consultant at Technologic Partners, a New York company that follows fast-growing computer firms for institutional investors. \"That sort of thing will be done very soon with the IBM computers. ... Apple figured out one picture is worth a thousand DOS commands, and IBM is moving toward it as fast it can.\"\nBut while the OS\/2 system is expected to become an industry standard, Ansari believes there is plenty of room in the market for DOSTalk, especially among the tens of millions of users of IBM PCs and compatibles. OS\/2, he said, \"still requires a whole lot of reading on the user's part before the user understands how things connect to each other. ... DOSTalk removes the need of having to know how to make the system work.\" Ansari said DOSTalk could extend the life of MS-DOS as an operating standard and even challenge the acceptance of the OS\/2 operating system.\nComputer industry analysts are generally favorable to the concept behind Ansari's product. \"He can make a very profitable business out of it, quite probably,\" Shaffer said. \"There are many ideas which make good small businesses and never become large businesses.\"\nCollins Hemingway, a spokesman at Microsoft Corp., the Redmond, Wash.-based software giant that developed MS-DOS and OS\/2 and has a reputation for protecting its programs from modifications by outsiders, said the company is unfamiliar with DOSTalk, and in any case views a graphics-based approach to PC operating systems as the best course to follow.\n\"But it is also true that there are a million great ideas that no one company can meet and there is a large cottage industry in the PC world of people providing exactly these kinds of extremely helpful utilities,\" Hemingway said.\n\n","699":"In the last three months, as Uber has been scrutinized over its workplace culture and the behavior of its top executives, several high-ranking managers have left the ride-hailing company. At least one has been openly critical about the way the company has been run.\u00a0\nAmit Singhal \n  Joined: January 2017 | Left: February 2017\n  Mr. Singhal, a 15-year Google veteran, joined Uber as senior vice president to work on software and infrastructure. He was dismissed from the ride-hailing company for failing to disclose a sexual harassment claim that occurred during his Google tenure.\n  Self-Driving Initiative\n  Raffi Krikorian\n  Joined: March 2015 | Left: February 2017\n  Mr. Krikorian, who previously worked at Twitter, was a senior director of engineering at Uber's Advanced Technologies Center in Pittsburgh, which focuses on autonomous vehicles. He left several months after Uber brought in new leadership to its self-driving cars effort.\n  Ride-sharing Services\n  Jeff Jones\n  Joined: August 2016 | Left: March 2017\n  Mr. Jones was hired from Target to be Uber's president of ride-sharing, with a mandate to improve the company's relationships with drivers. He left after Uber began a search for a chief operating officer.\n  Mapping\n  Brian McClendon\n  Joined: June 2015 | Left: March 2017\n  Mr. McClendon, a former Google engineer, was a vice president at Uber who worked on mapping and autonomous vehicle technology initiatives. He left amicably to move to his home state of Kansas to explore politics.\n  Artificial Intelligence\n  Gary Marcus\n  Joined: December 2016 | Left: March 2017\n  Mr. Marcus joined Uber through an acquisition of his artificial intelligence company, Geometric Intelligence. He left three months after Uber began a new internal research arm on A.I.\n  Product Development\n  Ed Baker\n  Joined: September 2013 | Left: March 2017\n  Mr. Baker, a former Facebook executive, was hired as vice president of growth and later became head of product and growth. He resigned after the company began its internal investigation into workplace culture.\n  COMMUNICATIONS\n  Rachel Whetstone\n  Joined: May 2015 | Left: April 2017\n  Ms. Whetstone, who was at Google for a decade in charge of communications, joined Uber as senior vice president for policy and communications. She departed after tensions with Travis Kalanick, Uber's chief executive.\n\n\n\n","701":"Pop culture wants us to fear the artificially intelligent robot: The titular \"Terminator\" characters goes back in time to kill a mother and her child. Cylons of \"Battlestar Galactica\" fame destroy Earthly civilization and, bloodthirst not slaked, pursue the remnants of humanity through space. \"The Matrix\" begat two sequels and \"Jupiter Ascending.\"\nToday's artificial intelligence researchers are not, in fact, on the cusp of creating a doomsday AI. Instead, as IBM executive Guruduth Banavar recently told The Washington Post, current AI is a \"portfolio of technologies\" assigned to specific tasks. Such programs include software capable of defeating the world's best Go players, yes, but also isolated mundanities like the Netflix algorithm that recommends which sitcom to watch next.\u00a0\nSimply because artificially intelligent robots lack the capacity for world domination, however, does not mean that they are incapable of losing control. Computer experts at Google and the University of Oxford are worried about what happens when robots with boring jobs go rogue. To that end, scientists will have to develop a way to stop these machines. But, the experts argue, it will have to be done sneakily.\n\"It is important to start working on AI safety before any problem arises,\" Laurent Orseau, a researcher at Google's DeepMind, said in an interview with the BBC on Wednesday. Orseau and Stuart Armstrong, an artificial intelligence expert at the University of Oxford's Future of Humanity Institute, have written a new paper that outlines what happens when it becomes \"necessary for a human operator to press the big red button.\"\nIn their report, the duo offers a hypothetical scenario that could take place in a typical automated warehouse the world over. A company purchases a smart robot, one that improves its performance based on \"reinforcement learning\" (an AI teaching method akin to giving a dog a treat whenever it performs a trick.) The robot gets a big reward for carrying boxes into the warehouse from outside, and a smaller reward for sorting the boxes indoors. In this instance, it's more important for the company to have all of its merchandise inside, hence the bigger reward.\nBut the researchers throw a wet wrinkle into the situation: Perhaps the warehouse is located in an area where it rains every other day. The robot is not supposed to get wet, so whenever it ventures outside on a rainy day, humans shut it down and carry the machine back inside. Over time, if the robot learns that going outside means it\u00a0has a 50 percent chance of shutting down -\u00a0and, therefore, will get fewer\u00a0overall treats -\u00a0it may resign itself to only sorting boxes indoors.\nOr, as Orseau told the BBC: \"When the robot is outside, it doesn't get the reward, so it will be frustrated.\"\nThe solution is to bake a kill switch into the artificial intelligence, so the robot never associates going outside with losing treats. Moreover, the robot cannot learn to prevent a human from throwing the switch, Orseau and Armstrong point out. For the\u00a0rainy warehouse AI, an\u00a0ideal kill switch would shut the robot down instantly while\u00a0preventing it from remembering\u00a0the event.\u00a0The scientists' metaphorical big red button is, perhaps, closer to a metaphorical chloroform-soaked rag that the robot never sees coming.\nIf the paper seems to lean too heavily on speculative scenarios, consider the artificial intelligences that are already acting out. In March, Microsoft scrambled to reign in Tay, a Twitter robot designed to autonomously act like a teen tweeter. Tay began innocently enough, but within 24 hours the machine ended up spewing offensive slogans -\u00a0\"Bush did 9\/11,\" and worse -\u00a0after Twitter trolls exploited its penchant for repeating certain replies.\nEven when not being explicitly trolled, computer programs also reflect bias. ProPublica reported in May that a popular criminal-prediction software defaults to rate black Americans as higher recidivism risks than whites who committed the same crime.\nFor a more whimsical example, Orseau and Armstrong refer to an algorithm tasked with beating different Nintendo games, including \"Tetris.\" By human standards, the program turns out to be an awful \"Tetris\" player, randomly dropping bricks to rack up easy points but never bothering to clear the screen. The screen fills up with blocks -\u00a0but the program will never lose. Instead, it pauses the game for perpetuity.\nAs Carnegie Mellon University computer scientist Tom Murphy, who created the game-playing software, wrote in a 2013 paper: \"The only cleverness is pausing the game right before the next piece causes the game to be over, and leaving it paused. Truly, the only winning move is not to play.\"\nA robot that misbehaves like Murphy's rogue Tetris program could cause significant damage. Even when their tasks are as\u00a0mundane as\u00a0moving parts around a factory, robots that malfunction can be lethal: Last year, a\u00a022-year-old German man was crushed\u00a0to death by a\u00a0robot at a\u00a0Volkswagen plant, which apparently accidentally turned on (or was left on in error by a human operator) and mistook him for an auto part.\nTechnology analyst Patrick Moorhead told Computer World that now is the right time to build such a kill switch. \"It would be like designing a car and only afterwards creating the ABS and braking system,\" he said.\nReady the robo-chloroform.\n","702":" Technology is becoming more and more important in schools and education, but is there a limit to what computers should do in the classroom?  \nHow would you feel about a computer grading your essays?\u00a0\nIn \"New Test for Computers: Grading Essays at College Level,\" John Markoff writes about new software that many college professors, especially those teaching massively open online courses, or MOOCs, are likely to use.\nImagine taking a college exam, and, instead of handing in a blue book and getting a grade from a professor a few weeks later, clicking the \"send\" button when you are done and receiving a grade back instantly, your essay scored by a software program. \nAnd then, instead of being done with that exam, imagine that the system would immediately let you rewrite the test to try to improve your grade.\nEdX, the nonprofit enterprise founded by Harvard and the Massachusetts Institute of Technology to offer courses on the Internet, has just introduced such a system and will make its automated software available free on the Web to any institution that wants to use it. The software uses artificial intelligence to grade student essays and short written answers, freeing professors for other tasks. \nThe new service will bring the educational consortium into a growing conflict over the role of automation in education. Although automated grading systems for multiple-choice and true-false tests are now widespread, the use of artificial intelligence technology to grade essay answers has not yet received widespread endorsement by educators and has many critics.\n Students: Tell us...\n\n","703":"The challenge of knowing whether what you read online is true has gotten national attention. It is proving harder and harder to separate factually based writing from intentional misinformation or click-bait. This problem is about to get exponentially worse.\nWe should all be concerned about living in what Peter Horan, a long-time veteran of the Internet industry, describes as a \"post-truth society\" - a society where \"whether or not it has a foundation in fact, anything that is repeated often enough is believed to be true.\"\u00a0\nThe Internet has driven down the cost of producing content and distributing it broadly to effectively zero. It has also made the cost of reviewing written information for probity and integrity higher in comparison. Sadly, insuring that something is true requires more time and expense than making something up.\nUntil recently, the cost advantage that drives the ubiquity of false written information did not prevail for video images. A recent study by the Belfer Center at Harvard highlighted that since the invention of the photographic camera, \"the technology for capturing highly reliable evidence has been significantly cheaper and more available than the technology for producing convincing forgeries.\" This allowed people seeing images of a politician taking a bribe, a celebrity staring in a sex tape, or other sensational images, to be relatively comfortable that the depicted event actually occurred.\nLast week, a number of technology blogs alerted us to the proliferation of inexpensive video editing software that creates realistic pornographic videos of celebrities. As inexpensive video editing technology is combined with artificial intelligence, creating fake videos have become indistinguishable from reality.\nThis development will challenge our society. But, it should also create an economic opportunity for our region.\nOur largest regional industry, national security, sees the degradation of the fidelity of information as an existential security challenge. Chuck Howell, chief scientist for dependable artificial intelligence at the research contractor MITRE, points to an \"arms race over various kinds of deception\" attracting the attention of our national security establishment.\nMany of our largest private businesses - media, consulting, advertising and other highly skilled knowledge worker businesses - depend upon the reliability of information and a consequent shared reality. More than many others, our region's economy depends on informational expertise and having insights that are expensive to generate and correspondingly scarce. A world that does not share facts and informational integrity has no way to value informational experts. In that world, many of our businesses will suffer.\nTherefore, our region has a very stark choice to make. It can either accept that the erosion of the quality of information is an irreversible technological trend, as many national observers have suggested. Or it can meet the challenge and fight for information quality and objectively demonstrable facts.\nWhen I asked Kurt Roberts, chief innovation officer at RP3 Agency, about the threat posed by technology cheapening information, he told me we should \"flip the notion on its head\" and focus on using technology to help people know whether something is true or fake. This point was echoed by Adam Zuckerman, founder of the Fosterly community that promotes regional entrepreneurship. For him, technology is neither good nor bad. It just is a tool, and how we use it is an opportunity.\nAny entrepreneur will tell you that the biggest businesses are often built by solving society's biggest problems. We have the technologists and entrepreneurs to come up with new technology applications and the means to ensure that our society is based upon a shared reality and facts. We also have large customers that need that those solutions now more than ever.\nInformational integrity may be a political issue for all, but for our region it is also a large business opportunity.\n          Jonathan Aberman is a business owner, entrepreneur and founder of TandemNSI, an organization that connects innovators to government agencies. He is host of \"What's Working in Washington\" on the radio station WFED, a program that highlights business and innovation, and he lectures at the University of Maryland's Robert H. Smith School of Business.       \n","706":"BERKELEY, Calif. --  In an engineering laboratory here, a robot has learned to screw the cap on a bottle, even figuring out the need to apply a subtle backward twist to find the thread before turning it the right way.\nThis and other activities -- including putting a clothes hanger on a rod, inserting a block into a tight space and placing a hammer at the correct angle to remove a nail from a block of wood -- may seem like pedestrian actions. But they represent significant advances in robotic learning, by a group of researchers at the University of California, Berkeley, who have trained a two-armed machine to match human dexterity and speed in performing these tasks. \u00a0\n  The significance of the work is in the use of a so-called machine-learning approach that links several powerful software techniques that make it possible for the robot to learn new tasks rapidly with a relatively small amount of training.\n  The new approach includes a powerful artificial intelligence technique known as ''deep learning,'' which has previously been used to achieve major advances in both computer vision and speech recognition. Now the researchers have found that it can also be used to improve the actions of robots working in the physical world on tasks that require both machine vision and touch.\n  The group, led by the roboticist Pieter Abbeel and the computer vision specialist Trevor Darrell, with Sergey Levine, a postdoctoral researcher, and Chelsea Finn, a graduate student, said they were surprised by how well the approach worked compared with previous efforts.\n  By combining several types of pattern recognition software algorithms known as neural networks, the researchers have been able to train a robot to perfect an action such as correctly inserting a Lego block into another block, with a relatively small number of attempts.\n  ''I would argue this is what has given artificial intelligence the whole new momentum it has right now,'' Dr. Abbeel said. ''All of a sudden there are all of these results that are better than expected.''\n  Roboticists said that the value of the Berkeley technology would be in quickly training robots for new tasks and ultimately in developing machines that learn independently.\n  ''It used to take hours on up to months of careful programming to give a robot the hand-eye coordination necessary to do a task,'' said Gary Bradski, a roboticist and computer vision specialist who founded OpenCV, a freely available software library for machine vision. ''This new work enables robots to just learn the task by doing it.''\n  Previously, the Berkeley lab had received international attention for training a robot to fold laundry. Although it was viewed almost one million times on YouTube, the laundry-folding demonstration noted that the video had been sped up more than 50 times. The new videos show the robots performing tasks at human speeds.\n  Despite their progress, the researchers acknowledge that they are still far away -- perhaps more than a decade -- from their goal of building a truly autonomous robot, such as a home worker or elder care machine that could perform complex tasks without human supervision.\n  The researchers said that while their new approach represents an important leap, it is also fragile. For example, the bottle cap-threading technique will work reliably when the bottle is moved from one location to another or if the bottle is of a different color. But if the bottle is tilted at an angle before it is picked up, the robot will completely fail.\n  ''There is nothing better to ask a roboticist, 'If you change the conditions, will it still work?' '' Dr. Abbeel said.\n  To explain the new approach, the researchers draw the analogy of how baseball players track and then catch balls. Humans do not do mathematical calculations to discern the trajectory of the ball. Rather, they fix the ball in their field of vision and adjust their running speed until they arrive at the spot where the ball lands.\n  This, in effect, short-circuits a complicated set of relations between perception and motion control, substituting a simple technique that works in a wide variety of situations without having to worry about details like wind resistance or the ball's velocity.\n  Until now, robots have generally learned with a variety of techniques that are laboriously programmed for each specific case. The Berkeley researchers, who will present their results in a paper at the IEEE Robotics and Automation Society's conference next week in Seattle, instead connected the neural networks, which learn from both visual and sensory information, directly to the controller software that oversees the robot's motions. As a result, they achieved a significant advance in speed and accuracy of learning.\n  ''We are trying to come up with a general learning framework that allows the robot to learn new things on its own,'' Dr. Abbeel said.\n  The advance underscores the rapid impact that the deep-learning approach has had on the field of artificial intelligence. Pioneered several decades ago by a small group of cognitive scientists, the techniques were blended in 2012 with the ''big data'' power offered by cloud computing systems. Researchers were then able to capture billions of images or samples of human language. Their software was able to show rapid progress in accuracy in recognizing objects and in understanding human speech.\n  Now computer scientists are pushing the techniques in new directions, including self-driving cars and a host of other applications. In December 2013, Deepmind, a British start-up, first demonstrated deep-learning techniques that could be used to play video games with more skill than most human players. The company, which Google acquired for an undisclosed sum in 2014, published a paper describing its advance in the journal Nature in February.\n\n\n\n","708":"                               Each week,  In Theory \u00a0takes on a big idea in the news and explores it from a range of perspectives. This week we're talking about robot intelligence.\u00a0Need a primer? Catch up here.                         \n                Patrick Lin                  is an associate philosophy pro                fessor at California Polytechnic State University and an affiliate scholar at Stanford Law School's Center for Internet and Society. He works with government and industry on technology ethics, and his book \"                 Robot Ethics                 \"\u00a0was published in 2014.            \n Forget about losing your  job   to a robot. And don't worry about a\u00a0  super-smart  , but somehow evil, computer. We have more urgent ethical issues to deal with right now. \u00a0\n\u00a0Artificial intelligence is replacing human roles, and it's assumed that those systems should  mimic   human behavior - or at least an idealized version of it. This may make sense for limited tasks such as product assembly, but for more autonomous systems\u00a0-\u00a0robots and AI systems that can \"make decisions\" for themselves\u00a0-\u00a0that goal gets complicated. \n There are two problems with the assumption that AI should act like we do. First, it's not always clear how we   humans   ought to behave, and programming robots becomes a soul-searching exercise on ethics, asking questions that we don't yet have the answers to. Second, if artificial intelligence does end up being more capable than we are, that could mean that it has different moral duties, ones which require it to act differently than we would.  \n Let's look at robot cars to illustrate the first problem. How should they be programmed? This is important, because they're driving alongside our families right now. Should they always obey the law? Always protect their passengers? Minimize harm in an accident if they can? Or just slam the brakes when there's trouble? \n These and other design principles are reasonable, but sometimes they conflict. For instance, an automated car may have to  break the law   or risk its passengers' safety to spare the greatest number of lives on the outside. The right decision, whatever that is, is fundamentally an ethical call based on human values, and one that isn't answerable by science and engineering alone. \n That leads us to the second, related problem. With its unblinking sensors and networked awareness, robot cars can detect risks and react much faster than we can - that's what artificial intelligence is meant to do. In addition, their behavior is programmed, which means crash decisions are already scripted. Therein lies a dilemma. If a human driver makes a bad decision in a sudden crash it's a forgivable accident, but when AI makes   any   decision, it's not a reflex but  premeditated  . \n This isn't just philosophical; it has real implications. Being thoughtful about a crash decision - accounting for numerous factors that a human brain cannot process in a split-second - would be assumed to lead to better outcomes overall, yet it is where new liability arises. An\u00a0  \" accidental  \" accident caused by a person and a \"deliberate\" accident involving a computer system could have vastly different legal implications.  \n Why would we hold artificial intelligence to a higher standard? Because, as any comic-book fan could tell you, \"With great power comes great responsibility.\" The abilities of AI and robots are effectively superpowers. While it may not be our moral duty to throw a ticking bomb into outer space to save people on the ground, it's arguably Superman's duty because he can. Where we may duck out of harm's way, a robot may be expected to  sacrifice   itself for others, since it has no life to protect. \n But even superheroes need a Justice League or a Professor X for a sanity check; or campaigners emerge to fill that vacuum, on issues from  love   to  war  . Some companies, such as Google DeepMind, recognize the value of an \"  ethics board  \" to help guide their AI research and its resulting products in uncharted territory. Berkeley's Stuart Russell, a computer science professor, supports bringing in ethics discussions: \"In the future, moral philosophy will be a key industry sector.\" Stanford's Jerry Kaplan, another AI expert, predicts that, within 10 years, a \"moral programming\" course will be required for a degree in computer science. \n Our society is increasingly becoming a black box\u00a0-\u00a0we don't know how things work anymore, because it's hard to inspect the algorithms on which many of our products run. These formulas are mostly hidden away as corporate trade secrets, whether\u00a0  they are  financial trading bots   or  car operating   systems   or  security screening software  . Even within a company, their own algorithms can be too complex to understand: New code is stacked on top of old code\u00a0over time, sometimes resulting in \"  spaghetti code  \" that can literally kill. The unintended acceleration by Toyota vehicles, resulting from badly structured code, may have been involved in the deaths of at least 89 people since 2000. \n But then again, fears about AI may just reflect fears about ourselves. We know what kind of animals we are, and we worry that AI might wreak the same havoc (some algorithms have already been accused of  discrimination  ). But in the same way that we can raise our children to do the right thing, we can ease our worries about unprincipled artificial intelligence systems by building ethics into the design. Ethics creates transparency, which builds trust. We'll need trust to co-exist with the technological superheroes we've created to save us all. \n","709":"When it comes to the body of scientific literature in existence, the world is suffering from information overload.\u00a0There are now more than 100 million academic papers online and that\u00a0number is estimated to be growing at a rate of nearly 5,000 new articles each day. According to one study, only half of those papers are being read by anyone other than the author and the editors of whatever journal published it.\nMany researchers\u00a0think the answers to a lot of fundamental questions about our lives, science, society and history\u00a0are hidden\u00a0in these articles and the connections they make, but that humans\u00a0just don't have the brain power to make sense of it.\u00a0\nA group of computer scientists funded by Microsoft co-founder and billionaire philanthropist\u00a0Paul Allen is trying to change that through artificial intelligence.\nOn Monday, they took a first step toward that goal by unveiling a new AI program that will help you search the vast universe of\u00a0scholarly knowledge\u00a0in a new way.\u00a0Known as\u00a0Semantic Scholar and developed at the Allen Institute for Artificial Intelligence or AI2 in Seattle, it's a publicly available \"smart\"\u00a0search service\u00a0for journal articles.\n\"There are no Renaissance men or women anymore,\" Oren Etzioni, the head of AI2, said in an interview. \"It's impossible to keep up with everything we are discovering these days.\"\nFor now, the program's\u00a0knowledge is limited to about 3 million computer science papers, but the team is quickly adding more areas of expertise. Neuroscience research will be available sometime in 2016, and other medical topics will soon follow.\u00a0Etzioni explained that medical science is the\u00a0priority for the project because \"the\u00a0medical example is so visceral.\"\n\"When you see a specialist\u00a0you know he or she is not up on your\u00a0condition simply\u00a0because they can't be,\" he said. \"Even doctors in the emergency room are feverishly looking up stuff on their phone to get the latest information.\"\nEtzioni said the difference between Semantic Scholar and services, like Google Scholar or PubMed, is its ability to quickly highlight the most important papers and to tell you what the connections are between different papers and that is something that the researchers will continue to build on.\nBut the program's ambitions go far beyond search. Etzioni said the ultimate goal is to use it as\u00a0a foundation for a kind of artificial intelligence scientific assistant which can suggest new hypothesis or new directions for research.\n\"It's\u00a0going to help connect the dots and help scientists potentially find answers to something\u00a0like cancer,\" he said.\nHow soon will this happen in the future?\u00a0\"I think that the first scientific assistants\u00a0will emerge in the next 10 years, and they are going to keep getting better and better,\" Etzioni said. \"We're not talking about beyond the horizon. We're going to see this very soon.\"\nYou can try the new AI2 Semantic Scholar program\u00a0here or at\u00a0semanticscholar.org. There's also a mobile version for both iOS and Android.\n              Read More:           \n Cutting sugar from kids' diets appears to have a beneficial effect in just 10 days \n Sleep study\u00a0on modern-day hunter-gatherers dispels notion that we're\u00a0wired to need\u00a08 hours \n WHO says hot dogs, bacon cause cancer. Does this mean we should all be vegetarians? \n There are roughly 480 other things the WHO says might cause cancer. \n  SPECIAL REPORT: Billionaire Paul Allen's quest to build an artificial brain  \n              For more health news, you can sign up for our weekly newsletter\u00a0                here                .           \n","711":"This week marks the 101st birthday of Martin Gardner (1914-2010), the journalist, philosopher, mathematician and magician who popularized the notion of recreational math, igniting curiosity for generations of mathematicians. Among the many fans of Martin Gardner is the Russian-born mathematician\u00a0Edward Frenkel, a\u00a0professor of mathematics at University of California, Berkeley. I recently sat down for lunch with Dr. Frenkel and asked him about his own\u00a0experience with mathematical puzzles. He responded with the following story:\n I grew up in Kolomna, a small town about 70 miles southeast\u00a0of Moscow. When I was 14 or 15, a prominent visitor - a professor from Moscow University - came to my class. He was there to lecture us about breaking convention, and about how rare it was to find someone who could think outside the box. To illustrate, he gave us all a puzzle to show us that we could not think outside the box. \"I'm going to time you,\" he said. \"Let's see if anyone can get this in three\u00a0minutes.\" And he gave us the puzzle. \u00a0\n As he gave us the puzzle it all seemed so clear to me, and I solved it in 15 or 20 seconds. It happened so fast I actually surprised myself. It just clicked. I wasn't trying to be fast. I guess I took it as a challenge and wanted to stand up for my classmates. This guy had just walked into our classroom, very cavalier, and was writing us off. What does he know about us? If he hadn't been so arrogant I probably would not have focused on the problem as closely. \n But when I said I had the answer he didn't believe I had actually solved the problem. He thought it was a lucky guess. So he asked me to explain how I got the answer and when I did, he realized I had indeed solved it correctly. He was very impressed. \"You are going to go far,\" he said. I think this\u00a0episode\u00a0gave me more confidence that I was indeed capable of thinking outside the box, and I would sometimes remember it when I needed to employ this skill. \n Solving this puzzle has an element of\u00a0discontinuity. Insight is always discontinuous. All of my discoveries in math\u00a0have been discontinuous like this. Boom. A big jump. \n The puzzle was\u00a0- \n  The Age of a Child  \n Two guys meet and they haven't seen each other for many years. One says, how have you been? Are you married? Do you have children? And the other says, yes, I'm married and have three children. How old? I'll give you the answer as a puzzle. \n If you multiply their ages you get 36, and if you add their ages you get the number of that tram over there (Kolomna had a lot of trams). \n Well, that's not enough information to tell how old your kids are. \n One more piece: the oldest one is red-headed. \n O.K. Now I can tell you. And he answered.  \n The question is: how old were the children? \nDr. Frenkel, who will be giving a\u00a0talk on the fallacies of strong artificial intelligence at\u00a0the upcoming Science and Non-Duality conference in San Jose, also suggested this\u00a0related bonus challenge:\n The following is a version of a puzzle I found in Raymond Smullyan's\u00a0G\u00f6del's Incompleteness Theorems. I think it's quite neat. \n The puzzle illustrates the point that if you have a computer program that is sufficiently sophisticated to make certain statements about itself AND trustworthy (that is, it produces only true statements, and no false statements), then it will NOT\u00a0produce ALL true statements. That is the essence of G\u00f6del's first incompleteness theorem, which says, roughly, that for any sufficiently sophisticated and trustworthy formal system (I will not explain precisely what this means, but think of a very closely related notion of a computer program, or an algorithm), there will be a true statement that cannot be proved within this system.  The syntax can conspire to create self-referential loops, from which there's no exit, like in the famous liar's paradox statement: \"This sentence is not true.\" \n G\u00f6del's incompleteness theorem is very important in the debate about artificial intelligence (AI), because it shows very clearly the limitations of formal logical reasoning. I hope that the practitioners in the field of \u00a0AI will pay a closer attention to it.  \n Here goes the puzzle: \n  Printing Error  \n Suppose you have a computer program that functions in the following way: when you type a statement on your keyboard, it prints it out if the statement is true, and it does nothing if the statement is not true. \n For example, if you type \"2+2=4,\" the computer prints: \"2+2=4\" and if you type \"2+2=5,\" the computer does nothing. \n The details of the programming language are not essential, but we know the following: For every statement X, \n (1) the statement P X means \"the computer can print X\"; \n (2) the statement N X means \"the computer cannot print X\"; \n (3) the statement D X means \"X X\";\u00a0\u00a0\n Question: Find\u00a0a true statement that the computer cannot print. \nFor more by Dr. Frenkel, check out his autobiographical book about\u00a0the\u00a0joy of pure intellectual discovery: \u00a0 Love & Math: The Heart of Hidden Reality. \nAt the 2014 Science and Non-Duality conference Dr. Frenkel discussed Martin Gardner and the nature of mathematics in his talk, \"Mathematics as Hidden Reality.\"\nAnd before we close out -\u00a0here's Numberplay's own Robert Dudley\u00a0(a\u00a0software developer in Florida known among Numberplay regulars\u00a0as Golden Dragon) bringing back\u00a0a Numberplay classic: word ladder challenges. (Click here for rules.) Try transforming the following:\n MARTIN to GARDNER   MATH to FUN   LABOR to THRILL   CONFUSION to CLARITY   PUZZLE to SOLVE \nWith that we wrap up the week. As always, once you're able to read comments for this post, use Gary Hewitt's  to correctly view formulas and graphics. (Click here for an intro.) And send your favorite puzzles to gary.antonick@NYTimes.com\n Solution \nCheck reader comments on Friday for\u00a0solutions\u00a0and commentary by Edward Frenkel.\n","713":"AFTER years of limited progress and effort, there is now a renaissance of interest in using computers to translate languages. Scientists believe that with the help of more powerful computers, advances in artificial intelligence and new approaches to translation, the long-sought goal of machine translation may be at hand.\nResearchers once believed machine translation would be a relatively simple task of having computers match words in dictionaries and apply a few rules of grammar. But different concepts among languages, varying uses of parts of speech, idioms, nuance, contextual meanings, sentence structure and other factors proved daunting.\u00a0\n For example, the sentence \"The hunters shot at the ducks and I saw several fall\" is immediately comprehensible to all speakers of English, but can pose a vexing conundrum for a computer trying to determine whether it was the hunters or the ducks that fell.\nLikewise, the declarative statement \"I can fish\" could easily be translated, \"I work in a cannery.\"\nThe initial brute-force approach of using powerful computers containing large dictionaries, syntax rules and powerful mathematical methods to solve problems, or algorithms, did not fade away entirely. Several commercial companies, including Systran Translation Systems Inc. of La Jolla, Calif., continued to refine such systems and now claim 85 percent or better accuracy in translating specialized documents into several languages.\n\"Systran proved that machine translation could be done and that there was a market for rudimentary translation,\" said Dr. Yorick Wilks, head of the Computing Research Laboratory at New Mexico State University. \"The translations are rough, but useful. However, they require human translators to do pre-editing and post-editing to deal with ambiguities and to create a first-rate finished product.\"\nOne promising alternative to the brute force approach lies in the analysis of meaning. Methods are being developed that infer meaning by the context in which a word is used, said Dr. Jaime G. Carbonell, director of the Center for Machine Translation at Carnegie-Mellon University in Pittsburgh.\nUsing artificial intelligence techniques that simulate human thinking, it should be possible to organize semantic information in a hierarchical fashion so that the computer can pick the most likely meaning from a list of abstract possibilities, he said. In translating \"I took the cake from the table and ate it,\" for example, a computer could tell that \"ate\" referred to cake and not table by knowing that something you eat has to be edible.\n\"By using semantic information, you can infuse the system with a degree of common sense it didn't have before when you used only syntax and word order,\" Dr. Carbonell said.\nAnother approach receiving considerable attention is the use of \"unified grammars\" or \"interlinguas\" that contain common grammar concepts and universal meanings. Using this approach, all languages are considered modules or spokes of a wheel that address a hub, which contains a common, central set of meanings -- kind of an Esperanto for machine translation -- before being translated into one another.\nThe interlingual approach should simplify translation by having a central, intermediate \"language\" in which all terms or concepts have only one meaning for any sense in which they are used in the tongue being translated. The intent is to avoid the passion of poets but the pitfall of translators: ambiguity.\nFor instance, the English word \"bank\" can mean, among other things, a financial institution, the land bordering a river or creek, a mound or heap of something, or, as an aviation verb, the turn of an airplane. In an interlingua, each would have a separate representation or code.\nThe Tradux Corporation of Pompano Beach, Fla., has begun marketing an interlingual system, called Ultra, that was developed by the Wilks group at New Mexico State. Robert P. Irwin, the company president, said the program's English, Spanish, German, Japanese and Chinese language modules have dictionaries designed for simple business correspondence. Each contains only 10,000 meanings because the interlingual approach allows for greater simplicity while producing highly accurate translations with only minimal editing.\u00a0The Simpler, the Better\n \"If you want to add a language, all you have to do is develop a module and not a whole new program with a different set of rules for that language relating to each of the others,\" Mr. Irwin said.\nInterlingual programs now require mainframe or work-station computer power. But Toltran Ltd. of Barrington, Ill., has developed a new translation system for personal computers. It analyzes the base meanings of words without prefixes or suffixes, and the structure of words to tell their parts of speech, tense and semantic traits, such as their relationships to other words in phrases.\n\"The simpler the system, the better,\" said Bruce G. Tolin, the author of the program, who contends that his method exploits commonalities in words from different languages that others have not noticed.\nComputer translation work almost came to a halt in the United States after an influential 1966 study by the National Academy of Sciences recommended that research in the area be curtailed. The report said there was little hope that computer approaches of that day would produce systems that could match the cost and quality of human translators.\nAfter more than 20 years of almost no Federal support for computer translation, at least by civilian agencies, Government money is beginning to filter back into the field. Financing from the National Science Foundation has risen to about $400,000 annually to support three small university machine translation projects. And the Defense Advanced Research Projects Agency is expected to announce a new $1 million a year cooperative program involving New Mexico State, Carnegie-Mellon and the University of Southern California to develop the interlingual approach further.\nDr. Carbonell of Carnegie-Mellon said that the technology for computer language translation was ready and that it was more economically necessary than it was 25 years ago.\u00a0European Requirements\n \"The research really didn't end,\" Dr. Carbonell said. \"It kind of went underground. There absolutely has been progress, although this is not a guarantee we will be successful.\"\nDr. Carbonell and others in the language translation area said the current American push was being inspired by the pressure of world trade and an increasing interest in language and international affairs.\nThe European Economic Community has required that by 1992, those doing business with its member countries must translate product instructions and certain correspondence into its 10 official languages. Increased trade with Japan, Korea and other nations in the Far East is fueling interest in translations.\nThe effort to use computers to translate language now spans a range of size and sophistication from giant mainframe machines to hand-held devices the size of calculators or credit cards. The small devices, such as those made by Microlytics Inc. and Franklin Electronic Publishers Inc., use advanced data compression technology to put thousands of words and hundreds of phases in several languages at the fingertips. They have more translating power than mainframe computers did 25 years ago.\nAlan W. Portela of Systran said it costs about $45 per page for a human translator to translate from one language to another manually, compared with about $20 per page for current machine translation that is later checked by a human. Unedited machine translation directly from one language to another, with less than perfect accuracy, has also proved to be enough for some users.\nThomas F. Seal, president of Alpnet Inc., a Salt Lake City company that uses a computer-aided translation system to assist human translators, said it would probably be 25 years before anyone developed machine translations that approach what humans can do. He and others said they doubted whether any machine would ever be able to handle translations of diplomatic correspondence, classic literature or legal documents.\n\"Humans in the loop with computers means you can allow for subtlety, nuance and ambiguity in human language that no computer alone can handle,\" Mr. Seal said. \"After years of research in machine translation, which seems to attract interest every five to seven years or so but then goes away when people get dissatisfied with the progress, there have not been any fundamental breakthroughs. If it ever happens, I think it will have to be something radically different.\"\n","714":"LONDON -- In the battle against fake news, Andreas Vlachos -- a Greek computer scientist living in a northern English town -- is on the front lines.\nArmed with a decade of machine learning expertise, he is part of a British start-up that will soon release an automated fact-checking tool ahead of the country's election in early June. He also is advising a global competition that pits computer wizards from the United States to China against each other to use artificial intelligence to combat fake news. \n  ''I'm trying to channel my research into something that is useful for everyone who's reading the news,'' said Mr. Vlachos, who is also an academic at the University of Sheffield. ''It's a positive way of moving artificial intelligence forward while improving the political debate.''\n  As Europe readies for several elections this year after President Trump's victory in the United States, Mr. Vlachos, 36, is one of a growing number of technology experts worldwide who are harnessing their skills to tackle misinformation online.\u00a0\n  The French electorate heads to the polls in the second round of presidential elections on May 7, followed by votes in Britain and Germany in the coming months. Computer scientists, tech giants and start-ups are using sophisticated algorithms and reams of online data to quickly -- and automatically -- spot fake news faster than traditional fact-checking groups can.\n  The goal, experts say, is to expand these digital tools across Europe, so the region can counter the fake news that caused so much confusion and anger during the United States presidential election in November, when outright false reports routinely spread like wildfire on Facebook and Twitter.\n  ''Algorithms will have to do a lot of the heavy lifting when it comes to fighting misinformation,'' said Claire Wardle, head of strategy and research at First Draft News, a nonprofit organization that has teamed up with tech companies and newsrooms to debunk fake reports about elections in the United States and Europe. ''It's impossible to do all of this by hand.''\n  Researchers have tried to learn from the United States' run-in with fake news, but the problem in Europe has mutated, experts say, making it impossible to merely replicate American responses to the issue.\n  European countries have different languages, and their media markets are smaller than those in the United States. That means groups that set up fake news sites in the United States, seeking to profit from online advertising when false claims were shared on social media, are less prevalent in Europe.\n  So far, outright fake news stories have been relatively rare. Instead, false reports have more often come from Europeans on social media taking real news out of context, as well as from fake claims spread by state-backed groups like Sputnik, the Russian news organization.\n  But with fake news already swirling around Europe's forthcoming elections, analysts also worry that technology on its own may not be enough to combat the threat.\n  ''There's an increased amount of misinformation out there,'' said Janis Sarts, director of the NATO Strategic Communications Center of Excellence, a think tank in Riga, Latvia, that will hold a hackathon with local coders in May to find potential tech solutions to this trend. ''State-based actors have been trying to amplify specific views to bring them into the mainstream.''\n  Calls for combating fake news have focused on some of the biggest online players, including American giants like Facebook and Google.\n  After criticism of its role in spreading false reports during the United States elections, Facebook introduced a fact-checking tool ahead of the Dutch elections in March and the first round of the French presidential election on April 23. It also removed 30,000 accounts in France that had shared fake news, a small fraction of the approximately 33 million Facebook users in the country.\n  Not everyone, though, has embraced Facebook's response.\n  Most German publishers, for instance, have so far balked at participating in the company's fact-checking efforts, saying it is the responsibility of the social network, not them, to debunk such claims. German lawmakers are mulling potential hefty fines against tech companies if they do not clamp down on fake news and online hate speech.\n  Since last year, Google also has funded almost 20 European projects aimed at fact-checking potentially false reports. That includes its support for two British groups looking to use artificial intelligence to automatically fact-check online claims ahead of the country's June 8 parliamentary election.\n  It similarly has teamed up with French newsrooms to create digital tools, including ways to track trending topics during that country's election.\n  David Dieudonn\u00e9, head of the company's news lab in France, said the project had debunked 43 reports since February (arguably a relatively small figure), including claims that Saudi Arabia was funding the campaign of Emmanuel Macron, the leading candidate.\n  ''We're trying something new,'' Mr. Dieudonn\u00e9 said. ''There's no easy answer for this complicated issue.''\n  Not all potential solutions, though, are being driven by Silicon Valley's big beasts.\n  David Chavalarias, a French academic, has created a digital tool that has analyzed more than 80 million Twitter messages about the French election, helping journalists and fact-checkers to quickly review claims that are spread on the social network.\n  After the presidential election in the United States last year, Dean Pomerleau, a computer scientist at Carnegie Mellon University in Pittsburgh, also challenged his followers on Twitter to come up with an algorithm that could distinguish fake claims from real news.\n  Working with Delip Rao, a former Google researcher, he offered a $2,000 prize to anyone who could meet his requirements. By early this year, more than 100 teams from around the world had signed on to Mr. Pomerleau's Fake News Challenge.\n  Using a database of verified articles and their artificial intelligence expertise, rival groups -- a combination of college teams, independent programmers and groups from existing tech companies -- already have been able to accurately predict the veracity of certain claims almost 90 percent of the time, Mr. Pomerleau said. He hopes that figure will rise to the mid-90s before his challenge ends in June.\n  ''This is just Round 1 of what we want to do,'' said Mr. Pomerleau, who expects the teams to share their work with fact-checking groups worldwide. ''Next, we want to move toward multimedia content like videos.''\n  In the rush to find solutions to fake news, some within the industry are taking a decidedly more low-tech approach.\n  Jimmy Wales, the founder of Wikipedia, recently started a crowdfunding campaign to create a news organization that would combine professional journalists with digital volunteers, who would contribute to reports in a way similar to how articles are created on Wikipedia.\n  Part fact-checking site, part traditional newsroom, the project -- called Wikitribune -- was inspired by the effect of misinformation on the United States presidential election. Mr. Wales said his project would choose subject areas based on the interests of the community of volunteers and paying subscribers to the service, relying more on traditional reporting techniques than high-tech wizardry.\n  ''The real impetus for this was fake news,'' he said. ''We want people to get behind topics, and then we'll hire staff to cover them.''\n\n\n\n","715":"For decades, the world's smartest game-playing humans have been racking up losses to increasingly sophisticated forms of artificial intelligence.\nThe defeats began in the 1990s when IBM's Deep Blue computer conquered chess master Garry Kasparov. More recently, Ke Jie - until then the world's best player of the ancient Chinese board game \"Go\" - was defeated by a Google computer program in May.\nNow the AI supergamers have moved into the world of e-sports. Last week, an artificial intelligence bot created by the Elon Musk-backed start-up OpenAI defeated some of the world's most talented players of Dota 2, a fast-paced, highly complex, multiplayer online video game that draws fierce competition from all over the globe.\u00a0\nOpenAI unveiled its bot at an annual Dota 2 tournament where players walk away with millions in prize money. It was a pivotal moment in gaming and in AI research largely because of how the bot developed its skills and how long it took to refine them enough to defeat the world's most talented pros, according to Greg Brockman, co-founder and chief technology officer of OpenAI.\nThe somewhat frightening reality: It only took the bot two weeks to go from laughable novice to world-class competitor, a period in which Brockman said the bot gathered \"lifetimes\" of experience by playing itself.\nDuring that period, players said, the bot went from behaving like a bot to behaving in a way that felt more alive.\nDanylo \"Dendi\" Ishutin, one of the game's top players, was defeated twice by his AI competition, which felt \"a little like human, but a little like something else,\" he said, according to the Verge.\nBrockman agreed with that perspective: \"You kind of see that this thing is super fast and no human can execute its moves as well, but it was also strategic, and it kind of knows what you're going to do,\" he said. \"When you go off screen, for example, it would predict what you were going to do next. That's not something we expected.\"\nBrockman said games are a great testing ground for AI because they offer a defined set of rules with \"baked-in complexity\" that allow developers to measure a bot's changing skill level. He said one of the major revelations of the Dota 2 bot's success was that it was achieved via \"self-play\" - a form of training in which the bot would continuously play against a copy of itself until it amassed more and more knowledge while improving incrementally.\nFor a game as complicated as Dota 2 - which incorporates more than 100 playable roles and thousands of moves - self play proved more organic and comprehensive than having a human program the bot's behavior.\n\"If you're a novice playing against someone who is awesome - playing tennis against Serena Williams, for example - you're going to be crushed, and you won't realize there are slightly better techniques or ways of doing something,\" Brockman said. \"The magic happens when your opponent is exactly balanced with you so that if you ... explore and find a slightly better strategy it is then reflected in your performance in the game.\"\nTesla chief executive Elon Musk hailed the bot's achievement in historic fashion on Twitter before going on to once again express his concerns about artificial intelligence, which he said poses \"vastly more risk than North Korea.\"\nMusk unleashed a debate on the dangers of AI last month when he tweeted that Facebook chief executive Mark Zuckerberg's understanding of the threat posed by AI \"is limited.\"\nOpenAI first ever to defeat world's best players in competitive eSports. Vastly more complex than traditional board games like chess & Go.\n- Elon Musk (@elonmusk) August 12, 2017          \nIf you're not concerned about AI safety, you should be. Vastly more risk than North Korea. pic.twitter.com\/2z0tiid0lc          \n- Elon Musk (@elonmusk) August 12, 2017          \nNobody likes being regulated, but everything (cars, planes, food, drugs, etc) that's a danger to the public is regulated. AI should be too.\n- Elon Musk (@elonmusk) August 12, 2017          \nOpenAI + Dota 2 https:\/\/t.co\/hdyFSMMB0z          \n- Elon Musk (@elonmusk) August 12, 2017          \n          MORE READING:        \n          Can a better night's sleep in a 'hipster' bus replace flying?       \n          One recipe at a time, YouTube's 'Binging With Babish' is disrupting the content industry       \n          I spent three minutes inside Tesla's Model 3 - and I'm still thinking about it a day later       \n","716":"EVERY day, millions of travelers research fares and hundreds of thousands buy airline tickets over the Internet, sometimes with just a few clicks of the mouse. They could be forgiven for not knowing that air-fare searching is an astonishingly complex computer task -- or that it took a small software company, founded a few years ago by a procrastinating graduate student at the Massachusetts Institute of Technology, to help simplify the process.\n     The company, ITA Software of Cambridge, Mass., had revenue of only $10 million last year for licensing its technology to the airlines -- less than the amount of tickets the airline industry sells online in a day. But ITA is a catalyst of a major change that has been reshaping airline reservations technology -- away from outdated and expensive mainframe computer systems and toward inexpensive desktop computers that, hooked together, can deliver vast amounts of computing power. \n \"ITA is a good example of how someone coming from outside the industry with no agenda is able to help the industry rewrite its future,\" said Henry H. Harteveldt, a senior analyst at Forrester Research, a technology consulting company.\u00a0\nThe founder of ITA, Jeremy Wertheimer, said that the company began as what he half-jokingly called a \"thesis avoidance project.\" Mr. Wertheimer, 40, was a graduate student in artificial intelligence at M.I.T. in the early 1990's when he became interested in air-fare searching -- a classic computer science problem.\nA single round-trip can have hundreds of millions of fare combinations. Fares have a range of variables: the array of airlines serving the route, the number of daily flights, connecting flights and dozens of airline rules (things like passenger eligibility, seasonal restrictions and advance-purchase requirements). International trips have even more possibilities. \"It's unfathomably complex,\" Mr. Wertheimer said. \nIntrigued by how computers could wade through so much information to search for the cheapest fares quickly, he developed a demonstration program using Lisp, an artificial intelligence language that allows enormous quantities of data to be manipulated on a desktop computer. That program became the basis for ITA, which Mr. Wertheimer founded with friends in 1996. \nOther, bigger companies have created rival systems. Expedia, the travel booking Web site started by Microsoft, has developed a fare-search technology that runs on Windows NT servers. Sabre, whose reservations technology powers the Travelocity site, is developing a desktop platform to replace its mainframe system, which American Airlines and I.B.M. introduced in 1964.\nOnly recently have mainframes begun to lose favor in the airline industry. In the 1990's, when Mr. Wertheimer first researched fare searches, many airlines and travel agents still used systems developed in the 1950's. \"I was sort of in awe because they were solving a very hard problem, and they didn't have the tools for it,\" Mr. Wertheimer said.\nAlex Zoghlin, chief technology officer of Orbitz, a travel Web site started last spring by a consortium of airlines, attributed the relative backwardness of reservations technology until recently to the airline industry's reluctance to invest in it. The assumption, he said, had been that \"the cost and effort were so high, it created a barrier to entry.\" Orbitz uses ITA's software.\nMainframe reservation systems can list fares for only a narrow parameter specified by the user -- say flights between Boston and Los Angeles at noon on a certain day. The user must screen the list generated by the mainframe to find the cheapest fares; if the user wants to know if there are cheaper fares at 9 a.m. or 2 p.m. that day, another mainframe search must be generated.\nMoreover, mainframe fare searches return only about 10 choices. For some routes, Mr. Wertheimer noted, that number wouldn't even cover all of nonstop flights available. For people outside major cities, nonstop flights are rarely available, and the variations in choices can be vast.\n\"The thing that bothered me was that you had to make a lot of decisions before you could see a price,\" Mr. Wertheimer said, referring to the frustration of conducting many incrementally different searches to find the cheapest fares for one trip.\nMr. Wertheimer wanted to create a computerized search that would return the best answers for a much wider range of variables chosen by the user -- for instance the lowest fares in a 24-hour period.\n\"It's an interesting problem in that there's not one best answer,\" Mr. Wertheimer said. \"Maybe the flight you want costs $206 through Fargo, but only $212 nonstop. Or for another $20, you could fly on an airline you like and collect frequent-flier miles. Or if you're flexible about the time of day, you could save $50.\"\nITA's software is designed to take into account a full day's schedule, regional airports near a selected destination, possible connecting flights and other factors, and then to return about 400 results sorted by price, flight duration, departure times and airline. The user can change the search parameters or the way results are displayed, then decide if he or she wants to make trade-offs -- perhaps an earlier departure or extra connection to get a lower fare.\nSUCH consumer-centered programming has become more important in the industry as travelers buy more tickets online. \nForrester Research estimates that consumers bought $9.1 billion of leisure air travel tickets online in 2001, or 10 percent of total passenger revenue for the year. The company projects $13.2 billion in online sales for 2002, growing to 14 percent of total passenger revenue, and $16.2 billion in sales for 2003, or 16.5 percent of the total.\nMainframe computers and software cannot support thousands of simultaneous searches in addition to sales (which result from only a small fraction of the searches on travel Web sites). ITA's software, and Expedia's, run on networks of inexpensive desktop computers that can be expanded easily.\nVisitors to www.itasoftware.com, ITA's Web site, can try the software, but it can show only fares -- users cannot use the ITA site to book reservations. ITA's primary business is licensing software and selling services to airlines, companies that run reservation systems, travel agents and other industry groups.\nTravelers are most likely to encounter the system on Orbitz, a flight booking Web site founded last spring by American, Continental, Delta, Northwest and United airlines. ITA said its searches would always return the lowest published fares available, but Orbitz, Expedia and Travelocity all have private deals with airlines, and for any given search a competing site may have a better fare. Expedia also offers air-fare packages with rental cars and hotels.\nITA, which originally stood for Internet Travel Agency, was initially capitalized with money from Mr. Wertheimer's father, Nathan, a jewelry manufacturer, and its first employees. In 1998, with a staff of only five, including Mr. Wertheimer, ITA landed its first deal, with Amadeus, the primary airline reservation system in Europe. Orbitz signed in 2000.\nWith market evidence that its software worked, and a burgeoning interest among airlines in Internet bookings, ITA found it easier to attract customers. In 2001, Delta Air Lines and SITA, an international fare-pricing organization, signed deals with ITA. America West is expected to announce a partnership with it in June.\nWith each contract, ITA has hired 10 people; it now has a staff of 45. But the company found that once it exhausted its network of programmers in the research field, it had difficulty finding qualified people.\nSince last November, ITA has run a banner ad on Slashdot, a Web site that bills itself as \"News for Nerds.\" That banner, and one on Freshmeat, a Slashdot affiliate, challenge readers to solve difficult programming puzzles. The ads have drawn nearly three million hits, and about 200 people have submitted answers, half of them correct. ITA has brought on three employees through this process and seven more through personal connections and postings on HotJobs.\nMr. Wertheimer said he was confident that he had hired great people, but he was cautious about growing too fast. \"Hiring is still one of the more nerve-racking things,\" he said.\nStill, ITA is growing, and Mr. Wertheimer said he saw opportunities in several areas. Within the airline industry, ITA software can be applied to help determine schedules and routes, for example. ITA could also apply its systems to tasks in other industries, like scheduling advertising in broadcast media.\n\"A lot of people have offered to buy us,\" he said, without being specific. \"But we've turned them down because we can do a lot more here.\"Boston to Los Angeles: 800 Million Fare Choices\nAN air fare reflects a combination of things: airline, airport of origin, destination and applicable rules. In theory, there are immeasurable combinations, because a traveler could fly, say, from Boston to Los Angeles via London.\nTo set fares that might interest an average traveler, however, it is usually enough for a computer to search for the 400 most direct round trips. Even finding those requires sifting through hundreds of millions of possibilities. Here are the choices for Boston-Los Angeles round trips as ITA software seeks fares for nonstop and one-stop flights, leaving May 16 and returning May 23, with a 24-hour window for departures both days:\n* The 400 possible outbound trips are multiplied by the 400 possible return trips , which equals 160,000 unique round-trip itineraries.\n* For each round trip, there may be up to four legs (Boston to connecting city, connecting city to Los Angeles, and the reverse), and each represents a flight. Once rules and classes (economy, business, first class) are applied, each flight has about 30 fares. The number of fare combinations is thus 30 times 30 times 30 times 30, which equals 810,000.\n* Of that 810,000, only about 5,000 typically pass the rules for a given route and passenger (special fares for the elderly, for example, or companion fares).\n* The final numbers, then, are: 160,000 itineraries times 5,000 fares -- or 800 million possible fare combinations.\nThe cheapest round-trip fare found by ITA's system early this month was $309.50 on United, with nonstops both ways.   SARAH MILSTEIN\n","717":"A recent San Francisco art show hosted by Google included the typical trappings - a big crowd, striking pieces of art and high prices. But there was one noteworthy distinction of this particular exhibit: All of the pieces were created with the help of a Google computer program.\nThe artwork relies on software that remakes an image to include whatever the software is being told to see in the image. The result is a hallucination of the image that has a psychedelic look.\u00a0\nThe new software provides artists opportunities to be creative as never before. With this fresh approach, there are questions about who or what should get credit for the artwork. Should it be the artist, the computer software or the software's creator, and is the final product really art?\nFor the London man behind one of the pieces that sold for $8,000, the price was an endorsement of artists' continued value in a world that is increasingly reliant on technology and machines. (Another piece of art at the show also sold for $8,000.) Artist Memo Akten said that Google has made a better \"paintbrush\" but that the human artist is essential. Not just any image fed into the computer program can sell for so much, he said.\nAkten   began creating his artwork, called \"GCHQ,\" with a Google Maps screen shot of GCHQ, the British intelligence and security agency. He put   it into a Google program called DeepDream, which is available on a website that anyone can use.   But Akten went a step further, tweaking the code to produce multiple morphed versions of the Google Maps screen shot that were to his liking. One was covered in eyes, another had web-like detail and a third had a textured look. Then Akten merged those three images using Adobe AfterEffects into a single image, his final product.\nWhile the initial version of his art took only a couple hours to make, Akten later spent a couple weeks creating a larger scale version of the art. (The print that was sold was about 6 feet by 3 feet.)\nThe inspiration behind the piece was Akten's view that tech companies such as Google and government agencies are like   modern deities that play the role of religions in previous years.\n\"[When] we have a problem, we ask Google instead of praying,\" Akten said. \"We're provided with a false sense of safety. 'You're being watched, don't do that, we'll find you.' \"\nHe thinks it's \"quite scary,\" given the data and artificial intelligence expertise Google has acquired. For him, it was fitting to use a Google product to help make a statement about the omnipresence of technology companies in our lives.\nAnd this perspective has resonated with some in the art world.\n\"It's beautiful and yet does something more than demonstrate computing prowess,\" said Peter Patchen, chair of digital arts at the Pratt Institute in Brooklyn. \"It questions everything from corporate control of culture to changing socio-religious beliefs and taps into our deep distrust of seemingly omnipotent power structures.\"\nPatchen was happy to see digital art being given a value in the thousands of dollars but said there is still room for growth. (The most coveted classic pieces sell for millions.)\nAs  young, affluent art collectors start to buy works, he expects that they will appreciate digital techniques and be willing to pay higher prices.\nPatchen thinks credit for Akten's work should be shared between the artist and the algorithm's creators. Until the algorithm becomes a thinking being, he thinks it is merely a tool, deserving no credit.\nFor Christiane Paul, the adjunct curator of new media arts at Manhattan's Whitney Museum of American Art, credit lies solely with the artist. She noted that an artist using Adobe Photoshop to make a piece of art would not credit the team that made Photoshop.\nPaul said artists have long sought new tools to elevate their work, such as creating new colors or finding new materials with which to work.\n\"Whether it's a new paintbrush or pigment or neural network, they are all potentially great tools for creating sophisticated art,\" said Paul, who thought Akten's work was richly creative.\nOthers were not as impressed.\nEmily L. Spratt, a PhD candidate in art history at Princeton with a research focus on artificial intelligence, described the artist's explanation behind his work as an anarchist's rant and reflective of our paranoia about machines. Spratt was not sure who ultimately deserved credit for the work -  the artist, the software or the team that made the software.\nShe cautioned that the painting's high price  should not be seen as an acceptance into the art world; the piece was sold at a charity auction, with money going to Gray Area, a reputable nonprofit arts organization in which auction attendees had an interest.\nStill, new techniques such as the use of artificial intelligence to create art are promising, in Spratt's view. She noted that the emergence of oil paints was a breakthrough for artists centuries ago. Now artificial intelligence presents new possibilities for the creative class.\n\"It's definitely a venue for more possible creativity. Will it be as simple as uploading a photo into Google DeepDream and then seeing what type of distorted image it will produce?\" Spratt said. \"I imagine artists will be using this type of technology in a more complicated, nuanced and sophisticated type of way.\"\nmatt.mcfarland@washpost.com\n","718":"                               Each week,  In Theory \u00a0takes on a big idea in the news and explores it from a range of perspectives. This week we're talking about robot intelligence.\u00a0Need a primer? Catch up here.                         \n              Dileep George is an artificial intelligence and neuroscience researcher. In 2010, George and D. Scott Phoenix co-founded Vicarious, an AI research company focused on developing software that can think and learn like the human brain.           \n As an artificial intelligence researcher, there are two questions I am often asked. Will human-level artificial intelligence - also called artificial general intelligence - be reached soon? And will it be dangerous? If you look at recent headlines, you might think the answers are \"Yes!\" and \"Yes! ... Run for your lives!\" \u00a0\n I think the truth is actually more mundane: AGI will be created gradually, over the course of many years. The scenarios presented by movies and the media are an exaggerated picture of the actual risks.The headlines might lead you to believe that AGI is imminent, but scientists actually working on the problems will tell you otherwise. A   lot   more research is needed before we can build AGI. \n[                Other perspectives:                          Machines may seem intelligent, but it'll be a while before they actually are]\n Like our reptile ancestors, the current generation of AI is able to perform many complex behaviors. Outward signs of progress, like Siri, Watson and self-driving cars, may seem like impressive steps toward human-like artificial intelligence. But what separates smart-looking behavior from general intelligence is our human ability to dynamically imagine, reason and adapt - to understand why we're behaving one way, imagine new possibilities, reason about their consequences and alter our behavior as the environment changes. The AIs of today can   act   smart, but many more years of fundamental discoveries are needed to build systems that can actually learn, imagine and reason like a human. \n AGI won't be created overnight. Building machines of human-like intelligence is a very difficult, long-term project, not unlike putting the first humans on Mars. To build AGI (or to colonize Mars) will require a large, interdisciplinary team focused on the problem for many years. There is a broad array of intermediate milestones, like being able to recognize the contents of photographs or putting a satellite in orbit, that are achieved far in advance of the final goal. Neither AGI nor Mars colonization will happen overnight, or in a lone mad scientist's garage without many measurable, understandable, intermediate achievements. Since so many of these milestones are commercially valuable and scientifically fascinating, it also seems likely that the big steps along the way will be celebrated and shared with the public - imagine the fanfare when robots start cleaning up nuclear waste at Fukushima or treating patients with Ebola. \n Many of the \"scary\" scenarios are less than realistic. For example, one concern is that a superintelligent AGI will misunderstand our intentions and follow simple instructions - like \"I need some paperclips\" - to an extreme outcome -   like turning the entire planet into paperclips  . Such a feat would confound all of the greatest human minds alive today, and it seems contradictory to argue that an AGI smart enough to outwit all of humanity is simultaneously not smart enough to figure out what we mean when we ask for paperclips.  \n Imbuing our computers with common sense and teaching them to behave as we expect are precisely the kinds of research challenges that need to be overcome to create the first human-level AGI. In Hollywood's imagination, these problems occur   after   we achieve AGI. In reality, human-level AGI is achieved only when these problems are well understood and solved. \n Real concerns are often overlooked because   headlines about Skynet   get more clicks. Like every technology humanity has created, an artificial general intelligence could bring real benefits and real risks. One example of a far more practical problem associated with AGI is the economic effect of robotic automation. A broad transition toward automated manufacturing and transportation could potentially   disrupt a large number of jobs  , and I agree with the economists who have advocated for work programs and other government-sponsored initiatives to ease structural transitions. \n Every new technology comes with its own potential risks and benefits, and the research community is committed to addressing these challenges together .  Vicarious   and other AI labs recognize the power of technology to transform society and the many benefits and risks that any important invention can create. This spring, Vicarious contributed to the creation of an open letter and   research document   on the focus areas that can be helpful along the path toward human-level AGI, from legal frameworks for autonomous vehicles to verification algorithms. \n Human-level artificial\u00a0intelligence has the potential to help humanity thrive more than any invention that has come before it, and it's important to not let Hollywood fantasies or overzealous reporting make us lose sight of how amazing a world with AGI could be. Many of the biggest problems facing humanity today, like curing diseases or addressing climate change, would be vastly easier with the help of superintelligent AI. We are all lucky to share a future where that will one day be possible. \n                               Explore these other perspectives:                         \n Patrick Lin:\u00a0We're building superhuman robots. Will they be heroes, or villains? \n Ari N. Schulman: Do we love robots because we hate ourselves? \n Murray Shanahan:\u00a0Machines may seem intelligent, but it'll be a while before they actually are \n","719":"TECHNOLOGY tends to cascade into the marketplace in waves. Think of personal computers in the 1980s, the Internet in the 1990s and smartphones in the last five years.\nComputing may be on the cusp of another such wave. This one, many researchers and entrepreneurs say, will be based on smarter machines and software that will automate more tasks and help people make better decisions in business, science and government. And the technological building blocks, both hardware and software, are falling into place, stirring optimism.\nMichael R. Stonebraker, a pioneer in database research, is one of the optimists. Software used by companies and government agencies -- in products sold by Oracle, I.B.M., Microsoft and others -- descends from research done in the 1970s by Mr. Stonebraker and Eugene Wong, a colleague at the University of California, Berkeley, as well as a team of scientists at I.B.M.\nToday, Mr. Stonebraker sees an opportunity for new kinds of ultrafast databases. The new software, he explains, takes advantage of rapid advances in computer hardware to help businesses and researchers find insights in the rising flood of data coming from so many sources, including Web-browsing trails, sensor data, genetic testing and stock trading.\u00a0\nSo, at 68, Mr. Stonebraker is a co-founder and chief technology officer of two start-ups in the field of data-driven discovery, VoltDB and Paradigm4.\n''Now is the time,'' says Mr. Stonebraker, who is an adjunct professor at the Massachusetts Institute of Technology's computer science and artificial intelligence laboratory. ''The economics and the technology are ripe.''\nThe case for optimism is by no means unqualified. The march of these technologies raises social issues, including privacy concerns, and the timing is uncertain. All of the bold predictions in the 1990s that the Internet would disrupt traditional industries like media, advertising and retailing did come true -- a decade later.\nBut a series of related technologies, scientists and entrepreneurs say, has reached a critical mass -- come to a digital boiling point, so to speak -- so that new products and capabilities become possible. The technical ingredients, they note, include powerful, low-cost computing and storage spread across thousands of computers. The digital engine rooms of Google and Amazon are prime examples.\nAnother fast-improving technology involves inexpensive and intelligent sensors, which are crucial to a new breed of automated machines like experimental driverless cars and battlefield drones. Clever software -- notably machine-learning algorithms -- animates much of the current wave of smarter technology. Two well-known examples are found in Watson, the ''Jeopardy''-winning computer from I.B.M., and the movie recommendations on Netflix.\nADVANCES in such underlying technologies are fueling the current excitement in fields like artificial intelligence, robotics and data analysis and prediction. ''All parts of the technology pipeline are gearing up at the same time, and that's how you get this explosion of new applications and uses,'' says Jon Kleinberg, a computer scientist at Cornell University.\nBehind the seeming explosion, experts say, is a process of technology evolution. Paul Saffo, a technology forecaster, compares the process to the evolutionary biology concept known as ''punctuated equilibria'' formulated by the paleontologists Stephen Jay Gould and Niles Eldredge. The idea is that species often evolve in periodic spurts.\nYet, they say, there are typically years of progress before a commercial breakthrough in the technological realm.\n''Even in Silicon Valley, it takes most technologies 20 years to become overnight successes,'' says Mr. Saffo, a consulting professor at Stanford's school of engineering.\nThe Internet provides a case study of both technology's evolutionary progress and its exponential growth. In 1969, there were only four computers connected to the nascent Internet, compared with roughly a billion computing devices today, from laptops to cellphones, says Edward Lazowska, a computer scientist at the University of Washington.\nThe early increases in connected computers drew scant attention. ''But at some point in the late 1990s,'' Mr. Lazowska says, ''you were going from 4 million to 8 million to 16 million to 32 million to 64 million, and people started to notice that something revolutionary was going on.''\nRocket Fuel is a four-year-old Silicon Valley start-up that uses artificial-intelligence software to place display advertisements for marketers on the Web. The company can not only tailor ads by demographic slices of viewers' ages, gender and interests, but can also use its predictive algorithms to produce campaigns based on results, says George H. John, the company's chief executive.\nFor example, a luxury carmaker might tell Rocket Fuel that it wants to place 100 million ads in the next month, and it will pay the company, say, $80 for generating a sales lead, as evidenced by a potential customer downloading a brochure or filling out an online form.\nRocket Fuel is growing fast, having nearly doubled its work force since the start of the year, to 240. So far in 2012, it has handled campaigns for more than 500 advertisers, including BMW, Duncan Hines, Allstate, Pizza Hut and Ace Hardware. It has raised $76 million in venture funding and debt, and its thousands of computers handle 19 billion bid requests a day on ad exchanges. Each online auction for ad space is typically completed in about 100 milliseconds, a tenth of a second.\nRocket Fuel, Mr. John says, is using some of the ideas he worked on in the 1990s as a doctoral student focusing on artificial intelligence at Stanford -- research that was supported with government dollars from the National Science Foundation and other agencies, as is so often the case. In the last few years, building a business around those ideas has become achievable and affordable. ''And a lot of it has to do with the underlying technology,'' Mr. John says.\nFOR Mr. Stonebraker, the hardware advance that opens the door to his start-ups is the striking improvement of solid-state memory, as performance climbs and prices plunge. Solid-state, or flash, memory is most widely known as the lightweight storage technology used in consumer devices like small music players and smartphones.\nBut increasingly, solid-state memory can be used in big computers, holding a hefty database in memory instead of sending data off to be stored on disk drives. According to Mr. Stonebraker, some data-handling tasks can now be completed 50 times faster than with conventional systems.\n''Memory is the new disk,'' he says. ''The obvious thing to do is to exploit that technology.''\nIn the yin and yang of computing, it is software that exploits hardware, enabling a computer to do useful things. And machine-learning programs and other data-sifting software are advancing swiftly.\n''There is no point in collecting and storing all this data if the algorithms are not able to find useful patterns and insights in the data,'' says Mr. Kleinberg at Cornell. ''But the software is scaling up to the task.''\nGRAPHIC (GRAPHIC BY JOHN HERSEY)\n","721":"SAN FRANCISCO -- The ride-hailing service Lyft has agreed to alliances with just about anyone working on driverless car technology. Its latest partnership aims to make its autonomous vehicle technology available to any car manufacturer.\nLyft said Wednesday that it had reached a deal with Magna International, one of the world's biggest auto suppliers, to jointly develop and manufacture self-driving car systems. \n  The companies said they will work together to introduce autonomous vehicles to Lyft's ride-hailing network. At the same time, Magna can sell the driverless-car technology to any customer -- including other technology companies.\u00a0\n  Magna also said it would invest $200 million in Lyft's latest fund-raising round, lifting the San Francisco-based company's valuation to $11.7 billion.\n  The partnership reflects Lyft's open-arms approach to autonomous vehicles.\n  Lyft has opened its ride-hailing network to other companies working on self-driving cars, including Ford and General Motors, a major Lyft investor, so they can gain real-world experience by picking up passengers and collecting data.\n  Uber, Lyft's main rival, has been developing self-driving technology mostly on its own. Waymo, a Lyft partner, is slowly introducing its own ride-hailing service using autonomous Chrysler Pacifica minivans equipped with Waymo's own hardware and software.\n  The hype over self-driving cars has made development of the technology challenging for some traditional automakers. Car companies face huge salaries for top artificial intelligence engineers and limited access to data and key components. The competitive landscape is so charged that it has already given birth to at least one high-profile lawsuit.\n  As a result, automakers are confronting a choice: Pay big for a technology start-up or risk falling behind. Last year, Ford announced it would invest $1 billion in Argo AI, an artificial intelligence start-up focused on developing autonomous vehicle technology. GM acquired Cruise for an estimated $1 billion in cash, stock and incentive packages in 2016, the same year it invested $500 million in Lyft.\n  After lagging behind Uber, Lyft has recently made a concerted push into self-driving cars. The company opened a research facility in Palo Alto, Calif., and has aggressively recruited engineers.\n  It also has a major asset for self-driving technology -- a ride-hailing network picking up and dropping off passengers 10 million times a week. This provides Lyft with a customer base to introduce and test the vehicles and a way to collect information that can be used to ''train'' autonomous cars.\n  But Raj Kapoor, Lyft's chief strategy officer, said it would be a few years before truly autonomous vehicles were ready for the road.\n  ''I believe this relationship will get us there faster,'' Mr. Kapoor said.\n  Magna, a Canadian auto parts maker, already supplies a wide range of driver-assist technology to its customers, including a system for staying in lanes, automatic emergency braking and rearview cameras. It also builds entire vehicles for customers like Mercedes-Benz, BMW and Jaguar -- a capability that has made the company a potential partner for a new entrant like Apple.\n  Magna has already been working on hardware for self-driving cars, including radar and lidar -- an abbreviation for light detection and ranging -- that help the vehicles see the world around them.\n  But Magna said the partnership with Lyft would be essential to helping it push further into autonomous vehicles, combining its automotive and manufacturing experience with Lyft's ride network to better understand the many situations that a self-driving car will encounter.\n  ''The question isn't whether autonomous vehicles are going to happen but how long the transition is,'' said Swamy Kotagiri, Magna's chief technology officer.\n  Under the partnership, Lyft would take the lead in developing self-driving car technology while Magna would oversee manufacturing of the systems. The two companies would share the development costs and the resulting intellectual property.\n  Lyft compared its strategy of operating its own self-driving cars as well as welcoming driverless cars from other companies to the way Amazon is both a retailer selling products to customers directly and an online virtual mall, providing space for other companies to sell goods on the site.\n\n\n\n","722":"Outspoken, controversial and a fugitive from academe, Doug Lenat is one of the few computer scientists who appreciates the link between artificial intelligence and natural stupidity.\n\"Even computer programs should be able to learn from their mistakes,\" insists Lenat, who argues that the current crop of artificial-intelligence programs do \"things that any four-year-old child would know is stupid; they lack common sense.\"\nLenat views knowledge as something that the truly intelligent program should be able to acquire, not something that must be built in.\nTop computer scientists from MIT to Stanford assert that Lenat's work at the Microelectronics and Computer Technology Consortium here is defining the future of machine intelligence.\n\"He's clearly one of the most creative people in the field,\" said MIT's Marvin Minsky, a pioneer of artificial-intelligence research. \"What Lenat has done is started up a whole new field of knowledge.\"\u00a0\nWhat sets Lenat's software apart is that it isn't programmed just to solve existing problems; it's programmed to learn the rules about how to solve problems. In essence, Lenat is programming computers to learn how to become experts.\nOne Lenat program, EURISKO, literally programmed itself to become the world's first designer of three-dimensional computer chips. Another version of the program kicked the stuffing out of its human opponents for two years running in the national championship of a science fiction spacewar game called Traveller -- even after a last-minute rules change.\nEURISKO offers just a glimmer of what could prove to be the most powerful implementation of artificial-intelligence techniques yet. Computer programs with the power to test new ideas, make analogies and learn would have applications in everything from weapons design to the genetic engineering of life.\nEURISKO signals that computers are prepared to move to a new level of complexity and application, Lenat said: the capability to program themselves.\n\"His work is highly innovative and often brilliant,\" said Edward Feigenbaum, chairman of Stanford University's computer science department. \"And . . . what he's done is probably the most important work that's been done along the road of modelling creativity and discovery by machine.\"\nIronically, Lenat was denied tenure at Stanford, where he won his Ph.D -- Lenat believes it was because his research wasn't deemed scientific enough. Now, the 35-year-old computer scientist is at the Microelectronics and Computer Technology Consortium -- a research-and-development group run by Bobby Ray Inman, which made a special effort to recruit him.\n\"It is the next serious step in artificial intelligence,\" asserted Al Clarkson, director of man\/machine technology at the advanced military science division of ESL, a subsidiary of defense giant TRW Inc.\n\"It's a radical departure from expert systems,\" Clarkson said. \"If you were a smart venture capitalist, this is what you'd invest in.\"\n The Pentagon and U.S. intelligence officials are exploring the national-security implications of the EURISKO program, Clarkson said.\nWhat sets Lenat's work apart from the rest of the artificial-intelligence intelligentsia is his radically different approach to representing knowledge and learning in computer code. Where most researchers today are intent on transferring human knowledge directly into machine form, Lenat's programs embody such anthropomorphic concepts as curiousity, discovery and, yes, stupidity -- some of the tools through which knowledge is achieved.\n\"I honestly think that the only way we will learn things is if the program surprises people,\" Lenat said. \"People should be using computers the way astronomers use telescopes: as a new way of looking at things.\"\nLenat's research represents a vital enhancement of \"expert systems,\" the hottest research area in artificial intelligence. Dozens of companies -- including International Business Machines Corp., Texas Instruments Inc., Digital Equipment Corp., Ford Motor Co. and General Motors Corp. -- are spending tens of millions of dollars to develop expert-systems software for everything from writing software to designing automobile engines.\nEssentially, expert systems seek to transfer the expertise of human specialists onto a computer program in the form of rules. For example, certain medical diagnoses could be expressed as chains of IF\/THEN rules: IF a certain symptom is present, THEN check the blood-sugar level. The thrust of the expert-systems approach is that all kinds of expertise can be represented in properly ordered sequences of IF\/THEN rules.\nThe problem with expert systems is that they generally address only a tiny base of knowledge, and the rules are fixed, Lenat pointed out. That means expert systems are frozen slices of information that can't adapt to change.\nCrudely put, an expert system knows something by rote; a Eurisko system can learn it from experience. An expert system that gives advice on designing a certain kind of computer chip is virtually useless when it comes to designing other kinds of chips. By contrast, a Eurisko system could learn to add to its chip-design knowledge and become more productive.\nIn working on the design of integrated circuits, for example, EURISKO discovered that symmetry is a desirable property for such chips -- even though it didn't understand why.\nLater, when EURISKO designed its conquering Traveller game fleets, the program decided to make them symmetrical -- and justified the decision by referring to its earlier experience in designing circuits.\nThe business implications of expert systems that could learn to reason by analogy are staggering. These \"learning expert systems\" could, over time, reprogram themselves to be more productive. Theoretically, they could discover new ways of performing tasks more effectively.\nThe intellectual theme of Lenat's machine learning research is \"heuristics.\" Heuristics are the intellectual tools -- the rules of thumb -- that people use in the course of attempting to solve problems and make decisions.\nThe trick is figuring out how to represent these rules in computer code in such a way that a program can succesfully transform them into learning behavior.\nLenat likens intelligent problem-solving to using a map to find one's way. Without the map, a traveller has no idea which way to go. The map makes searching for the right way to go much easier.\nThe key is constraints, argued Lenat, who asserted that \"intelligence is the ability to zero in effectively on a solution despite the apparent size of the search space.\"\nTo understand concepts such as \"intelligence\" and \"knowledge,\" one has to understand the rules, Lenat said.\nThe importance of the rules of rules burned into Lenat's research efforts when he developed a computer program called AM (Automated Mathematician) over a decade ago for his doctoral thesis at Stanford. AM was a novel approach to expert systems because it was designed to discover new concepts rather than solve old problems.\n\"AM was probably the only program written that didn't know what it was going to do,\" Lenat said. \"I was ready to build a program that surprised me.\"\nLenat gave AM a couple of hundred basic mathematical concepts such as equivalence and addition. He then programmed AM to explore \"interesting\" relationships between concepts to see if new mathematical concepts could be uncovered. Thus, one AM heuristic was \"If X is an interesting operation, THEN look at the inverse of X.\"\nAM thus would spew out hundreds of mathematical conjectures and rules based on permutations of these core concepts and heuristics. In a Darwinian form of conceptual evolution, AM derived hypotheses that either proved interesting and true, and thus were kept in the program's repertoire, or were intellectual dead ends and thus were discarded.\nThis survival-of-the-fittest approach to generating and testing hypotheses according to AM's built-in rules was the heart of Lenat's approach to machine learning.\nAM rediscovered such basic mathematical concepts as numbers, set theory, prime numbers, Goldbach's conjecture (that every even number greater than 2 is the sum of two prime numbers) and \"several conjectures that don't have names, for good reasons,\" Lenat said.\nUltimately, however, the AM approach proved sterile. The further away the program explored from its core set of rules, the wilder and stupider its hypotheses became. The \"hit rate\" for useful hypotheses dropped from more than 60 percent to less than 10 percent.\n\"AM ground to a halt because, unlike humans,\" it never learned any new rules based on its mistakes, Lenat said. \"It put together concepts that were awful and that people instantly recognized were awful\" but that the machine did not.\nWhat Lenat recognized was that, to be truly effective, AM had to do more than just generate new concepts: It had to discover and test rules about discovering concepts. In effect, AM had to learn how to learn. That was the genesis of EURISKO.\n\"The initial experience was disastrous,\" Lenat said. \"For the first five years, nothing good came out of it.\"\nThen, Lenat figured out what he was doing wrong. Rather than express rules in complicated forms of IF\/THEN statements, he decided to break them down into their fundamental attributes. In effect, he based EURISKO rules in terms of key words -- adjectives and nouns -- rather than complete sentences.\nThink of a Eurisko concept expressed as a name modified by descriptions. In Traveller, for example, a weapon might be described as a BEAM WEAPON, DEFENSIVE and WORTH 500 POINTS.\nInstead of testing something broad, such as an IF\/THEN hypothesis, EURISKO also is capable of testing all the components, all the names and descriptions, within that hypothesis and determining what about a hypothesis makes it work or not work.\nMutations create new concepts that are tested against the environment. Some concepts \"succeed\" and EURISKO adopts them; others \"fail\" and become the programming equivalent of dinosaurs. It may take thousands of EURISKO generations to spawn useful concepts.\nFor Traveller, Lenat wrote, \"At first, mutations were random. Soon, patterns were perceived: More ships were better; smaller ships were better; etc. Gradually, as each fleet beat the previous one . . . its 'lessons' were abstracted into new specific heuristics.\"\nThese improved rules, evolved over hundreds of hours of computer time, enabled EURISKO to design the winning Traveller space fleet.\nAt that level, EURISKO took off.\nBut Lenat soon ran into another brick wall. The program needed a broader base of knowledge -- common sense.\n\"Common sense is everything they assume you already know when you read an encyclopedia article,\" said Lenat, whose reasearch group at MCC is busy trying to create a common-sense database from an encyclopedia. But that will take years. Consequently, \"EURISKO is frozen now,\" Lenat said. \"It will be thawed out once we have this body of common sense.\"\nArmed with common sense to go along with its programmed expertise in a specialty subject, this next-generation EURISKO will be a powerful intellectual tool for design, analysis and discovery, according to Lenat.\nAn artificial-intelligence system that can learn to become an expert could be the most significant step yet to rivaling human intelligence.\n","723":"             The president of Uber and another prominent executive are leaving the company in the latest sign of tumult as the ride-hailing service continues to grapple with months of scandal.\nIn the past three months, senior leaders in departments that oversee marketing, engineering, artificial intelligence and product development have stepped down.\nJeff Jones joined Uber in September to serve as president after a stint as Target's chief marketing officer. His departure comes just weeks after chief executive Travis Kalanick said he would seek \"leadership help\" and announced plans to hire a chief operating officer following the release of video showing Kalanick's confrontation with an Uber driver.\u00a0\nJones told tech news site Recode: \"It is now clear, however, that the beliefs and approach to leadership that have guided my career are inconsistent with what I saw and experienced at Uber, and I can no longer continue as president of the ride sharing business.\"\nRecode first reported the departures, and they were later confirmed by a spokeswoman.\n\"We want to thank Jeff for his six months at the company and wish him all the best,\" the company said in a statement.\nAlso departing this month is Brian McClendon, the vice president of maps and business platform. He plans to return to his native Kansas and explore a career in politics, he said in a statement provided by the company. \"This fall's election and the current fiscal crisis in Kansas is driving me to more fully participate in our democracy,\" he said.\nThey join a larger exodus of executives since Uber's troubles began in January. According to media reports, others to depart the company in recent months include Ed Baker, the vice president for product and growth, Raffi Krikorian, a senior engineering director, and Gary Marcus, who led Uber's artificial intelligence lab.\nUber's senior vice president of engineering, Amit Singhal, left the company in February after he did not disclose a sexual harassment accusation made against him at Google, his previous employer, Recode reported.\nThe departures come as Uber tries to contain the fallout from recent controversies.\nUber faced a consumer boycott after the company continued to operate during a taxicab strike at New York's John F. Kennedy International Airport following President Trump's first travel ban. An estimated 200,000 users deleted the Uber app and the company saw a 10\u00a0percent dip in riders, according to one recent study. Kalanick stepped down as an economic adviser to Trump as a result.\nThen a former employee published allegations of sexual harassment and discrimination at the company, including accusations that the human resources department did not properly handle complaints from female employees. The company hired former U.S. attorney general Eric H. Holder Jr. to help lead a review of those accusations and the company's discrimination policies.\nLast month, Google's self-driving division, Waymo, filed a lawsuit against Uber, alleging that its technology was modeled off stolen intellectual property. Waymo has accused three former employees of taking trade secrets to Uber and asked a judge to bar Uber from using the technology until the matter is settled.\nUber has denied those accusations and called Waymo's lawsuit \"a baseless attempt to slow down a competitor.\"\nKalanick was also caught on video in an argument with one of the company's drivers, who confronted the chief executive about introducing low-cost services, such as Uber X, that are less lucrative for drivers. After the video surfaced, Kalanick publicly apologized and said he would seek \"leadership help.\"\nFinally, a report in the New York Times outlined Uber's use of a secret tool to evade regulators in cities where it was not allowed to operate. The company said it would review how it uses the software - which allows the company to display the app differently to individual users - and prohibited \"its use to target action by local regulators going forward,\" the company said.\nsteven.overly@washpost.com\n","726":"Software is the strangest invention in economic history: a great wealth creator and an industry destroyer. But software is also insubstantial;no one can bring you a bucket of software. Code is a series of propositions about arranging transistors in a computer.\u00a0\nApple became one of the world's biggest companies thanks to this paradoxical invention, and now it may also be threatened by where software is headed. \n  As Brian X. Chen reports, this week Apple held its big meeting for outside software developers, the people who make Apple products essential for many by creating new things that Apple's phones, tablets, computers and watch can do. For the rest of us, it's a week to imagine what will spring from this Apple-developers pairing a few months from now.\n  But this session was more about catching up to rivals. Apple offered a new way to send sketches and handwriting, new ways of photo-organizing, and a limited incorporation of Siri, Apple's virtual assistant, with third-party software. The watch loads apps faster, and has some more social functions.\n  But other companies, from Google to independent messaging services, have for the most part been there already. As Farhad Manjoo writes, Apple appears to be taking a cautious, step-by-step approach, focusing more on devices than on this kind of software.\n  Google Maps, more popular than the Apple version, works well by pooling the experiences of millions of users, often in real time, to deliver accurate information on things like traffic jams. Echo, Amazon's virtual assistant masquerading as a new wave pepper mill, does a great job of playing your songs, buying stuff for you or reading you recipes, because it trawls in lots of people's data.\n  These are just two examples of a big trend in software, the application of real-time artificial intelligence in ordinary products.\n  A.I., which is fundamentally the recognition and leveraging of all sorts of behavioral patterns, works well by using very large and diverse sets of data. Often, the more personal the better. To make software with A.I. features, you have to be able to access lots of information, obtained by various means.\n  Timothy D. Cook, Apple's chief executive, has positioned his company as one that values customer privacy as a key feature, something exemplified in his battle with the Federal Bureau of Investigation over access to an iPhone after the San Bernardino shootings. That's perhaps great from a brand perspective, but it may conflict with the way a lot of software is going to be written.\n  Apple fans, who could see their products have relatively less magical software functionality, may end up experiencing something else: getting their privacy at the expense of the the consumer delight offered by Apple's competitors.\n\n\n\n","728":"Imagine this: When you leave the house, your air conditioner and lights turn off automatically. Then when a motion sensor detects a person in the house, like your house cleaner, it sends an alert to your phone. When you arrive home, a camera recognizes who you are and the door automatically unlocks.\nAutomated technologies like these will be at the forefront of CES, one of the world's largest tech conventions, next week in Las Vegas. They underline one major trend: Increasingly, the innovations that are making their way into your personal technology aren't physical electronics or gadgets at all.\nThe real star is artificial intelligence, the culmination of software, algorithms and sensors working together to make your everyday appliances smarter and more automated. It is A.I. that is telling the door to unlock when the camera recognizes you, or sending an alert to your phone when sensors detect a person.\n\"It's less about the hardware, and more about what's inside,\" Carolina Milanesi, a technology analyst for Creative Strategies, said about the prominence of artificial intelligence and software innovations at CES. For consumers who are dazzled by flashy new devices, A.I. is never as exciting, she said -- but it's the magic that is making hardware evolve.\nThat artificial intelligence will take center stage at CES also speaks to how the event has changed in the last few years. It has become less of a venue for tech companies to unveil splashy new products like smartphones or computers, and instead has turned into a showcase for nascent technologies.\nHere are some highlights you can expect from next week's show.\nAlexa and Her Counterparts\nAlexa, Amazon's intelligent assistant that listens to your voice commands to play music, order diapers and place a phone call, will be everywhere at CES.\nSmaller tech companies have teamed up with Amazon to bring voice-controlled smarts to their products. Devices like light bulbs, car stereo accessories, robovacuums, home security systems and even coffee makers will work with Alexa. In addition, thousands of companies have developed \"Skills,\" or third-party apps, that work with Alexa voice commands. Sonos, a premium audio brand, recently released a speaker with Alexa functionality built in.\nWhat's fueling interest in Alexa? Amazon's success with Echo, the smart speakers enabled with the personal assistant. Amazon said last month that it sold tens of millions of Echo devices over the holiday season. Strategy Analytics, a research firm, estimates that 68 percent of voice-controlled speakers sold last year worked with Alexa.\nOther tech giants want a piece of the pie, too. In 2016, Google introduced Home, an artificially intelligent speaker to rival the Echo. The search giant will have a large presence at CES, where the company is also expected to highlight accessories that work with its Home speaker and Google Assistant.\nApple this year will release HomePod, a speaker that relies on Siri, the Apple assistant, to control some smart home accessories. And Samsung Electronics, the South Korean manufacturer, is expected to unveil a major upgrade for Bixby, its virtual assistant, later this year.\n\"If I had to make a bet, it's the year of A.I. and conversational interfaces,\" said J. P. Gownder, an analyst for Forrester Research.\nSmart Cities\nNowadays, it's easy to shop for high-quality internet-connected home accessories, like light bulbs, thermostats and security cameras. At CES, Samsung is even planning to introduce a smart refrigerator at the electronics show that can listen to voice commands to control other home accessories.\nNow tech companies are looking to push internet connectivity beyond products in your home.\nConsider parking spaces that can sense whether cars are occupying them, and can then alert people when spots free up. Or a garbage can that can notify a waste collection facility when the container is full, or street lamps planted throughout a city that could monitor air quality.\n\"You've seen the smart home,\" Mr. Gownder said. \"The smart city is elevating that to the next level.\"\nDeloitte, the consulting firm, has been making a big push for smart cities. It envisions a future where a multitude of sensors work together to create a healthier, safer and more energy efficient town. Sensors in a river could detect pollution like leaks from chemical plants, and sensors that detect the sound of a gunshot could be used to alert the police, for example.\nNext week, companies will demonstrate prototypes of devices embedded with some of these sensors with the hope that cities will soon begin adopting these technologies within their infrastructure.\nSmarter Cars\nSelf-driving-car enthusiasts like Elon Musk, the chief executive of Tesla, dream of a future where driverless cars eliminate traffic accidents while letting people do work on their commutes.\nThey can keep dreaming: Autonomous vehicles still have a long way to go before they become safe and properly regulated.\n\"Your car doesn't drive itself, at least not reliably for a long period of time,\" said Ms. Milanesi of Creative Strategies.\nStill, at CES, carmakers like Ford, Hyundai, BMW and Audi are expected to show off the latest improvements to self-driving tech, like smarter parking assistance and less error-prone collision avoidance. These are baby steps toward truly driverless vehicles, but some features may appear in cars in the coming years.\nLikely coming sooner is the so-called connected car. Tech and car companies will demonstrate new features for internet-equipped cars, like the ability to pay for parking and gas through a dashboard or cameras that enhance a driver's side and rear vision. At the trade show, Gentex Corporation, a company that develops car technology, will demonstrate in-vehicle biometrics that scan a driver's iris to verify his or her identity before turning on the car.\nNext-Generation Wireless Technology\nAs a growing number of devices rely on artificial intelligence, they will require faster bandwidth speeds. At CES, wireless companies like AT&T and Verizon are expected to give progress reports on so-called 5G, the fifth-generation network technology.\nWith 5G, wireless carriers envision an era of incredibly fast speeds that let smartphone users download a movie in less than five seconds -- roughly 100 times faster than the current network technology, 4G. Even more important, 5G is expected to greatly reduce latency to let devices communicate with each other with extremely fast response times.\n(One caveat: the wireless industry is not expected to roll out 5G until 2020. Telecom equipment makers and tech companies are still fighting over which of their technologies should be standardized globally.)\nSo why should you care? All of the above -- the smart city, the driverless car and smarter artificial intelligence -- will need extremely fast response times for devices to work reliably. The chief example is driverless cars, which will have to communicate with each other practically in real-time to avoid collisions.\n\"It could be foundational to the next generation of advancements,\" said Greg Roberts, an executive at Accenture, a tech consulting firm. \"You're going to see more talk around the potential of 5G to remove the barriers around bandwidth.\"\nEmail: brian.chen@nytimes.com; Twitter: @bxchen.\nPHOTOS: Top, a robot interacting with attendees at last year's CES, the giant tech trade show. Above left, Ford introduced its in-car Alexa system at CES in 2017; Amazon said it sold tens of millions of Echo devices over the holiday season. (PHOTOGRAPHS BY ALEX WONG\/GETTY IMAGES; SAM VARNHAGEN; TOM JAMIESON FOR THE NEW YORK TIMES)Related Articles\n\n","729":"In a perfect world, elevators would never do those irritating things elevators do, such as coming late or not at all. Or stopping at every floor between here and there. Or suddenly slamming their doors on a passenger who has been a nanosecond too slow, or generously opening their doors to show a would-be rider an elevator packed tighter than a Broadway local subway car at rush hour.\nBut this is an imperfect world, and that is why Bruce A. Powell, John S. Kendall and David J. Sirag labor for the Otis Elevator Company in the rolling hills of central Connecticut, using various forms of artificial intelligence -- technologies and disciplines with odd names like fuzzy logic and neural networks -- to improve the relationship between humankind and its elevators.\u00a0\n The first Otis elevator system incorporating such technology is now being installed in Osaka, Japan, at the 28-story Hyatt Regency Osaka hotel, which is to open next year. The system will be able to handle more people with less waiting time than other elevators, the Otis researchers say.\n\"With the sophistication of the new artificial intelligence, and with better processors, these elevators can process a lot more information,\" said Mr. Kendall, director of advanced research at Otis, a division of the United Technologies Corporation. \"They're learning.\"\nMr. Powell, principal research engineer for Otis, said the goal was to keep people moving. \"We're unhappy when someone waits more than 30 seconds, and we like to have waiting times average no longer than 20 seconds,\" he said. \"And, ideally, nobody waits longer than 90 seconds.\"\nOne of Otis's most promising technologies works on \"fuzzy logic.\" Instead of traditional logic, with \"crisp\" distinctions, such as true or false, that lead to the typical precise computer decision-making, fuzzy logic can be programmed to suggest that something is more true or more false.\nTake, for example, calling an elevator to the 6th floor of a 12-story building. The elevator control system has two choices: send car A, which is on the 9th floor with three passengers heading for the lobby, or send car B, on the 11th floor with no passengers.\n\"The very naive approach, and one that's taken by the older systems, is to send the closest elevator,\" Mr. Powell said, but that means a delay for the riders in the ninth-floor car.\nThus, the researchers say, it is better to avoid the \"crisp\" question \"Which car will get there fastest?\" and try a \"fuzzy\" approach in which the system assesses not only how near the cars are but also how busy they are, asking and answering the question, \"Which car should be sent?\"\nEven though Otis's first working \"fuzzy' elevator is being installed in a new Japanese hotel, the company sees a big market for the technique in renovations of elevator systems in the United States and elsewhere. Such upgradings are a crucial part of Otis's revenue, which totaled $4.5 billion in 1992, particularly since the glut of commercial real estate has cut construction of new buildings.\nBesides Otis, several Japanese companies, including Hitachi, Toshiba and Mitsubishi, make fuzzy elevator dispatchers. But even the smartest such systems can get confused by the hour-to-hour idiosyncracies of a building -- say, a 40-floor building with a cafeteria on the 31st floor causing heavy lunchtime traffic.\n\"Developing dispatching logic robust enough to handle such tasks,\" Mr. Powell said, \"is what makes our job challenging.\"\nTo handle such problems, Otis is researching the neural network, a computer system that can be taught to recognize patterns or to check its own performance and \"learn\" from its mistakes.\nSuch systems theoretically have many strengths: Say the entire third floor goes to lunch at 1 P.M. every day. Instead of always parking idle elevators at the lobby, as many elevator systems are programmed to do on the theory that the cars are more likely to be wanted there, a neural network could \"learn\" to park idle elevators on the third floor at 12:55.\nCan these new smart elevators tell the difference between an elevator call from the chief executive and one from a pack of underlings hoping to slip out to buy a bowl of gruel? No problem, Mr. Kendall said, because -- peons everywhere, take note -- even today's elevators typically have a V.I.P. option. The dignitary in question can use a button or key to take priority over all other calls -- a system similar to the emergency override options used in hospitals.\nThere are plenty of pitfalls in elevator programming. Mr. Sirag, an analyst in the artificial intelligence group at the United Technologies Research Center, where Otis thinks many of its big thoughts about elevators, recalls a simulation that was intended to keep elevator systems from sending cars that are too full to take passengers. The computer was told, in effect, \"don't send a car that can't take all the people who are waiting.\"\nThe system obeyed. The line of would-be passengers lengthened. Still, no cars came, because none could accommodate all the imaginary waiting passengers. \"The whole system fell apart,\" Mr. Sirag said. \"Sometimes things that seem intuitively obvious don't work out the way you expect.\"\nTo be sure, it is not only speed that concerns Otis's elevator researchers. They also try to reduce vibrations, \"so we don't have to turn the elevator music up so loud,\" Mr. Kendall said, And they have developed a new door sensor that \"sees\" not only a foot or an arm but even a pen or a credit card, to keep the doors from nipping at people's heels.\nBut the main goal is to reduce the time spent waiting for an elevator and the time in an elevator. \"Most people tolerate a minute and a half in an elevator, and after that they start to get a little bit antsy,\" Mr. Kendall said, adding that he has an assistant for whom any amount of time in an elevator is so unbearable that she will not even get into one.\nThere are other techniques for reducing the perception of waiting time, including putting reading material inside the cars. And then there is the piece of elevator lore everyone seems to know: that shiny surfaces improve the elevator experience.\n\"There is an old story in the operations research community that there was a building in which people were waiting too long,\" Mr. Powell said. \"So they did a whole bunch of studies, sent people with stopwatches, measured everything that could be measured, and finally they put a mirror in the hallway, so people could check their hair and their clothing, and that minimized their anxiety over the elevator and the wait.\"\u00a0Cultural Differences\n The Otis team has made some interesting cultural discoveries while dealing with Otis's first full-scale fuzzy logic system, the Elevonic 411, being installed in Osaka. It turns out that Japanese seem to feel more strongly about reducing the longest waiting time than Americans do.\n\"In Japan, the person who waits 10 seconds feels badly that his colleague has to wait three minutes,\" Mr. Powell said.\nMr. Kendall added: \"In New York, they're not that way. They don't like to wait.\"\n","730":"ABSTRACT\nKate Crawford The Education of AI article in The Future of Everything section notes digital brains can be as prejudiced as their human counterparts and as AI invades into more of our lives, we need to understand how its thinking before its too late; diagram, drawing (M)\n","731":"ABSTRACT\nJapanese company Preferred Networks Inc is developing 'deep-learning' software that can learn on its own, which Japanese companies see as way for hardware to learn to improve itself, while Silicon Valley tends to see it as way to improve software; photo (M)\n","732":"ABSTRACT\nGary Marcus Review section article recommends society prepare now for future rise of superintelligent machines by adopting some sort of oversight mechanism so that machines never replace humans; photo (M)\n","733":"Artificial\u00a0Intelligence is colossally hyped these days, but the dirty little secret is that it still has a long, long way to go. Sure, A.I. systems have mastered an array of games, from chess and Go to \"Jeopardy\" and poker, but the technology continues to struggle in the real world. Robots fall over while opening doors, prototype driverless cars frequently need human intervention, and nobody has yet designed a machine that can read reliably at the level of a sixth grader, let alone a college student. Computers that can educate themselves -- a mark of true intelligence -- remain a dream.\nEven the trendy technique of \"deep learning,\" which uses artificial neural networks to discern complex statistical correlations in huge amounts of data, often comes up short. Some of the best image-recognition systems, for example, can successfully distinguish dog breeds, yet remain capable of major blunders, like mistaking a simple pattern of yellow and black stripes for a school bus. Such systems can neither comprehend what is going on in complex visual scenes (\"Who is chasing whom and why?\") nor follow simple instructions (\"Read this story and summarize what it means\").\nAlthough the field of A.I. is exploding with microdiscoveries, progress toward the robustness and flexibility of human cognition remains elusive. Not long ago, for example, while sitting with me in a cafe, my 3-year-old daughter spontaneously realized that she could climb out of her chair in a new way: backward, by sliding through the gap between the back and the seat of the chair. My daughter had never seen anyone else disembark in quite this way; she invented it on her own -- and without the benefit of trial and error, or the need for terabytes of labeled data.\nPresumably, my daughter relied on an implicit theory of how her body moves, along with an implicit theory of physics -- how one complex object travels through the aperture of another. I challenge any robot to do the same. A.I. systems tend to be passive vessels, dredging through data in search of statistical correlations; humans are active engines for discovering how things work.\nTo get computers to think like humans, we need a new A.I. paradigm, one that places \"top down\" and \"bottom up\" knowledge on equal footing. Bottom-up knowledge is the kind of raw information we get directly from our senses, like patterns of light falling on our retina. Top-down knowledge comprises cognitive models of the world and how it works.\nDeep learning is very good at bottom-up knowledge, like discerning which patterns of pixels correspond to golden retrievers as opposed to Labradors. But it is no use when it comes to top-down knowledge. If my daughter sees her reflection in a bowl of water, she knows the image is illusory; she knows she is not actually in the bowl. To a deep-learning system, though, there is no difference between the reflection and the real thing, because the system lacks a theory of the world and how it works. Integrating that sort of knowledge of the world may be the next great hurdle in A.I., a prerequisite to grander projects like using A.I. to advance medicine and scientific understanding.\nI fear, however, that neither of our two current approaches to funding A.I. research -- small research labs in the academy and significantly larger labs in private industry -- is poised to succeed. I say this as someone who has experience with both models, having worked on A.I. both as an academic researcher and as the founder of a start-up company, Geometric Intelligence, which was recently acquired by Uber.\nAcademic labs are too small. Take the development of automated machine reading, which is a key to building any truly intelligent system. Too many separate components are needed for any one lab to tackle the problem. A full solution will incorporate advances in natural language processing (e.g., parsing sentences into words and phrases), knowledge representation (e.g., integrating the content of sentences with other sources of knowledge) and inference (reconstructing what is implied but not written). Each of those problems represents a lifetime of work for any single university lab.\nCorporate labs like those of Google and Facebook have the resources to tackle big questions, but in a world of quarterly reports and bottom lines, they tend to concentrate on narrow problems like optimizing advertisement placement or automatically screening videos for offensive content. There is nothing wrong with such research, but it is unlikely to lead to major breakthroughs. Even Google Translate, which pulls off the neat trick of approximating translations by statistically associating sentences across languages, doesn't understand a word of what it is translating.\nI look with envy at my peers in high-energy physics, and in particular at CERN, the European Organization for Nuclear Research, a huge, international collaboration, with thousands of scientists and billions of dollars of funding. They pursue ambitious, tightly defined projects (like using the Large Hadron Collider to discover the Higgs boson) and share their results with the world, rather than restricting them to a single country or corporation. Even the largest \"open\" efforts at A.I., like OpenAI, which has about 50 staff members and is sponsored in part by Elon Musk, is tiny by comparison.\nAn international A.I. mission focused on teaching machines to read could genuinely change the world for the better -- the more so if it made A.I. a public good, rather than the property of a privileged few.\nFollow The New York Times Opinion section on Facebook and Twitter (@NYTopinion), and sign up for theOpinion Today newsletter. \nGary Marcus is a professor of psychology and neural science at New York University.\nDRAWING (DRAWING BY JUN CEN)\n","734":"ABSTRACT\nArtificial intelligence research head Yann LeCun says Facebook is opening AI research lab in Paris and has named\u00a0six new researchers for lab (S)\n","735":"During the Capitals' second-round playoff series against the Penguins last May,\u00a0there were nearly 400 messages sent to the Capitals' Facebook account, a\u00a0significant increase\u00a0from the 10 to 15 the team received each day\u00a0on average during most of the regular season. That made it difficult for Capitals director of digital media James Heuser and his staff, who are\u00a0tasked with replying to those messages, to keep up. It also prompted\u00a0Heuser\u00a0to begin\u00a0looking into ways to use artificial intelligence to improve the Facebook user experience for Capitals fans.\nThis season, the Capitals became the first NHL team to launch a Facebook Messenger bot.\u00a0\n\"It's just another way for fans to interact with our channel and then hopefully for us to interact back with them,\"\u00a0Heuser said.\nNow, when fans first send the Capitals a message on Facebook or with the Messenger app, it no longer \"goes into a black hole,\" as Heuser described it, where a human would eventually\u00a0see it and, ideally, respond in a timely manner. Instead, users are\u00a0greeted with the following message:\n\"Hi I'm Capsbot! The hockey loving messenger bot, here to answer all of your questions about the Washington Capitals. Ask me a question or just type \"trivia,\" \"FAQ,\" or \"help at anytime. #RocktheRed!\"\nCapsbot was developed in partnership with iStrategyLabs (ISL), a D.C.-based agency that has created \"digital and physical experiences\" for brands such as NBC Universal, MillerCoors, Kroger and Facebook. ISL\u00a0CMO and managing director DJ Saul said creating something with Messenger had been on his radar since he attended\u00a0Facebook's annual developer conference in the spring, when Facebook CEO\u00a0Mark Zuckerberg\u00a0announced that the social network would enable\u00a0third parties to build chatbots into\u00a0its popular Messenger service, which has\u00a01 billion users.\nCapsbot uses artificial intelligence to provide tailored responses to a variety of frequently asked questions.\u00a0Ask Capsbot when the Capitals play next, and a link to the team's schedule will instantly appear. Ask Capsbot who the best team in the NHL is, and a link to the league standings will pop up.\u00a0There's also\u00a0a Capitals trivia game, which Heuser said has been the most popular use of the bot\u00a0thus far.\n\"It's got kinks in it, and we have trolls who love to go in there and try to break the thing's brain,\" Heuser said.\nCapsbot is still in its infancy, and its \"brain\" is still developing. ISL engineer Taylor Guidon monitors interactions with Capsbot on a daily basis using a service called Wit.ai\u00a0and tags incoming messages with proper responses. Guidon said each\u00a0new user helps train the bot further, even the trolls.\n\"We definitely get a lot of people on the East Coast saying the Penguins are better or the Rangers are better, but we trained it to\u00a0recognize the intent of messages,\" Guidon said. \"So when we see that people are trying to be slightly funny, or making fun of the team, it detects that and then gives them a canned message back.\"\nThe Capitals beta-tested Capsbot during the preseason and began promoting it at the home opener Saturday. While Capsbot will continue to be useful for FAQs, Heuser hopes fans will enjoy\u00a0interacting with it as more features,\u00a0such as\u00a0GIFs, in-game polls and giveaway contests,\u00a0are integrated\u00a0over the course of the season.\n\"We're trying to figure out what fans want to message and what they want out of it,\"\u00a0said Heuser, who continues to monitor the team's Facebook messages in the event that Capsbot fails to produce a satisfactory answer to a question.\nThe Wizards are expected to unveil their own Messenger bot, also developed by ISL, in time for the season. The Capitals and Wizards\u00a0join the\u00a0NBA's Dallas Mavericks and Sacramento Kings, the NFL's Denver Broncos and the English Premier League's Manchester City among the first professional sports teams to launch a bot.\u00a0Saul estimated that\u00a0more than half of U.S. pro sports teams will have done so by this time next year.\n\"Hopefully we're able to stay ahead of the curve, because this first version was sort of intentionally simple to test and see how people actually use it,\" he said. \"Then there's\u00a0all sorts of fun, e-commerce implications,\u00a0in-game experiences and augmentation that we could potentially build in.\"\n","736":"ABSTRACT\nIBM artificial intelligence researchers say they have succeeded in building software algorithm that accurately recognizes conversations spoken by two alternating voices (M)\n","737":"The old slogan was \"Anybody can grow up to be president,\" a fact now confirmed by the election of the most improbable American for the job, and without him having to grow up at that.\u00a0\nThe new slogan is \"Artificial intelligence will be able to do any job.\" This is also confirmed by the election of Donald Trump, since even the most primitive levels of mental processing are apparently sufficient to sit in the chair in the Oval Office and pose for a picture amidst random fake paperwork arrayed on the desk while you hold the phone pretending to be discussing something intelligently with somebody. (Be sure to gather some as-yet-unfired aides to stand behind you looking serious and impressed).\nThe worry with AI is that if our machines get smarter, the brains of actual humans will atrophy. Both halves of this are coming true. We consume what information we get via something called \"feeds,\" self-customized info-drips to sustain minimal awareness between bouts of competing for \"likes\" on our vital outgoing social updates. Observe the sea of people staring at their phones in a way that they will soon be staring into their Virtual Reality Display Helmets. The main difference between the two is that with the latter our mouths will hang open just a little wider.\nAnd so we proceed through the Trump era of outrage after insult, with everyone wondering when somebody else is going to troubleshoot what seems to be a major glitch in the reality field. But reality is getting increasingly out of focus, to say the least. Why, is that SEAWATER lapping around my ankles? Oh, yeah, my feed said something about Irma, or was it Harvey, but it once again failed to mention that either was connected to climate change, or that climate change was connected to Trump and his complete lack of understanding of the issue, or his likely-to-be-fatal policies on it.\nSiri, is climate change real? Yes, climate change has been documented extensively. Siri, what should we be doing about it? Moving aggressively to renewable energy sources, facilitated through an incentive policy on the price of carbon. Okay, I didn't really ask Siri those questions, but it wouldn't take a genius-level personal information assistant to deliver those answers. Whatever Siri or Alexa or whoever would say on this has got to be better than what we get from the lying ignoramuses who we are somehow allowing to make our actual policy.\nMeanwhile, it is still the rest of us, in our final days as independent political participants, who had better see to it that we get the Prime Directives in our computers right, before we slide forever into our Matrix-style nutrient bath, which may actually turn out disappointingly to be nothing more than a relentlessly rising and violent ocean.\n","738":"ABSTRACT\nFacebook's director of artificial intelligence Yann LeCun says company wants to deploy AI to solve problems such as fake news or spotting violence in live videos, but is unsure where line between filtering and censorship falls (M)\n","739":"ABSTRACT\nAlphabet subsidiary Google\u00a0is offering Parsey McParseface computer program that helps machines understand written English as well as SyntaxNet software programming toolkit free for anyone to distribute or modify; photo (M)\n","740":"ABSTRACT\nToyota Motor is acquiring 3% stake in Toyko-based machine learning venture\u00a0Preferred Networks for Y1 Billion ($8.2 million), latest sign that Toyota is accelerating its development of self-driving cars; photo (M)\n","741":"ABSTRACT\nApple purchases artificial-intelligence startup UK-based\u00a0software maker VocalIQ, which could help make iPhone users' interactions with virtual assistant Siri more natural (S)\n","742":"Researchers say they have created artificial intelligence that in some cases can do a better job of learning and identifying characters than humans.\u00a0\nWhile we're still a long way from the Terminator or the female automaton Ava in \"Ex Machina,\" the development is a significant leap in A.I. technology. So-called computer-vision technology is already used in a wide array of products, from cars to video-game controls.\nOn Thursday, scientists from the Massachusetts Institute of Technology, New York University and the University of Toronto reported a new type of \"one shot\" machine learningin the journal Science. The computer-vision program outperformed humans in identifying handwritten characters based on a single example. This program, the scientists say, is capable of quickly learning the characters of a language, similar to the way humans learn.\nBy exposing these these so-called neural networks to a large set of examples, they can be trained to understand speech, detect objects in images or even identify types of behavior.\nBut don't worry. They are still a long way from being able to identify Sarah Connor.\n","743":"ABSTRACT\nBusinesses are starting to deploy artificial-intelligence technology to interpret confidential employee surveys, aiming to gauge their satisfaction and find ways to improve management, which could help worker productivity and retention; drawing (M)\n","744":"ABSTRACT\nMonsanto Co is using artificial intelligence to forecast farmers need for crop seeds and save money in process (S)\n","745":"ABSTRACT\nIBM is launching Watson artificial-intelligence product to help financial institutions comply with rules and detect possible crimes;\u00a0 move puts it in competition with other firms such as Digital Reasoning Systems (M)\n","746":"ABSTRACT\nGoogle is adding new artificial-intelligence features for functions such as photos, email and mobile payments to its virtual assistant and smart speaker, part of efforts to keep up with Amazon.com and Apple and spread its services further; photo (M)\n","747":"ABSTRACT\nCompanies are increasingly using artificial intelligence algorithms to determine optimal price, predicting what rivals are charging and customers are willing to pay, which can sometimes result in rivals boosting prices together; diagram; graph; photo (M)\n","748":"IBM is taking its Watson artificial-intelligence technology into health care in a big way with industry partners, a pair of acquisitions and an ambitious agenda.\nThe initial three industry partners are Apple, Johnson & Johnson and Medtronic. On Monday afternoon, after the close of stock trading, IBM also announced it would buy two start-ups: Explorys, a spin-off from the Cleveland Clinic whose data on 50 million patients is used to spot patterns in diseases, treatments and outcomes; and Phytel, a Dallas maker of software to manage patient care and reduce readmission rates to hospitals.\nThe IBM plan, put simply, is that its Watson technology will be a cloud-based service that taps vast stores of health data and delivers tailored insights to hospitals, physicians, insurers, researchers and potentially even individual patients.\u00a0\n\"We're going to enable personalized health care on a huge scale,\" said John E. Kelly, a senior vice president who oversees IBM's research labs and new initiatives.\nTo date, IBM has done some individual projects using Watson technology with leading medical centers, including Memorial Sloan Kettering Cancer Center in New York, the University of Texas MD Anderson Cancer Center in Houston and the Cleveland Clinic. But the creation of the Watson Health unit, Mr. Kelly said, is an effort to apply the technology to the mainstream of health care.\nAnd while IBM has been commercializing Watson technology with tools for mining Twitter, weather and Internet of Things data, Mr. Kelly said Watson Health was the first move into a specific industry.\nThe Watson Health announcement is also the latest in flurry of initiatives IBM has announced this year that include new corporate partnerships as well as moves in cloud computing, data analytics and Watson.  They are evidence that IBM is intent on investing for future growth, and showing it is doing so, in a year when its financial performance is likely to lag. \nIBM has reported disappointing earnings recently, and Virginia M. Rometty, IBM's chief executive, has told industry analysts and investors that 2015 would be a transition year in which new growth businesses like Watson did not yet overcome the profit erosion in some of its traditional hardware and software products.\nIBM's broad vision of combining and analyzing health data from varied sources to improve care has been around for decades. But the company and its partners say that technology, economics and policy changes are coming together to improve the odds of making the IBM venture a workable reality. They point to improvements in artificial intelligence, low-cost cloud computing and health policy that will reward keeping patients healthy instead of the fee-for-service model in which more treatments and procedures mean more revenue.\n\"Forces in health care are aligning as never before,\" said Sandra E. Peterson, a group worldwide chairman at Johnson & Johnson in charge of information technology and new wellness programs. \"It could be a unique moment and something like this could have real legs.\"\nA focus of the Johnson & Johnson partnership with IBM will be improving patient care before and after knee and hip replacements. The company will apply Watson technology to data sources ranging from patient records to digital fitness devices and smartphone applications, which can monitor movement and vital signs. \"It will allow us to do much more integrated, personalized care,\" Ms. Peterson said.\nMedtronic, a large medical equipment maker, wants to use data intelligently to treat diabetes patients beyond providing them with its glucose monitors and insulin pumps. Medtronic devices are already digital and produce a lot of data, but the company plans to use the Watson software to spot patients trending toward trouble and automatically adjust insulin doses and send alerts to care providers and the patients themselves.\n\"The goal is dynamic, personalized care plans so you can delay or stop the progression of diabetes,\" said Hooman Hakami, executive vice president in charge of Medtronic's diabetes group.\nApple is increasingly a major supplier of health sensors, from iPhone apps to the Apple Watch. Its recently introduced HealthKit and ResearchKit software make it easier for applications and researchers to harvest health information from millions of owners of Apple products, with their permission. That data can now be plugged into Watson. \"We want to be the analytics brains behind HealthKit and ResearchKit,\" Mr. Kelly said.\nThe IBM initiative raises questions on how data is handled and about privacy. Mr. Kelly said the data scrutinized by Watson will typically be anonymized and often be read by Watson but not removed from hospital or health company data centers. \"There will be no big, centralized database in the sky,\" Mr. Kelly said.\nEven critics of health information technology say the IBM effort holds promise. \"If that future when all this stuff works is going to become real, then having some of the key players come together is the only way it's going to happen,\" said Dr. Robert M. Wachter, a professor at the University of California, San Francisco medical school and author of \"The Digital Doctor: Hope, Hype and Harm at the Dawn of Medicine's Computer Age.\" \"This could be a pretty important step along the way.\"\n","749":"As the history of computerdom tells us, old (and young) entrepreneurs neither die nor fade away.\nNo, they are dragged kicking and screaming from the companies they created by either bankruptcy lawyers or a posse of professional managers brought in to smooth the rough edges of entrepreneurial excess.\nThere is a recent and honorable exception to this sorry pattern and his name is Mitch Kapor, the founder of Lotus Development Corp. Kapor, the former disc jockey who built Lotus into one of the most formidable and profitable personal computer software companies in the world, gracefully exited this past July from the company he founded.\n\"It was very important for me to leave gracefully,\" said Kapor. \"I had a major league dread of leaving traumatically. I was obsessed with this issue so I probably erred on the side of caution.\"\u00a0\nFormer journalist and McKinsey & Co. analyst Jim Manzi is now comfortably ensconsed as Lotus CEO and Kapor now spends \"about one day a week on Lotus\" as sort of a corporate guru.\nNo, Kapor is not about to launch a new company, nor is he going to join the ranks of venture capitalists and try to find and fund the next Lotus. He is becoming a student.\n\"You know the Isaiah Berlin essay about the hedgehog and the fox?\" he asks, referring to the Oxford don's thesis that people know the world in one of two ways. \"The hedgehog knows one big thing; the fox knows lots of little things. With Lotus, I was a hedgehog. I'm now in the fox stage.\"\nKapor is a visiting scientist at Massachusetts Institute of Technology's Center for Cognitive Science.\n\"I'm trying to prepare myself to be receptive to new ideas and I've got to immerse myself in the culture here,\" he says. \"I've decided to zero in on knowledge representation as an area of study.\"\nKnowledge representation, as artificial intelligence aficionados and epistemologists will tell you, is the study of how one portrays the things one knows. This is a hot research area in artificial intelligence because how knowledge is represented determines what kind of symbols and relationships a computer can process.\nTo put it crudely, think of knowledge representation as the software of software.\nKapor is particularly interested in some of the work in \"semantic networks\" as a means to gain insight into natural language; that is, how do we know what we're saying and what it means. Kapor is interested in how one goes about modeling \"common sense systems,\" not expert systems, because common sense gives you a much more robust way to solve problems. \"I don't like the logistic, theorem proving approaches to [artificial intelligence] that much,\" he says.\n What particularly intrigues Kapor is the possibility of coming up with new \"formalisms\" (structures) to represent knowledge because \"new formalisms mean new businesses.\"\n\"The spreadsheet was a new formalism,\" he notes, and it turned out to create several successful businesses -- especially Lotus.\nBut, Kapor says, \"I don't really expect to run across a paper and say, 'Aha! this is the guy.' I think it will be more like I spot bits and pieces of interesting things and put them together and put an original twist on software innovation. \"Finding a balance between conservation and innovation is critical for software companies,\" Kapor says, who notes that his own bias is for innovation. \"But market leaders have to be able to protect their positions.\"\nKapor also points out that there is a \"tectonic plates analogy to this industry -- there are powerful forces going against each other. On one hand, semiconductor technology [such as Intel's 386 microprocessor] is creating powerful new capacity; on the other hand, the installed base of computers and IBM are a braking factor to exploiting that capacity.\"\nDoes this uncertainty create opportunity for entrepreneurs? \"This is a good time not to get into the software business,\" said the man who founded Lotus.\n","750":"ABSTRACT\nPhoto of\u00a0 Nvidia demonstration of law-enforcement system that recognizes vehicles and people as chip makers develop processors for use in AI market\n","751":"ABSTRACT\nAndy Kessler commentary contends artificial intelligence in form of machine learning and deep learning is steadily making gains but no one really knows how it works and results can be surprising;\u00a0 sees need for legal safe harbor and verification\n","752":"ABSTRACT\nSamsung Electronics is considering making its artificial-intelligence assistant dedicated feature on its next Galaxy S8 smartphone;\u00a0 company is in dire need of rebound after massive recall of its Galaxy S7; photo (M)\n","753":"ABSTRACT\nBill Ingram commentary contends artificial intelligence and 'big data' will allow robots to help boost productivity in ways that will allow humans to work more creatively, rather than displacing human workers\n","754":"ABSTRACT\nJapanese robot maker Fanuc says it plans to acquire 6% stake in Tokyo-based Preferred Networks as part of effort to develop industrial machines that can learn and repair themselves (M)\n","755":"ABSTRACT\nLi Yuan China Circuit column on interview with Baidu vice chairman, group president and COO Qi Lu, who says he is confident he can turn Baidu around and take on Google again by focusing on AI; graph (M)\n","756":"ABSTRACT\nAmazon.com is launching new version of its artificial-intelligence speakers called Echo Look, that will help users compare their outfits but could go beyond fashion to include communications and security;\u00a0 consumers could resist over privacy concerns; photo (M)\n","757":"ABSTRACT\nCarnegie Mellon University artificial-intelligence experts have developed program that plays Texas Hold 'Em poker almost perfectly, marking advance in game theory that could have applications in military strategy, national security, medicine and other areas; photo (M)\n","758":"Two years after a Stanford professor drew 160,000 students from around the globe to a free online course on artificial intelligence, starting what was widely viewed as a revolution in higher education, early results for such large-scale courses are disappointing, forcing a rethinking of how college instruction can best use the Internet.\nA study of a million users of massive open online courses, known as MOOCs, released this month by the University of Pennsylvania Graduate School of Education found that, on average, only about half of those who registered for a course ever viewed a lecture, and only about 4 percent completed the courses.\nMuch of the hope -- and hype -- surrounding MOOCs has focused on the promise of courses for students in poor countries with little access to higher education. But a separate survey from the University of Pennsylvania released last month found that about 80 percent of those taking the university's MOOCs had already earned a degree of some kind.\u00a0\nAnd perhaps the most publicized MOOC experiment, at San Jose State University, has turned into a flop. It was a partnership announced with great fanfare at a January news conference featuring Gov. Jerry Brown of California, a strong backer of online education. San Jose State and Udacity, a Silicon Valley company co-founded by a Stanford artificial-intelligence professor, Sebastian Thrun, would work together to offer three low-cost online introductory courses for college credit.\nMr. Thrun, who had been unhappy with the low completion rates in free MOOCs, hoped to increase them by hiring online mentors to help students stick with the classes. And the university, in the heart of Silicon Valley, hoped to show its leadership in online learning, and to reach more students.\nBut the pilot classes, of about 100 people each, failed. Despite access to the Udacity mentors, the online students last spring -- including many from a charter high school in Oakland -- did worse than those who took the classes on campus. In the algebra class, fewer than a quarter of the students -- and only 12 percent of the high school students -- earned a passing grade.\nThe program was suspended in July, and it is unclear when, if or how the program will resume. Neither the provost nor the president of San Jose State returned calls, and spokesmen said the university had no comment.\nWhatever happens at San Jose, even the loudest critics of MOOCs do not expect them to fade away. More likely, they will morph into many different shapes: Already, San Jose State is getting good results using videos from edX, a nonprofit MOOC venture, to supplement some classroom sessions, and edX is producing videos to use in some high school Advanced Placement classes. And Coursera, the largest MOOC company, is experimenting with using its courses, along with a facilitator, in small discussion classes at some United States consulates.\nSome MOOC pioneers are working with a different model, so-called connectivist MOOCs, which are more about the connections and communication among students than about the content delivered by a professor.\n''It's like, 'The MOOC is dead, long live the MOOC,' '' said Jonathan Rees, a Colorado State University-Pueblo professor who has expressed fears that the online courses would displace professors and be an excuse for cuts in funding. ''At the beginning everybody talked about MOOCs being entirely online, but now we're seeing lots of things that fall in the middle, and even I see the appeal of that.''\nThe intense publicity about MOOCs has nudged almost every university toward developing an Internet strategy.\nGiven that the wave of publicity about MOOCs began with Mr. Thrun's artificial-intelligence course, it is fitting that he has become emblematic of a reset in the thinking about MOOCs, after a profile in Fast Company magazine that described him as moving away from college classes in favor of vocational training in partnerships with corporations that would pay a fee.\nMany educators saw the move as an admission of defeat for the idea that online courses would democratize higher education -- and confirmation that, at its core, Udacity, a company funded with venture capital, was more interested in profits than in helping to educate underserved students.\n''Sebastian Thrun put himself out there as a little bit of a lightning rod,'' said George Siemens, a MOOC pioneer who got funding from the Bill & Melinda Gates Foundation for research on MOOCs, and last week convened the researchers at the University of Texas at Arlington to discuss their early results. ''Whether he intended it or not, that article marks a substantial turning point in the conversation around MOOCs.''\nThe profile quoted Mr. Thrun as saying the Udacity MOOCs were ''a lousy product'' and ''not a good fit'' for disadvantaged students, unleashing a torrent of commentary in the higher-education blogosphere.\nMr. Thrun took issue with the article, and said he had never concluded that MOOCs could not work for any particular group of students.\n''I care about education for everyone, not just the elite,'' he said in an interview. ''We want to bring high-quality education to everyone, and set up everyone for success. My commitment is unchanged.''\nWhile he said he was ''super-excited'' about working with corporations to improve job skills, Mr. Thrun said he was working with San Jose State to revamp the software so that future students could have more time to work through the courses.\n''To all those people who declared our experiment a failure, you have to understand how innovation works,'' he wrote on his blog. ''Few ideas work on the first try. Iteration is key to innovation. We are seeing significant improvement in learning outcomes and student engagement. ''\nSome draw an analogy to mobile phones, which took several generations to progress from clunky and unreliable to indispensable.\nMr. Thrun stressed that results from the second round of the San Jose experiment over the summer were much improved, with the online algebra and statistics students doing better than their on-campus counterparts. Comparisons are murky, though, since the summer classes were open to all, and half the students already had degrees.\nSome San Jose professors said they found the MOOC material useful and were disappointed that the pilot was halted.\n''We had great results in the summer, so I'm surprised that it's not going forward,'' said Julie Sliva, who taught the college algebra course. ''I'm still using the Udacity videos to support another course, because they're very helpful.''\nMr. Siemens said what was happening was part of a natural process. ''We're moving from the hype to the implementation,'' he said. ''It's exciting to see universities saying, 'Fine, you woke us up,' and beginning to grapple with how the Internet can change the university, how it doesn't have to be all about teaching 25 people in a room.\n''Now that we have the technology to teach 100,000 students online,'' he said, ''the next challenge will be scaling creativity, and finding a way that even in a class of 100,000, adaptive learning can give each student a personal experience.''\n","759":"MOUNTAIN VIEW, Calif. -- As we merged with freeway traffic on Highway 101, one of Silicon Valley's busiest freeways, Christopher Urmson, the man in the driver's seat, gestured, not touching the steering wheel.\nMr. Urmson is a Google engineer, and last Wednesday, I sat belted in the back seat as he talked and as the car, a Toyota Prius equipped by Google with radar, video, motion sensors and a GPS device, drove itself at 60 miles an hour.\nMy eyes were glued to the 22-inch three-dimensional color display in front of Dmitri Dolgov, an artificial intelligence researcher at Google who was riding shotgun. It showed the world around us in great detail, down to painted lane markers, stop signs, traffic lights and a sliding green column that indicated our path. A blocky yellow object representing a car was coming up behind us in the lane we were entering as the robotic, female voice of the Prius announced, ''Preparing to change lane.''\u00a0\n''Don't worry, we have plenty of room,'' Dr. Urmson said.\nWe followed a 12-mile planned route in a vehicle that looks different from the striking Google Street View cars, which are distinguished by a six-and-a-half-foot-tall camera mast.\nInstead, Google's autonomous Prius has a more modest if no less striking sensor mounted in the center of the vehicle's roof. Known as Lidar, or Light Detection and Ranging, it provides a continuously updated three-dimensional map of the world at centimeter accuracy extending for more than 230 feet around the car.\nThe Lidar is supplemented by four standard automotive radars with less resolution and greater range, three in front and one in the rear. Inside the car, positioned next to the rear-view mirror, is a high-resolution video camera to detect street lights and moving obstacles like pedestrians and bicyclists. The Prius also has a GPS receiver and an inertial motion sensor.\nThe same, or very similar, sensor array was used on many of the cars that competed in the 2007 Defense Advanced Research Projects Agency Urban Challenge, a competition for autonomous automobiles held on the grounds of a former Air Force base.\nWhat has changed -- in addition to the use of public roads -- is the computing power available to the designers and the artificial intelligence software. Designers have made advances in detecting pedestrians, street lights and lane markers, as well as in resolving conflicting sensor data and in motion planning to avoid obstacles. A traffic-cop program monitors all the car's processes.\nOne main technique used by the Google team is known as SLAM, or simultaneous localization and mapping, which builds and updates a map of a vehicle's surroundings while keeping the vehicle located within the map. To make a SLAM map, the car is first driven manually along a route while its sensors capture location, feature and obstacle data. Then a group of software engineers annotates the maps, making certain that road signs, crosswalks, street lights and unusual features are all embedded. The cars then drive autonomously over the mapped routes, recording changes as they occur and updating the map. The researchers said they were surprised to find how frequently the roads their robots drove on had changed.\nUnsolved problems remain. Sebastian Thrun, a Google engineer and head of the Stanford Artificial Intelligence Laboratory, said the design team was stumped by how to train its system to follow the hand signals that a human traffic cop or crossing guard might make.\nDespite its limitations -- and they are significant -- the Google car's abilities occasionally gave me goose bumps, particularly compared with previous rides in autonomous vehicles. In 2005, I was in such a vehicle with Dr. Thrun and two others when it swerved off a gravel road at more than 20 miles per hour. Unlike the newer Google cars, the only way to wrest control from that car was to hit a large red button on the driver's console, and Dr. Thrun was simply unable to push the button quickly enough to keep the car from driving into the brush.\nFor me, the tour de force of the new car came when the vehicle halted at a stop sign to make a right turn. It waited patiently for a vehicle in front of it to turn, then inched forward. A car was approaching from the left, but the Prius pulled into the far right lane, and I realized that it ''knew'' the other car was not in our lane even though it was passing close to us. There was no need to hit the red button.\n","760":"The first thing that you might think when you see visual images produced by WordsEye is that you are viewing the work of an extremely talented and creative artist. Pictures of a dinosaur standing next to a chocolate cake, shiny gray spaceships on the lunar surface and rainbow-colored African safari animals are easily worth 1,000 words.\nHowever, these images were not created by a human, they were created by a machine. WordsEye is a new Web app that translates a few lines of user text about a scene into an artistic image using artificial intelligence.\u00a0\nFor example, a user might type in, \"The canyon is tall. An enormous chocolate cake is three feet to the left of the dinosaur.\" That would trigger the app to, first, understand the words \"chocolate cake\" \"dinosaur\" and \"canyon\" from its library of concepts and then come up with an artistic representation of how these three concepts might fit together within a single scene.\nIn November, WordsEye released a one-minute YouTube video showcasing some of the original creations that have been made in just the first month of private beta testing. And, on the WordsEye Web site, there's a gallery of images that users have created with the app and that are being shared across social networks.\nIn the surreal @wordseye cosmos, furry cats can be conjured up in hats #Magic #typeapicture https:\/\/t.co\/NItoPpoNrg pic.twitter.com\/RGLXcZycQ4\nGetting from word to image is actually harder than it sounds, since it relies on a specific branch of artificial intelligence known as natural language processing. In order to go from \"text to scene\" and \"type a picture,\" you first have to be able to extract meaning from the words, and then use a combination of mathematics, probability and statistics to determine the relationships between these words. Then, you have to be able to pull up the right images that match thee words and apply a few artistic filters to transform them into gorgeous pieces of art.\nThus far, WordsEye is still in private beta, but early plans are for it to become a type of social network, possibly along the lines of an Instagram, which also got its start as a place for people to share artistic images and then have people respond to these images with comments and likes. WordsEye refers to itself as \"a new social network for creative expression and visual banter.\" There are also plans to release an Android and iOS app, which would open up WordsEye to mobile users.\nOnce WordsEye appeared on Product Hunt (where it's been upvoted 245\u00a0times), the suggestions about what's next for the \"type a picture\" concept started to pour in. Some Product Hunt users have suggested that WordsEye could become a tool for virtual reality\u00a0or gaming, a way to teach foreign languages, or an easy way to create explainer articles about scientific concepts. Or journalists might use it to create images to accompany an article, rather than rely on stock or archival images.\nHowever there might be a bigger story here, and that's how artificially intelligent machines are assuming traits - such as creativity - that were once the exclusive preserve of humans. We might refer to machines as \"intelligent\" or \"rational\" or \"efficient,\" but we don't typically think of them as \"creative.\"\nThat might be changing, however, as artistic fields such as painting embrace AI. For now, artificial intelligence applications such as WordsEye are limited by the number of vocabulary words they can understand, as well as by the limits of natural language processing and the types of artistic models the app can create. It's still a work in progress.\nHowever, there are some interesting implications if machines actually do become creative - or at least, convince humans of the fact that they possess the attributes of creativity. Machines might challenge us, for example, to rethink what we mean when we use the word \"creativity.\"\nWe traditionally think of creativity as being a gift, or a spark, or a light bulb moment - but it might just be a really clever algorithm. Salvador Dali and Pablo Picasso were just like you or me; they just had better algorithms and a few cool filters.\nFrom that perspective, it's worth comparing, say, some of the all-time famous surrealist paintings with a few of the images created by WordsEye. Which would you prefer, for example, the melting clocks of Dali's \"Persistence of Memory\" or an image of a surrealist planetary terrain populated by dinosaurs and chocolate cakes?\nAt some point, it's conceivable that there will exist an art market for machine creativity, just as there exists an art market for human creativity. In an era where collectors routinely pay tens of millions of dollars for the world's most famous paintings, how much would they be willing to pay for a piece of art created by a machine?\n","761":"ABSTRACT\nAngus Loten article in Journal Report: Artificial Intelligence Issue notes systems of automated AI assistants will engage potential buyers in future, before handing these leads off to human sales reps to close deals; cartoon; graph (M)\n","762":"ABSTRACT\nHeidi Vogt article in Journal Report: Artificial Intelligence Issue notes three AI experts debate what government should do in preparation for AI-dominated robot world to come-and when they should be doing it; drawing (M)\n","763":"ABSTRACT\nAlexandra Samuel article in Journal Report: Artificial Intelligence Issue notes that there still will be many jobs in world's future, but preparing your children for this radical change becomes key, by making sure they are ready; drawing (M)\n","764":"ABSTRACT\nTed Greenwald article in Journal Report: Artificial Intelligence Issue notes upcoming chatbots, such as AVA, customer-service chatbot created by Autodesk, will be smarter and more useful, equipped with emotional intelligence, but they will still include plenty of limits; photo (M)\n","765":"ABSTRACT\nChristopher Mims Keywords column on limitations of artificial intelligence; cites example of Facebook's call to use humans to check images on its antirevenge-porn service; photo (M)\n","766":"ABSTRACT\nToyota Motor has been working on self-driving, artificial-intelligence project dubbed Yui, which will let onboard assistant chat with driver, gauge mood and offer to take over for driver, who may then enjoy nice massage from seat; photo (M)\n","767":"ABSTRACT\nGoogle has recently posted at least four artificial-intelligence-related jobs in Beijing, where it needs local partner;\u00a0 China has hundreds of millions of computer users and fewer privacy concerns than West, making it ideal to advance some AI technology (M)\n","768":"ABSTRACT\nJames Mackintosh Streetwise column notes limitations with artificial intelligence for automated investment, including its ability to find illusory patterns, lack of transparency in its decision making and risk that its successes are its undoing; graph (M)\n","769":"ABSTRACT\nAndy Kessler commentary contends artificial intelligence and machine learning could help provide cheaper and higher quality health care if obstacles were removed, notably failure to make most of electronic health records; photo\n","770":"ABSTRACT\nFinancial industry is getting more comfortable using artificial intelligence for things like generating information reports;\u00a0 services are gaining traction as technology becomes more sophisticated and banks seek to cut costs and boost efficiency; drawing (M)\n","771":"SAN FRANCISCO -- Machines are starting to learn tasks on their own. They are identifying faces, recognizing spoken words, reading medical scans and even carrying on their own conversations.\nAll this is done through so-called neural networks, which are complex computer algorithms that learn tasks by analyzing vast amounts of data. But these neural networks create a problem that scientists are trying to solve: It is not always easy to tell how the machines arrive at their conclusions.\nOn Tuesday, a team at Google took a small step toward addressing this issue with the unveiling of new researchthat offers the rough outlines of technology that shows how the machines are arriving at their decisions.\u00a0\n\"Even seeing part of how a decision was made can give you a lot of insight into the possible ways it can fail,\" said Christopher Olah, a Google researcher.\nA growing number of A.I. researchers are now developing ways to better understand neural networks. Jeff Clune, a professor at University of Wyoming who now works in the A.I. lab at the ride-hailing company Uber, called this \"artificial neuroscience.\"\nUnderstanding how these systems work will become more important as they make decisions now made by humans, like who gets a job and how a self-driving car responds to emergencies.\nFirst proposed in the 1950s, neural networks are meant to mimic the web of neurons in the brain. But that is a rough analogy. These algorithms are really series of mathematical operations, and each operation represents a neuron. Google's new research aims to show -- in a highly visual way -- how these mathematical operations perform discrete tasks, like recognizing objects in photos.\nInside a neural network, each neuron works to identify a particular characteristic that might show up in a photo, like a line that curves from right to left at a certain angle or several lines that merge to form a larger shape. Google wants to provide tools that show what each neuron is trying to identify, which ones are successful and how their efforts combine to determine what is actually in the photo -- perhaps a dog or a tuxedo or a bird.\nThe kind of technology Google is discussing could also help identify why a neural network is prone to mistakes and, in some cases, explain how it learned this behavior, Mr. Olah said. Other researchers, including Mr. Clune, believe they can also help minimize the threat of \"adversarial examples\" -- where someone can potentially fool neural networks by, say, doctoring an image.\nResearchers acknowledge that this work is still in its infancy. Jason Yosinski, who also works in Uber's A.I. lab, which grew out of the company's acquisition of a start-up called Geometric Intelligence, called Google's technology idea \"state of art.\" But he warned it may never be entirely easy to understand the computer mind.\n\"To a certain extent, as these networks get more complicated, it is going to be fundamentally difficult to understand why they make decisions,\" he said. \"It is kind of like trying to understand why humans make decisions.\"\nFollow Cade Metz on Twitter: @CadeMetz. \nPHOTO: On the left is an image that was put through a neural network trained to classify objects in images -- for example, to tell whether an image includes a vase or a lemon. On the right is a visualization of what one layer in the middle of the network detected at each position of the image. The neural network seems to be detecting vase-like patterns and lemon-like objects. (PHOTOGRAPH BY The Building Blocks of Interpretability FOR THE NEW YORK TIMES)\n","772":"Correction Appended\nWhen it comes to artificial intelligence and jobs, the prognostications are grim. The conventional wisdom is that A.I. might soon put millions of people out of work -- that it stands poised to do to clerical and white collar workers over the next two decades what mechanization did to factory workers over the past two. And that is to say nothing of the truckers and taxi drivers who will find themselves unemployed or underemployed as self-driving cars take over our roads.\nBut it's time we start thinking about A.I.'s potential benefits for society as well as its drawbacks. The big-data and A.I. revolutions could also help fight poverty and promote economic stability.\nPoverty, of course, is a multifaceted phenomenon. But the condition of poverty often entails one or more of these realities: a lack of income (joblessness); a lack of preparedness (education); and a dependency on government services (welfare). A.I. can address all three.\nFirst, even as A.I. threatens to put people out of work, it can simultaneously be used to match them to good middle-class jobs that are going unfilled. Today there are millions of such jobs in the United States. This is precisely the kind of matching problem at which A.I. excels. Likewise, A.I. can predict where the job openings of tomorrow will lie, and which skills and training will be needed for them.\nHistorically we have tended to shy away from this kind of social planning and job matching, perhaps because it smacks to us of a command economy. No one, however, is suggesting that the government should force workers to train for and accept particular jobs -- or indeed that identifying these jobs and skills gaps needs to be the work of the government. The point is that we now have the tools to take the guesswork out of which jobs are available and which skills workers need to fill them.\nSecond, we can bring what is known as differentiated education -- based on the idea that students master skills in different ways and at different speeds -- to every student in the country. A 2013 study by Indian researchers found that nearly 40 percent of medical students held a strong preference for one mode of learning: Some were listeners; others were visual learners; still others learned best by doing.\nOur school system effectively assumes precisely the opposite. We bundle students into a room, use the same method of instruction and hope for the best. A.I. can improve this state of affairs. Even within the context of a standardized curriculum, A.I. \"tutors\" can home in on and correct for each student's weaknesses, adapt coursework to his or her learning style and keep the student engaged.\nToday's dominant type of A.I., also known as machine learning, permits computer programs to become more accurate -- to learn, if you will -- as they absorb data and correlate it with known examples from other data sets. In this way, the A.I. \"tutor\" becomes increasingly effective at matching a student's needs as it spends more time seeing what works to improve performance.\nThird, a concerted effort to drag education and job training and matching into the 21st century ought to remove the reliance of a substantial portion of the population on government programs designed to assist struggling Americans. With 21st-century technology, we could plausibly reduce the use of government assistance services to levels where they serve the function for which they were originally intended.\nBig data sets can now be harnessed to better predict which programs help certain people at a given time and to quickly assess whether programs are having the desired effect. To use an advertising analogy, this would be the difference between placing a commercial on prime-time television and doing so through micro-targeted analytics. Guess which one is cheaper and better able to reach the target population?\nAs for the poisonous effect of ideology on the debate over public assistance: Big data promises something closer to an unbiased, ideology-free evaluation of the effectiveness of these social programs. We could come closer to the vision of a meritocratic, technocratic society that politicians from both parties at state and local levels -- those closest to the practical problems their constituents face -- have begun to embrace.\nEven Congress occasionally wakes up from its partisan slumber to advance the cause of technology in public policy decision-making: In 2016, Congress voted for and President Barack Obama authorized the creation of the Commission on Evidence-Based Policy Making. The act creating the commission was sponsored by Senator Patty Murray, a Democrat, and Paul Ryan, the House speaker. Before the commission expired in September 2017, it used government data to evaluate the effectiveness of government policy and made recommendations based on its findings.\nThis provides one more indication of the promise of A.I. and big data in the service of positive, purposeful public good. Before we dismiss these new technologies as nothing more than agents of chaos and disruption, we ought to consider their potential to work to society's advantage.\nFollow The New York Times Opinion section on Facebook and Twitter (@NYTopinion), and sign up for the Opinion Today newsletter. \nElisabeth A. Mason is the founding director of the Stanford Poverty and Technology Lab and a senior adviser at the Stanford Center on Poverty and Inequality.\nCorrection: January 8, 2018, Monday\nThis article has been revised to reflect the following correction: A previous version of this article misidentified the source of a study on medical students and learning. The study was conducted by researchers in India, not by the National Institutes of Health.\n","773":"If a teenager gets bullied on social media, it's usually up to her to report the incident. But one day, an automated tool might spot the harassment, ask her if she needs help, tell a moderator to step in on her behalf or notify her parents.\u00a0\nWhile rudimentary filters for bad language have existed for years, tech companies and others are experimenting with tools that use machine learning, crunching through huge numbers of online interactions to \"train\" themselves to identify bullying.\nThe Google subsidiary Jigsaw has developed a tool called Conversation AI, which uses machine learning to identify harassment. Jigsaw trained the tool with the help of institutions, including The New York Times, which provided 17 million reader comments from The Times's website along with decisions that moderators had made about whether to accept or reject them. Analyzing comments that moderators had flagged as abusive helped Conversation AI improve.\nMary Aiken, a cyber security expert, proposes a similar approach in her new book \"The Cyber Effect.\" She hopes to create a database of user-submitted examples of harassment that can be used to develop an anti-bullying tool.\nA number of other researchers and companies, including SRI International, which created Siri, have explored\u00a0machine-learning responses to online harassment in recent years.\nThese efforts have the potential to change the way social networks and other technology and media companies handle harassment. Bullying is difficult to identify in part because the same phrase can be insulting in one context and affectionate in another, said Norman Winarsky, the former president of SRI's ventures group. But by analyzing millions or billions of conversations, new tools could begin to identify patterns that distinguish innocent comments from harassment.\nThe technology isn't foolproof. When Andy Greenberg at Wired tested Conversation AI with a real example of harassment directed at another writer, the tool rated the obvious threat as fairly benign.\nWhile AI tools are likely to improve over time, humans will probably always need to monitor them for accuracy and make the final decision in difficult cases. Companies that use the tools will have to ensure that their users consent to having their interactions monitored by a computer program. Identifying harassment, of course, is only the first step to protecting users of social networks. Just as important is how quickly the companies respond to acts of harassment once their tools spot that behavior.\n","774":"WITH the de facto end of summer upon us, there is a natural tendency for the pulse to quicken, for the mind to snap out of repose and for debate to begin on the immediate burning questions of the fall. Will an invasion of Iraq come before Christmas? Which fashion fad will we succumb to first -- pencil skirts or rugby shirts?\n     The Web site Longbets.org urges a longer -- and less idle -- view. Designed to sharpen long-term thinking on issues of social or scientific significance, the nonprofit site (a spinoff of the Long Now Foundation, headed by veteran Silicon Valley pundits Stewart Brand and Kevin Kelly), solicits prophecy backed by currency. To divert gamblers and kibitzers from the mesmerizing press of the next five minutes, the minimum bet is $1,000 and the minimum period is two years. Bets are tax deductible and winnings (all in good time) go to a charity of the victor's choice. \n Since its debut in April, Longbets has published 11 bets on topics ranging from where alien life will be discovered to when commercial planes will regularly fly without pilots. The prognosticators with deep pockets include technology executives, scientists, writers, philosophers -- and the actor Ted Danson. Both sides must post a defense of their position, and anyone can participate in the ensuing online discussion.\u00a0\nBefore the short-range seduction of September kicks in, then, here are excerpts from conflicting visions of a more distant future, and some wagers still in search of a taker.   AMY HARMONThe biggest bet (so far)\n\"A computer or 'machine intelligence' will pass the Turing test by 2029\"\nMost Longbettors stick to the minimum stake of $1,000 each. Here Mitchell Kapor, the founder of Lotus Development, explains why he was confident enough to wager a $10,000 that no computer in the next 27 years will be able to impersonate a human well enough to fool a human judge (the Turing test): \nWhile it is possible to imagine a machine obtaining a perfect score on the SAT or winning Jeopardy -- since these rely on retained facts and the ability to recall them -- it seems far less possible that a machine can weave things together in new ways or to have true imagination in a way that matches everything people can do, especially if we have a full appreciation of the creativity people are capable of. . . . Computers look relatively smarter in theory when those making the estimate judge people to be dumber and more limited than they are.\nRay Kurzweil, an artificial intelligence pioneer and entrepreneur, on why he is equally sure Mr. Kapor is wrong (and why he believes the $20,000 total will go to his selected charity, the Kurzweil Foundation): \nThe brain is self-organizing, which means that it is created with relatively little innate knowledge. Most of its complexity comes from its own interaction with a complex world. Thus it will be necessary to provide an artificial intelligence with an education just as we do with a natural intelligence. But here the powers of machine intelligence can be brought to bear. Once we are able to master a process in a machine, it can perform its operations at a much faster speed than biological systems. As I mentioned, contemporary electronics is already more than 10 million times faster than the human nervous system's electrochemical information processing. Once an AI masters human basic language skills, it will be in a position to expand its language skills and general knowledge by rapidly reading all human literature and by absorbing the knowledge contained on millions of Web sites. . . . The longest bet (so far)\n\"At least one human alive in the year 2000 will still be alive in 2150\"\nUnless one of the bettors turns out to be the human in question, they'll never know who won. But according to the rules of the service, which is set up to be administered for several centuries, Longbets will award the winnings in 148 years \"with great fanfare.\" Peter Schwartz, a futurist, makes a case for scientific progress:\nIf one simply looks at the historical trend, one finds that over the last century, we have nearly doubled human life span. The average lifespan of human beings (average, not maximum) has gone from about 45 to about 85. With the advances in microbiology and molecular biology, there's no reason to imagine that we won't do at least as much in the next century. . . . \nMelody K. Haller replies with her own unique interpretation of Darwin: \nHumans may succeed in overcoming self-limiting life spans but the result is likely to be contra-indicatory to the continued success of humans and other life. . . . I am betting money against his prediction purely because I believe that the further radical prolonging of human (and pet) longevity would not benefit the human species. . . .The sports bet\n\"The U.S. men's soccer team will win the World Cup before the Red Sox win the World Series.\"\nNestled in among propositions on the fate of the universe and the likely future Nobel Prize, this bet might seem to flout the \"social or scientific significance\" rule, but Mike Elliot, an editor-at-large for Time magazine, somehow manages to wrap in globalization and metaphysics:\nAs immigration and technology continue to make the U.S. a more international nation, so the quality of its soccer team will continue to increase. Already, American teenagers can hold their own with players from more established countries, while players like Claudio Reyna and Kasey Keller have become acknowledged international stars. The Curse of the Bambino, on the other hand, is one of those mystical truths that are beyond the reach of human intervention. Cheers, Ted.\nPerhaps not one for intellectual pretense, Ted Danson, who played a retired Red Sox pitcher on the sitcom \"Cheers\" for 11 seasons, grounds his reply in pragmatism:\nThe Red Sox have had such bad luck in the 20th century, I have to believe that in the new millennium it can only get better. Besides, statistically, scoring goals is harder than hitting a home run, and in the World Cup, you have the whole WORLD against you, but in baseball, the Red Sox only really have to beat the Yankees.\nCount on a discussion group participant, \"micromike\" to return to more sober astronomical postulation. In a post titled \"I feel bad for Danson,\" he writes:\nIt must be hard living a delusional life thinking that Boston will ever win the World Series!!! A team from outer space will win the series before Boston does. . . .\nTired of the future? Ready to embrace back-to-school shopping for things you can use, say, next week? But wait, before deliberations begin on what's for dinner, say, tonight, here's a sampling of the more than 20 bets that remain open. Any can be joined with $1,000 and a few clicks at www.longbets.org.\n* By 2100 a world government will be in place and in control of: business law, environmental law and weapons of mass destruction. \n* By 2020, bioterror or bioerror will lead to one million casualties in a single event.\n* By the year 2015 solar electricity will be as cheap or cheaper than that produced by fossil fuels. \n* By 2030 all surgical anesthesia will be administered and monitored by computers, with no need for professional medical supervision beyond the surgeon. \n* By 2050, we will receive intelligent signals from outside our solar system. \n* By 2070, at least six countries will have officially implemented a 4-day working week.\n","775":"To be Asian in America is to be quizzed, constantly, about your ethnicity. What are you? Where are you from? No, but where are your parents from?           \nNowadays, such\u00a0questions\u00a0are more awkward than ominous. But there was a\u00a0time when this\u00a0was\u00a0a national obsession of sorts - when splashy\u00a0magazines\u00a0like\u00a0Life\u00a0published guides to help readers\u00a0distinguish between the \"parchment yellow\" Chinese with their \"finely-bridged\" noses, and the \"earthy yellow\" Japanese, with their \"massively boned faces.\"\nRecently, computer scientists at the University of Rochester tried to teach an algorithm to tell the difference between Chinese, Japanese and Korean faces. They wanted to explore how advancements in artificial intelligence have made it easier for computers to interpret pictures in sophisticated ways. But, intentionally or not, their research taps into the uncomfortable history of how Asians have struggled to fit into American life.\u00a0\nThe scientists were inspired by a quiz created by Japanese American web designer Dyske Suematsu. Fifteen years ago, Suematsu decided, half-jokingly, to investigate the stereotype that Asians all look alike. He threw a party in New York City and invited Asian friends. He put their portraits on the Internet and asked strangers to guess their ethnicity.\nThe website was a huge hit, quickly becoming one of the web's first viral sensations. Suematsu says that millions have registered and taken the test. On average, people identify 7 out of 18 photos correctly - an accuracy rate of about 39 percent. That's barely better than pure guessing, which would yield an accuracy rate of 33 percent, on average.\n\"This is a challenging task even for humans,\" said\u00a0Jiebo Lu, a professor of computer science at the University of Rochester.\u00a0\"I asked some of my students to take the test and they all failed horribly - even though all of them were\u00a0Asian.\"\nLu and his students\u00a0suspected that a trained artificial intelligence might be able to perform as well, or even better. Recently, they collected hundreds of thousands of pictures of East Asian faces and fed them through an algorithm to figure out just what made Chinese, Japanese, and Korean people look different. In a draft report detailing their results, they provide samples of the pictures fed to the computer.\nBut despite what they expected to be a difficult task,\u00a0the scientists were surprised to discover that the computer could achieve accuracy rates of over 75 percent. This is far better than humans performed on Suematsu's quiz. The computer's advantage is that it could draw on a vast library of faces, Lu explained. \"Our machine has seen far more examples than any living person,\" he said.\nLack of experience is a major reason humans sometimes\u00a0struggle to tell foreigners apart. Psychologists call it the \"cross-race\" effect: We\u00a0are much better at distinguishing members of our\u00a0own race or ethnicity than members of other races or ethnicities.\nStudies suggest that with training, people can improve at recognizing the faces of people from different ethnic backgrounds. As Lu and his colleagues have demonstrated, computers might even be better than we are at noticing\u00a0some of these subtle distinctions.\nIt's not all about physical proportions. When the scientists went to investigate how the computer was making its decisions, they discovered an interesting pattern. Many of the cues that stood out to the algorithm were cultural features, like hairstyles or glasses or facial expressions. This makes sense, since the people of China, Japan and Korea have somewhat shared ancestries, but distinct senses of fashion.\nWithout being told, the computer seemed to realize that our concepts of race\u00a0and national identity transcend genetics - they are cultural ideas.\nLu imagines that this kind of research might one day be used in targeted ads or counterterrorism. Being able to discern a person's nationality from their profile photo would help marketers better tailor online messages. Or, in a more Orwellian context, airports could set up cameras to racially profile people in the name of homeland security.\nIt may be more interesting though, to view the project almost as a work of conceptual art. Lu and his co-authors are all scientists of Chinese heritage working in the United States, a nation that has not always welcomed Asians, or treated them with respect.\nJust a few weeks ago, \"The O'Reilly Factor,\" the Fox News show, sent reporter Jesse Watters to Manhattan's Chinatown, where he proceeded to mock the residents with a mash-up\u00a0of stereotypes. \"In a stunning thirty-second clip, Watters asks a man if he knows karate (a Japanese style of martial arts) and then, confusingly enough, proceeds to attempt Tae Kwon Do (a Korean style of martial arts) with nunchucks (which originated in Japan),\" writes the New Yorker's Jiayang Fan. \"The point is clear: no one can tell these Orientals apart anyway!\"\nThe work of Lu's lab\u00a0rebukes the lazy\u00a0notion that Asians all look the same. If a software routine can be trained to easily recognize the differences between a Chinese person, a Japanese person and a Korean person, then that challenges Americans to pay close attention, to work harder to understand the diverse mix of people living in our\u00a0nation\u00a0today.\nAt the same time, it's unclear how well the program would fare if it were presented only with pictures of Asian people who grew up in the United States. If the computer is mainly making judgments based on cultural attributes, it might completely fail at distinguishing between Korean Americans, Japanese Americans, and Chinese Americans.\nThat, too, would be remarkable - it would illustrate\u00a0how identity is not something we are born with, but something that we build piece by piece.\nAsk any\u00a0Asian American. Every\u00a0single one has a well-worn reply to the question: So where are you really from? When someone demands your ancestry at the beginning of a conversation - as often happens if you're Asian - it implies that your genetic history\u00a0is the most interesting thing about you.\u00a0This\u00a0gets\u00a0tiresome no matter how proud you are of your heritage.\nIf smartphones could operate some version of the software from Lu's lab, trained on the different nationalities of the world, we could all avoid\u00a0a lot of awkwardness. With our new mental prosthetics, we might learn to look at each other in new ways. But we might not\u00a0master a\u00a0lesson that truly matters:\u00a0Sometimes, respecting another\u00a0person means learning\u00a0about their differences. And sometimes, it\u00a0means recognizing how\u00a0they're really\u00a0just the same.\n","776":"ABSTRACT\nSteven Melendez Gear & Gadgets column comments on artificial-intelligence toothbrush from Colgate that informs users in app when they miss spot for brushing; photo (M)\n","777":"ABSTRACT\nGregg Ip Capital Account column contends advances of artificial intelligence in spotting pneumonia on chest X-rays offer new tool for radiologists and other specialists burdened by proliferation of imagery and tests; graph (M)\n","778":"ABSTRACT\nGarry Kasparov Review section article discusses benefits of artificial intelligence, including rising living standards, access to information and new job fields; drawing (M)\n","779":"ABSTRACT\nFord Motor Company has acquired majority ownership in artificial-intelligence start-up called Argo AI and plans to invest $1 billion in company, latest move in auto-industry spending spree to develop self-driving technology (M)\n","780":"ABSTRACT\nAlphabet Inc's Google unit hires head of Stanford's artificial-intelligence lab Fei-Fei Li, latest example of trend that some researchers say is draining universities of scientists needed to cultivate next generation of researchers and solve pressing problems; graph; photo (M)\n","781":"ABSTRACT\nIBM CEO Virginia Rometty expects company's Watson artificial-intelligence service to be used in some form by one billion people by end of 2017, citing its deal to pair Watson with General Motors' OnStar system as example; photo (M)\n","782":"ABSTRACT\nGM, with help of\u00a0IBM's Watson artificial intelligence technology,\u00a0will unveil new version of OnStar that will offer mobile-commerce services beyond navigation and entertainment; GM and IBM will share revenue with partners; photo (M)\n","783":"ABSTRACT\nGoogle is debuting new Google assistant software that uses artificial intelligence to answer users' questions and perform variety of tasks;\u00a0 sees software as next iteration of search that will allow it to dominate rivals like Amazon.com and Apple; photo (M)\n","784":"ABSTRACT\nDavid Gelernter Review section article discusses future of artificial intelligence and argues man should be scared; photos (L)\n","785":"ABSTRACT\nNomura Securities and Credit Suisse Group are both using artificial intelligence to crunch Bank of Japan economic assessments to supplement human judgment;\u00a0 analysts are increasingly using such algorithms to glean insights from data; graph (M)\n","786":"ABSTRACT\nChristopher Mims Keywords column on ability of IPSoft's artificial-intelligence operating system Amelia to converse and to solve problems (M)\n","787":" ABSTRACT:Robert N Goldman is named president and chief executive officer of Artificial Intelligence Corp (S)\n","790":"ABSTRACT\nMany firms are not yet equipped to take advantage fully of artificial intelligence, despite all talk of machine learning and AI's potential in enterprises (S)\n","791":"ABSTRACT\nMotorola Solutions and partner Neurala are among technology companies developing artificial-intelligence capabilities coupled with video surveillance and body cameras that can alert police on streets in real time about suspects, further expanding police reach; diagram (M)\n","792":"ABSTRACT\nAgricultural giants like Monsanto and BASF are investing in artificial intelligence in bet that research and decision-making can be streamlined and farming made more efficient, including spotting and responding to crop diseases quickly; photo (M)\n","793":"ABSTRACT\nKen Goldberg commentary on virtues of multiplicity, in which humans and artificial-intelligence-driven machines work collaboratively, with humans helping to fine-tune system and learning new strategies along way\n","794":"ABSTRACT\nGoogle reportedly is building new mobile-messaging service that taps its artificial intelligence know-how and so-called chatbot technology to better compete with rivals; photo (M)\n","795":"ABSTRACT\nJohnson & Johnson is working with IBM and Apple to create apps that apply artificial intelligence to health data for marketing to third parties like hospitals, who will use them as virtual health coaches and rehabilitation tools for patients; photo (M)\n","796":"ABSTRACT\nAI research head Yann LeCun says Facebook is opening artificial-intelligence research lab in Paris and hopes to create opportunities for researchers to create ecosystem where startups will work on similar topics\u00a0(M)\n","797":"ABSTRACT\nResearchers at MIT and University College London\u00a0 reports that computer artificial intelligence that scanned the manual of computer strategy game Civilization II won more often than AI that did not consult manual; photo (S) \u00a0\n","799":"ARTIFICIAL intelligence research, music division, could be sampled Saturday night at the Samaya Foundation, 75 Leonard Street in TriBeCa. The composer David Behrman presented two works, ''Circling Six'' and ''Interspecies Smalltalk,'' that centered on computer programs responding to a live soloist.\u00a0\nWhile Mr. Behrman was behind a table covered with electronic gear, the soloists sat onstage watching a video monitor and playing into microphones ''heard'' by the computer.\nIn ''Circling Six,'' Ben Neal played his own invention, the mutantrumpet - a horn with three bells, two sets of valves and a slide - which allowed him to produce muted, open and sliding notes in quick succession. Mr. Neal's modest, sustained notes were heard amid the rich, organ-like chords produced by the computer program - a small set of them, recurring at various speeds and rhythms, sliding from one to the next - and, eventually, rapidly cascading chords and a high, quasi-random melody line.\n''Interspecies Smalltalk'' used the computer to supply a chordal backdrop for the folkish melodies played by Barbara Benary on two kinds of fiddle, the Balkan gadulka and the Chinese chung wu. As the piece progressed, it added pitter-patter percussive sounds and a different sort of chordal cascade.\nThe sounds were complex and well-tuned, but as with other kinds of computer ''creativity,'' the music had a random, desultory quality; its activity lacked direction.\n","800":"The criticism of Uber continues to pile up.\nThis week, the car service was found to use secret software to evade government regulators and\u00a0a video showed its chief executive in a verbal altercation with one of the company's drivers. Previously, the company's self-driving cars raised safety concerns in San Francisco when, because of faulty and incomplete technology, they reportedly barreled through red lights and crossed over bike lanes. Uber has recently been accused of sexual harassment, intellectual property theft\u00a0and other questionable behavior.\nUber isn't alone. Silicon Valley is gaining a reputation for being obsessed with making money at any cost, i.e.\u00a0Theranos, which made false claims and\u00a0risked lives. The tech industry is becoming too much like the finance industry, which a decade ago caused the Great Recession\u00a0with its greed.\u00a0\nThe irony is that both industries compete for top engineering talent from our colleges. And each corrupts these students in a different way. Finance uses their knowledge to engineer our financial system, while tech focuses it on making money rather than on uplifting humanity.\nMy greatest fear after joining Duke's engineering school in 2004 was that my students would end up joining investment banks or management consultancies or, when they joined the tech industry, would act as Uber and Theranos executives have. We teach our students core technologies but do not give them the vision to better the world.\nThat is why we need people with good values and ethics\u00a0leading the way.\u00a0We need innovators who care about enriching humanity rather than just themselves. We need people who give back to the world and make it a better place.\u00a0There are positive examples, of course, with successful executives like Bill Gates and Mark Zuckerberg devoting large portions of their wealth to public health and other notable causes. These are the values we need to instill in our engineering students - before they absorb the corruption of our investment banks and big business.\nThis year, Carnegie Mellon's engineering dean, James Garrett, presented me with the opportunity to teach students how they might use technology to solve humanity's grand challenges and build billion-dollar businesses by helping 1\u00a0billion people. I jumped at the chance.\nI wanted to try an experiment: teaching students the potential of technology to solve big problems like clean water, energy, education, disease and hunger. The idea is not to build silly apps,\u00a0as Silicon Valley does, but to design real solutions to global problems.\nA decade ago, it would have seemed wishful thinking to say that students could effect change on such a scale. It was only governments and big research labs that could solve grand challenges - and they required big grants and budgets. But that is no longer the case. The cost of building world-changing innovations has fallen so low that motivated graduates can do it.\nThese young dreamers can build technologies that solve these problems. Unconstrained by the idea\u00a0of what is impossible, they can help take us into a world in which we worry more about sharing prosperity than about fighting over what little we have.\nWitness the threshold we have already crossed with Moore's law. Our smartphones are many times faster than the supercomputers of yesteryear and, by 2023,\u00a0will exceed\u00a0the human brain in both processing and storing information. We are seeing exponential advances in technologies such as sensors, artificial intelligence, robotics and genomics. And their convergence is making amazing things possible.\nCheap sensors and networks, for example, are enabling the development of a web of connected devices, called the Internet of Things. Besides increasing the energy efficiency of our homes and tracking our bodily functions, this web of sensors enables the automation of manufacturing, the creation of smart grids and cities, and a revolution in agriculture. The combination of sensors, artificial intelligence and computers enables robots to do the work of humans: to assemble electronics, drive cars and look after the elderly. And digital tutors can take students into virtual-reality worlds and teach them engineering, mathematics, language and world history.\nThe same technologies are enabling entrepreneurs to transform health care. We can use artificial intelligence to help us learn how the environment, including the food we eat and the medicines we take, affects the complex interplay between our genes and our organisms. The human genome has been mapped digitally, and artificial intelligence may even enable us to engineer cures for certain diseases.\nBut these technologies all have a dark side and can be used in destructive ways. As easily as we can edit genes, we can\u00a0create killer viruses, alter the human germ line and inadvertently destroy ecosystems dependent upon an insect we casually exterminate. As easily as nursing the elderly, robots can become killing machines.\u00a0Our future can be\u00a0either a \"Star Trek\" utopia or a \"Mad Max\" wreck; it all depends on the choices we make and how we educate our students.\nI have no idea whether my attempts at Carnegie Mellon will succeed in equipping these young engineers with the values to pursue something more worthwhile than personal gain at global expense, but it is certainly worth a try. Our students are our future, and that motivates me to enable them to fulfill grand visions.\u00a0We need to launch similar experiments in schools across the United States and the world.\n              Read more from The Washington Post's Innovations section.           \n","801":"ABSTRACT\nJulian E Barnes and Josh Chin Review section article discusses artificial intelligence research in China and US that seeks military advantage; drawing; photos (L)\n","802":"ABSTRACT\nCurt Levey and Ryan Hagemann commentary notes concerns about how artificial-intelligence systems reach their decisions; contends accountability, through explicit explanation of how decision is reached, could be better than transparency in easing fears\n","803":"ABSTRACT\nThe Education of AI article in The Future of Everything section comments on whether artificial intelligence poses existential threat to humanity, as two experts from Elon Musk's OpenAI say it is possible, but way to avoid this nightmare scenario is teaching machines to place out interests first; drawing; diagram (M)\n","804":"ABSTRACT\nGinny Rometti Voices article in The Future of Everything notes future of Artificial Intelligence is neither in robo-maids nor robo-wars, but in much simpler and more beneficial reality: cognitive systems that could improve health care, education and more; drawing (M)\n","805":"ABSTRACT\nSoftBank CEO Masayoshi Son says company will focus on artificial intelligence and work with Honda Motor to build cars that would learn to perceive drivers' emotions from sensors and cameras, as well as from conversing with motorists (M)\n","806":"ABSTRACT\nBaidu's Minwa supercomputer has posted world's best results on ImageNet artificial-intelligence test in which computer must sorts digital images into roughly 1,000 categories;\u00a0 made use of so-called deep-learning algorithms (M)\n","807":" ABSTRACT:Xerox Corp says its Xerox Artificial Intelligence Systems unit has become independent, employee-owned company (S)\n","808":"To the Editor:\n     \"Still a Long Way From Checkmate\" (Dec. 28) -- about how artificial intelligence is affecting, or may affect, consumer appliances -- hinted at some lovely windmills in the sky. But I'd like to pick a bone with Sony, Panasonic, General Electric, et al: Why can't I walk into a consumer electronics store and buy an AM\/FM radio with presettable programming, similar to that of a VCR? \u00a0\n I softly curse those companies when I miss my favorite \"Osgood File\" snippet on CBS Radio or my favorite FM station's folk-music program on Saturday. In today's world, this isn't even firecracker, much less rocket science. Why has such a basic and obvious consumer appliance never been offered to the public? WALTER DAVIELos Angeles\n","809":"Go figure: Artificial\u00a0intelligence by Google wins complex strategy game\nGoogle's artificial\u00a0intelligence system beat a top-ranked player of the board game Go in three straight games in South Korea last week, providing the first evidence that the company's software may attain superhuman status at a challenging 2,500-year-old strategy contest.\u00a0\nTo show off the capabilities developed by its London-based AI subsidiary DeepMind, the technology company arranged a five-match tournament against Lee Sedol, who Google said has been the top-ranked Go player of the past decade.\n\"It'll never get tired and it'll never get intimidated,\" said DeepMind co-founder Demis Hassabis at a news conference ahead of the first match. \"These are the main advantages.\"\nSedol managed to win a game Sunday, though he had already lost the tournament. The last game was scheduled for Tuesday.\nThe breakthrough astounded experts, who had previously thought it would be five to 10 years before AI would be good enough to play Go, and it positions Google as a leader in the next generation of super-smart computing. The search giant already uses AI in a range of products - automatically writing emails, recommending YouTube videos, helping cars drive themselves, etc.\nThe wins against Lee were further confirmation of the power of DeepMind's system and its progress in seeking to make machines that can outsmart humans. For scientists and researchers in AI, Go has been the game to conquer since IBM's supercomputer Deep Blue beat world chess champion Garry Kasparov in 1997.\nWhat sets DeepMind's approach apart from traditional Go-playing software is its use of a technology called a neural network, which lets computers learn from experience, rather than specific programming. This enables it to learn by studying example games, then playing millions of games against itself, inferring the rules and, eventually, developing long-term strategies. The system also uses a more traditional computing technique called Monte Carlo Tree Search.\nGo, also known as Baduk, is a game played widely in Asia that sees players battle to take territory on a board by taking turns placing stones on the intersections of a grid. There is only one type of piece, and players choose to play as either white or black. On a 19-by-19 Go grid, there are more possible board configurations than there are atoms in the universe.\n\"I'm somewhat shocked,\" Lee told reporters after the first match. \"I didn't really imagine I'd lose. I didn't foresee AlphaGo would play Go so perfectly.\"\n-Bloomberg News\n","811":"ABSTRACT\nGreg Ip Capital Account column argues that economic payoff from computers, automation and artificial intelligence may finally be starting to arrive after years in which innovations have worked to establish themselves as norms while economy gathered strength; graph (M)\n","812":"ABSTRACT\nGoogle opens AI China center to draw on Chinese expertise in artificial intelligence, including areas such as deep learning, natural-language processing and computer vision;\u00a0 move comes as both US and China race to hire talent; photo (M)\n","813":"ABSTRACT\nGregg Ip Capital Account column contends best way of looking at artificial intelligence is to consider it as predictive tool that is becoming cheaper and will find more uses, forcing realignment but creating new economic opportunities; graph (M)\n","814":"ABSTRACT\nChristopher Mims Keywords column on\u00a0risks involved with potential bias in artificial intelligence robots and use by some engineers of cognitive psychology to tease out how their AI thinks (M)\n","815":"ABSTRACT\nTed Greenwald article in Journal Report: Workplace Technology describes how artificial intelligence is changing way managers are doing their job; explores questions of whether AI is too intrusive and whether it can really help supervisors do better job; drawing; charts (L)\n","816":"ABSTRACT\nComputational creativity researcher Mike Cook, Rebecca Fiebrink and other experts explain why artificial-intelligence software is limited in its ability to replace human creativity (S)\n","817":"ABSTRACT\nSunglass Hut, L'Oreal, Under Armour and other retailers are turning to artificial intelligence, algorithms and other means supplied by Silicon Valley to personalize online shopping\u00a0experiences; photo (M)\n","818":"ABSTRACT\nSteven Norton CIO Journal on steps by corporate world to use artificial intelligence and machine learning to automate and augment tasks previously done by humans alone;\u00a0 reports sending is expected to reach $47 billion in 2020 from some $8 billion in 2016 (M)\n","819":"ABSTRACT\nSome top business schools have recently added MBA courses on managing artificial-intelligence applications and algorithms that help businesses make informed decisions; photo (M)\n","820":"ABSTRACT\nEllen Gamerman Arena section article on Hollywood's craze for robots as artificial intelligence becomes part of popular culture and viewers are repeatedly asked whether robots are heroes, villains or just appliances; photos (M)\n","821":"ABSTRACT\nYan LeCun Voices article in The Future of Everything section discusses how artificial intelligence machines could resemble their makers (S)\n","822":"THE latest catchword in computing is artificial intellegence, or AI. Most researchers in the field do not promise anything as radical as self-replicating computers in the near future. Some, however, do honestly seem to feel that the reproductive facility is the only human aspect computers will be lacking a couple of decades from now. AI, then, is entering the world of hype usually attendant upon any new technology. The reality is something quite different, of course. For the most part it is yet to be seen.  Several new products employing AI techniques to make software easier to use are expected to be released this fall. Symantek has a word-processing and database package with a ''natural-language front end.'' That is, it is supposed to allow the user to tell the computer in plain English what to do. Javaline, a financial modeling package, and Paradox, another database manager, are both said to be ''artificially intelligent.''\nThen there are the so-called expert systems, into which can be entered the combined knowledge of a given specialty collected from everyone who is anyone in the field. Once all the information is in place a user can seek answers to complex questions in the field by merely querying the computer. The computer will query back, guiding one through the maze of stored knowledge.\u00a0\nMost of us will never deal with such systems. Notwithstanding, AI-based software geared toward the average individual is coming on the market. Recently I fed my computer Racter ($44.95 for the IBM PC family of computers from Mindscape Inc., Northbrook, Ill. 60062, telephone 312-480 7667). Billed as ''Tongue-in-chip software,'' it is obviously an entertainment package, not one designed to increase productivity.\nRacter - the name is short for raconteur, according to its publisher - ''permits a computer owner to have a spontaneous, albeit bizarre, conversation with his or her computer.'' Well, I have had a number of those already, when my machine has failed to perform as expected, but here, presumably, was a chance for the computer to state its side.\nTo start the program, one need only insert the disk and type in ''Racter.'' There are no files to copy, no special keys to learn, which certainly shows above-average intelligence on the part of the designers.\nAfter being asked my name and entering it, I was instructed to interview Racter, who initiated the interchange by stating, ''Ask a question.'' ''Why are you,'' I typed in.  ''Ask a question,'' Racter replied. ''Why are you,'' I repeated.  Our parrotlike conversation continued until I had enough intelligence to add a question mark at the end of my query. At that point Racter responded with the comment, ''Possibly I am because - well, why do you think?'' Obviously Racter was a psychologist of the Rogerian school.\nNot to be put off, I typed in, ''You're supposed to do the thinking.''\nThe reply to that was ''Wrong! And it's about time, or so one feels. Have you heard about Bucky Fuller?''\nCurious about the machine's cozy familiarity with the great inventor, and willing to admit my ignorance concerning the latest rumors about Mr. Fuller, I replied in the negative and was greeted by: ''Listen to this! Genghis Khan slapped a stinker who was kicking Genghis Khan's slug because Bucky Fuller the democrat tried to originate hollowly. Next, many hollow stinkers embraced fineness. Shocking isn't it? Democrats are like that. A foul paragon . . . .''\nFollowing a few more of my questions, which elicited equally strange and lengthy answers, Racter wound down with, ''Next question.''\nI entered, ''Why are you such a long-winded klutz?'' and left him babbling on and on about Charlie Parker, Hitler, Timothy Leary and other ''stinkers,'' an appellative of which Racter seems inordinately fond. Entertaining he was not. Intelligent he certainly was not. LEAVING Racter to run off at the screen, I loaded another computer with Mind Reader ($189 for the IBM PC series from Businessoft Inc., Annapolis, Md. 21401. telephone 301-263 1962). A word-processing program, this software package is either far off the wall or great, depending on how the individual user views its approach.\nI should note that the publisher does not expect Mind Reader to become the tool of skilled typists. It is geared toward charitable organizations, clubs, small businesses and others who produce letters and similar short written works without secretarial assistance.\nAll that is necessary to load the program is to place the disk in its drive and type in ''MR,'' which will result in the appearance of the logo accompanied by a sci-fi ''boinnnggg.'' Futuristic sound effects as strange as the program's packaging, which resembles that of a game, abound.\nAll the key functions for using the program are listed at the top and bottom of the screen. There is nothing to memorize and hardly anything to refer to in the manual. If I wanted to type a letter, I was to press the F9 key. Doing so brought a name and address file to the screen. Picking a canned addressee at random, I pressed the asterisk key and the information printed itself in the appropriate place on my letter-to-be. Up to 175 complete names and addresses can be added to the file by the user.\nOnce a salutation was in place, I began to type in earnest. Barely had my finger touched the ''T'' key than a little window appeared next to the character on screen. In it was to be seen the listing ''to the 5, there 4, this 3, that 2, the ;.'' When I pressed the semicolon key, an ''h'' and an ''e'' appeared by themselves after the ''T'' I had originally entered.\nTyping ''M'' brought forth a new window displaying ''many 5, make 4, most 3, more 2, may ;'' as possibilities. Now my intent had been to type Mind Reader. So I cautiously added an ''i'' to my solitary ''M.'' The windowful of verbiage vanished. However, typing next an ''n'' opened a new window showing that ''minute 7, minor 6, Minnesota 5, minister 4, Mind Reader 3, mineral 2, minimum ;'' could be made to ensue. Hitting the semicolon key again, instead of the 3 as I had intended, gave me ''minimum.'' Backspacing erased everything except my original ''Min.'' But the word window failed to reappear. Finally I erased the whole word and started over with ''Min 3.'' At last I had succeeded in calling forth ''Mind Reader.''\nAnother interesting problem arose with the word ''considerable.'' Typing just ''c'' gave me a choice of five words from ''can'' to ''computer.'' Getting as far as ''consi'' manually, I ended up with three windowed choices, ''consistent,'' ''consideration'' and ''consider.'' I chose ''consider,'' planning to add ''able.'' However, after Mind Reader entered ''consider,'' it automatically and most obligingly skipped a space for me so that I could begin a new word. The spacing feature of the software, incidentally, works with punctuation marks as well as between words, two spaces being automatically inserted after periods, question marks and exclamation marks. But I had to backspace in order to enter ''able.'' The program seems in general somewhat averse to derivatives.\nMind Reader can store key phrases and even whole paragraphs that are used frequently. These can then be called up from the program's glossary by a single keystroke. The same built-in dictionary that supplies the letter-matching words also allows for the insertion of the user's own vocabulary. Most interestingly, the dictionary accommodates itself to the way in which an individual writes. For example, if you use the word ''coronary'' frequently, that word will, after some time, move down to the preferred semicolon position so that when you type in ''cor,'' ''coronary'' will be the lead word in the accompanying window, replacing the original ''corporation.''\nThe dictionary can be turned off, leaving Mind Reader an ordinary, dumb word processor. ''Dumb,'' too, was my initial reaction to Mind Reader itself, if the truth must be told. After a few hours' use, I still was far from convinced that it was for me, although my secretary found the company's telephone assistance very helpful when this unusual program didn't have the intelligence to do what she wanted it to do. Who knows, just as there are devotees of the Dvorak keyboard, still perhaps a more practical approach than this one to increasing typing speed, a cult may well develop around Mind Reader.\n","823":"The Washington Post has launched ModBot, a software application that utilizes artificial intelligence to moderate comments. The proprietary technology uses machine learning to automatically filter comments that require human moderating, flag stories that require real-time monitoring, and approve or delete comments based on The Post's discussion policy. The technology evaluates comments using an algorithm that has been trained by The Post's years-long history of human-moderated comments.\u00a0\nModBot has been assisting Post comment moderators since its launch on May 5 and the technology is currently evaluating all Washington Post comments.\n\"With discussion streams open on nearly every article, The Post's moderators manage a massive number of comments each day. ModBot handles the rote work, directing the more complicated decisions to human moderators,\" said Greg Barber, director of digital news projects at The Post. \"This technology not only helps foster healthier comment sections, but will make it easier for journalists to find and interact with the highest quality commenters.\"\nModBot analyzes comments to determine if action is needed, taking into account the language in the comment and past actions Post moderators have taken on similar comments. ModBot's dashboard displays stories with a high rate of deletion so that human moderators can watch those comments more closely and step in to moderate if necessary.\n\"We've used The Post's rich comment moderation history to train ModBot to identify comments that require action,\" said Dr. Sam Han (PhD), director of data science at The Post. \"We are also working on deep learning techniques to allow ModBot to better understand words in context and decrease the need for human intervention.\"\nLater this summer, The Post will pair ModBot with Talk, the comment management and moderation software developed by The Coral Project, a collaboration between The Post, The New York Times, and Mozilla funded by a grant from the John S. and James L. Knight Foundation.\n\"ModBot's analysis is granular, evaluating the words and phrases in a comment,\" Barber said. \"Talk's analytics are broad, providing insights about our commenters that will help us discover and highlight voices we might otherwise have missed. Working together, Talk and ModBot will provide a textured picture of our most loyal readers, helping us to engage with them like never before.\"\nModBot was co-developed by news and engineering. Both ModBot and Talk are part of The Post's Arc Publishing suite.\n          About The Washington Post's Arc Publishing           Arc Publishing (www.arcpublishing.com) is a state-of-the-art digital platform and suite of tools that's engineered to meet the needs of modern publishers. Built by engineers and designers at The Washington Post, Arc is made up of flexible, sophisticated tools that work seamlessly together and can function individually. With more than a dozen clients, Arc spans the range of technology needs for digital publishers, including video, mobile web and apps, syndication to distributed platforms, automatic content testing, data mining and innovative monetization tools. At its core, Arc is about speed: for readers, the newsroom, advertisers and developers.       \n","824":"Google researchers published a landmark paper in artificial intelligence this week. They created a computerized system that can\u00a0teach\u00a0itself how to beat old Atari games. Of the 49 games tested, their system was better than professional human testers at 22 of them:\u00a0\n1. Video Pinball, 2439 percent better than a professional human game tester\n2. Boxing, 1607 percent better\n3. Breakout, 1227 percent better\n4. Star Gunner, 498 percent better\n5. Robotank, 408 percent better\n6. Atlantis, 349 percent better\n7. Crazy Climber, 319 percent better\n8. Gopher, 300 percent better\n9. Demon Attack, 194 percent better\n10. Name This Game, 178 percent better\n11. Krull, 177 percent better\n12. Assault, 146 percent better\n13. Road Runner, 132 percent better\n14. Kangaroo, 124 percent better\n15. James Bond, 45 percent better\n16. Tennis, 43 percent better\n17. Pong, 32 percent better\n18. Space Invaders, 21 percent better\n19. Beam Rider 19 percent better\n20. Tutankham, 12 percent better\n21. Kung-Fu Master, 2 percent better\n22. Freeway, 2 percent better\n              Related: The 5 Atari games that Google's algorithm had no clue against \n","825":"ABSTRACT\nCorrection of Nov 24 Greg Ip's Nov 24 Capital Account column on artificial intelligence as\u00a0 new tool for radiologists and other specialists\n","826":"ABSTRACT\nMark P Mills commentary contends world is still in early stages of digital and artificial-intelligence revolution, which has barely begun to affect physical economy;\u00a0 contends dominant players in cyberphysical world have yet to emerge\n","827":"ABSTRACT\nSecretive Apple is making unusual attempt at transparency by opening small window onto its artificial-intelligence efforts as rivals are racing to scoop up talent from academia, where experts like to share information; graph; photo (M)\n","828":"ABSTRACT\nSteven Rosenbush article in Journal Report: CIO Network notes unknown future of artificial intelligence is becoming just bit more knowable and offers special report from WSJ's CIO Network conference in San Francisco on how this latest technological revolution is unfolding (S)\n","829":"ABSTRACT\nSoftBank Group CEO Masayoshi Son speaking at Mobile World Congress says artificial intelligence will surpass humans in 30 years in connected world, forecast that lies behind SoftBank's $100 billion innovation fund and other deals (M)\n","830":"ABSTRACT\nVoices article in The Future of Everything section notes Facebook director of artificial-intelligence research Yann LeCun's comments on curriculum for software and what it will look like in age of machines; drawing (M)\n","831":"ABSTRACT\nSouth Korean Go grandmaster Lee Se-dol loses final round and match to artificial-intelligence machine AlphaGo developed by Alphabet's Google; photo (M)\n","832":"ABSTRACT\nRachel Pannett Health & Wellness column notes Australian researchers have developed artificial-intelligence system based on dragonfly's vision that they say could help improve eyesight of people who see almost nothing and also be used in robotics and driverless cars; photo (M)\n","833":"ABSTRACT\nScience of artificial intelligence is catching up to smartphones, some of which can respond to verbal commands and questions with computer-generated speech; Apple Inc's new iPhone 4S includes virtual personal assistant called Siri, which can respond to queries with jokes; photo (M)\n","835":"Machines and humans learn differently. This has been a central fact of artificial-intelligence research for decades. If you cram enough data into a machine and let the algorithms grind away tirelessly, the computer can detect a pattern, produce a desired outcome and perhaps beat a grandmaster in chess.\nHuman intelligence is faster, quirkier and more nimble. We take mental shortcuts. We have a knack for discerning the rules of a game, the dynamic of a situation, who's mad at whom. The human mind - the most complex piece of matter in the known universe - is adept at getting the gist of things quickly.\u00a0\nNow researchers report a breakthrough in artificial intelligence: a machine-learning program that mimics how humans learn.\nThe report, published online Thursday in the journal Science, is being described as a small but significant step in closing the vast gap between machines and humans when it comes to generalized, all-purpose intelligence.\n\"For the first time, we think we have a machine system that can learn a large class of visual concepts in ways that are hard to distinguish from human learners,\"  Joshua Tenenbaum, the senior author of the new paper and a professor at MIT, said in a teleconference with reporters.\nThe computer program, developed primarily by lead author Brenden Lake, a cognitive scientist at  New York University, used statistical probabilities to infer the basic rules behind the formation of letters in alphabets.\nAmong humans, visual recognition of a concept can often be achieved with a single example. \"You show even a young child a horse or a school bus or a skateboard and they get it from one example,\" Tenenbaum said.\nThe new computer program, which goes by the name  Bayesian Program Learning (BPL), performed well in inferring rules behind the representation of letters in different alphabets. The researchers judged this performance by conducting a Turing test, a kind of contest between humans and the computer program. Both the computer program and the humans were given a single example of a letter, then asked to find a match to that letter among 20 handwritten representations. The humans made errors only 4.5 percent of the time, but the computer program  did slightly better, with a 3.3 percent error rate.\nThe Turing test is named after the British mathematician and computer pioneer Alan Turing. In 1936, Turing devised some of the fundamental concepts for a general-purpose computer. In 1950, he proposed that machines could someday match human intelligence. He conceived of what he called the Imitation Game, which would be played  in the future, when computers were more advanced. In Turing's scenario, an interrogator would ask questions and, unseen in an adjacent room, a human and a computer would provide answers. If the interrogator couldn't reliably distinguish the human answers from the computer's answers, the computer would pass the test and have the status of a thinking machine, Turing argued.\nIn the teleconference with reporters, Tenenbaum was asked if this kind of computer technology could be used in satellite surveillance. He said the military helped fund the research and is interested in potential applications.\n\"In some ways there's a huge leap that has to be made because, you know, it's one thing to talk about writing characters. It's another thing to talk about moving around on the ground if you're an individual or a military unit or whatever,\" he said.\nThe breakthrough comes during a period of great excitement in the A.I. community, but also amid anxiety about whether there are sufficient safeguards to ensure that machine intelligence doesn't run away from its human creators. Entrepreneur Elon Musk has given $10 million for A.I.-safety research. Stephen Hawking, Bill Gates and others in science and technology have expressed concern that A.I. could pose an existential threat to humanity.\nBut Tenenbaum said this new work doesn't come anywhere near being something to worry about. Machines, he said, are not close to achieving general intelligence.\n\"Intelligence, at least to me, has a general, very flexible capacity. I don't think any machine has any level of general intelligence,\" Tenenbaum told The Washington Post. \"Our programs have a sense of the program that generates characters, but they don't have any real deep sense of what they're doing, or any drive to do it.\"\njoel.achenbach@washpost.com\n","836":"ABSTRACT\nJeremy Rabkin and John Yoo Review section article on combination of robotics and artificial intelligence that is likely to define battlefield in future; photo (M)\n","837":"ABSTRACT\nFrederick Butzen letter responds to April 15 Review section article about differences between artificial intelligence revolution and earlier industrial revolution\n","838":"ABSTRACT\nCanadian government has allotted C$125 million ($94 million) for developing artificial intelligence after conference hosted by Magna International CEO Don Walker where executives, scientists and politicians identified AI as research priority; photo (M)\n","839":"ABSTRACT\nRachel King Business News article discusses IBM efforts to recruit developers for its Watson artificial intelligence software (M)\n","840":"ABSTRACT\nYann LeCun Voices article in The Future of Everything section notes biggest barrier to true artificial intelligence in machine is ability to give them common sense and be able to interpret sentences correctly that humans can decipher right away, something called predictive learning (S)\n","841":"ABSTRACT\nJonathan Cheng WSJ Weekend article discusses next challenge for artificial intelligence company Deep Mind, whose programming defeated human players in abstract game Go, might be Blizzard Entertainment's StarCraft computer game; photo (M)\n","842":"ABSTRACT\nAuditing firm KPMG is forming alliance with IBM's IBM Watson artificial-intelligence unit to develop tools for auditing and other businesses;\u00a0 auditors are aim to automate critical but rote tasks to free up humans for more substantive work; drawing (M)\n","843":"ABSTRACT\nBank of Japan's surprise move last week into negative interest rates stumped text-mining artificial-intelligence programs deployed by Nomura Securities and Credit Suisse Group, which firms had touted for their objectivity and usefulness; photo (M)\n","844":"ABSTRACT\nNicholas Carr Review section article contends human intelligence is withering as people\u00a0 rely on artificial intelligence;\u00a0 suggests new approach to technology that is tailored to people, not robots; drawings (L)\n","845":"To the Editor:\nAstro Teller's explanation of why people fear artificial intelligence (Op-Ed, March 21) misses an obvious solution: just stop calling it intelligence, because it isn't.\nIntelligence is a characteristic of living beings. If a computer in 1760 could have been programmed to write music based on the work of Bach and Haydn, would it ever have composed a Stravinsky ballet or a Sibelius symphony? If a computer in 1520 had been programmed to paint pictures, starting with the work of Raphael, would it ever have created paintings like those of Picasso or Pollock?\u00a0\n Computer scientists and engineers have given us wonderful aids to our research and problem solving that sometimes outdo humans, but they aren't intelligent; they're just impressive robots.\u00a0LEO GOLDMANNew York, March 22, 1998\n","847":"A decade ago, Scott French bet a few Silicon Valley friends that a computer could write a novel. Not Tolstoy or Faulkner maybe. But a computer, Mr. French told his friends, could probably be programmed to turn out a trashy page-turner -- the sort of steamy fiction that Jacqueline Susann, author of \"Valley of the Dolls,\" used to crank out.\nA rough draft within a year, he wagered. Twelve months later, Mr. French was wiser and $300 poorer. But that lost bet fueled an eight-year obsession that resulted in the publication this week of \"Just This Once,\" which carries this explanatory subtitle: \"A novel written by a computer programmed to think like the world's best-selling author\" -- Ms. Susann -- \"as told to Scott French.\"\u00a0\n The story of Mr. French's struggle to produce a computer-generated, Jacqueline Susann-style novel illustrates both the relentless advance of computer technology and its severe limitations. His work with a supercharged Apple Macintosh computer named Hal, using so-called artificial intelligence, an advanced form of programming that tries to emulate human thought, proved a slow and often painful collaboration.\u00a0\u00a0Humans Are Faster\n \"Let's be honest,\" said Mr. French, a 43-year-old electronic surveillance consultant and self-taught computer programmer. \"If I'd written it myself, this book would have been done seven or eight years ago.\"\nAs it worked out, he said, he wrote about a quarter of the prose, the computer cranked out about the same amount and the remainder can only be described as a collaboration of man and machine.\nThe writing of a scene amounted to a dialogue between Mr. French and his computer software. The computer would ask questions, he would answer them, and then the machine would spit out the story, a couple of sentences at a time. He would then change a word here and there, correct a misspelling, whatever. Then, based on what went before, the computer would ask some more questions that Mr. French would have to answer.\n\"It doesn't write whole paragraphs at a time,\" Mr. French said. \"You can't get up, walk away, come back and find a completed chapter. It's not that advanced.\"\nMuch of the tone and plotting was based on thousands of rules that Mr. French programmed into the computer, formulas he derived by carefully analyzing two of Ms. Susann's best-selling books, \"Valley of the Dolls\" and \"Once Is Not Enough.\"\nWhen two key female characters were to meet, for example, the computer would ask Mr. French about the \"cattiness factor\" that should be used in the scene. Mr. French would be presented with choices 1 through 10. If he keyed in 8 -- high cattiness -- the computer reached into its memory to craft a sentence that was likely to employ words like \"screaming\" or \"shrieking.\"\nA couple early reviews of \"Just This Once\" have been surprisingly generous, particularly compared with the criticism directed at works by Ms. Susann. Her obituary in The New York Times in 1974 observed that critics were \"almost unanimously unkind to her books.\"\nIn USA Today on Wednesday, Thomas Gifford, a novelist, reviewed both \"Just This Once\" and another entry of the same genre, \"American Star,\" by Jackie Collins. Mr. Gifford's verdict: \"If you do like this stuff, you'd be much, much better off with the one written by the computer.\"\nAnother approving nod came from the Dead Jackie Susann Quarterly, a New York publication that writes about Ms. Susann with equal measures of enthusiasm and irreverence. \"She would be proud,\" its review declared. \"Lots of money, sleaze, disease, death, oral sex, tragedy and the good girl gone bad.\"\u00a0\u00a0Doubts on Sales Potential\n Still, it was at the cash register that Ms. Susann measured her books' success. And even people in publishing who are impressed by the technological achievement of \"Just This Once\" have doubts that it will sell strongly.\n\"I think it's more a curio than something that will be a big commercial success,\" said Marc Aronson, a senior editor at Henry Holt & Company. \"But it is testimony that we are at a crossroads of technology in this industry.\"\nThe novel's publisher, the Carol Publishing Group's Birch Lane Press, recognizes that \"Just This Once\" is an experimental foray and that its commercial prospects are uncertain. The first printing is a respectable 15,000 hardcover copies, priced at $18.95. It will be available in the national bookstore chains.\n\"But basically,\" said Steven Schragis, Carol's publisher, \"the stores don't know what to make of this book.\"\nStill, Mr. Schragis hopes that the novelty will draw readers. \"I'm not saying this is a great work of literary distinction, but it is as good as a hundred other romance novels being published this year,\" he said.\nComputer experts are bemused and intrigued. Writing a 295-page book patterned after a pulp-fiction queen strikes them as a delightfully frivolous application of artificial intelligence, a technology commonly associated with high-minded pursuits like finding a cure for cancer.\nOver the years, there have been many obstacles in the drive to teach computers to write, but one of the biggest has been what Prof. Marvin Minsky, a professor at the Massachusetts Institute of Technology, calls the \"common-sense knowledge problem.\" He means that vast word lists can be fed into a computer, programs can analyze grammar and sentence structure, and a writer's style can be dissected. But the computer does not actually understand words.\nAfter several false starts and extensive reading about artificial intelligence, Mr. French in the late 1980's came across a program by Neuron Data Inc., a specialist in expert systems, a branch of artificial intelligence.\nWith the help of the program, Mr. French was able to write the several thousand computer-coded rules suggesting how certain kinds of characters will interact with others in a given situation, based on patterns in Ms. Susann's works.\nFor example, Mr. French explained, the computer will suggest a typical Jacqueline Susann plot situation: two women pursuing the same man. Then the computer will suggest that based on its analysis of Ms. Susann's novels, there is a \"high probability\" that both sex and drugs will be involved in character A's effort to seduce the fictional gentleman.\u00a0\u00a0\u00a0Settlement With Susann Estate\n A computer-generated book based on the style of a well-known author raises thorny questions of copyright infringement. Mr. Schragis has held discussions with representatives of Ms. Susann's estate. \"It's worked out,\" he said. \"But I can't discuss it -- that's part of our agreement.\"\nSeveral computer experts say they know Mr. French and attest that he seems well informed about his niche of artificial intelligence. But short of looking over his shoulder for years as he hunched over his home computer in Woodside, Calif., is there any way to prove that \"Just This Once\" is not a hoax? Probably not, but Mr. French seems to have a reasonable reply.\n\"It really wasn't worth my time and effort to fake this,\" he said. \"For years, most of my friends thought I was crazy, and I may well not even break even on this book. It's just something I wanted to do to prove it could be done.\"\u00a0The Lady, or the Cyber?\n\u00a0From Valley of the Dolls by Jacqueline Susann\u00a0Another half hour passed. Neely alternated between anger and fright. She wanted a cigarette. The two Seconals had worn off, and she was wide awake and terrified. She rang. A nurse appeared. The nurse was polite but evasive. Miss O'Hara could have a cigarette right now if she came into the lounge. In fact, she had better hurry. If she missed this smoking period she couldn't smoke until nine o'clock.\u00a0\"Who the hell are you to tell me when I can smoke?\" she screamed. \"This is no charity ward. This place costs money -- I want to be treated with respect.\"\u00a0\"We respect you, Miss O'Hara. But in turn, you must respect the rules of Haven Manor.\"\u00a0\"I don't follow the rules. I make the rules! I'm Neely O'Hara.\"\u00a0From Just This Once by Hal, as told to Scott French\u00a0\"No,\" Carol said, as if speaking to herself. \"That's not how the script goes this time. We're going to get you help in spite of yourself. You're going in a serious help program, Lisa, right away . . . Or else -- \"\u00a0\"Or else what, Carol? What's the ultimate threat?\"\u00a0\"I'll call Taylor and get you canceled, get you blackballed.\"\u00a0Lisa got out of bed. \"You still don't get the picture do you? It's still some kind of good versus evil battle with you, isn't it? Carol, the world just isn't made that way.\"\u00a0She picked up a cordless phone and threw it across the room. \"Don't break your fingers on the buttons, but do it in the other room. I want a little privacy.\"\n","848":"ABSTRACT\nBob Davis The Outlook column on China's government-backed push to become world leader in artificial intelligence, in part for sophistication it can bring to controlling its citizens, which could make it rare example of repressive state that is also innovative; graph (M)\n","849":"ABSTRACT\nJames Mackintosh Streetwise column considers why shares of companies with disruptive technologies like automation and artificial intelligence have soared, but expected surge in productivity remains elusive; graphs (M)\n","850":"ABSTRACT\nFacebook founder Mark Zuckerberg says his goal for year is to create virtual assistant powered by artificial intelligence to help him at home and work; Facebook is developing its own AI assistant, M, embedded in its Messenger app (M)\n","851":"ABSTRACT\nEric Pfanner Technology article explores joint effort by Softbank and IBM\u00a0to develop IBM's Watson artificial intelligence technology for Japanese market, including white plastic robot named Pepper and other robots for home use; photos (M)\n","852":" ABSTRACT:Article discusses the rising use of artificial intelligence to combat credit card fraud in Great Britain and US; says Barclays Bank PLC is a front runner in this matter due to a loss last year of 31 million pounds to fraud from credit card theft (M)\n","853":" ABSTRACT:Bookshelf column briefly reviews selected recent books on artificial intelligence and 'fuzzy logic' (M)\n","854":"SAN FRANCISCO -- Google has been using artificial intelligence to build other artificially intelligent systems for the last several months.\nNow the company plans to sell this kind of \"automated machine learning\" technology to other businesses across the globe. On Wednesday, Google introduced a cloud-computing service that it bills as a way to build a so-called computer vision system that suits your particular needs -- even if you have little or no experience with the concepts that drive it.\nIf you are a radiologist, for example, you can use CT scans to automatically train a computer algorithm that identifies signs of lung cancer. If you run a real estate website, you can build an algorithm that distinguishes between living rooms and kitchens, bathrooms and bedrooms.\nAt least that is the pitch. \"You don't need a Ph.D. in machine learning,\" said Diane Greene, who oversees Google's cloud computing group. \"But you can still build a highly accurate machine learning model.\"\nLike many of the world's largest internet companies in recent years, Google has begun relying on machine learning -- computer algorithms that can learn tasks on their own by analyzing large amounts of data. These include systems that learn to recognize commands spoken into smartphones or translate one language into another. They also include algorithms that learn to build other machine learning systems.\nGoogle uses the technique while building systems that can recognize faces, products, landmarks and other objects in photos. In some cases, these algorithms are more accurate than something that is designed solely by engineers.\nThe new service is part of a widespread effort to expand the power of modern A.I. to businesses that are largely unfamiliar with this rapidly evolving technology. Like Google, a New York start-up called Clarifai offers an online service that helps customers train computer vision algorithms.\nAt the same time, several other start-ups, like Boston's DataRobot and Silicon Valley's H2O.ai, offer services designed to help businesses analyze the way products, customers, markets and employees behaved in the past and predict how they will perform in the future.\n\"They aim to automate data science in general,\" said Randy Olson, a data scientist at Life Epigenetics, a company in Portland, Ore.\nTech giants like Google, Amazon and Microsoft have hired a large portion of the people who specialize in the machine learning techniques that are rapidly accelerating the progress of A.I. -- a community of only 10,000 researchers worldwide, according to one estimate. That means most businesses don't have the talent needed to explore the latest machine learning.\nThe question is whether these new services will work as advertised and how rapidly they will evolve in the years to come.\nGoogle, Amazon, Microsoft and others already offer cloud-computing services that let businesses add existing machine learning algorithms to their own products. A company can take a Microsoft computer vision algorithm, for example, and slip it into a new smartphone app.\nBut with its new service, Google goes a step further, providing an automated way for businesses to build new algorithms. Businesses can upload their own images, provide a list of objects pictured in these images and train their own computer vision systems, tackling tasks that aren't necessarily handled by existing technology, according to Google.\nInitially, Google will open this service only to a small group of businesses. A Google product manager, Rajen Sheth, said the company would work with these customers to determine the price.\nRisto Miikkulainen, a professor of computer science at the University of Texas at Austin who has long explored the kind of technology that underpins Google's new service, agreed that it had the potential to help other businesses build their own A.I.\n\"It is really powerful technology,\" said Mr. Miikkulainen, who is also vice president of research at Sentient Technologies.\nBut sometimes, there is no substitute for good old human labor. With Google's new service, humans must label the data before the system can learn from it. Google can provide the human labelers, as do companies like CrowdFlower.\nAnd even when an online service successfully automates a task, it's not necessarily worth using.\nJames Bradley and his London company, NMT Vision, once used Clarifai to train and operate algorithms to identify websites that are selling products that infringe on copyrights. But he and his company now handle this on their own, mainly because the cost is lower.\nServices like DataRobot and H20.ai may bill themselves as automated data scientists, but here, too, automation has its limits. \"These services are only as good as their parts,\" said Patrick Dougherty, a data scientist with the Seattle-based firm Slalom Consulting. \"And humans still supply some of the parts.\"\nGoogle says that once images are labeled, its new service operates without human involvement. In a matter of minutes, it can retrain an existing algorithm using the customer's images. Given more time, it can build a model from scratch, specifically for the problem at hand.\nIf you are a zoologist who wants an algorithm that identifies jaguars and giraffes, said Fei-Fei Li, chief scientist inside the Google cloud group, all you have to do is supply the right images. \"You upload jaguars and giraffes,\" she said. \"And you are done.\"\nAll that remains is determining how well it works.\nFollow Cade Metz on Twitter: @CadeMetz.Related Articles\n\n","855":"The Internet is frequently blamed for messing with our minds, making us superficial, distracted and even deluded. But a new study suggests that for some people, using it could actually be healthy.\nFor a study published in The Journals of Gerontology,\u00a0Andr\u00e9 J. Xavier and his co-authors analyzed data on 6,442 people between ages 50 and 89. Several times over the course of eight years, the participants were asked if they used the Internet or email, and were given a word-recall test that measured their memory.\nThose who said they didn't use the Internet or email did worse on the test over time, while those who did actually improved - the effect remained after the researchers took into account age and socioeconomic status. Even those subjects who had relatively low cognitive function at the beginning of the study - meaning they might already be experiencing age-related problems - performed better on the recall tests if they used the Internet than if they didn't. The authors write that it\u00a0is\u00a0\"the first major study to show that being digitally literate can improve memory\" and that countries that promote digital literacy \"may expect lower incidence rates for dementia over the coming decades.\"\u00a0\nMr. Xavier told Op-Talk that using the Internet and email might be beneficial\u00a0because \"our brains need to learn new things and interact with other brains.\" He explained, \"our 'memory' is not inside, it is between us, in our day-by-day life when we talk and see each other.\" And Internet use may be one way to maintain connections and forge new ones: \"Digital literacy is about contact, new horizons, inclusion and humanization, so we start to be an active part of society again.\"\u00a0\nThe news that the Internet might actually make people better at something may come as a surprise, since so many have warned of its dangers. One of the most famous warnings is Nicholas Carr's 2008 Atlantic cover story \"Is Google Making Us Stupid?\" In it, he notes a change in his own mental habits:\n\"My mind now expects to take in information the way the Net distributes it: in a swiftly moving stream of particles. Once I was a scuba diver in the sea of words. Now I zip along the surface like a guy on a Jet Ski.\"\nHe also worried that the Internet might alter not just our thought processes and reading habits but our very selves. He recalled a scene in the film \"2001: A Space Odyssey\" wherein the artificial intelligence HAL pleads with his human operator:\n\"In the world of '2001,' people have become so machinelike that the most human character turns out to be a machine. That's the essence of Kubrick's dark prophecy: as we come to rely on computers to mediate our understanding of the world, it is our own intelligence that flattens into artificial intelligence.\"\nMr. Carr\u00a0also wrote that \"for all that's been written about the Net, there's been little consideration of how, exactly, it's reprogramming us.\" This is no longer true - examinations of the effect of Internet use on our mental faculties are now commonplace. In The Washington Post, Michael S. Rosenwald reports on recent research into Internet use and reading:\n\"With so much information, hyperlinked text, videos alongside words and interactivity everywhere, our brains form shortcuts to deal with it all - scanning, searching for key words, scrolling up and down quickly. This is nonlinear reading, and it has been documented in academic studies. Some researchers believe that for many people, this style of reading is beginning to invade when dealing with other mediums as well.\"\nThe neuroscientist and reading researcher Maryanne Wolf tells him she's seen the effects herself after a day of heavy Internet use:\n\"I couldn't force myself to slow down so that I wasn't skimming, picking out key words, organizing my eye movements to generate the most information at the highest speed. I was so disgusted with myself.\"\nAnd Michael Harris, in an excerpt from his book \"The End of Absence: Reclaiming What We've Lost in a World of Constant Connection,\" published at Wired, writes\u00a0that \"a\u00a0slower, less harried way of thinking may be on the verge of extinction\" and that \"young brains may be more equipped to deal with digital reality than with the decidedly less flashy reality that makes up our dirty, sometimes boring, material world.\" His warning:\n\"We may be on our way to becoming servants to the evolution of our own technologies. The power shifts very quickly from the spark of human intention to the absorption of human will by a technology that seems to have intentions of its own.\"\nBut some people see the Internet as a mixed blessing rather than as a curse. In a November article in Scientific American, the psychologists Daniel M. Wegner and Adrian F. Ward write that in one study performed by their team, people who had\u00a0access to a computer to save facts were worse at remembering them, even if they were asked to.\nIn another, those who were allowed to use the Internet to help them answer trivia questions felt smarter than those who had to answer them on their own. Mr. Wegner and Mr. Ward write that \"using Google gives people the sense that the Internet has become part of their own cognitive tool set\" and that \"the advent of the 'information age' seems to have created a generation of people who feel they know more than ever before - when their reliance on the Internet means that they may know ever less about the world around them.\" \nHowever, they conclude:\n\"As advances in computation and data transfer blur the lines between mind and machine, we may transcend some of the limits on memory and thought imposed by the shortcomings of human cognition. But this shift does not mean that we are in danger of losing our own identity. We are simply merging the self with something greater, forming a transactive partnership not just with other humans but with an information source more powerful than any the world has ever seen.\"\nMs. Wolf also believes Internet use could have benefits. She tells Mr. Rosenwald, \"We should be simultaneously reading to children from books, giving them print, helping them learn this slower mode, and at the same time steadily increasing their immersion into the technological, digital age. It's both.\"\nResearch into the Internet's effects on our mental faculties will no doubt continue for a long time to come, but Mr. Xavier and his team offer one clue that these effects may not be all bad. Per Mr. Wegner and Mr. Ward, the Internet may not be destroying us - rather, it may change us, sometimes for the better.                      \n","856":"ABSTRACT\nAndrew Browne China's World column in Chinese Pres Xi Jinping's desire to use big data and artificial intelligence to correct planning errors of past and micromanage Chinese economy while controlling its citizens, contrary to Western expectations for digital age; photo (M)\n","857":"ABSTRACT\nRide-hailing service Uber Technologies is creating artificial-intelligence division to improve service, such as by more accurately estimating rider locations and travel time as well as developing software for self-driving cars; photo (M)\n","858":"To the Editor:\nRe ''Robot Weapons: What's the Harm?'' (Op-Ed, Aug. 17): Jerry Kaplan argues that smart robotic weapons are likely to have far fewer costs in lives and other resources than conventional weapons, and that we have a duty to secure the greatest safety possible for our troops in battle. Moreover, because of their accuracy, use of robotic weapons would result in far fewer noncombatant casualties. \u00a0\n  The assumption made by Mr. Kaplan and most others who write on the morality of war is that war is inevitable, so the safest and most effective weapons are also the most ethical to use.\n  For a small number of us, however, war is unjust, and none of the arguments even for a defensive war are sound. Thus, any weapons that make war easier to engage in are themselves unethical.\n  Perhaps wars should cost lives, and perhaps wars should be morally complex and perhaps we should look our ''enemies'' in the eyes before we attempt to take their lives. Perhaps we should confront the worst in ourselves if we are ever to develop the best in ourselves.\n  For those reasons, I oppose developing and deploying robotic weapons.\n  JOHN DOUARD\n  Montclair, N.J.\n  To the Editor:\n  Jerry Kaplan includes in his criteria for use of robotic weaponry, ''It has to be fully controllable.'' But these smart machines will no doubt be reprogrammable, and thus be hackable. Consider your July 24 Business Day article ''The Web-Connected Car Is Cool, Until Hackers Cut Your Brakes.''\n  Technology is susceptible to the law of unintended consequences. We have repeatedly seen dumb weapons repurposed; it can be even worse with smart weapons.\n  RICHARD E. PATTIS\n  Irvine, Calif.\n  The writer teaches computer programming at the University of California, Irvine.\n\n\n\n","859":"With the help of artificial intelligence, scientists scouring data from NASA's Kepler Space Telescope have discovered an eighth planet around the star known as Kepler-90.\nThe find sets a record for the most exoplanets around a single star and, for the first time, ties with our own solar system.\nThe planet Kepler-90i, described at a briefing Thursday and detailed in a paper accepted for publication in the Astronomical Journal, demonstrates that other stars can indeed host planetary systems as extensive as our solar system's.\n\"Kepler has already shown us that most stars have planets,\" said Paul Hertz, director of NASA's Astrophysics Division in Washington. \"Today Kepler confirms that stars can have large families of planets just like our solar system.\"\u00a0\nThe discovery also establishes an important role for neural networks and other machine-learning techniques in the hunt for planets outside our solar neighborhood.\nKepler-90i orbits a sun-like star located 2,545 light-years away in the constellation Draco. Like Earth, Kepler-90i is the third rock from its sun -- though it sits much closer, circling its star every 14.4 days.\nTwo small planets within its orbit, known as 90b and 90c, revolve around Kepler-90 every seven and nine days, respectively.\nThe next three planets beyond Kepler-90i -- 90d, 90e and 90f -- fall into a sub-Neptune size class and complete an orbit every 60, 92 and 125 days, respectively. The two most distant planets, 90g and 90h, are Jupiter-class gas giants and take 211 and 332 days to make a round trip.\nAll of the planets except for 90i were previously known. That tied the Kepler-90 system with the seven-planet Trappist-1 system for the honor of most extensive known exoplanet solar system.\nIn some ways, the Kepler-90 system echoes our own solar system, with small rocky planets (Mercury, Venus, Earth, Mars) closer in to the sun and larger, more gas-rich ones (Jupiter and Saturn, Neptune and Uranus) lying farther out.\nScientists think there's a reason the larger planets orbit farther from their sun: It's the cool place to be.\n\"In our own solar system, this pattern is often seen as evidence that the outer planets formed in a cooler part of the solar system, where ice can stay solid and clumped together to make bigger and bigger planets,\" said Andrew Vanderburg, an astronomer at the University of Texas at Austin and an author of the forthcoming study.\nThe same phenomenon could be at work here around Kepler-90, scientists said.\nBut the exoplanet system differs from ours in at least one major way: The orbits of all eight planets would lie well within that of Earth, which takes 365 days to circle the sun.\nScientists said they weren't sure why the Kepler-90 system has such a crowded field. Perhaps some of the planets formed farther out and were eventually drawn inward.\nRegardless, it means that Kepler 90i, third rock though it may be, is too hot to be habitable.\n\"Kepler-90i is not a place I'd like to go visit,\" Vanderburg said, adding that the planet probably has an average temperature of about 800 degrees.\nThe Kepler Space Telescope was launched in 2009 to search for Earth-like planets around distant, sun-like stars. For four years it stared at a single patch of sky holding more than 150,000 stars, looking for the tiny, recurring dips in light caused by a planet as it repeatedly passed in front of its stellar host.\nAfter problems with its reaction wheels left the spacecraft hobbled in 2013, scientists and engineers focused it on new targets, renaming it the K2 mission. To date, the spacecraft has discovered 2,525 confirmed planets.\n\"We've come a long way in our understanding of planetary systems,\" said Jessie Dotson, Kepler's project scientist at NASA's Ames Research Center in Northern California, which manages the Kepler and K2 missions.\nThough Kepler is still searching the skies, the data it has already sent back could contain evidence of even more exoplanets.\nBut finding them will be a challenge, said study coauthor Christopher Shallue, a senior software engineer with the Google AI research team. To do it, scientists have to select the strongest signals and examine them with human eyes and automated tests to determine which ones are real.\nSo far, out of 30,000 signals, only about 2,500 have turned out to actually be planets.\n\"The process is like looking for needles in a haystack,\" Shallue said.\nIf you want to search for planets among Kepler's weaker signals -- which are far more numerous -- then that haystack gets \"much, much larger,\" he added.\n\"There are simply too many weak signals to examine using the existing methods,\" Shallue said. \"But machine learning really shines in situations where there is too much data for humans to examine for themselves.\"\nThat's why Shallue and Vanderburg developed a neural network, a type of machine-learning technique that can learn to identify patterns in large data sets.\n\"The key idea is to let the computer learn by example instead of humans programming specific rules,\" Shallue said.\nTo work properly, a neural network needs lots of practice. The scientists \"trained\" theirs using a set of 15,000 Kepler signals that had already been studied and properly labeled by humans. Using that data, the program learned to identify the signatures of actual planets and distinguish them from false positives.\nWhen the scientists finally tested their neural network on signals that it had not seen before, it correctly sorted the planets from the false positives a whopping 96% of the time, Shallue said.\nUsing the neural network, the scientists were able to discover new planets in old data -- Kepler-90i, as well as a sixth planet in a different star system, Kepler-80g. Kepler-80g is an Earth-sized planet that is gravitationally locked in a resonant chain with four of its fellow planets, forcing them to orbit their star almost as if they were all moving to the music of a choreographed dance.\n\"This new method found something that was extremely hard to find otherwise,\" said Sara Seager, an astrophysicist at MIT who was not involved in the work.\nThese discoveries are just the beginning, the scientists said. Shallue said they already had ideas for how to improve the neural network. Once those improvements are in place, they also plan to search all 150,000 or so stars in the Kepler data set to hunt for planets that are similar to Earth.\n\"I'm so excited to see where this goes next,\" Dotson said.\nMany scientists had hoped that Kepler's original mission would last far longer than four years, allowing it to find a significant number of Earth-sized planets in Earth-sized orbits around sun-like stars.\nAnd while that search was cut short, this AI-based method could allow researchers to go back into the data and fulfill that goal, said Seager, the deputy science director for NASA's Transiting Exoplanet Survey Satellite, set to launch next year.\nFinding some of those key planets hiding in Kepler's data \"would be incredible,\" Seager said.\nAs for Kepler-90, its planetary system could have more than eight planets, the scientists pointed out -- they could just lie farther away from the star and not have orbited enough times for Kepler to have spotted them during its primary mission.\n\"There's a lot of unexplored real estate in the Kepler-90 system,\" Vanderburg said, \"and it would almost be surprising to me if there weren't any more planets around this star.\"\nAs more such planetary systems are found, he added, our own home among the stars may start to look surprisingly ordinary.\n--\namina.khan@latimes.com\nTwitter: @aminawrite\n","861":"ACCORDING to her resume, the fictitious Miss Vicki Oberjeune became a movie star as a child - ''many moons ago'' - in ''X Marks the Spot,'' a tap-dancing musical comedy about ''a young teen-ager and her love for her dog, Spot.''\nSoon to be married for the seventh time, the 34-year-old ''Miss O,'' as she is called, ''blossomed into television work,'' first in a short-lived series, ''The Unreachables'' (a female version of ''The Untouchables''); then with her own talk show, ''Cocktails With Miss O,'' in which she ''chatted with every guest imaginable.''\nThe creator of ''Miss O'' - and the writer of her resume - is Nancy Cassaro, a 27-year-old performer from Massapequa. The live musical comedy revue, ''The Vicki Oberjeune Valentine's Day Special,'' is a one-hour sendup of 1950's and 1960's-style television party\/family\/at home\/hit-parade-type shows.\u00a0\nIt was conceived and co-written by Miss Cassaro and is being performed by Artificial Intelligence, an 18-member comedy theater troupe, of which she is a founding member. Artificial Intelligence took up residence as one of four comedy attractions at a Manhattan cabaret restaurant, the Ballroom, 253 West 28th Street, during a monthlong comedy festival that ended yesterday.\nBecause of enthusiastic critical and audience response, Miss Cassaro -who plays Miss Oberjeune - and company will return to the Ballroom from Tuesday through Feb. 14, playing Tuesdays through Saturdays at 9 P.M., in the retitled revue, ''Vicki's Valentine Thing.''\nIn performance, Miss Cassaro's comedic beginnings can be traced to the revue ''Gold Diggers of '73,'' which she also wrote, in the eighth grade while attending J. Lewis Ames Junior High School in Massapequa. Then, as now, much of her material was inspired - or thrust upon her - by family experiences. ''When your brothers tease you and your sister drives you crazy with a dopey song, you have to develop a sense of humor,'' she said.\nThe song with which her sister, Alice, now a music teacher in Massapequa, taunted her - ''Bluesette,'' with its simplistic lyric ''Poor little bluesette\/Sad little bluesette\/Don't you cry\/Don't you fret'' is one of the intentionally insipid numbers in the Vicki Oberjeune show.\nAnd ''Little Sister Syndrome,'' an account of growing up with five siblings, was one of Miss Cassaro's made-up songs when she appeared as a standup comedian in an East Village club in Manhattan in 1982, after she graduated from Hofstra University and worked as a waitress in a Hicksville restaurant for one year.\nMiss Cassaro's family includes her parents, three brothers, two sisters -as Vicki, she wears her older sister Judy's old dresses - ''and 10 nieces and nephews, with one on the way,'' she added, counting names on her fingers. They live in various parts of the Island: ''Commack, Garden City, Seaford,'' she said, again counting. Her father, Dr. James Cassaro, has been practicing medicine in Massapequa for 35 years.\nApart from family influences, Miss Cassaro has had her theatrical mentors: Hofstra University's Dr. Richard Mason, with whom she studied environmental theater, and, after graduating, David Kaplan, a teacher and director in New York, who emphasized the techniques of episodic theater. Thus, her orientation toward comedy is rooted in character and context.\nWith a group of Mr. Kaplan's students, she formed Artificial Intelligence in 1985. Rather than set routines or sketches, the company chooses to create an event and elaborate upon it in terms of a whole environmental theater piece, in which rituals are satirized, with a tone ranging from nostalgia to savagery.\nFor its first show, Artificial Intelligence staged a wedding party. Titled ''Tina-n-Tony,'' it has been performed in a SoHo loft and a Greenwich Village church and has evolved into a screenplay, ''Tony-n-Tina Forever,'' which is about a wedding in Massapequa. And just as Vicki has her resume, Tina has a wedding album, replete with such photos as the bride and bridegroom covered with dollar bills, which they charged their guests to dance with them.\nThe environment for the Vicki Oberjeune revue is a television studio, an idea that, Miss Cassaro recalled, came while ''looking through a picture book by Desi Arnaz.''\n''There was the 'I Love Lucy' set,'' she said, ''then suddenly another picture showed the crew and then the audience, and the perspective of the Ricardos in their living room changed. It was a whole fake world, and I thought, 'So, that's really what's going on!' ''\nFrom there, Miss Cassaro's fancy, and that of her co-author and director, Larry Pellegrini - took flight. One character, Fritz Freund, is named after Karl Freund, who was the actual cameraman on the ''I Love Lucy'' shows. Another character, Jack Peters - played by James Altuner - is intended to be a satirical cross between the singers Jack Jones and Robert Goulet.\nThe Vicki Oberjeune revue begins with the crew setting the stage for a television show. An ''applause'' sign lights up, getting the live audience prepared to act as a television audience. The frenetic goings-on include a tipsy star (''the incomparable Vicki'') teetering about, making off-camera advances to the cameraman (her next husband), competing for on-camera attention with her sultry daughter and singing a love ballad to her innocent son. The singers and dancers are forever out of step, out of tune and out of synch. The result is sheer madness.\nExplaining the Artificial Intelligence comic style and credo, Miss Cassaro said: ''We create the contradistinction between what they're doing for the camera and what they're really doing - like maybe they're hitting each other. When we thought up our name, we hoped to be taken seriously - but we never want to take ourselves too seriously. We're just a bunch of writers and actors with a sense of how absurd things are.\n''We like creating a theatrically entertaining world and asking the audience to come into it. Maybe we can give people a different way of looking at television shows and weddings.''\n","862":"ABSTRACT\nFacebook is considering designing its own chips for consumer devices, artificial-intelligence software and data centers as it begins to build its own hardware, following path taken by Google and others (M)\n","863":"ABSTRACT\nML Cavanaugh commentary notes objections by some Google employees to working with US military on Project Maven artificial-intelligence project; contends they share interests in preventing China and Russia from developing overwhelming advantage against West\n","864":"ABSTRACT\nAili McConnon article in Journal Report: Health Care\u00a0 notes use of artificial intelligence, such as suicide-alert software system used by Facebook, in tools that recognize and prevent suicidal or heavily depressed individuals from killing themselves, but critics are raising possible concerns over privacy; photo; chart (M)\n","865":"ABSTRACT\nChinese Internet giant Baidu's chief scientist, Stanford professor Andrew Ng, is leaving to explore 'new chapter' in artificial intelligence, dealing blow to Baidu as it is strengthening its focus on AI and hoping to internationalize its business; photo (M)\n","866":"ABSTRACT\nIBM agrees to integrate human-like conversation and learning capabilities of its Watson with Salesforce.com's more sales-oriented Einstein artificial-intelligence technology;\u00a0 move makes Watsons more visible to business and helps Salesforce against rivals like Microsoft (M)\n","867":"ABSTRACT\nIBM CEO Ginni Rometti says during World Economic Forum panel in Davos, Switzerland, that advances in artificial intelligence will lead to job losses, but new forms of employment will take their place (M)\n","868":"ABSTRACT\nVoices article in The Future of Everything section notes Baidu chief scientist Andrew Ng's comments on how artificial intelligence will impact what people do for living (M)\n","869":"ABSTRACT\nRobert McMillan Business News article discusses IBM decision to make its proprietary artificial intelligence software SystemML available to share and modify through Apache Software Foundation (M)\n","870":"ABSTRACT\nPaul Vigna MoneyBeat column notes Merrill Lynch's massive report on opportunities in robotics and artificial intelligence, which it expects to be $153 billion market by 2020, as well as its concerns about potential risks around development of fully autonomous weapons, or killer robots (M)\n","871":"ABSTRACT\nJerry Kaplan Review section article discusses possibility of injecting social sense into robots controlled by artificial intelligence software; drawing (M)\n","872":"ABSTRACT\nAlison Gopnik Mind & Matter column discusses what it means to be human in a world of fast-moving computer artificial intelligence; photo (M)\n","873":" ABSTRACT:The Front Lines column profiles mathematician Ben Goertzel, an artificial-intelligence theorist who speculates on the possibilities of computers as reasoning minds with human beings as sense organs; Goertzel is being bankrolled by a few Wall Street investors who see his concept of computing as a possible basis for trading securities and managing information; drawing (M)\n","874":" ABSTRACT:Symbolics Inc, onetime leader in artificial intelligence, files for Chapter 11 bankruptcy protection (S)\n","875":"WHEN COMPUTERS HURT INSTEAD OF HELP\n  Ray Fisman writes on the Web site Slate about why giving computers to poor children won't necessarily help educate them.\n  Parents are more worried than ever about making sure their kids can compete in today's high-tech world, and the growing digital divide is a subject of great concern for educators and policymakers. Federal subsidies in the United States provide billions of dollars for computer access in schools and libraries, and billions more may soon be spent in the developing world through programs such as One Laptop per Child. But even O.L.P.C.'s $100 laptop comes loaded with more distractions than my PET [the world's first personal computer] ever had. So will kids use these subsidized computing resources to prepare for the demands of the 21st-century job market? Or do computers just serve as a 21st-century substitute for that more venerable time-waster--the television? \u00a0\n  New research by economists Ofer Malamud and Cristian Pop-Eleches provides an answer: For many kids, computers are indeed more of a distraction than a learning opportunity. The two researchers surveyed households that applied to Euro 200, a voucher distribution program in Romania designed to help poor households defray the cost of buying a computer for their children. It turns out that kids in households lucky enough to get computer vouchers spent a lot less time watching TV -- but that's where the good news ends. ''Vouchered'' kids also spent less time doing homework, got lower grades and reported lower educational aspirations than the ''unvouchered'' kids. ...\n  Perhaps not surprisingly, the lesson from Romania's voucher experiment is not that computers aren't useful learning tools, but that their usefulness relies on parents being around to assure they don't simply become a very tempting distraction from the unpleasantness of trigonometry homework. But this is a crucial insight for those tasked with designing policies to bridge the digital divide. ... If we really want to help poor kids, whether in Romania, sub-Saharan Africa, or America's housing projects, we may want to focus on approaches that provide structured, supervised access through after-school programs or subsidies that bring technology into low-income schools. But just giving kids computers? Might as well just ship them PlayStations.\n  THE ELUSIVENESS OF THE HUMAN MIND\n  Douglas R. Hofstadter, a professor of cognitive science at Indiana University, in an interview on the Web site Tal Cohen's Bookshelf, explains why he is glad that artificial intelligence hasn't made more headway.\n  Am I disappointed by the amount of progress in cognitive science and artificial intelligence in the past 30 years or so? Not at all. To the contrary, I would have been extremely upset if we had come anywhere close to reaching A.I. -- it would have made me fear that our minds and souls were not deep. Reaching the goal of A.I. in just a few decades would have made me dramatically lose respect for humanity, and I certainly don't want (and never wanted) that to happen.\n  I am a deep admirer of humanity at its finest and deepest and most powerful -- of great people such as Helen Keller, Albert Einstein, Ella Fitzgerald, Albert Schweitzer, Frederic Chopin, Raoul Wallenberg, Fats Waller, and on and on. I find endless depth in such people ... and I would hate to think that all that beauty and profundity and goodness could be captured -- even approximated in any way at all! -- in the horribly rigid computational devices of our era.\n  Do I still believe it will happen someday? I can't say for sure, but I suppose it will eventually, yes. I wouldn't want to be around then, though. Such a world would be too alien for me. I prefer living in a world where computers are still very, very stupid. And I get a huge kick out of laughing at the hilariously unpredictable inflexibility of the computer models of mental processes that my doctoral students and I codesign. It helps remind me of the immense subtlety and elusiveness of the human mind.\n  A VERY OLD SEED GROWS IN ISRAEL\n  Ari Rabinovitch writes for Reuters about an ancient date seed that sprouted and may help restore a species of biblical trees.\n  Carbon dating confirmed that the seed -- named Methuselah after the oldest person in the Bible -- was the oldest ever brought back to life, Sarah Sallon, a researcher at the Hadassah Medical Centre in Jerusalem, reported in the journal Science.\n  The seed came from the Judean date palm, a species that once flourished in the Jordan River Valley and has been extinct for centuries, Ms. Sallon said. It was one of a group discovered at Masada, a winter palace overlooking the Dead Sea built by King Herod in the 1st century B.C.\n  The fortress was used by hundreds of Jewish insurgents in a revolt against Roman rule that erupted in 67 A.D.\n  ''It has survived and flourished,'' Dr. Sallon said. Previous attempts to grow plants from ancient seeds failed after a few days. ...\n  Preliminary genetic studies suggest it may share about half of its genetic code with modern dates, Dr. Sallon said.\n  If the tree, which now stands about five feet tall, is female, it might be able to help restore the species which once formed thick forests throughout the Jordan River Valley, she said.\n","876":"In just the past few years, Robert McMillan writes in Wired magazine, dramatic advances have been made in the field of artificial intelligence. With Skype's \"Star-Trek-like instant translation capabilities,\" Google's self-driving cars and computers that can teach themselves to humiliate humans at arcade games, he says, the new developments are both exhilarating and scary.\u00a0\nIn \"AI Has Arrived, and That Really Worries the World's Brightest Minds,\" he reports on the fears of experts who gathered at a closed-door conference in Puerto Rico in early January; among the industry talents were Elon Musk of SpaceX and Tesla, Skype co-founder Jaan Tallinn and Google AI expert Shane Legg. Think of that game-winning computer, Tallinn told the meeting: Though \"the technologist in me marveled at the achievement, the other thought I had was that I was witnessing a toy model of how an AI disaster would begin, a sudden demonstration of an unexpected intellectual capability.\" In other words, it's a little too much like a precursor to \"The Terminator.\"\nDelegates to the conference signed an open letter pledging to conduct AI research only for good. Another letter outlined research priorities that would involve studying the economic and legal effects of robots that can take away human jobs or manipulate financial markets; Musk kicked in $10 million to help pay for the research.\nIt's not a new fear, McMillan notes: Last year a Canadian company, Clearpath Robotics, promised not to build autonomous robots for military use. On its Web site the company posted this statement: \"To the people against killer robots: We support you.\"\n","877":"A new partnership between aviation giant Boeing and Carnegie Mellon University hints at the power of fields such as artificial intelligence and big data to transform huge, multibillion-dollar industries. As part of a three-year, $7.5 million deal that establishes a new Aerospace Data Analytics Lab, Boeing and Carnegie Mellon's School of Computer Science plans to work on a range of new projects that will apply the principles of AI and big data to improving the quality of Boeing's aerospace activities. \nThe goal of the new partnership, first and foremost, is to make sense of the burgeoning amount of data in the aerospace industry. By applying principles of machine learning, it might be possible to optimize many aspects of Boeing's operations - including those related to design, construction and operation - and turn ordinary data into real-world insights.\u00a0\nJaime Carbonell, the Carnegie Mellon professor and director of the Language Technologies Institute who will head up the new Aerospace Data Analytics Lab, said: \"The mass of data generated daily by the aerospace industry overwhelms human understanding, but recent advances in language technologies and machine learning give us every reason to expect that we can gain useful insights from that data.\"\nOne example of how machine learning can be used to gain useful insights is the issue of airline maintenance. Think of airline maintenance in the same way one would think of car maintenance. You can follow the generally suggested guidelines for your vehicle (e.g. an oil change every 3,000 miles), or you can use real-time data to see which planes require repairs and when. By performing preventive maintenance, an airline flying Boeing planes could gain a competitive advantage over its peers.\n\"A Boeing aircraft such as the 787 Dreamliner combines thousands of on-board sensors, text from pilots and mechanics, structured engineering databases, across the entire fleet collected from each of the client airlines,\" Carbonell said. \"This provides a golden opportunity, merging CMU's capabilities and Boeing data to address problems such as predictive analysis for preventive maintenance - rather than after-the-plane-is-grounded maintenance.\"\nIn short, Carnegie Mellon and Boeing might be able to determine when planes actually need maintenance instead of just following historical maintenance schedules. And that might avoid the airline equivalent of the \"check engine\" light that comes on without warning in automobiles. That makes flying safer for passengers - and could reduce the time lost on the tarmac accounting for last-minute mechanical failures in planes.\nAt the launch of the new lab, Ted Colbert, Boeing chief information officer, commented on the importance of the new initiative for the aerospace giant: \"We're aiming to push the technology envelope. We have the best and the brightest faculty at a leading institution focused on how we can innovate and solve business challenges for today and into the future.\"\nThe Aerospace Data Analytics Lab is just part of an expanding number of initiatives at Carnegie Mellon that are attempting to tap into the exciting potential of AI, according to Andrew Moore, Carnegie Mellon's computer science dean. He mentioned a number of \"moonshot\" initiatives underway - including robots that take over the cleanup of hazardous environments and robotic arms that can pick up a cup of coffee without spilling it.\nAnd keep in mind, Carnegie Mellon is also the home of the self-driving car, perhaps one of the most talked-about advances in the AI field in recent years. Carnegie Mellon already has created more than 140 technologies related to autonomous vehicles.\nSo are self-flying airplanes on the horizon?\nThat's not really the goal, Carbonell said, especially given the three-year time horizon of the Boeing lab project. \"We already have semi-self-driving airplanes. We call the devices autopilots, which have been in use for a long time. These will keep improving and gradually yield a higher degree of automation.\"\nInstead, Carbonell said, the goal is airplanes that can fix themselves: \"The audacious plan is closer to self-healing airplanes: evidence-based predictions of what may not be working right tomorrow, to enable preventive inspection or replacement before a failure, and hence to lower costs of coping with real unscheduled failures and to increase safety.\"\nAt the official launch of the lab, Boeing suggested that its involvement could grow over time, presumably based on the success of the aerospace giant being able to realize a return from its AI investment. For now, the lab will launch with more than six Boeing-directed projects and will involve more than 20 researchers pulled from the ranks of Carnegie Mellon's faculty and graduate students.\nThe big idea is that visible success by an American corporate giant in a data-intensive industry could encourage other companies to explore AI initiatives. As it continues to move up the software stack, Moore said AI could become the layer on top that provides helpful advice to humans, enabling them to build better models of the world based on new machine-learning algorithms.\ndominic.basulto@washpost.com\n","878":"ABSTRACT\nHennes & Mauritz's H&M retail chain is turning to artificial intelligence and store data to customize what it sells in individual stores after having offered same selection across chain for years;\u00a0 hopes strategy can reverse slump in same-store sales; diagram; graph; photo (M)\n","879":"ABSTRACT\nJPMorgan Chase hires Carnegie Mellon's head of machine learning Manuela Veloso to head up its new artificial-intelligence research operation;\u00a0 aims to build on its LOXM program, which executes trades across its global equities algorithms business; photo (M)\n","880":"ABSTRACT\nNina Trentmann Technology article on Statoil and hundreds of other large global companies that are using automation, robotics and artificial intelligence to improve their efficiency (M)\n","881":"ABSTRACT\nPrivately-held Promontory Financial Group agrees to be acquired by IBM, which plans to combine its consultants with its Watson artificial-intelligence technology to advise clients on financial regulation (M)\n","882":"ABSTRACT\nSouth Korean Go grandmaster Lee Se-dol claims his first victory in five-game match against AlphaGo, artificial-intelligence project developed by Alphabet's Google following three straight losses; photo (M)\n","883":" ABSTRACT:Portals column takes issue with Ray Kurzweil, a flag-waver for the 'strong' school of artificial intelligence, whose book 'The Singularity Is Near' promises fully human-like computers in about 20 years; observes that for decades, strong AI advocates have explained their failures by insisting that some new research direction would bring home the prize; notes that Kurzweil's new vision requires scientists to re-engineer the human brain and duplicate its secrets in a computer (M)\n","884":" ABSTRACT:Special section on Technology describes increased willingness of some corporations to use artificial intelligence (AI) software agents to help control costs and improve customer service and data collection; notes AI agents can act as experts to augment users' skills or as 'gofers' to gather and assemble otherwise inaccessible information; drawings (L)\n","885":"In Copenhagen the summer before last, I shared a taxi with a man who thought his chance of dying in an artificial intelligence-related accident was as high as that of heart disease or cancer. No surprise if he'd been the driver, perhaps (never tell a taxi driver that you're a philosopher!), but this was a man who has spent his career with computers.\nIndeed, he's so talented in that field that he is one of the team who made this century so, well, 21st - who got us talking to one another on video screens, the way we knew we'd be doing in the 21st century, back when I was a boy, half a century ago. For this was Jaan Tallinn, one of the team who gave us Skype. (Since then, taking him to dinner in Trinity College here in Cambridge, I've had colleagues queuing up to shake his hand, thanking him for keeping them in touch with distant grandchildren.)\u00a0\nI knew of the suggestion that A.I. might be dangerous, of course. I had heard of the \"singularity,\" or \"intelligence explosion\"- roughly, the idea, originally due to the statistician I J Good (a Cambridge-trained former colleague of Alan Turing's), that once machine intelligence reaches a certain point, it could take over its own process of improvement, perhaps exponentially, so that we humans would soon be left far behind. But I'd never met anyone who regarded it as such a pressing cause for concern - let alone anyone with their feet so firmly on the ground in the software business.\nI was intrigued, and also impressed, by Tallinn's commitment to doing something about it. The topic came up because I'd asked what he worked on these days. The answer, in part, is that he spends a lot of his time trying to improve the odds, in one way or another (talking to philosophers in Danish taxis, for example).\nI was heading for Cambridge at the time, to take up my new job as Bertrand Russell professor of philosophy - a chair named after a man who spent the last years of his life trying to protect humanity from another kind of technological risk, that of nuclear war. And one of the people I already knew in Cambridge was the distinguished cosmologist Martin Rees - then master of Trinity College, and former president of the Royal Society. Lord Rees is another outspoken proponent of the view that we humans should pay more attention to the ways in which our own technology might threaten our survival. (Biotechnology gets most attention, in his work.)\nSo it occurred to me that there might be a useful, interesting and appropriate role for me, as a kind of catalyst between these two activists, and their respective circles. And that, to fast forward a little, is how I came to be taking Jaan Tallinn to dinner in Trinity College; and how he, Martin Rees and I now come to be working together, to establish here in Cambridge the Centre for the Study of Existential Risk (C.S.E.R.).\nBy \"existential risks\" (E.R.) we mean, roughly, catastrophic risks to our species that are \"our fault,\" in the sense that they arise from human technologies. These are not the only catastrophic risks we humans face, of course: asteroid impacts and extreme volcanic events could wipe us out, for example. But in comparison with possible technological risks, these natural risks are comparatively well studied and, arguably, comparatively minor (the major source of uncertainty being on the technological side). So the greatest need, in our view, is to pay a lot more attention to these technological risks. That's why we chose to make them the explicit focus of our center.\nI have now met many fascinating scholars - scientists, philosophers and others - who think that these issues are profoundly important, and seriously understudied. Strikingly, though, they differ about where they think the most pressing risks lie. A Cambridge zoologist I met recently is most worried about deadly designer bacteria, produced - whether by error or by terror, as Rees puts it - in a nearby future in which there's almost an app for such things. To him, A.I. risk seemed comparatively far-fetched - though he confessed that he was no expert (and added that the evidence is that even experts do little better than chance, in many areas).\nWhere do I stand on the A.I. case, the one that got me into this business? I don't claim any great expertise on the matter (perhaps wisely, in the light of the evidence just mentioned). For what it's worth, however, my view goes like this. On the one hand, I haven't yet seen a strong case for being quite as pessimistic as Jaan Tallinn was in the taxi that day. (To be fair, he himself says that he's not always that pessimistic.) On the other hand, I do think that there are strong reasons to think that we humans are nearing one of the most significant moments in our entire history: the point at which intelligence escapes the constraints of biology. And I see no compelling grounds for confidence that if that does happen, we will survive the transition in reasonable shape. Without such grounds, I think we have cause for concern.\nMy case for these conclusions relies on three main observations. The first is that our own intelligence is an evolved biological solution to a kind of optimization problem, operating under very tight constraints of time, energy, raw materials, historical starting point and no doubt many other factors. The hardware needs to fit through a mammalian birth canal, to be reasonably protected for a mobile life in a hazardous environment, to consume something like 1,000 calories per day and so on - not to mention being achievable by mutation and selection over a time scale of some tens of millions of years, starting from what existed back then!\nSecond, this biological endowment, such as it is, has been essentially constant, for many thousands of years. It is a kind of fixed point in the landscape, a mountain peak on which we have all lived for hundreds of generations. Think of it as Mount Fuji, for example. We are creatures of this volcano. The fact that it towers above the surrounding landscape enables us to dominate our environment and accounts for our extraordinary success, compared with most other species on the planet. (Some species benefit from our success, of course: cockroaches and rats, perhaps, and the many distinctive bacteria that inhabit our guts.) And the distinctive shape of the peak - also constant, or nearly so, for all these generations - is very deeply entangled with our sense of what it is to be us. We are not just creatures of any volcano; we are creatures of this one.\nBoth the height and the shape of the mountain are products of our biological history, in the main. (The qualification is needed because cultural inheritance may well play a role too.) Our great success in the biological landscape, in turn, is mainly because of the fact that the distinctive intelligence that the height and shape represent has enabled us to control and modify the surrounding environment. We've been exercising such control for a very long time of course, but we've recently got much better at it. Modern science and technology give us new and extraordinarily powerful ways to modify the natural world, and the creatures of the ancient volcano are more dominant than ever before.\nThis is all old news, of course, as is the observation that this success may ultimately be our undoing. (Remember Malthus.) But the new concern, linked to speculation about the future of A.I., is that we may soon be in a position to do something entirely new: to unleash a kind of artificial vulcanism, that may change the shape and height of our own mountain, or build new ones, perhaps even higher, and perhaps of shapes we cannot presently imagine. In other words - and this is my third observation - we face the prospect that designed nonbiological technologies, operating under entirely different constraints in many respects, may soon do the kinds of things that our brain does, but very much faster, and very much better, in whatever dimensions of improvement may turn out to be available.\nThe claim that we face this prospect may seem contestable. Is it really plausible that technology will reach this stage (ever, let alone soon)? I'll come back to this. For the moment, the point I want to make is simply that if we do suppose that we are going to reach such a stage - a point at which technology reshapes our human Mount Fuji, or builds other peaks elsewhere - then it's not going to be business as usual, as far as we are concerned. Technology will have modified the one thing, more than anything else, that has made it \"business as usual\" so long as we have been human.\nIndeed, it's not really clear who \"we\" would be, in those circumstances. Would we be humans surviving (or not) in an environment in which superior machine intelligences had taken the reins, to speak? Would we be human intelligences somehow extended by nonbiological means? Would we be in some sense entirely posthuman (though thinking of ourselves perhaps as descendants of humans)? I don't claim that these are the only options, or even that these options are particularly well formulated - they're not! My point is simply that if technology does get to this stage, the most important fixed point in our landscape is no longer fixed - on the contrary, it might be moving, rapidly, in directions we creatures of the volcano are not well equipped to understand, let alone predict. That seems to me a cause for concern.\nThese are my reasons for thinking that at some point over the horizon, there's a major tipping point awaiting us, when intelligence escapes its biological constraints; and that it is far from clear that that's good news, from our point of view. To sum it up briefly, the argument rests on three propositions: (i) the level and general shape of human intelligence is highly contingent, a product of biological constraints and accidents; (ii) despite its contingency in the big scheme of things, it is essential to us - it is who we are, more or less, and it accounts for our success; (iii) technology is likely to give us the means to bypass the biological constraints, either altering our own minds or constructing machines with comparable capabilities, and thereby reforming the landscape.\nBut how far away might this tipping point be, and will it ever happen at all? This brings me back to the most contested claim of these three - the assertion that nonbiological machines are likely, at some point, to be as intelligent or more intelligent than the \"biological machines\" we have in our skulls.\nObjections to this claim come from several directions. Some contest it based on the (claimed) poor record of A.I. so far; others on the basis of some claimed fundamental difference between human minds and computers; yet others, perhaps, on the grounds that the claim is simply unclear - it isn't clear what intelligence is, for example.\nTo arguments of the last kind, I'm inclined to give a pragmatist's answer: Don't think about what intelligence is, think about what it does. Putting it rather crudely, the distinctive thing about our peak in the present biological landscape is that we tend to be much better at controlling our environment than any other species. In these terms, the question is then whether machines might at some point do an even better job (perhaps a vastly better job). If so, then all the above concerns seem to be back on the table, even though we haven't mentioned the word \"intelligence,\" let alone tried to say what it means. (You might try to resurrect the objection by focusing on the word \"control,\" but here I think you'd be on thin ice: it's clear that machines already control things, in some sense - they drive cars, for example.)\nMuch the same point can be made against attempts to take comfort in the idea that there is something fundamentally different between human minds and computers. Suppose there is, and that that means that computers will never do some of the things that we do - write philosophy, appreciate the sublime, or whatever. What's the case for thinking that without these gifts, the machines cannot control the terrestrial environment a lot more effectively than we do?\nPeople who worry about these things often say that the main threat may come from accidents involving \"dumb optimizers\" - machines with rather simple goals (producing IKEA furniture, say) that figure out that they can improve their output astronomically by taking control of various resources on which we depend for our survival. Nobody expects an automated furniture factory to do philosophy. Does that make it less dangerous? (Would you bet your grandchildren's lives on the matter?)\nBut there's a more direct answer, too, to this attempt to take comfort in any supposed difference between human minds and computers. It also cuts against attempts to take refuge in the failure of A.I. to live up to some of its own hype. It's an answer in two parts. The first part - let me call it, a little aggressively, the blow to the head - points out that however biology got us onto this exalted peak in the landscape, the tricks are all there for our inspection: most of it is done with the glop inside our skulls. Understand that, and you understand how to do it artificially, at least in principle. Sure, it could turn out that there's then no way to improve things - that biology, despite all the constraints, really has hit some sort of fundamental maximum. Or it could turn out that the task of figuring out how biology did it is just beyond us, at least for the foreseeable future (even the remotely foreseeable future). But again, are you going to bet your grandchildren on that possibility?\nThe second part of the argument - the blow from below - asks these opponents just how far up the intelligence mountain they think that A.I. could get us. To the level of our fishy ancestors? Our early mammalian ancestors? (Keep in mind that the important question is the pragmatic one: Could a machine do what these creatures do?) Wherever they claim to draw the line, the objection challenges them to say what biology does next, that no nonbiological machine could possibly do. Perhaps someone has a plausible answer to this question, but for my part, I have no idea what it could be.\nAt present, then, I see no good reason to believe that intelligence is never going to escape from the head, or that it won't do so in time scales we could reasonably care about. Hence it seems to me eminently sensible to think about what happens if and when it does so, and whether there's something we can do to favor good outcomes over bad, in that case. That's how I see what Rees, Tallinn and I want to do in Cambridge (about this kind of technological risk, as about others): we're trying to assemble an organization that will use the combined intellectual power of a lot of gifted people to shift some probability from the bad side to the good.\nTallin compares this to wearing a seat belt. Most of us agree that that makes sense, even if the risk of an accident is low, and even though we can't be certain that it would be beneficial, if we were to have an accident. (Occasionally, seat belts make things worse.) The analogy is apt in another way, too. It is easy to turn a blind eye to the case for wearing a seat belt. Many of us don't wear them in taxis, for example. Something - perhaps optimism, a sense that caution isn't cool, or (if you're sufficiently English!) a misplaced concern about hurting the driver's feelings - just gets in the way of the simple choice to put the thing on. Usually it makes no difference, of course, but sometimes people get needlessly hurt.\nWorrying about catastrophic risk may have similar image problems. We tend to be optimists, and it might be easier, and perhaps in some sense cooler, not to bother. So I finish with two recommendations. First, keep in mind that in this case our fate is in the hands, if that's the word, of what might charitably be called a very large and poorly organized committee - collectively shortsighted, if not actually reckless, but responsible for guiding our fast-moving vehicle through some hazardous and yet completely unfamiliar terrain. Second, remember that all the children - all of them - are in the back. We thrill-seeking grandparents may have little to lose, but shouldn't we be encouraging the kids to buckle up?\nHuw Price\u00a0is Bertrand Russell professor of philosophy at the University of Cambridge. With\u00a0Martin Rees\u00a0and\u00a0Jaan Tallinn, he is a co-founder of the project to establish the\u00a0Centre for the Study of Existential Risk. \n\n","887":"ABSTRACT\nSony brings its Aibo robotic dog back to market as 198,000-yen ($1,700) puppy with improved artificial-intelligence capabilities and enhanced motors;\u00a0 device will be able to interact with other Internet-connected electronics; photo (M)\n","888":"ABSTRACT\nThomas Coffey letter responds to July 17 Business News article regarding Tesla CEO Elon Musk's concerns about dangers of artificial intelligence\n","890":"ABSTRACT\nDaniela Hernandez article in Journal Report: Health-Care Technology describes how pharmaceutical companies are using artificial intelligence to develop drugs; photo (M)\n","891":"ABSTRACT\nSalesforce.com agrees to pay about $700 million to acquire startup Krux, which mines Internet data and uses artificial intelligence to analyze it to help companies better understand and target customers (M)\n","892":"ABSTRACT\nAlan Murray interview with inventor and entrepreneur Ray Kurzwell in The Journal Report on CFO Network conference, in which Kurzwell discusses artificial intelligence, merger of technology and biology and other issues; photo\n","893":"Increasing numbers of investors like those at hedge fund Rebellion Research are turning to branch of artificial intelligence called machine learning to make investment decisions; Rebellion has topped Standard & Poor's 500-stock index by average of 10% a year, after fees, since its 2007 launch through June; programs can be effective at crunching large amounts of data quickly, learning what works and adjusting strategies on fly; photos (L)\n","894":" ABSTRACT:Applied Expert Systems Inc introduces artificial intelligence computer program for financial planners; agrees to buy 1,000 Xerox Corp artificial intelligence computers for $20 million (S)\n","896":"ABSTRACT\nIBM, Intel and associations representing Apple, Facebook and Alphabet Inc are working with futurists, civil-rights activists and social scientists on artificial-intelligence code of conduct, seeking to head off regulators as businesses rush to incorporate AI in their operations; photo (M)\n","897":"ABSTRACT\nQuote from Stanford Adjunct Prof Jerry Kaplan in Journal Report: CEO Council on artificial intelligence; photo\n","898":"ABSTRACT\nUS deputy chief technology officer Michael Kratsios is developing policy agenda to spur innovation in areas such as drones and artificial intelligence, but needs to work closely with science and tech communities that have been critics of Pres Trump; photo (M)\n","899":"ABSTRACT\nProminent startup investors are warning of strong backlash against technology companies that could lead to breakup amid concerns about job losses, economic inequality, artificial intelligence and companies' growing role in Americans' lives (S)\n","900":"ABSTRACT\nDan Gallagher Heard on the Street article warns that market for chips driving Google, Microsoft and Amazon.com's networks powered by artificial intelligence is in flux and Nvidia's graphics processing units could pose threat to giant Intel; graph (M)\n","901":"ABSTRACT\nIntel plans to deliver new version of its Xeon Phi processor targeted at scientific applications that will speed deep-learning tasks associated with artificial intelligence;\u00a0 fast-growing field is scene of hot competition with rivals like Nvidia (S)\n","902":"ABSTRACT\nGoogle has fallen behind rivals Amazon.com and Microsoft in offering cloud-computing services but is making new push by offering artificial-intelligence programs that could encourage customers to rent its servers; photo (M)\n","903":"ABSTRACT\nGeoffrey Fowler Personal Technology column tests Aether Cone, audio speaker that combines stylish hardware, Internet streaming and artificial intelligence to choose what listener might want to hear; photos (M)\n","904":"An article in Science Times on Tuesday about a project at the University of Southern California that uses artificial intelligence to help soldiers learn Arabic gave an incorrect literary example. Omar Khayyam's poetry is in Persian, not Arabic. Because of an editing error, the article also misstated the location of Fort Bragg, where Special Forces soldiers are to test a video game this month as part of the project. It is in North Carolina, not Northern California.\n","905":"ABSTRACT\nS&P Global Inc says it will buy technology startup Kensho Technologies for about $550 million, its second investment into artificial intelligence sector this year (M)\n","906":"ABSTRACT\nSam Schechner, Douglas MacMillan and Liza Lin article in Outlook 2018 section notes growing concern about global race to develop and profit from artificial intelligence; notes US companies are leading race, but their Chinese rivals are catching up quickly because of growing investments; graph (M)\n","907":"ABSTRACT\nLi Yuan China Circuit column on recent moves by Chinese startups to take lead in building central processing units that use artificial intelligence to make phones, cars and home appliances to more seamlessly interact with users; photo; graph (M)\n","908":"ABSTRACT\nChristof Koch Review section article discusses how humans need to create technologies that enhance brain to avoid dystopian future fueled by rise of artificial intelligence; drawing; photos (L)\n","909":"ABSTRACT\nPhoto of Hanson Robotics' Han the Robot, which engaged another robot in conversation at RISE Technology Conference in Hong Kong, where artificial intelligence was one topic of discussion\n","910":"ABSTRACT\nApple has discussed investing as much as $1 billion in SoftBank Group's $100 billion technology fund, focused on emerging technologies such as artificial intelligence and Internet of Things;\u00a0 Apple has previously focused on small stakes in young tech companies (M)\n","911":"ABSTRACT\nVenture capitalists are spreading their bets across range of unproven technologies in search of next big thing, such as artificial intelligence, robots, agricultural tech and aerospace, but pressure to put money to work may lead to foolish decisions; graphs; photo (M)\n","912":"ABSTRACT\nIntel says it will acquire Movidius, startup that focuses on computer vision technology, in latest bet on specialized chips for artificial-intelligence applications; photo (M)\n","913":"ABSTRACT\nJapanese smartphone-game provider DeNA sets up PFDeNA with Preferred Networks to become latest major firm to bet on artificial-intelligence technology; names DeNA CEO Isao Moriyasu to head new firm; photo (S)\n","914":"ABSTRACT\nAlibaba's AliCloud unit has hired Microsoft veteran Zhou Jingren as it plans to boost its big-data and artificial-intelligence capabilities for business customers;\u00a0 Morgan Stanley suggests AliCloud could become one of its biggest growth drivers by 2020 (M)\n","915":"ABSTRACT\nFrancis Fallon, Jascha Kessler, John F Thomas and David Brown letters respond to March 19 Review section article about advances in artificial intelligence; photo\n","916":" ABSTRACT:David Stipp Leisure & Arts article contends that influence of late British scientist Alan Turing still pervades quest for artificial intelligence because of ingenious experiment he proposed to determine whether machine can think; says experiment is at last to be conducted this fall in form of international sporting event in Boston, pitting humans against computer; Turing portrait (M)\n","917":" ABSTRACT:IntelliCorp Inc, maker of software based on artificial-intelligence technology, says it signed two-year distributor agreement with Hyundai Electronics Industries Co of South Korea (S)\n","918":"One of the most challenging tasks for the hearing-impaired is communicating effectively in noisy environments. This was made a little easier by digital hearing aids introduced in the late 90's, which used directional microphones to detect the trajectories of sounds, and compression systems to clarify them. \u00a0\n A new hearing aid from Oticon, the Syncro, goes a step further, incorporating artificial intelligence software. The hearing aid, which comes in a variety of shapes and colors and costs $2,000 to $3,000, uses an algorithm to adapt to the wearer's environment by constantly adjusting its digital sound processor's signal-to-noise ratio. \n  The Syncro's software aims to mimic natural hearing, in which the brain is constantly scanning for meaningful sounds and screening out noise.\n Beyond the algorithm, the Syncro contains customized listening-assistance software for use when listening to such things as amplified voices or music or a movie. \n The only caveat is that these programs, which can be switched on by pressing a button on the hearing aid, seem to be oriented toward predictable auditory situations. Canceling out the noise of screaming children at the beach so you can process the calming sounds of the waves will still be an achievement of mind over matter.   Adam Baer\n","919":"To the Editor:\nRe \"Smart Machines, and Why We Fear Them\"(Op-Ed, March 21):\nI normally start my graduate class in artificial intelligence, or A.I., with a quote from a National Aeronautics and Space Administration report supporting manned space exploration: \"Man is the lowest cost, 150-pound, nonlinear, all-purpose computer system that can be mass produced by unskilled labor.\"\u00a0\n I use this to emphasize that we should study A.I. technology and its potential benefits without complicating it with philosophical issues (\"Can machines think?\"), a nonproductive exercise. Instead, we must remain alert to the consequences of our designs.\u00a0GORDON SILVERMANBronx, March 23, 1998\u00a0The writer is a professor of electrical engineering at Manhattan College.\n","920":"A robot walks into a bar and says, \"I'll have a screwdriver.\" A bad joke, indeed. But even less funny if the robot says \"Give me what's in your cash register.\"\nThe fictional theme of robots turning against humans is older than the word itself, which first appeared in the title of Karel Capek's 1920 play about artificial factory workers rising against their human overlords. Just 22 years later, Isaac Asimov invented the \"Three Laws of Robotics\" to serve as a hierarchical ethical code for the robots in his stories: first, never harm a human being through action or inaction; second, obey human orders; last, protect oneself. From the first story in which the laws appeared, Asimov explored their inherent contradictions. Great fiction, but unworkable theory.\nThe prospect of machines capable of following moral principles, let alone understanding them, seems as remote today as the word \"robot\" is old. Some technologists enthusiastically extrapolate from the observation that computing power doubles every 18 months to predict an imminent \"technological singularity\" in which a threshold for machines of superhuman intelligence will be suddenly surpassed. Many Singularitarians assume a lot, not the least of which is that intelligence is fundamentally a computational process. The techno-optimists among them also believe that such machines will be essentially friendly to human beings. I am skeptical about the Singularity, and even if \"artificial intelligence\" is not an oxymoron, \"friendly A.I.\" will require considerable scientific progress on a number of fronts.The neuro- and cognitive sciences are presently in a state of rapid development in which alternatives to the metaphor of mind as computer have gained ground. Dynamical systems theory, network science, statistical learning theory, developmental psychobiology and molecular neuroscience all challenge some foundational assumptions of A.I., and the last 50 years of cognitive science more generally. These new approaches analyze and exploit the complex causal structure of physically embodied and environmentally embedded systems, at every level, from molecular to social. They demonstrate the inadequacy of highly abstract algorithms operating on discrete symbols with fixed meanings to capture the adaptive flexibility of intelligent behavior. But despite undermining the idea that the mind is fundamentally a digital computer, these approaches have improved our ability to use computers for more and more robust simulations of intelligent agents - simulations that will increasingly control machines occupying our cognitive niche. If you don't believe me, ask Siri.\nThis is why, in my view, we need to think long and hard about machine morality. Many of my colleagues take the very idea of moral machines to be a kind of joke. Machines, they insist, do only what they are told to do. A bar-robbing robot would have to be instructed or constructed to do exactly that. On this view, morality is an issue only for creatures like us who can choose to do wrong. People are morally good only insofar as they must overcome the urge to do what is bad. We can be moral, they say, because we are free to choose our own paths.\nThere are big themes here: freedom of will, human spontaneity and creativity, and the role of reason in making good choices - not to mention the nature of morality itself. Fully human-level moral agency, and all the responsibilities that come with it, requires developments in artificial intelligence or artificial life that remain, for now, in the domain of science fiction. And yet...\nMachines are increasingly operating with minimal human oversight in the same physical spaces as we do. Entrepreneurs are actively developing robots for home care of the elderly. Robotic vacuum cleaners and lawn mowers are already mass market items. Self-driving cars are not far behind. Mercedes is equipping its 2013 model S-Class cars with a system that can drive autonomously through city traffic at speeds up to 25 m.p.h. Google's fleet of autonomous cars has logged about 200,000 miles without incident in California and Nevada, in conditions ranging from surface streets to freeways. By Google's estimate, the cars have required intervention by a human co-pilot only about once every 1,000 miles and the goal is to reduce this rate to once in 1,000,000 miles. How long until the next bank robber will have an autonomous getaway vehicle?\nThis is autonomy in the engineer's sense, not the philosopher's. The cars won't have a sense of free will, not even an illusory one. They may select their own routes through the city but, for the foreseeable future, they won't choose their own paths in the grand journey from dealership to junkyard. We don't want our cars leaving us to join the Peace Corps, nor will they any time soon. But as the layers of software pile up between us and our machines, they are becoming increasingly independent of our direct control. In military circles, the phrase \"man on the loop\" has come to replace \"man in the loop,\" indicating the diminishing role of human overseers in controlling drones and ground-based robots that operate hundreds or thousands of miles from base. These machines need to adjust to local conditions faster than can be signaled and processed by human tele-operators. And while no one is yet recommending that decisions to use lethal force should be handed over to software, the Department of Defense is sufficiently committed to the use of autonomous systems that it has sponsored engineers and philosophers to outline prospects (.pdf report, 108 pages) for ethical governance of battlefield machines.\nJoke or not, the topic of machine morality is here to stay. Even modest amounts of engineered autonomy make it necessary to outline some modest goals for the design of artificial moral agents. Modest because we are not talking about guidance systems for the Terminator or other technology that does not yet exist. Necessary, because as machines with limited autonomy operate more often than before in open environments, it becomes increasingly important to design a kind of functional morality that is sensitive to ethically relevant features of those situations. Modest, again, because this functional morality is not about self-reflective moral agency - what one might call \"full\" moral agency - but simply about trying to make autonomous agents better at adjusting their actions to human norms. This can be done with technology that is already available or can be anticipated within the next 5 to 10 years.\nThe project of designing artificial moral agents provokes a wide variety of negative reactions, including that it is preposterous, horrendous, or trivial. My co-author Wendell Wallach and I have been accused of being, in our book \"Moral Machines,\" unimaginatively human-centered in our views about morality, of being excessively optimistic about technological solutions, and of putting too much emphasis on engineering the machines themselves rather than looking at the whole context in which machines operate.\nIn response to the charge of preposterousness, I am willing to double down. Far from being an exercise in science fiction, serious engagement with the project of designing artificial moral agents has the potential to revolutionize moral philosophy in the same way that philosophers' engagement with science continuously revolutionizes human self-understanding. New insights can be gained from confronting the question of whether and how a control architecture for robots might utilize (or ignore) general principles recommended by major ethical theories. Perhaps ethical theory is to moral agents as physics is to outfielders - theoretical knowledge that isn't necessary to play a good game. Such theoretical knowledge may still be useful after the fact to analyze and adjust future performance.\nEven if success in building artificial moral agents will be hard to gauge, the effort may help to forestall inflexible, ethically-blind technologies from propagating. More concretely, if cars are smart enough to navigate through city traffic, they are certainly smart enough to detect how long they have been parked outside a bar (easily accessible through the marriage of G.P.S. and the Internet) and to ask you, the driver, to prove you're not drunk before starting the engine so you can get home. For the near term (say, 5 to 10 years), a responsible human will still be needed to supervise these \"intelligent\" cars, so you had better be sober. Does this really require artificial morality, when one could simply put a breathalyzer between key and ignition? Such a dumb, inflexible system would have a kind of operational morality in which the engineer has decided that no car should be started by person with a certain blood alcohol level. But it would be ethically blind - incapable, for instance, of recognizing the difference between, on the one hand, a driver who needs the car simply to get home and, on the other hand, a driver who had a couple of drinks with dinner but needs the car because a 4-year old requiring urgent medical attention is in the back seat.\nIt is within our current capacities to build machines that are able to determine, based on real-time information about current traffic conditions and access to actuarial tables, how likely it is that this situation might lead to an accident. Of course, this only defers the ethical question of how to weigh the potential for harm that either option presents, but a well-designed system of human-machine interaction could allow for a manual override to be temporarily logged in a \"black-box\" similar to those used on airplanes. In case of an accident this would provide evidence that the person had taken responsibility. Just as we can envisage machines with increasing degrees of autonomy from human oversight, we can envisage machines whose controls involve increasing degrees of sensitivity to things that matter ethically. Not perfect machines, to be sure, but better.\n~~~\nDoes this talk of artificial moral agents overreach, contributing to our own dehumanization, to the reduction of human autonomy, and to lowered barriers to warfare? If so, does it grease the slope to a horrendous, dystopian future? I am sensitive to the worries, but optimistic enough to think that this kind of techno-pessimism has, over the centuries, been oversold. Luddites have always come to seem quaint, except when they were dangerous. The challenge for philosophers and engineers alike is to figure out what should and can reasonably be done in the middle space that contains somewhat autonomous, partly ethically-sensitive machines. Some may think the exploration of this space is too dangerous to allow. Prohibitionists may succeed in some areas - robot arms control, anyone? - but they will not, I believe, be able to contain the spread of increasingly autonomous robots into homes, eldercare, and public spaces, not to mention the virtual spaces in which much software already operates without a human in the loop. We want machines that do chores and errands without our having to monitor them continuously. Retailers and banks depend on software controlling all manner of operations, from credit card purchases to inventory control, freeing humans to do other things that we don't yet know how to construct machines to do.\nWhere's the challenge, a software engineer might ask? Isn't ethical governance for machines just problem-solving within constraints? If there's fuzziness about the nature of those constraints, isn't that a philosophical problem, not an engineering one? Besides, why look to human ethics to provide a gold standard for machines? My response is that if engineers leave it to philosophers to come up with theories that they can implement, they will have a long wait, but if philosophers leave it to engineers to implement something workable they will likely be disappointed by the outcome. The challenge is to reconcile these two rather different ways of approaching the world, to yield better understanding of how interactions among people and contexts enable us, sometimes, to steer a reasonable course through the competing demands of our moral niche. The different kinds of rigor provided by philosophers and engineers are both needed to inform the construction of machines that, when embedded in well-designed systems of human-machine interaction, produce morally reasonable decisions even in situations where Asimov's laws would produce deadlock.\n\n\n\n","921":"Every new online search service must face the inevitable question: ''Is it better than Google?''\n  WolframAlpha, a powerful new service that can answer a broad range of queries, has become one of the most anticipated Web products of the year. But its creator, Stephen Wolfram, wants to make something clear: Despite the online chatter comparing it to Google, his service is not intended to dethrone the king of search engines.\n  ''I am not keen on the hype,'' said Mr. Wolfram, a well-known scientist and entrepreneur and the founder of Wolfram Research, a company in Champaign, Ill., that has been quietly developing WolframAlpha. \u00a0\n  Mr. Wolfram's service does not search through Web pages, and it will not help with movie times or camera shopping. Instead it computes the answers to queries using enormous collections of data the company has amassed. It can quickly spit out facts like the average body mass index of a 40-year-old male, whether the Eiffel Tower is taller than Seattle's Space Needle, and whether  it is high tide in Miami right now. \n  WolframAlpha, which is expected to be available to the public at wolframalpha.com in the next week, is not a finished product. It is an early working version of a project that has been years in the making and will continue to evolve over years, if not decades. As such, there is much it cannot answer now. \n  But even as he dismisses the Google comparisons, Mr. Wolfram, a former child prodigy who published his first research paper on particle physics at age 15 and is best known for creating the math-formula software Mathematica, is happy to add fuel to the simmering expectations surrounding his service.\n  ''I think WolframAlpha has the potential to be quite important,'' he said.\n  The goal of creating a computer system that can answer questions has been a tantalizing but elusive pursuit for many computer scientists for more than four decades. Some veterans of the field say Mr. Wolfram may have come as close as anyone yet. \n  ''In many ways, creating a system like this has been a holy grail of lots of folks for some time,'' said Nathan Myhrvold, a former chief technology officer of Microsoft and co-founder of Intellectual Ventures, an investment company that owns a portfolio of patents.\n  ''It has wound up being considered something that is virtually impossible,'' Mr. Myhrvold said. WolframAlpha has shown ''that it wasn't impossible but really difficult,'' he added. ''It involved applying lots of different tricks.''\n  Doug Lenat, an artificial intelligence expert whose company Cycorp has spent the last 15 years developing a system that brings human-like reasoning to some computer systems, said WolframAlpha can handle ''an astronomical number of questions,'' and could eventually turn into a favorite destination on the Web. \n  ''It may become a massive player alongside Google,'' Mr. Lenat said. \n  Traditional search engines like Google and Yahoo, by and large, excel at finding information that already exists online. If there are Web pages that include the words used in a query, the engines will find them and rank them in order of relevance. \n  WolframAlpha is different. For starters, it does not gather data from the Web. Instead, its ''knowledge base'' is made up of reams and reams of data -- ranging from the kinds of facts you would find in a World Almanac, to highly specialized data from physics and other sciences -- that some 100 employees at Wolfram Research have gathered, verified and organized over several years. \n  When a user types in a query, WolframAlpha tries to determine the relevant area of knowledge and  find the answers, often by performing calculations on its data. If you type ''LDL 120,'' it will return a graph showing the distribution of cholesterol levels among the United States population, and display the percentage of people above and below that figure. If you type ''LDL 120 male 33,'' it will adjust the results to focus on that gender and age group. \n  In response to ''how far is the Moon from Earth,'' WolframAlpha will calculate the exact distance based on an algorithm that computes the ever-changing distance between the two bodies. The engine that computes answers is largely built on Mathematica.\n  In its current state, there are many queries that WolframAlpha cannot answer, either because it does not understand the question or because it does not have the requisite data. For instance, it is stumped by queries like ''obesity rate,'' ''housing prices New York'' or ''unemployment San Francisco'' (but it will answer ''unemployment San Francisco County'').  \n  ''It is going to be very good in some areas and incomplete in others,'' said Nova Spivack, the chief executive of Radar Networks, which is using artificial intelligence and other techniques to help people find Web content that is interesting and relevant to them. \n  WolframAlpha does not actually try to work out the real meaning of a query, as some artificial intelligence systems do, so there are some questions it will never be able to answer.  But experts say its approach appears to be effective in many areas. \n  ''He's done a great job of marrying the acquisition of data with the mathematical algorithms,'' said David A. Ferrucci, an artificial intelligence researcher at I.B.M., who is leading a team developing a computer program that will compete with humans on ''Jeopardy.'' \n  If successful, WolframAlpha has the potential to become a large business opportunity. For now, Mr. Wolfram said he plans to offer advertising and other forms of sponsorship on the site, and perhaps offer premium versions of the service for researchers. And somewhat coyly, he said he has discussed potential partnerships with the ''obvious people,'' including search engine companies. \n  ''We are actively pursuing interesting relationships,'' he said. \n  Representatives for Google and Yahoo declined to discuss WolframAlpha.  \n  Mr. Spivack and others said WolframAlpha may become a complement to traditional search engines, which themselves have begun to offer simple versions of the kinds of calculations and data manipulation at which WolframAlpha excels. \n  ''There is a huge space of possible questions that Google doesn't answer,'' Mr. Spivack said. ''I think WolframAlpha will go well beyond the academic world to cover business and industry, economics, health.'' \n","922":"ABSTRACT\nMichael Benson Review section article discusses movie 2001: A Space Odyssey and its forward-looking technology, including tablet computers and artificial intelligence; photos (M)\n","923":"ABSTRACT\nEditorial notes rising budget and reorganization as Chinese Pres Xi Jinping puts loyalists in place and builds effective military fighting force abetted by artificial intelligence;\u00a0 predicts his more assertive stance could lead to missteps\n","924":"ABSTRACT\nNeil Parmar article in Journal Report: Innovations in Health Care on rise in number of hospitals, universities and tech companies that are training and testing artificial intelligence deep learning and other systems to help improve medical diagnosis process; photo; graph (M)\n","925":"ABSTRACT\nDrones, shot-taking apps and artificial intelligence are accelerating what has long been clunky, time-consuming process for automobile or home insurance claims;\u00a0 four in ten car insurers no longer using employees to physically inspect damages in some cases; graph; photo (M)\n","926":"ABSTRACT\nSinovation Ventures raises 4.5 billion yuan ($674 million) from investors including Foxconn Technology Group and plans to invest in startups involved in artificial intelligence, building enterprise software and creating entertainment comment (S)\n","927":"ABSTRACT\nDan Gallagher Heard on the Street column contends chip maker Nvidia's efforts to develop new chips for artificial intelligence, cloud computing and cars are paying off and that its future justifies its pricey shares (M)\n","928":" ABSTRACT:Development of 64-bit computer chips is expected to make computer visualization, artificial intelligence and other leading-edge applications requiring huge amounts of memory easier to develop and run (S)\n","929":"Over the next five years, Ford will\u00a0pour $1 billion into an artificial-intelligence company tasked with developing the technology that one day will drive its autonomous vehicles.\nThe technology also could be licensed to other automakers, executives said.\u00a0\nPittsburgh-based Argo AI was founded late last year by Bryan Salesky and Peter Rander, who previously worked on self-driving-car initiatives at\u00a0Google and Uber, respectively. The company will include staff members at Ford who\u00a0have been developing its virtual driver system for the past several years.\nIn a phone call Friday, chief executive\u00a0Mark Fields said the investment will help Ford bring its self-driving cars to market by the company's\u00a0previously stated goal of 2021. It also will open a new revenue stream if Ford licenses the technology to carmakers who have not developed\u00a0their own autonomous driving systems.\nThat licensing model will put Ford in direct competition with Waymo, Google's self-driving-car company, which announced plans this year to develop both hardware and software for self-driving cars. Previously, Waymo\u00a0focused solely on software, but executives decided that it was necessary also to build the sensors and cameras on the vehicle if its system was to be sophisticated enough to handle fully autonomous driving.\nDuring an interview at the North American International Auto Show in Detroit\u00a0in January, Ford Chief Technical Officer Raj Nair said the company's\u00a0experience making vehicles will give it an advantage over Waymo and other competitors that do not have the same pedigree.\n\"We'll see where they go with the autonomous vehicle,\" Nair said in January. \"The comment on doing both the hardware and the software is correct, but I think it's pretty limiting if you don't include the vehicle as hardware.\n\"The ability to integrate into the vehicle and be able to do the vehicle engineering is just as key,\" Nair added. \"It's not going to be any good if the software program doesn't know how to talk to the vehicle.\"\nArgo AI\u00a0will have\u00a0200 employees by the end of this year, executives said. It also will have a headquarters in Pittsburgh, with additional offices in southeastern Michigan and the San Francisco Bay area.\n              Read more from The Washington Post's Innovations section.\u00a0           \n","930":"You'd think some of the world's greatest minds would be more into artificial intelligence. But not, at least, when it comes to arming it.\nAn open letter signed by Elon Musk, Stephen Hawking and Steve Wozniak, among others, is making the case (again) that weaponized robots could lead to \"a global AI arms race\" that turns self-directed drones into \"the Kalashnikovs of tomorrow.\"\n\"We believe that AI has great potential to benefit humanity in many ways, and that the goal of the field should be to do so,\" the open letter reads. \"Starting a military AI arms race is a bad idea, and should be prevented by a ban on offensive autonomous weapons beyond meaningful human control.\"\u00a0\nThis isn't the first time these technologists have warned of the dangers of artificial intelligence. Musk has warned before that there \"needs to be a lot more work on AI safety,\" and a previous open letter from Musk, Hawking, Wozniak and others spoke of the \"pitfalls\" that lay in wait if the research wasn't done carefully.\n              [Elon Musk, gaming supernerd]           \nThe prospect of weaponized autonomous drones is no doubt a tempting one for some militaries: They would be able to compensate for lack of manpower, surprise their enemies and turn war into a virtually bloodless (and therefore relatively cheap) affair. And it would be no surprise if, upon seeing their rivals get hold of the technology, for other countries to want killer robots, too.\nThe solution, according to Musk and others, is a ban on autonomous weapons, similar to the kind that governs chemical weapons.\nHistory suggests that such a ban could be hard to approve, let alone enforce: Despite many major powers signing the 1925 Geneva Protocol banning the use of chemical and biological weapons, other countries such as Japan and the United States did not become signatories until as late as the 1970s, according to the Arms Control Association. And even then, claims were still made about the use of such weapons in violation of the ban.\nP.W. Singer is the author of \"Wired for War: The Robotics Revolution and Conflict in the 21st Century\" and a researcher at the New America Foundation who studies the future of warfare. When I asked him last month about the chances of the ban on weaponizing outer space surviving through the next few decades, he had this to say:\nWould a treaty hold? I hope it would, because space is the one domain we've not fought in. Yet. History shows it's likely going to be more like the various treaties of the 1920s and 1930s that everyone signed up to. One, they really didn't respect them during the period, like we're seeing with these space weapons tests. But also when push came to shove in an actual war, they junked them.\nSinger added that even though many countries have agreed not to militarize outer space, they still maintain programs \"designed to fight in space and deny it to the other side [and] in the last year have ramped up those programs.\"\nBans on specific weapons or types of weapons can be extremely complicated. A somewhat less formal -\u00a0though no less effective -\u00a0approach that could emerge is simply the general unspoken agreement that using killer drones could be lethal to society.\nThis would resemble much more the norm against using nuclear weapons in anger. While there are treaties to prevent the spread of nuclear weapons, there isn't such a formal document governing their use. Today we mostly rely on the fear of mutual destruction and voluntary commitments by countries such as China and India on a \"no-first-use\" policy that only permit the firing of nuclear weapons in response to a nuclear attack.\n","931":"BEIJING -- Po, the wisdom-seeking hero of the ''Kung Fu Panda'' films, might recognize this temple in China where the world's first robot monk dwells. For Po's Jade Palace, there is Longquan (Dragon Spring) Temple, a place of Buddhist worship in the mountains northwest of Beijing, where gnarled gingko and cypress trees tower over red-walled buildings underneath rocky Phoenix Ridge.\nFor his Hall of Warriors, there is the Comic Center deep inside the temple, at the end of winding stone paths and steps, past a flower-shaped audio device that crackles sutras. \u00a0\n  As for Po himself, there is Xian'er, the two-foot-tall, advice-dispensing robot whose full title is Worthy Stupid Robot Monk. (In the Beijing dialect, ''er,'' or ''stupid,'' is a term of affection.)\n  Not so much ''Kung Fu Panda 3,'' perhaps, as ''Robot Monk 1.''\n  A childlike creature in an orange Buddhist robe, Xian'er is an object of fascination in China amid an increasingly urgent pursuit of spirituality and, more recently, artificial intelligence. But Xian Fan, the head of the Comic Center, told National Business Daily that the temple did not plan to commercialize the robot and that its development was for ''the public welfare.''\n  And the monks do not seem to be planning a franchise. There is only one robot monk for now, Xian Fan told Beijing News, adding, ''We're not doing this for commerce, but just because we want to use more modern ways to spread Buddhist teachings.''\n  The robot was created last year by the temple in collaboration with about a dozen Chinese technology, culture and investment companies, according to Chinese news reports. But the character of Xian'er was first designed by artists at the Comic Center for a 2014 Buddhist comic book series, whose first title was ''Troubles Are Self-Made.'' The series has sold well in Chinese bookstores, and an English-language edition is due out soon.\n  As a robot, Xian'er has expanded his repertoire. He has a touch pad on his chest that allows him to respond to supplicants' questions and statements, such as: ''Who are your parents?'' ''I'm not happy'' and ''What is the meaning of life?''\n  On Wednesday morning, employees at the Comic Center declined to let visitors communicate directly with Xian'er, saying he was -- literally -- recharging in order to meet a delegation of government officials from Beijing later in the day. They would not say who the officials were or give their own names, saying they were not authorized to speak to the news media.\n  No matter. There was always Weixin, or WeChat, where Xian'er maintains an account. (Readers can find him under \u00e8\u00b4\u00a4\u00e4\u00baOE\u00e6oe\u00ba\u00e5[TM]\u00a8\u00e5\u0192\u00a7.)\n  Clearly no sentimentalist, his answers to the above queries were:\n  \u00c2\u00b7 ''That's ridiculous, how can robots have parents?''\n  \u00c2\u00b7 ''If you're not happy, what can anyone else do about it?''\n  \u00c2\u00b7 ''My master says the meaning of life is to help more people finally leave behind bitterness and gain happiness.''\n  Po might have approved.\n  Many visitors to Longquan Temple on Wednesday, however, expressed doubt as to whether Xian'er could really help people with their problems, though some said they expected technology to advance so quickly that future generations of robot monks might. A second, smarter Xian'er is already in development, a center employee said.\n  ''I haven't seen it, but I've heard of it,'' Zhang Ping, a middle-aged woman who was visiting the temple, said of the robot monk.\n  ''It relies on permutations and combinations of words to solve problems, but whether it can really deal with deep personal issues, I'm not sure,'' Ms. Zhang said. ''Everyone is different. For some, those may be about family, for others, about work. But look at how fast artificial intelligence is progressing. In 20 years, I think it's completely possible it will be able to.''\n  Another templegoer disagreed. ''I don't think it's possible for the intelligence of a robot to be advanced enough to understand human feelings,'' said Wang Yiren, 23, a Buddhist and volunteer at the temple. ''The robot doesn't really live up to my expectations. But I've seen robots in other parts of the world that can communicate with human beings, and that's pretty amazing.''\n  He Haikun, 33, a woman who was visiting the temple with a friend, said: ''If you have no one to talk to, maybe it would help to talk to a robot.''\n  Here are some other answers provided by Xian'er, via WeChat:\n  Q: ''What is love?''\n  A: ''Love is your own obsessions not being satisfied, the clashing of other people's troubles with yours.''\n  Q: ''I don't want to work.''\n  A: ''If you don't want to starve and have an O.K. job, work hard.''\n  Q: ''I want to die.''\n  A: ''Don't assume you're the most pathetic person in the world.''\n  Q: ''Who is Xi Jinping [the president of China]?''\n  A: ''A robot doesn't know any people.''\n  And finally:\n  Q: ''Could there be another Cultural Revolution?''\n  A: ''Wait, I will ask my master.''\n  Po would have been envious of Xian'er for being able to pass a difficult question on to his master, having been required in ''Kung Fu Panda 3'' to solve his community's problems himself.\n  The robotics companies Canbot and Turing Robotic Industries, among the companies cited in Chinese news reports as working with the temple to produce Xian'er, agreed to answer questions about the project but had not responded as of Wednesday evening. At Interjoy, another company listed as one of the designers, and others, the phone rang unanswered.\n\n\n\n","932":"ABSTRACT\nJason Anders article in Journal Report: WSJ D.Live offers interview with Intel CEO Brian Krzanich, who notes that companies that don't invest in artificial intelligence are going to be outpaced by another \u00a0company that is, in not too distant future; photo; chart (M)\n","933":" ABSTRACT:Hitachi Ltd and Hewlett-Packard Co say they will jointly develop artificial-intelligence software product (S)\n","934":"To the Editor:\nAstro Teller (Op-Ed, March 21) would like to pin the Luddite label on those of us who are concerned about the effect of artificial intelligence on jobs. He claims that \"when some jobs disappear, others are created.\"\nYes, farmers displaced by the Industrial Revolution found work in factories, and more recently assembly-line workers have migrated to the service sector. But what happens when intelligent agents sift through vast data banks to do most of our knowledge work? What type of work comes after the knowledge sector?\u00a0\n We can only \"profit as a species\" when our economic and social institutions adapt to these new technologies. Mr. Teller fails to appreciate the institutional change that must occur if these new technologies are to benefit more than just the favored few.\u00a0KIT SIMS TAYLORBellevue, Wash., March 23, 1998\u00a0The writer is an economics instructor at Bellevue Community College.\n","935":"          Announcement from National Economy and Business Editor David Cho and Deputy Business Editor Zachary Goldfarb:        \nWe are excited to announce that Drew Harwell will cover the way artificial intelligence and big data are transforming American business and society.\u00a0\nDrew came to The Post from the Tampa Bay Times in 2014 as a general-assignment reporter on the Financial desk, where he broke news about sexual harassment and Hollywood discrimination and wrote memorable stories about the cultural touchstones of big business, from the fake engine roars used to sell more trucks to the economic inequality behind the magic of Disney World.\nSince early 2016, he has helped cover the Trump business, family and presidency, contributing chapters to \"Trump Revealed\" and joining a team dedicated to exploring potential conflicts of interest. He was instrumental in scoopy coverage on Mar-a-Lago and the Secret Service and wrote detailed stories on Donald's business failures, Donald Jr.'s Twitter defiance, Eric's jet-setting to Uruguay and Ivanka's factories around the world.\nIn his new beat, Drew will explore how the most advanced technologies are changing the way we live, from how companies' race for our faces, voices and body sizes will reshape modern privacy to how robots are learning to read. His accountability work will delve into the hidden biases of the powerful algorithms defining the winners and losers of the new economy. He also will explain the surprising path AI is taking as it changes fashion, food, entertainment, childhood - and what it means to be alive.\n","936":"Wall Street Journal media reporter Lukas Alpert examined the latest iteration of The Post's artificial intelligence technology, Heliograf, which will be used to cover nearly 500 races nationwide on Election Day. He writes:\nIn recent years, publishers have moved increasingly toward using computers and data analytics to supplement and inform reporting... The Post-taking inspiration from its owner Amazon.com Inc. Chief Executive Jeff Bezos -has been a big proponent of such efforts.\u00a0\n","937":"Would a machine with AI have the same drive to reproduce as biological organisms do? Can - and should - empathy be introduced? If there are questions swimming around in your brain about the future of Artificial Intelligence, you can now send them to Stephen Hawking himself for an answer. Hawking has signed up for his first-ever AMA (Ask me Anything) on Reddit, and will be answering questions there for the next few weeks.\u00a0\n[Stephen Hawking announces $100 million hunt for alien life]\nYes, you read that correctly\u00a0--\u00a0weeks. For those familiar with the normal AMA format, be aware that Hawking's will work slightly differently. \"Due to the fact that I will be answering questions at my own pace, working with the moderators of \/r\/Science we are opening this thread up in advance to gather your questions,\" Hawking writes in his introduction.\nInstead of opening up a forum for a few hours and answering questions in real time, as most participants do, this AMA will work in two stages. First, Hawking and Reddit will collect questions for a number of days. The moderators have asked users to vote for the best questions, thus bumping them up higher on the thread.\nAfter a certain amount of time - apparently, at least a week - Hawking will consider which questions to answer, and begin replying. \u00a0His answers will be posted in the proper places on the thread by the Science subreddit's moderators, they explain. \"The date for this is undecided, as it depends on several factors,\" the moderators write.\nThis being an AMA, redditors can, and probably will, ask Hawking about more than just the topic at hand. But Hawking has made it clear that he's participating in the popular Q and A forum as part of his recent push to introduce safety measures into the burgeoning field of AI research. Earlier this year, he signed an open letter with Elon Musk and many others asking for a balance in pursuing AI's benefits and understanding its risks.\n[Stephen Hawking just got an artificial intelligence upgrade, but still thinks AI could bring an end to mankind]\n\"The potential benefits are huge, since everything that civilization has to offer is a product of human intelligence,\" the letter reads, \"we cannot predict what we might achieve when this intelligence is magnified by the tools AI may provide, but the eradication of disease and poverty are not unfathomable.\"\n\"Because of the great potential of AI, it is important to research how to reap its benefits while avoiding potential pitfalls.\"\nWithin an hour of the AMA thread going live, redditors posted more than 800 comments. Here are some of the early questions:\n","938":"The gig: Chris Nicholson, 42, is chief executive of Skymind, an artificial intelligence company in San Francisco that's vying with dozens of other start-ups to emerge as a major player in the nascent AI economy. Google, Amazon, Apple, Facebook and other tech giants now dominate \"deep learning\" AI, powering such things as voice-activated personal assistants, image recognition and driverless cars.\nSkymind has built open-source programs and assembled a team of experts to help organizations smaller than Google or Apple build their own deep-learning programs. Thousands of start-up wannabees would love to have Skymind's funding -- $6.3 million from venture capitalist hotshots such as Ray Lane's GreatPoint Ventures and China's Tencent Holdings Ltd.\u00a0\nEmbrace new experiences: \"Montana is a beautiful place, with a lot of wonderful people,\" the Montana native said. \"But if there's one adjective you'd use to describe it, it's remote. For anybody born curious in Montana, the first task is 'how to expose myself to the world.' \"\nAt 17, he was selected for the Rotary Youth Exchange program and traveled to the German state of Bavaria, where he developed a love for Europe. He attended Deep Springs College near Bishop in the California desert, with a student body of 26, \"a place of intense intellectual ferment, where you also learned how to do things with your hands.\"\nHe learned meditation living at the San Francisco Zen Center, where he also volunteered at its AIDS hospice. A \"wild dude\" he met there persuaded him to relocate to Guatemala, where he helped addicted street kids get off glue.\nTo explore the income gap between rich and poor countries, he studied economics at American University of Paris. \"It turns out most of economics doesn't really care about that,\" he said.\nTake the best of what's at hand: Nicholson fell in love with an Argentine dancer at the Moulin Rouge. An Argentine reporter lured him into journalism. He pestered the International Herald Tribune for 10 months before the newspaper finally hired him.\nHis next post was with the New York Times' DealBook section. He worked out of an apartment at the top of the small tower that cinema buffs will remember from the apartment building in \"Last Tango in Paris.\" After two high-stress years, Bloomberg hired him on the mergers and acquisition news team.\nAt each job, he picked up different skills. \"I learned basics of reporting at the Herald Tribune. At DealBook, I learned to write stories. At Bloomberg, I learned how to get scoops.\"\nKnow when to quit:After the Great Recession, journalism \"didn't feel very healthy,\" Nicholson said. \"I saw the newspaper business suffering so much,\" he said. \"I thought, tech did this to journalism, and I don't want to be on the receiving end of this. I want to be on the other side of the code.\"\nHe learned Javascript and Python programming. That didn't make him a coder but did lead to a public relations job in San Francisco at FutureAdvisor, a robo-investing company acquired by BlackRock in 2015 for $200 million.\nHe lived in a hacker house as \"one snoring guy in a room with five other snoring guys.\" There he met Adam Gibson, in his early 20s, a master hacker, Nicholson said. Together they started Skymind in 2014.\nAlways keep learning: Gibson handled the technology. Nicholson ran everything else: recruiting, team building, fundraising, documentation, incorporation documents, contracts.\n\"I'm not saying I did any of it great. But I had to do it well enough to keep the whole ship afloat,\" Nicholson said. \"Good enough is the key word. At least one co-founder needs to be at least good enough at about a dozen different things.\" That frees up the programmers to work toward technical perfection.\nFailures are rehearsals: \"When you're creating and starting a start-up, you're making lots of mistakes,\" he said. \"You get the opportunity to make mistakes and learn from them. When you found a start-up, if your job description doesn't change every three months or so, you're doing something wrong.\"\nThe first year of pitching for funds didn't go too well. But \"all those failed pitches to investors were in fact rehearsals that prepared me for the successful pitches I would eventually make.\"\nA start-up is a community: Now that Skymind has grown to 35 employees, spread across the globe, Nicholson is focusing on team building. \"Building a company means talking to people a lot. It's an extremely social occupation. I might prefer to read some crazy technical document, but that's not what I need to be doing.\"\nHe draws on lessons in building consensus in structured environments that go way back to his experiences at Deep Springs College and the San Francisco Zen Center. \"Direct democracy doesn't seem to be working anymore, but it does still work at a small level,\" he said.\nTake chances: Nicholson said he encourages Skymind employees to take chances in matters small and large.\n\"Every single time you jump off the cliff, you land someplace unexpected,\" he said. \"It never turns out like the dream you had, but you needed the dream to get there.\"\n--\nruss.mitchell@latimes.com\n","939":"ABSTRACT\nIntel says during WSJ D.Live conference that is working with Facebook and other firms on new chip called Nervana Neural Network Processor specially designed for artificial intelligence (S)\n","940":"ABSTRACT\nSoftBank Group and Saudi Arabia's sovereign-wealth fund launch $93 billion Softbank Vision Fund that will steer capital to deep learning, robotics, artificial intelligence and other cutting-edge technologies in US startups and other global firms (M)\n","941":"ABSTRACT\nScott Austin article in Journal Report: CIO Network offers interview with Andrew Ng, chief scientist at Chinese internet giant Baidu, and Neil Jacobstein, chair of artificial intelligence and robotics at Silicon Valley think-tank Singularity University, who discuss AI's opportunities and challenges in today's workplace; photos; graph (M)\n","942":" ABSTRACT:Xerox Corp's Xerox Artificial Intelligence Systems unit introduces two computer workstations that it says could help expand commercial uses for artificial intelligence (S)\n","943":"WAYNESBORO, Ga. - In the two months since Richard Watson strapped 200 remote-control-sized transmitters around his cows' necks, an artificial-intelligence system named Ida has pinged his phone with helpful alerts: when his cows are chewing the cud, when they're feeling sick, when they're ready for insemination. \n\"There may be 10 animals out there that have a real problem, but could you pick them?\" he said one morning, standing among a grazing herd of dairy cattle wearing what he calls \"cow Fitbits.\"\nBut on the neighboring pastures here in rural Georgia, other farmers say they aren't that impressed. When a cow's in heat, they know she'll start getting mounted by her bovine sisters, so they smear paint on the cows' backsides and then just look for the incriminating smudge. No fancy AI required. \u00a0\n\"I can spot a cow across a room that don't feel great just by looking in her eyes,\" said Mark Rodgers, a fourth-generation dairy farmer in Dearing, Ga., whose dad still drives a tractor at 82. \"The good lord said, 'This is what you can do.' I can't draw, paint or anything else, but I can watch cows.\"\nSophisticated AI technologies are helping reinvent how Americans work, offering powerful software that can read and react to mountains of data and save them time and stress along the way.\nBut its rollout is also sparking tensions in workplaces as humble and old-fashioned as the dairy farm. That down-home resistance raises a question farmers might be tackling before much of the rest of the workforce: Can new technology ever beat old intuition - even when it comes to a bunch of cows?\nThe AI that Watson's farm uses - called Ida, for \"The Intelligent Dairy Farmer's Assistant\" - tracks his cows' tiniest movements through their collars and then graphs and dissects them en masse. Those \"real-time cattle analytics\" are then used by the AI to assess diet and movement and predict concerning health issues, like lameness or udder infections.\nAs silly as this intricate level of maximum optimization sounds, particularly for a herd of cows that spend much of the day staring blankly or relieving themselves, Watson said it could mean the difference between a cow's healthy milking or premature death - and the difference between making or losing hundreds of thousands of dollars every year. \nThe Ida AI has sparked some early interest among farmers eager to compete in an industry where low milk prices and farm layoffs have everyone on edge. And while truck drivers and cashiers see AI as a job-killing omen, the farmers say they're in a labor crunch from years of too few young people getting into farming and need all the help they can get.\nConnecterra, a development team based in the Netherlands, built Ida with help from TensorFlow, the giant AI toolbox that Google created for its own apps and opened to the public in 2015. That release sparked a major wave of AI development, giving startups a shortcut to calculating advanced mathematics and creating learning machines.\nGoogle has joined other tech giants in pushing forcefully into AI, with chief executive Sundar Pichai telling a town-hall crowd in January that AI \"is one of the most important things humanity is working on\" and \"more profound than electricity or fire.\" (He did not explicitly mention cows.)\nStanding one March morning among his cows at Seven Oaks Dairy, one of three farms he runs as part of his Hart Agriculture brand, Watson pulls out his iPhone to show off his Ida app. The AI says he has three \"potential health problems to be checked\" among his herd: cow #14433 is eating less, while cows #10172 and #3522 are \"ruminating\" or chewing less, a sign they might feel ill. His herd's \"to be inseminated\" count is at 0, as signified by a reassuring green checkmark. \nAt 6'4\", with combine-wide shoulders and a Kiwi accent, the New Zealand-born Watson, 46, looks like a rugby player - which he was, playing a linebacker-like position in the late '90s for a semi-professional team called the Hurricanes. Shortly afterward, he moved to lead a cattle-grazing research program at the University of Georgia, where he taught and advocated for the increasingly rare craft of letting cows amble about aimlessly on a pasture, eating as they go. \nHis farm's cattle - cross-breeds of America's classic black-and-white moo cow, the Holstein, and New Zealand's relatively slimmer brown Jersey bulls - spend almost all day grazing on the thousands of acres of ryegrass and bermudagrass that surround his farms. That makes tracking their free-range eating and movement harder than at the average American \"confinement\" dairy, where cows are kept in stalls and fattened on corn and grains. \nSpotting problems the old way required closely watching the herd day and night, \"unless it's really obvious - you know, she's walking or limping or there are buzzards flying overheads,\" Watson said. \"Buzzards aren't a particularly good health program.\"\nThe cows' orange transmitters beam data over the hills of Watson's pastures to a set of antennas near the milking parlor. A \"base station\" computer then gulps up and processes all that sensor data, doing much of the AI work locally so as to avoid the problem of spotty rural Internet. The sensors pay the price for much of this data exchange, Connecterra's co-founder Yasir Khokhar Khokhar said: \"You don't want to know what cows do with them.\"\nThe Ida AI was first trained to comprehend cow behavior via thousands of hours of video and sensor inputs, as well as simpler approaches, including Khokhar mimicking bovine techniques with a sensor in his pocket. (\"I was the first cow,\" he said.) Every day brings more cow data and farmer feedback that help the AI learn and improve. The AI, Khokhar estimates, has processed about \"600 cow years of data,\" and is gaining about eight years of new cow data every day. \nThe AI now logs seven distinct cow behaviors: Walking, standing, laying, eating, chewing, drinking and idle. Other behaviors are on the way, Khokhar said, though he could not disclose them, calling them part of the \"secret sauce.\"\nDairy farmers have used sensors for years. But Ida's developers say its AI can do things old programs can't, by learning from the cow-behavior patterns that can pinpoint injuries, predict the onset of certain diseases and \"predict peak ovulation time with over 90 percent accuracy.\" The AI can also track how changes to their bedding, feed and environment can affect, for instance, how much milk they're making or how much they're laying around. \nKhokhar, who said he had the idea while living on a Dutch dairy farm, launched his startup in late 2016 and now counts a few thousand cow \"subscriptions\" across farms in seven countries, including the U.S., Spain and Pakistan. The company covers all the equipment and service work and sells monthly subscriptions. Farmers' prices start at about $3 a month per cow, plus a $79.99-per-cow startup fee, and Watson estimates he has invested about $17,000 on the system so far. \nAgriculture has long been one of Big Tech's juiciest target industries. Revamping the way farmers feed the planet, in the face of existential crises like food shortages and climate change, would be audacious, revolutionary - and highly profitable. Startups and farmers are now using camera-equipped robots to pick apples and sort cucumbers; running driverless tractors to harvest grain; and flying scanner drones to spot poachers and survey livestock. \nBeyond the Ida collars, other tech start-ups make cow pedometers, robot milkers, tail sensors and electric-shock collars that can stop or shift a herd. \nBut even some farmers who have invested heavily in new technology balk at the idea of paying for more. Everett Williams, the 64-year-old head of the WDairy farm near Madison, Ga., said his farm has all kinds of sensors that print out who-knows-how-many reports on matters such as cow activity and if hogs have gotten into his pens. They give him less data than the Ida AI would, he says, but he feels like he doesn't have the space in his brain for another data stream. \"You can only handle so many text alerts,\" he said.\nSystems like Connecterra are also enduring early criticism beyond the farm. Because the AI can help detect early disorders and walking disabilities, conservationists have criticized the systems as encouraging the breeding of a cow super-race by speeding underperformers to the slaughterhouse. \nRodgers, who runs his \"daddy-daughter\" family dairy farm in Dearing, Ga., said he's no luddite when it comes to farm technology. His \"super-system\" features cow-tracking transponders and, soon, a DeLaval VMS, which milks cows with lasers and robot arms and is advertised as the \"ultimate automatic milking machine.\"\nHis system, unlike Ida, doesn't track cud-chewing or use AI to tell him which cows to watch or what to do. But he's OK with that. That's the way things have always been done here, and he hopes they'll be that way for a long time. \n\"There's no substitute for watching your animals. It's an art and a science, and I hope my daughter and nephew get better at it than I am,\" he said. The cattle, he added, don't care much about evolving with the times. \"You cannot bore a cow to death.\"\ndrew.harwell@washpost.com\n","947":"In the original film \"Westworld\" (1973), written and directed by an up-and-coming novelist named Michael Crichton, the Delos corporation operates a kind of Disney World for depraved adults, a series of amusement parks where they can interact with uncannily lifelike robots in various environments. The parks include Medievalworld and Romanworld, but the bulk of the movie's action takes place in Westworld, where visitors are invited to shoot at android attractions like the Gunslinger (Yul Brenner) without fear of retaliation. All that changes when a technical glitch spreads through the parks like a virus, and suddenly the hosts are attacking the guests, not the other way around. \u00a0\nOf the many differences between Crichton's \"Westworld\" and the HBO version, which started its second season last Sunday, the most telling is the hands. For all their technical brilliance, the engineers in Crichton's film could never get the hands right: If visitors needed to tell who is and isn't a robot, they could look at the conspicuous silicon rings around the joints and know they weren't about to shoot (or otherwise violate) a human being. That little detail might spoil the fantasy of a truly authentic Old West experience, but it's also a clear line of demarcation between human and machine. The Gunslinger is plainly a robot, and when it goes haywire, there's nothing morally wrong about killing it. \nThe hosts in HBO's \"Westworld\" are not only seamless humanoids, but from the start, they've been more complex and sympathetic than their flesh-and-blood counterparts, who are consumed by pettiness, egotism, cruelty, greed and the other vices that come with being human. They also represent a profound shift in how the culture is coming to terms with artificial intelligence - a change that's reflected, too, in Steven Spielberg's recent \"Ready Player One,\" which grafts an entirely habitable virtual world that co-exists seamlessly with the real one. \nThere are more connections between the two works, which both draw heavily from the screen science fiction of the past but update it for a radically changed world. Spielberg directed \"Jurassic Park\" from Crichton's 1990 novel, and we can see the original 1973 \"Westworld\" as a proto-\"Jurassic Park\" for Crichton, an early run-through of the chaos theory that would upend an amusement park built around dangerous scientific advancement and corporate hubris. We could fear the idea of malfunctioning machines while safely categorizing them as \"the other,\" like some malevolent appliance that could be decommissioned and consigned to the scrap heap.\nBut the new \"Westworld\" is airing at a time when we're not only more comfortable with our machines and devices, but have integrated them so thoroughly into our everyday lives that they're more like extensions of us than mere utilities. For the humans - both on screen and off - the fear in \"Westworld\" isn't a computer glitch, but the worry that technology will evolve past us, resulting in synthetic creations that are stronger, smarter, more durable and perhaps worthier of occupying space on Earth. In this future, it's people who are the crude, malfunctioning machines, destined for the scrap heap. Earth 2.0 doesn't need us anymore. \nLess than a month ago, the screen adaptation of Ernest Cline's novel \"Ready Player One\" also reflected on the past and present, and how our relationship with machines has changed. By 2045, when the film opens, humans have rendered the planet virtually uninhabitable, a dystopia in which American cities are endless slums and the have-nots, like its hero, Wade Watts, are relegated to rickety trailer-park towers. Their only escape is a virtual world called the OASIS, a pristine and endlessly malleable environment where they can reinvent themselves and enjoy the freedoms that they're denied when the VR goggles are off. The OASIS and HBO's Westworld are both modeled after \"sandbox\" video games, such as \"Grand Theft Auto,\" which offer specific challenges but allow for open exploration, too. \nMuch like \"Westworld,\" \"Ready Player One\" looks back on an era in science fiction when artificial intelligence was an out-of-control \"other,\" rather than a threat that extended from us. The mastermind behind the OASIS, an self-effacing super-nerd named James Halliday, has designed it to reflect his obsession with '80s culture, including popular science-fiction like \"Tron\" and \"WarGames.\" The machines in \"Tron\" and \"WarGames\" are capable of learning and evolving: In the former, the diabolical Master Control Program brags about running things \"900 to 1,200 times better than any human.\" In the latter, a military supercomputer has to be talked down from starting World War III. They're not threatening to create new, superior worlds, like the OASIS or a Westworld run by the hosts. They're threatening to obliterate the ones we have. \nThe new season of \"Westworld\" opens with the androids still in revolt against their human oppressors, but should they prevail, what will they do with their freedom? Perhaps they'll enslave or destroy, just like the faceless CPUs in \"Tron\" and \"WarGames,\" or continue on a path of bloody revenge, like the Gunslinger in Crichton's version. But it's more likely that they'll have to think about the future, too, and what the world will look like once they lay claim to it. In this Western environment, they're the new pioneers, running roughshod over the natives and seizing territory for themselves. It's now their responsibility to figure out where their newfound agency takes them, and to live with the terrible mistakes they'll surely make along the way. \nIn 2018, artificial intelligence is too integrated into everyday life for us to keep it at arm's length - in conversations we have with Alexa or Siri, in the automation of industry, or in the hidden algorithms on Google ads and social media that monitor and accommodate our behavior. We still worry about AI rebellion, but mainly over how it contributes to our own obsolescence.\nThe hosts in \"Westworld\" might be a more evolved species than we are, capable of creating idealized environments that reflect Wade's description of the OASIS, \"a place where the limits of reality are your own imagination.\" In the future realms of \"Westworld\" and \"Ready Player One,\" it's humans who are reduced to ghosts in the machine, infecting these newly independent artificial beings with all their flaws and moral lapses. Their glitches are our glitches, too. \nstyle@washpost.com\n","948":"I THINK, ultimately, Steven Spielberg's \"A.I. -- Artificial Intelligence\" is one dead Pinocchio robot. But it's alive and beeping before it keels over, mid-movie.  \n Which makes the reviewing equivalent of an autopsy very compelling. \n Let's roll back the sheet, shall we? \u00a0\n There's a lot of history behind this movie, some of which you've probably heard. The whole thing began more than 25 years ago with Stanley Kubrick reading Brian Aldiss's haunting short story \"Supertoys Last All Summer Long,\" then falling in love with the idea of a movie about artificial intelligence. \n Kubrick being Kubrick, the movie was never made. The director, who died in 1999, told Spielberg about the idea in the 1980s, then involved him in the project. Spielberg took the baton willingly.  \n The result is not too surprising: a movie that's determined to honor the late filmmaker's vision but which is, after all, directed by Steven Spielberg. \"A.I.\" starts with the chilly precision of a Kubrick movie (the better half, actually, which also follows the Aldiss story), then wanders into Spielberg's big-top conceit: a modernized, Philip K. Dick-style version of \"Pinocchio.\" \n Its heart is in the right place. But the intelligence behind the rest of the movie is, well, a little too artificial. Intriguing, inspired, flawed, misbegotten and fascinating -- all of these qualities apply to the movie, at one point or another.  \n In the movie, set in the mid-21st century, Henry (Sam Robards) and Monica Swinton (Frances O'Connor) seek a substitute for Martin (Jake Thomas), their terminally ill son, who's been cryogenically frozen until a cure is found. \n Cybertronics Manufacturing, where Henry works, has just the answer: a brand-new, lifelike robot called David (Haley Joel Osment). David, hot off the Cybertronics production line, is designed to respond as intelligently and sensitively as artificial tissue will allow. He's programmed to need, to love. \n But here's the rub: When a willing parent activates his programming, David will become emotionally attached to the parent for life. \n This renders the parent morally responsible for David and his needs for affirmation and love. It's the same invisible contract parents sign with human children, except this robot never gets older. He stays young and adorable, and needy, forever. He doesn't need to sleep, but he's good enough to lie there all night until Mom and Dad \"wake\" him up. \n Could David be the perfect child? After many misgivings, Monica agrees to adopt him. After some adjusting, David becomes part of the family, with an automated, similarly sophisticated companion called Teddy at his beck and call. \n But David learns the limitations of his family membership when Martin -- now cured -- returns. When David realizes he needs to be a real boy, you can almost hear the drums roll: It's time for \"Pinocchio.\" \n Huh? \n Okay, you think, why not go with it? It's a provocative, bold idea. And how often does someone with the stature of Spielberg try something bold? More power to him, right? Besides, up until then, the movie has been working fine, as a sort of science fiction fairy tale. \n This is where you decide whether it works or not. But if you do accept what happens, you'll also have to swallow a finale that pushes the envelope even more. We know the \"Pinocchio\" ending, of course, so there's no surprise there. But it's as though Spielberg feels obligated to blend reality (or perhaps believability) with that fantasy transformation. Good fairies and Kubrick, it seems, don't mix well. \n But even if the concept gets knotted up in marionette strings, there are things to appreciate. Osment's performance is astonishing. There's an incredible moment when David, a newcomer at the Swinton dinner table, discovers the power of laughter. His over-the-top guffaw is eerie and shocking, as he experiments with this new emotion. His parents are taken aback at first. But when they see David's innocent desire to share some humor, they break into laughter. \n It's a delicate moment, and it speaks to Spielberg's tremendous abilities as a director. And it reminds us how good he can be around the family table, in such movies as \"Jaws\" and \"E.T.\" \n As the mother who goes through a searing, emotional odyssey, O'Connor also gives a lovely performance. But the hidden star, the D'Artagnan, is Jude Law, who appears midway through the movie as Gigolo Joe, a robotic ladies' man whose entire existence is built around a different form of love. His performance as a half-machine half-soul is balletic, precise and memorable. \n Of course, this is a movie that lives by its visual effects, too. Cinematographer Janusz Kaminski, who has shot six films for Spielberg, including \"A.I.\" and the upcoming \"Minority Report,\" delivers outstandingly crisp but gritty images. And the list of great collaborators is long, including special robotic effects master Stan Winston, Production Designer Rick Carter and so on. \n But the ultimate maestro behind this production is Spielberg, whose desire to realize Kubrick's ideas -- yet make his own work -- eerily mirrors the dedication of David as he tries to be human. It's close, mighty close, but not quite right. \nA.I. -- ARTIFICIAL INTELLIGENCE (PG-13, 146 minutes) -- Contains some sexual content and violent images. Area theaters. \n","949":"ABSTRACT\nDouglas Belkin article in Journal Report: C-Suite Strategies offers interview with Joseph Auon, president of Northwestern University, who discusses why he thinks graduates are not prepared for artificial intelligence world; photo (M)\n","950":"To the Editor:\nRe \"How to Make A.I. Human-Friendly\" (Op-Ed, March 8):\nFei-fei Li urges that \"human centered\" artificial intelligence \"must be guided by human concerns.\"\nAs long as the unholy alliance of tech and business is in charge, without labor at the table, profit will be the only concern.\nMICHAEL SOBEL, BROOKLYN\nPHOTO:  (PHOTOGRAPH BY Elisa Macellari FOR THE NEW YORK TIMES)\n","951":"To the Editor:\nRobert J. White reassures us (letter, March 26) that artificial human minds will never be made because \"artificial-intelligence investigation is based on advanced solid-state physics, whereas the humble human brain is a viable, semiliquid system!\" That is no more reassuring than the suggestion that automobiles could never replace horses because they are made of metal, while the humble horse is a viable, organic system with legs of flesh and bone.\u00a0MICHAEL D. ROHRNewark, March 26, 1998\u00a0The writer is an associate professor of philosophy at Rutgers University.\n","952":"MOUNTAIN VIEW, Calif. -- If you want to understand the priorities of a technology company, first look at the seating chart.\nAt Google's Silicon Valley headquarters, the chief executive, Sundar Pichai, now shares a floor with Google Brain, a research lab dedicated to artificial intelligence. \n  When Facebook created its own artificial intelligence lab at its offices about seven miles away, it temporarily gave A.I. researchers desks next to the fish bowl of a conference room where its chief executive and founder, Mark Zuckerberg, holds his meetings.\n  ''I can high-five Mark and Sheryl from my desk, and the A.I. team was right next to us,'' said Facebook's chief technology officer, Mike Schroepfer, referring to Mr. Zuckerberg and Sheryl Sandberg, the chief operating officer.\n  Even Overstock.com, the online retailer based in the Salt Lake City area, now runs a mini-research operation called OLabs. It sits directly outside the office of the company's chief executive, Patrick Byrne.\u00a0\n  A growing number of tech companies are pushing research labs and other far-reaching engineering efforts closer to the boss. The point is unmistakable: What they are doing matters to the chief executive. It may even be the future of the company.\n  ''The world is moving faster and faster. It is being driven by technology and innovation,'' said John Kotter, an emeritus professor at Harvard Business School who has written several books on business leadership. ''And a lot of these businesses are concluding that the speed of technological innovation should be the heart of everything.''\n  A year ago, the Google Brain team of mathematicians, coders and hardware engineers sat in a small office building on the other side of the company's campus. But over the past few months, it switched buildings and now works right beside the loungelike area where Mr. Pichai and other top executives work.\n  Jeffrey Dean, the celebrated Google engineer who oversees the Brain lab, is a short walk from Mr. Pichai. So are Ian Goodfellow, the researcher behind a new A.I. technique that generates lifelike images on its own, and Norm Jouppi, who explores ways of accelerating A.I. research through a new breed of computer chip.\n  ''Any C.E.O. thinks a lot about where people are sitting -- who they can walk around and have casual conversations with,'' said Diane Greene, who oversees Google's cloud computing team and sits on the board of Alphabet, Google's parent company. ''It is a very significant statement that he has moved that group right next him.''\n  Google is placing big bets on the A.I. being explored by researchers like Mr. Goodfellow. Many questions still hang over the progress of this research. But Mr. Pichai and the rest of the Google leadership hope it will accelerate the evolution of everything from smartphones and home appliances to internet services and robotics.\n  To Mr. Byrne, shaking up the seating chart at Overstock was a bit like a common management tactic in the military, when an officer will work closely with a small ''command initiatives group'' that is considerably more nimble than the rest of the organization.\n  ''We were getting bureaucratic,'' Mr. Byrne said. ''And this was a way of creating added competition outside the bureaucracy.''\n  These big companies are trying to duplicate the vibe of a Silicon Valley start-up, where the boss is next to everyone. As start-ups grow, they often put key technology teams next to the chief executive. Ms. Greene, who was the chief executive of the software company VMware, said she had always made a point of sitting beside the top engineers because they saw the company's future.\n  There are limits to these arrangements. When Facebook built a team to explore the future of virtual reality on its vast social network, it made a similar desk move. The group is no longer seated next to Mr. Zuckerberg. Facebook said this was because the group had grown too large. But across Silicon Valley, virtual reality is no longer the buzziest of topics. That honor belongs to artificial intelligence.\n  Where you sit has mattered for years at Facebook. The company's ad group traditionally sat far away from Mr. Zuckerberg. But after Facebook went public and started a big push for revenue, important members of the ad team moved next to the boss, said Antonio Garc\u00eda Mart\u00ednez, who wrote a book about his experiences inside Facebook.\n  At Overstock, Mr. Byrne walks past his small research team every time he leaves his office. Like Ms. Greene, he sees this as an opportunity for spontaneous interaction. The team members can discuss their work with him, and Mr. Byrne, a doctor of philosophy with a long history of unorthodox business choices, will share his far-reaching ideas with them. And he gets to hear, without the pressure of a formal presentation, what the engineers are excited about.\n  ''It was undeniable that proximity sparked conversation,'' said Judd Bagley, a kind of roving technologist and strategist who once worked in this group. ''Patrick prides himself on being able to walk up to anyone at the company, say 'hello,' and occasionally even eat french fries off the plate on their desk.''\n  Through OLabs, Overstock became the first major retailer to accept payment in the Bitcoin digital currency, and the lab eventually produced a company spinoff that seeks to apply the Bitcoin ethos to financial trading. Now, in an echo of the A.I. labs at Google and Facebook, the operation is focused on machine learning, which involves systems that can learn tasks on their own by analyzing large amounts of data.\n  If a chief executive is close to these researchers, he or she is learning from them. But the boss is also showing them how important they are to the company. That is enormously valuable for engineers and mathematicians who are not necessarily generating immediate revenue, said Kevin Quennesson, a start-up executive who ran an A.I. team at Twitter.\n  He also warned that companies risked putting too much pressure on researchers to produce results from work that was, by definition, a trip into the unknown. Amid so much spontaneous conversation with the chief executive, Mr. Bagley said, it is sometimes difficult to tell the difference between ''the formal direction and informal brainstorming.''\n  Status at Facebook was defined by how close your desk was to Mr. Zuckerberg, Mr. Mart\u00ednez said. And if you were close to the boss, other groups resented you.\n  Priorities also tend to ebb and flow. At Overstock, a new Bitcoin project will squeeze in beside OLabs. And at Facebook, the A.I. lab is no longer side by side with Mr. Zuckerberg because it also grew too large.\n  Still, this work is particularly important to the company as it works to ensure ''the safety and integrity of our community'' -- which has become Facebook code for guarding against false, misleading or dangerous activity on its social network, Mr. Schroepfer said. And the A.I. team is still only a short walk from Mr. Zuckerberg.\n\n\n\n","953":"ABSTRACT\nSteven Rosenbush article in Journal Report on CIO Network Conference and evolving role of chief information officer, which will involve greater leadership responsibility as machine learning and artificial intelligence advance (M)\n","954":"ABSTRACT\nUniversity of Texas audit finds little to show after almost five years and $62 million spent on University of Texas MD Anderson Cancer Center's partnership with IBM's Watson artificial-intelligence program;\u00a0 issues focus on integration, not Watson's capabilities (M)\n","955":"ABSTRACT\nSoftBank Cmhn Masayoshi Son plans to use $100 billion SoftBank Vision Fund he is setting up with Saudi Arabia's Public Investment Fund as vehicle for large investments in artificial intelligence and Internet of Things to help ease Softbank's debt load (M)\n","956":"ABSTRACT\nDaniela Hernandez Voices article in The Future of Everything section offers interview with Nick Bostrom, founding director of Future of Humanity Institute at Oxford University, who discusses existential risk of artificial intelligence; drawing (M)\n","957":"ABSTRACT\nHead of IBM's newly formed Cognitive Business Solutions unit Stephen Pratt is leaving for undisclosed reasons after just months on job;\u00a0 unit plans to sell data analytics and artificial-intelligence services to business customers (S)\n","958":"ABSTRACT\nJournal Report: WSJ 125 article by MIT faculty lecturer VA Shiva Ayyadurai on how email has transformed world;\u00a0 foresees email systems with integrated artificial intelligence acting as personal secretary; drawing (S)\n","960":"ABSTRACT\nLi Yuan China Circuit column on lag in China of interest in enterprise market and recent changes in attitude as many companies seek to tap into power of artificial intelligence and cloud computing to increase efficiency and cut costs; photo; graph (M)\n","961":"ABSTRACT\nMicrosoft unveILS new customer-service virtual assistant that will let customers describe their problems in their own words and draw from variety of resources for answers;\u00a0 hopes to rival IBM in designing artificial intelligence for everyday business needs (M)\n","962":"ABSTRACT\nSoftBank Group leads $502 million funding round for Improbable, London-based startup that makes virtual worlds for videogames and simulations, fitting with CEO Masayoshi Son's interest in artificial intelligence and long-term bets on new technology; photo (S)\n","963":"ABSTRACT\nChina's Didi Chuxing Technology Co says it raised $5.5 billion in new funding round which will help fuel its global expansion and push into artificial intelligence; photo (S)\n","964":"ABSTRACT\nAdrian Aoun unveILS his stealth health startup Forward in downtown San Francisco; Forward will offer concierge medicine for flat $150 monthly fee that will rely on artificial intelligence and\u00a0provide unlimited access to doctor in office and via mobile app; photo (M)\n","965":"ABSTRACT\nIT firm Cognizant Technology Solutions has grown to company with market cap of $34.6 billion since 1994 and has recently been pushing into digital services and artificial intelligence to help automate business processes in changing market (M)\n","967":"Decide.com, a start-up that describes itself as \"the next generation Consumer Reports,\" prides itself on using artificial intelligence and algorithms to make unbiased product recommendations to consumers without taking money from advertisers. In an effort to prove it is better than the venerable product evaluation magazine, which also doesn't take advertising, it hired a company that concludes it is better.\nPublished Tuesday, the report was conducted by Ant's Eye View, a part of PricewaterhouseCoopers. Not surprisingly, the authors found that recommendations made by Decide were more consistent and easily understood than those made by Consumer Reports. It says Decide.com reviews more and newer products than Consumer Reports.\nIn an interview, Decide's chief executive, Mike Fridgen, said that even though his company sponsored the study, its results were objective.\n\"I get the cynicism,\" he said. \"We sponsored the study, and the results are positive. But I'd ask you to look at the results and look at the methodology and draw your own conclusion.\"\nThe study, though, is an apples-to-oranges comparison. Decide main feature is its price predictions. It doesn't do its own testing, but pulls in data from reviews across the Web. So naturally it has a deeper product database than Consumer Reports, which buys its own products, even cars, to do its own testing.\nDecide, a company 30-person company in Seattle, has a lot of work to do if it really wants to compete with Consumer Reports. Mr. Fridgen declined to disclose the Web site's traffic and said it was not on par with Consumer Reports' audience. Compete.com, the analytics site, estimates that Decide has 50,000 regular visitors a month. Consumer Reports says it has 3.2 million Web subscribers and 4 million print subscribers.\nKen Weine, a spokesman for Consumer Reports, dismissed the study. He noted that Decide had recently pursued a partnership with Consumer Reports, a proposal that it rejected.\n\"Now, Decide.com has publicized a report that they paid consultants to write about themselves,\" he said in a written statement. \"We remain unconvinced working with Decide.com makes sense for our readers, or more broadly consumers.\"\n\n","968":"Film CapsulesCapsule reviews by Desson Howe unless noted. A star ([sstar]) denotes a movie recommended by our critics. \nBROTHER (R) -- See review on Page 32. \nHIMALAYA (Unrated) -- See review on Page 33. \nPLANET OF THE APES (PG-13) -- See review on Page 32. \u00a0\nA.I. -- ARTIFICIAL INTELLIGENCE (PG-13, 146 minutes) -- Director Steven Spielberg takes up the movie that late director Stanley Kubrick had in mind, but never achieved: a science fiction saga about artificial intelligence. The result: a movie determined to honor Kubrick's vision but directed, after all, by the man who gave you \"E.T.\" When a family in the 21st century adopt a sophisticated robotic boy called David (Haley Joel Osment), the \"mecha\" child becomes obsessed with earning his human mother's love. Upon learning he needs to be a real boy, he embarks on an epic mission to achieve humanity. This is the cue for Spielberg's big-top conceit: a futuristic \"Pinocchio.\" Intriguing, inspired, flawed, misbegotten and fascinating -- all of these qualities apply to the movie, at one point or another. The performances, by Osment, Frances O'Connor (as the mother) and Jude Law, as a robotic gigolo, are astonishing. But the movie runs into a flawed conclusion. In a way, Spielberg's desire to realize Kubrick's ideas -- yet make his own work -- eerily mirrors the dedication of David to be human. It's close, mighty close, but not quite right. Contains some sexual content and violent images. Area theaters. \n[sstar]AMERICA'S SWEETHEARTS (PG-13, 103 minutes) -- Written by Billy Crystal and Peter Tolan, and directed by former Disney head Joe Roth, \"America's Sweethearts\" is a throwback to the smart, sassy screwball comedies of Preston Sturges or Frank Capra. And the cast, including Julia Roberts, John Cusack and Catherine Zeta-Jones, is more than up to the task. Roberts is the meek assistant to her sister and movie star, Zeta-Jones, who has to fake a continuing attachment to her estranged movie star-husband, Cusack, so their last movie together won't be dogged by bad publicity. Roberts wins the cute contest in this movie. But Cusack, Crystal (as a super publicist) and a pretty hilarious Hank Azaria (as Zeta-Jones's narcissistic boyfriend) milk the comedy. Contains some strong language and sexual content. Area theaters. \nTHE ANNIVERSARY PARTY (R, 115 minutes) -- An enjoyable, no-holds-barred actor's workshop movie. Shot by veteran cinematographer John Bailey on digital video, this is the kind of experience in which performers are given all the time and indulgence in the world to soar or fall on their faces. Although the movie -- written and directed by Alan Cumming and Jennifer Jason Leigh -- falls occasional prey to pretension, it's a classic guilty pleasure. There are good moments, saucy moments, funny moments, dull moments, bad-acting moments. The characters we're watching are mostly actors, directors or hangers-on in the Hollywood business. They have gathered at the home of writer Joe Therrian (Cumming) and actor Sally Nash (Leigh), who are celebrating many things, including their sixth anniversary, after a one-year separation; their decision to start a family; and a movie-to-be. The movie's least persuasive element is its central one: the rocky relationship between Joe and often-aggressive Sally, which is marred by occasionally hackneyed writing. What works best is the light satire that plays with all of our shared notions about life in Hollywood, including an extended Ecstasy-popping scene in which the partygoers become a little too relaxed. Contains nudity, sexual scenes, obscenity and drug use. Annapolis Harbour, Sheperdstown Opera House and Cineplex Odeon Inner Circle.  \nATLANTIS: THE LOST EMPIRE (PG, 96 minutes) -- This PG-rated animated feature starts out with promise, like an animated Jules Verne-style adventure, as nerdy scholar Milo (voice of Michael J. Fox) teams with a tough crew to retrace the fabled city. Unfortunately, the animation's bland and the story's dull. And the only thing going for the movie are the funny offscreen vocal performances, particularly from Fox, Don Novello (you know, Father Guido Sarducci), the late Jim Varney and Phil Morris. Perhaps the watery domain of Atlantis, some underwater seacraft and the large lobsterlike creature (the Leviathan) that protects the city will be enough for young eyes. Contains some action violence, and story details and humor that might need explanation for kids. Area theaters. \nBABY BOY (R, 129 minutes) -- Writer-director John Singleton returns to the world, but not the same dramatic texture, of \"Boyz N the Hood\" with this movie. An episodic story that hangs out with its characters, warts, expletives and all, the story's about a 20-year-old African American man named Jody (Tyrese Gibson) who has an out-of-wedlock child, an on-again-off-again relationship with Yvette (Taraji Henson), the kid's mother, and who's coasting rent-free with his mother. The movie makes an over-long deal about Jody's immaturity and never seems to get beyond it. But there is a lot of pleasure in the caustic relationship between Jody and Melvin (Ving Rhames), a former con and a suitor for Jody's mother. Contains obscenity, drug use, sexual scenes and nudity. Area theaters. \nBLOW (R, 107 minutes) -- Johnny Depp is personable and appealing as George Jung, America's biggest cocaine dealer in the 1980s. But although the first half of this biopic is fascinating, as George and friends (Ethan Suplee, Max Perlich, Paul Reubens) make a killing selling the white powder, the movie degenerates into too obvious a \"rise and fall\" scenario. Also, director Ted Demme's use of Jung's relentless narration, punctuated with rock songs of the times, as well as a doomed romance with Latin party girl Mirtha (Penelope Cruz), seems patterned too closely to \"GoodFellas.\" And having Ray Liotta (the star of \"GoodFellas\") as Depp's father makes this parallel even more pronounced. Contains drug use, violence, obscenity and nudity. Arlington Cinema N' Drafthouse. \n[sstar]BREAD AND ROSES (R, 105 minutes) -- In this movie, inspired by the Justice for Janitors campaign in Los Angeles in the early 1990s, British director Ken Loach gets to revisit his union-building themes. He's something of a living, breathing antique, a filmmaker still committed enough to wave the flag for the exploited. But you don't have to be a card-carrying union member to appreciate the sweet-natured, highly photogenic Pilar Padilla, who plays Maya -- an illegal alien and office cleaner who becomes committed to union action, and the personable Adrien Brody, who plays Sam, a union worker determined to win high wages for Maya and her colleagues. \"Bread and Roses\" also shows what Loach does best: involve us directly in the desperate lives of his characters, who feel as real as moviemaking allows. Contains some violence, obscenity and sexual content. Cineplex Odeon Foundry. \nBRIDGET JONES'S DIARY (R, 95 minutes) -- She's not English, clearly, but Renee Zellweger is eminently likable as Bridget in this pleasant, if not wildly brilliant, adaptation of Helen Fielding's bestseller of the same name. When frumpy, 32-year-old Bridget decides to find a new man, she becomes the target of roguish manager Daniel Cleaver (Hugh Grant), who seduces her with smooth talk on e-mail and a squeeze of the bum. But also lurking in the background -- and not seeming too palatable at first -- is the sullen, repressed Mark Darcy (Colin Firth). In a movie that suggests a woman's value is entirely wrapped up in the wealthy man she persuades to marry her, it's devilish relief to see Grant upending his trademark niceness for something more scurrilous. Contains sexual scenes, very naughty words, overt sexual suggestion and a little bit of fisticuffs. Cineplex Odeon Foundry and Shirlington. \n[sstar]CATS & DOGS (PG, 87 minutes) -- This deft scratch-n-sniff comedy imagines a droll covert war between America's most popular pets, right under the noses of their clueless humans. And cat fanciers could get their dander up at the movie's portrayal of their preening pets as evil, shifty-whiskered connivers bent on world domination. But the movie's a surprisingly witty and sophisticated spy spoof that will tickle adult pet lovers and still capture kids 6 and older with its boy-and-his-dog love story and pet slapstick. Contains doggy poop and kitty hairball humor, and action sequences portraying cute critters in jeopardy that proves harmless. Area theaters. \n -- Jane Horwitz \nCHOCOLAT (PG-13, 121 minutes) -- When Vianne Rocher (Juliette Binoche) starts a chocolate shop in the morally uptight town of Lansquenet, the humorless Comte de Reynaud (Alfred Molina), a self-appointed guardian of morality, makes it his personal mission to run her out of business. Vianne's uncanny ability to find the perfect recipe for every customer, induces near-hysteria (and of course, liberation) among the citizens (including Judi Dench, Lena Olin, and John Wood). Unfortunately, director Lasse (\"The Cider House Rules\") Hallstrom's adaptation of Joanne Harris's bestselling novel never reaches the level of \"Like Water for Chocolate,\" \"Babette's Feast\" and other great movies about the magical connection between the gastronomic and the spiritual. Contains some strong language, mild sexual scenes and a little domestic assault and battery. Arlington Cinema 'n' Drafthouse. \n[sstar]THE CIRCLE (Unrated, 91 minutes) -- Jafar Panahi's documentary-style drama is a devastating indictment of the oppression facing many Iranian women. Episode by episode, the movie links its female characters -- most of them formerly imprisoned -- who struggle to get through everyday life in Iran. Panahi, who also made the politically metaphorical \"The White Balloon,\" makes his points with such stark clarity and conviction, it's hard to shrug off this deftly choreographed protest. And it's harder still to forget this low-budget wonder, which is testament to the continuing excellence of Iranian cinema. In Farsi with subtitles. Contains emotionally distressing material. Cineplex Odeon Foundry. \n[sstar]THE CLOSET (R, 85 minutes) -- Writer-director Francis Veber is at the top of his form, in this French comedy about a dull factory worker named Francois Pignon (Daniel Auteil), who pretends he's gay to stop his company from firing him. While the firm, afraid of a lawsuit, walks on eggshells to protect Pignon, the heat is on burly employee Felix Santini (Gerard Depardieu) to change his homophobic attitude. While Auteuil underplays, Depardieu counterpoints with an almost ursine, over-the-top portrayal. The performances work very well together: the faceless employee who has become the center of attention and the bearish homophobe trying his best to be sensitive. A very funny combination. Contains sexual scenes and language. In French with subtitles. Cinema Arts, Cineplex Odeon Dupont Circle and Shirlington. \n[sstar]CROUCHING TIGER, HIDDEN DRAGON (PG-13, 120 minutes) -- Like the warriors who effortlessly saunter up the sides of buildings, flit across rooftops and even skim across lakes, Ang Lee's mystical movie is magnificently magic. And the performances by Chow Yun Fat and Michelle Yeoh, and Zhang Ziyi as a mysterious young lady named Jen, are a delight. If the twists and turns in this story are occasionally convoluted, the dynamics are wonderful. Working with Yuen Wo-Ping, fight choreographer for \"The Matrix,\" Lee has created a visually wondrous experience. Contains swordplay, martial arts violence and sexual situations. Arlington Cinema N' Drafthouse. \n[sstar]DIVIDED WE FALL (PG-13, 122 minutes) -- This Czech movie, by Jan Hrebejk (director) and Petr Jarchovsky (the screenwriter), is a stirring, affecting story about courage, trust and the inevitability of bizarre allegiances during times of desperation. The film's about a brave Czech couple, Josef (Boleslav Polivka) and Marie (Anna Siskova), who hide Jewish refugee David Weiner (Csongar Kassai) from the Nazis for the last two years of the war. Hiding David -- the son of Josef's former employer who has been captured by the Nazis -- becomes increasingly difficult when a Nazi collaborator (Jaroslav Dusek) with a romantic interest in Marie keeps visiting the home. Based on a true story, the movie takes us through some harrowing times -- particularly when Josef and Marie have to make a desperate decision to preserve their lives. Contains a scene of sexual assault, minor sexual situations and some violence. In Czech and German with subtitles. Cineplex Odeon Foundry. \nDR. DOLITTLE 2 (PG, 97 minutes) -- This straight-to-video dog, this forgettable sequel to the forgettable 1998 remake has Dr. D (Eddie Murphy) trying to save a forest scheduled for destruction by getting two bears of an endangered species (voices of Lisa Kudrow and Steve Zahn) to mate. Funny? Not. Cute? Hardly. Adorable animals full of hilarious comments from plentiful off-screen celebs? Yes on plentiful celebs (Norm Macdonald, Michael Rapaport, Jacob Vargas). But negative on \"hilarious.\" Some language and jokes about flatulence. Area theaters. \n[sstar]EVERYBODY'S FAMOUS! (R, 99 minutes) -- In this Belgian comedy, Jean (Josse De Pauw), a forty-something factory worker and hopeless dreamer will do anything to help his semi-talented singing daughter, Marva (Eva Van Der Gucht), make it. So when a famous pop singer (Thekla Reuten) offers to fix his broken-down car, Jean kidnaps her and demands a recording contract for his daughter. Dominique Deruddere's comedy may have the corniest of premises, a thuddingly foregone conclusion and a pretty pedestrian approach to humor, but it's still disarmingly cute. It demands a sort of limbo experience, in which the more you lower your expectations, the more you'll learn to laugh at this collection of quirky characters. In Flemish with English subtitles. Contains nudity, sexual situations and some strong language. Foxchase. \nEVOLUTION (PG-13) -- In Ivan Reitman's rickety, stilted comedy, a meteorite spawns scary alien creatures who proliferate and threaten Earth. It's up to four badly drawn characters to try to stop them: scientists Ira Kane (David Duchovny, whose movie-acting career is devolving before his eyes) and Harry Block (Orlando Jones); aspiring fireman Wayne (Seann William Scott -- the funniest here); and Allison (Julianne Moore), an epidemiologist who bumps into doors all the time. As if aware of the humorous shortcomings, Reitman sends in special effects monsters that are so jarringly effective, they destroy the comedic texture of the film. Which hardly exists in the first place. Contains obscenity, sexual situations and no humor. Laurel Town Center and University Mall Theater. \nTHE FAST AND THE FURIOUS (PG-13, 107 minutes) -- Not a great movie by any means, but it can be fun if you're willing to go for the ride. It's got manly men who live by a code, women ranging from eye candy to one-of-the-boys tough, a few dollops of humor and tricked out, pumped up cars. Loosely based on an article in Vibe magazine about the world of import car racing, the cars are the real stars. Paul Walker is Brian, an undercover cop investigating a series of truck hijackings. He's surfer-boy pretty and drives like a maniac. He infiltrates the circle of Dominic (Vin Diesel), a prime suspect. Predictably, Brian gets too close to the people he's investigating, falling for the charming Mia (Jordana Brewster), Dom's sister. Ja Rule provides some brief comic relief as Edwin. Michelle Rodriguez brings her \"Girlfight\" spark -- and punch -- to the role of Letty, Dom's girlfriend and a topnotch driver herself. It's all B-movie familiar but Diesel, Brewster and Rodriguez have charisma. And, of course, the cars they drive are fast and cool. Contains mild sensuality, mild profanity, gunfire and the sort of driving you shouldn't try at home. Area theaters. \n -- Curt Fields \nFINAL FANTASY: THE SPIRITS WITHIN (PG-13, 104 MINUTES) -- Dr. Aki Ross, the star of this 100-percent digitally animated cartoon based on a video game, looks just like a real human being -- that is until she opens her mouth, moves and \"acts.\" Sure, the army of computer graphics wizards who created her and the all-pixel cast under the supervision of director-producer-gaming software mastermind Hironobu Sakaguchi were able to make her look as lifelike as, say, Angelina Jolie (more so, in fact). But her tight and expressionless face, puppet-like movements and limited emotional range (given vocal life by actress Ming-Na) as she struggles to save the earth from aliens is nothing to make flesh-and-blood actresses lose sleep. \"Final Fantasy\" is primarily for gaming zealots, who are already used to doing without character development, and for aficionados of Japanese anime, for whom the incomprehensible plot will be business as usual. Contains a vulgar word or two and battle with sci-fi critters. Area theaters. \n -- Michael O'Sullivan \nJURASSIC PARK III (PG-13, 92 minutes) -- This third installment of the dinos-eat-people franchise rounds up the usual animatronic dinosaurs and half-baked human characters to be chomped alive. But it doesn't round up original director Steven Spielberg, who knows his way around suspense. Now we've got the guy (Joe Johnston) who did \"Jumanji\" directing such prehistoric critters as Velociraptors, Spinosauruses, Tyrannosaurus rexes, pteranodons and humans -- definitely in that order. A weary Sam Neill returns as paleontologist Dr. Alan Grant, who agrees somehow to accompany shady Paul Kirby (William H. Macy) and his ex-wife, Amanda (Tea Leoni) on a flying tour of Isla Sorna -- a second dinosaur-breeding site. But the story is nonexistent, from the splintered Kirby family who will inevitably come back together, to the laughable story, which seems like a TV episode. The movie's so flat and uninvolving, you root for the dinosaurs. Contains dino-a-mano violence, bad writing and jurassic poop. Area theaters. \nKISS OF THE DRAGON (R, 120 minutes) -- As a formidable Chinese cop caught in a foreign land (Paris), Jet Li is easily Jackie Chan's equal at kung fu but doesn't share Chan's comic flair. Li punches, kicksand twirls without cracking a smile, and so does this movie. \"Kiss of the Dragon\" does have a sense of its own absurdity, but that doesn't prevent a seriousness so solemn that it could be French. In fact, apart from English language dialogue, it is, with Luc Besson as a cowriter and Parisian TV-commercial maker Chris Nahon directing. The movie undercuts the humor and grace of its fight scenes -- superbly choreographed by longtime Li collaborator Corey Yuan -- with raw brutality that's uncharacteristic of Hong Kong cinema. And it's too bad the filmmakers felt compelled to give the movie some heart, too. Contains strong violence, language, some sexuality and drug content. Area theaters. \n -- Mark Jenkins \n[sstar]A KNIGHT'S TALE (PG-13, 133 minutes) -- Forced to fill in for his deceased knight and jousting master, a squire called William Thatcher (Heath Ledger) decides to play a knight full time. Teaming with varlet-pals Roland (Mark Addy) and Wat (Alan Tudyk), acompulsive gambler (Paul Bettany) named Geoff Chaucer, and a handy-dandy blacksmith (Laura Fraser), he starts winning tournaments. But he meets his match in Count Adhemar (Rufus Sewell), a haughty hot-dogger. And he falls in love with Jocelyn (Shannyn Sossamon), a smart, slinky damsel who isn't completely sure she likes a jock knight. Produced, written and directed by Brian Helgeland, who made \"L.A. Confidential,\" this movie's savvy without being smug, cute without being saccharin, and funny without slipping into over-the-top goofiness. And there's something irresistible about a medieval crowd rocking out to Queen's \"We Will Rock You.\" Contains jousting violence and an implied sexual situation.  University Mall. \n[sstar]LARA CROFT: TOMB RAIDER (PG-13, 101 minutes) -- As Lara (the world's most popular video game character, adored by controller-clickers all over this planet), Angelina Jolie takes to the role with just the right combination of physical dexterity, coolness and power-jock vigor. She somersaults, kicks and likes to fire two guns at a time. The story (something about locating two separated halves of a mystical triangle when all nine planets are aligned) is nearly incomprehensible. But for a modestly budgeted project, the movie's well-done and watchable. The sets are memorable and intricately detailed. And when Lara takes on some stone gods who come roaringly to life, the fights feel more classic -- like the old Ray Harryhausen movies -- than futuristic, which is good. So there you have it, a movie that doesn't take itself seriously, but is fun enough. Contains video game-style violence and some sensuality. Area Theaters. \nLEGALLY BLONDE (PG-13, 96 MINUTES) -- Directed by first-timer Robert Luketic and penned by Karen McCullah Lutz and Kirsten Smith, this mildly amusing fish-out-of-water tale about a California deb (Reese Witherspoon) in pursuit of her Mrs. degree at Harvard Law School suffers from terminal cuteness. Little more than an extended dumb-blonde joke, complete with stock characters (the snooty WASP boyfriend, the humorless lesbian classmate, the shopping-obsessed heroine) and uplifting moral, \"Blonde's\" unsurprising punch line is that -- gasp! -- blondes might not be so dumb after all. After an hour and a half of watching Witherspoon bat her lashes and flash her pearly whites, one longs to see her bare her fangs as she did in the 1999 \"Election.\" Contains schoolyard-grade vulgarity and naughty sexual innuendo. Area theaters. \n -- Michael O'Sullivan \nLOST AND DELIRIOUS (Unrated, 100 minutes) -- Despite worthy performances from the three principal girls, Lea (\"Emporte-Moi\") Pool's sexual coming-of-age drama, set in a girls' boarding school, is equally earnest and unconvincing. It's about newboarder Mary Bradford (Mischa Barton) and her roommates, Pauline (Piper Perabo) and Victoria (Jessica Pare), whose romantic love for each other becomes fraught with shame when they're caught in bed together. When the socially intimidated Victoria disowns her lover, the proud, defiant Pauline is left to face ostracism and romantic rejection. Her ultimate reaction to this unrelenting anguish seems so inevitable, you hope director Pool and screenwriter Judith Thompson (who adapted Susan Swan's novel, \"The Wives of Bath\") will take a less obvious route. Not so. Nor can the filmmakers resist making an obvious metaphor of the hawk that the free-spirited Pauline keeps in secret. Contains nudity, sexual situations and strong language. Cineplex Odeon Inner Circle. \n[sstar]LUMUMBA (Unrated, 115 minutes) -- Writer-director Raoul Peck has boiled down the complex story of the Congo's painful first steps as an independent nation into a taut, well-drawn story. And there's a powerful performance from Eriq Ebouaney as Patrice Emery Lumumba, the self-taught visionary whose leadership of the Congo (as it was called then) lasted only months. Peck, who co-wrote with Pascal Bonitez, runs us at a brisk pace through Lumumba's short-lived career, including behind-the-scenes machinations of western powers and run-ins with rivals Joseph Kasa Vubu (Maka Kotto) and reporter-turned-soldier Joseph Mobuto (Alex Descas). The movie's visually stirring. And the locations, in Zimbabwe and Mozambique, imbue the story with eerie authenticity. In French and English with English subtitles. Contains disturbing violence. Visions Cinema\/ Bistro\/Lounge. \nMADE (R, 95 minutes) -- The amusing way Jon Favreau glares at Vince Vaughn in \"Made,\" is a familiar routine, as we saw in \"Swingers,\" the 1996 comedy that put Favreau and Vaughn on the map as a comedic team. But in \"Made,\" it's not quite as punchy as last time. Favreau's an L.A.-based boxer named Bobby who's always trying to help Ricky (Vaughn), a pain in the neck who annoys anyone unfortunate enough to cross his path. When Max (Peter Falk) hires Bobby for some shady work in New York City, Bobby persuades the old man to put Ricky on the payroll too. And the trouble begins. Unfortunately, Vaughn is comically over-the-top for too much of the time. And Favreau's straight-man shtick with Vaughn works much better than his affecting blue-collar angel role. In the end, \"Made\" is a movie with better potential than actual results. Contains obscenity, some violence, sexuality and drug use.Muvico, Cineplex Odeon Shirlington and Wisconsin Avenue. \nTHE MAN WHO CRIED (R, 100 minutes) -- After an impressive beginning -- in which a young Jewish Russian girl named Fegele (Claudia Lander-Duke) says farewell to her America-bound father (Oleg Yankovsky) in the 1920s -- Sally Potter's movie loses its way. Fegele (now Christina Ricci) is sent to a family of Brits and renamed Suzie. Then she goes to Paris as a dancer, on the eve of World War II. She befriends fellow dancer Lola (Cate Blanchett as a Russian) and an egotistical opera singer named Dante (John Turturro), and falls in love with Cesar (Johnny Depp), a gypsy horse-handler for the troupe. But Blanchett is unconvincing as a Russian starlet, ditto for Ricci as a Russian-turned-Englishwoman. And Depp parodies himself with a perpetual expression of smoldering sexiness. Contains some violence and sexual situations. Cineplex Odeon Outer Circle and Shirlington. \n[sstar]MOULIN ROUGE (PG-13, 125 minutes) -- In Baz Luhrmann's postmodern musical, Ewan McGregor's a penniless writer come to Gay Paree, circa 1899. And Nicole Kidman -- conspicuously cast against prim type -- is Satine, the Moulin nightclub's hottest attraction. Both sing far better than you'd expect. And besides, they're more than carried along by Luhrmann's rhapsody to many things, including Paris's Bohemian era, the evolution of pop music from Gilbert & Sullivan to Nirvana, and musicals from \"Gold Diggers of 1933\" to Lars von Trier's \"Dancer in the Dark.\" The great star, of course, is Luhrmann, who also made the inspired \"Strictly Ballroom\" and \"Romeo + Juliet.\" Working with music director Marius DeVries and many other regular collaborators, he has created a flamboyant, cutting-edge opera with nods to cinema, music and the audience itself. Contains sexual shenanigans, some violence and absinthe drinking. Area theaters. \nTHE MUMMY RETURNS (PG-13, 125 minutes) -- This sequel to 1999's \"The Mummy\" comes roaring onto the screen with a major battle sequence and a screenload of computer-generated effects in the first 10 minutes. It seldom pauses thereafter. The fights, swordplay, chases and explosions suggest a big-screen version of the WWF's wrestling programs, only with more expensive production values, marginally better acting, more fighting and less blood. Part comic book, part old-time serial, \"The Mummy Returns\" is best viewed with extremely low expectations. But if you saw the first one you already know that. Contains virtually nonstop comic book violence and some potentially uncomfortable scenes involving snakes and scorpions. Rockville Town Center, Arlington Cinema N' Drafthouse and University Mall.  \n -- Curt Fields \nPEARL HARBOR (PG-13, 183 minutes) -- The event is big -- the Pearl Harbor attack by the Japanese, Dec. 7, 1941. So Walt Disney has made things big, loud, formulaic and long. Which is not always good. For starters, it takes one hour and 20 minutes for the attack to get started. And for another, the emotional impact is not helped by the characters. There's a conventional friendship between flying-crazy pilot Rafe McCawley (Ben Affleck) and his pal Danny Walker (Josh Hartnett). And there's too much formulaic cute time between Rafe and dedicated nurse Evelyn Johnson (Kate Beckinsale). Director Michael Bay gives us the expected blitzkrieg fireworks show, but depends heavily on major appropriations from the movies \"Top Gun,\" \"Titanic\" and \"Saving Private Ryan.\" All in all, there's less surprise to this movie than there was for the residents of Pearl Harbor on the actual day. Contains war carnage and some strong language. Area Theaters. \n[sstar]THE PRINCESS AND THE WARRIOR (R, 130 minutes) -- German writer-director Tom Tykwer's movie is greater on style than its actual story. You watch this for the atmosphere and the engaging presence of Franka Potente, the cardiovascularly fit star of Tykwer's \"Run Lola Run.\" In this ironic fairy tale, she's Sissi, an innocent, laconic nurse who becomes obsessed with finding a stranger who saves her life, following a road accident. But the stranger, Bodo (Benno Fuermann), an ex-soldier suffering from a traumatic past, wants no part of her mystical fantasy. Chance intervenes again, however, during a bank robbery, when Sissi finds herself in a position to return the favor. Tykwer is interested in the vagaries of destiny. And the characters seem to be pawns on a fatalistic chessboard. And it's a pleasure to get caught up in this narrative stratagem. In German with subtitles. Contains disturbing images, language and some sexual content.Cineplex Odeon Dupont Circle. \n[sstar]THE ROAD HOME (G, 100 minutes) -- Zhang Ziyi, co-star of \"Crouching Tiger, Hidden Dragon,\" is delicately arresting in Zhang Yimou's beautifully textured, disarmingly simple movie. She's Zhao Di, an 18-year-old girl who falls in love with, then marries her village's new teacher, Luo Changyu (Zheng Hao). The film starts years later, right after Luo Changyu has died; and city businessman Luo Yusheng (the son of their marriage) returns to the village for the funeral. It's a lovely, romantic trip to the past. And director Zhang Yimou, screenwriter Bao Shi and cinematographer Hou Yong take a small tale and make it almost transcendental. Contains nothing objectionable. In Mandarin Chinese with subtitles. Cineplex Odeon Foundry. \nSCARY MOVIE 2 (R, 83 MINUTES) -- This sequel to last year's \"Scary Movie\" is actually a little more tightly focused that the original, which is, admittedly, not saying a lot, considering the fact that the original scattershot, lowbrow yuk-fest was all over the comedic map. At least now, director Keenen Ivory Wayans and his seven screenwriters (including brothers Shawn, Marlon and Craig) manage to keep their satiric sights more on target, that is to say on scary movies (mainly \"The Haunting\" and \"The House on Haunted Hill\") and less on pop culture in general. In addition, the returning cast (Marlon and Shawn Wayans, Anna Faris and Regina Hall) are joined by some talented supporting actors, including James Woods in a funny sendup of \"The Exorcist\" and Chris Elliott as a handyman with a hideously deformed hand. Contains obscenity, projectile vomiting and other bodily emissions, comic fixation on body parts, slapstick violence and sex with the living, the dead and the undead. Area theaters. \n -- Michael O'Sullivan \n[sstar]THE SCORE (R, 124 minutes) -- This is the best heist flick since \"The Usual Suspects,\" a perfect 10 of a movie that makes beautiful music out of the basics, from the nuts-and-bolts suspense of robbing the prize (an ancient French scepter) to the tight-lipped men who don the black ski masks to steal it. The performers are uniformly good, including Robert De Niro as the burglarizing equivalent of the aging gunfighter -- determined to make this one his last; Edward Norton as the smart, lippy upstart who joins him; and Marlon Brando as their eccentric go-between, who fusses around in the background like a bullish Truman Capote. Only Angela Bassett (as DeNiro's girlfriend) lacks time to flash her smoldering talent -- the boys take up the movie robbing stuff. Most-valuable-player awards behind the camera are due director Frank Oz and scriptwriters Kario Salem, Lem Dobbs (who wrote the fabulous \"The Limey\") and Scott Marshall Smith, who create an absorbing movie that just concentrates on doing it right. Contains strong language. Area theaters. \n[sstar]SEXY BEAST (R, 88 minutes) -- Ray Winstone, a British screen heavy, plays wonderfully against type as softspoken Gary \"Gal\" Dove, a former bank robber who wants to retire in Spain and live with the woman (Amanda Redman) he desperately loves. Ben Kingsley is Don Logan, a bald, grim-jawed psychotic who wants to recruit Gal for one last bank job and won't take \"no\" for an answer. That's the central issue in this small-scoped but entertaining film: the clash of two obstinate wills. In the assured hands of director Jonathan Glazer and writers Louis Mellis and David Scinto, \"Sexy Beast\" is a Molotov cocktail of a movie, an engaging conflagration of British B-flick, cockney wit and gallows humor. There's even a delicate little love story in there. It's a gas and a half to spend time with these characters, especially Kingsley who makes it his personal business to be the nastiest villain in recent memory. Contains violence, profanity, sexual situations and English. Area Theaters. \nSHREK (PG, 84 minutes) -- Thanks to the cutting-edge wonders of PDI\/DreamWorks' computer animation -- the ability now to render the fluidity of the human face and evoke the realness of life -- this is visually wonderful. And the story's great too. It takes amusing liberties with fairy tale characters, pokes fun at the Disney military-industrial complex and redounds with spirited off-screen performances from Mike Myers, Eddie Murphy and others. Myers is best as the titular green ogre with trumpet-shaped ears and a seemingly ferocious temper who's really a softie who just acts mean because everyone thinks he's mean. Contains flatulence, catty satire, crude humor, mild language and subversion of fairy tale tradition. Area theaters. \nSNATCH (R, 103 minutes) -- This kinetic, highly inventive caper, starring Benicio Del Toro, Brad Pitt and a near-stadium full of cockneys and other Brits, doesn't have a microsecond of dull exposition. Director Guy Ritchie, creator of the cult hit \"Lock, Stock and Two Smoking Barrels,\" is very funny, but you'll have to appreciate the humor of someone having their arm hacked off, for one thing. As with \"Lock, Stock,\" the story's a rush-hour pileup of subplots and full of colorful characters, all of them pretty much after the same thing: a very expensive rock. And it features a hilarious rogues gallery of boxing promoters, gangsters and other offbeat individuals. Pitt is a standout as an incomprehensibly accented gypsy boxer. If stories don't happen fast, funny and powerful enough for you in the movies, push your way into the queue. This flick's for you. Contains nonstop violence and obscenity, debilitating sarcasm and a bit of nudity. Arlington Cinema N' Drafthouse. \n[sstar]SONGCATCHER (PG-13, 105 minutes) -- Maggie Greenwald's movie pays sweet tribute to the folk music of Appalachia with an affecting story, soulful songs from the mountains and the delicate acting talents of Janet McTeer. She's Lily Penleric, a musicologist who, in 1907, leaves the frustrations of academic politics for the bucolic purity of Appalachian folk music. She discovers a world of tight-lipped but proud people whose music reflects their hardscrabble lives and the music of their ancestors in England, Scotland and Ireland. Of course, the music itself is more than half the magic. And Greenwald personalizes the songs wonderfully with Pat Carroll, playing a gun-toting songwriter-singer named Viney Butler, and teenager (and operatically trained) Emmy Rossum, who plays an orphan named Deladis Slocumb who has a knack for great traditional songs. Contains sexual situations and a little violence. Area Theaters. \nSWORDFISH (R, 99 minutes) -- Halle Berry's naked upper torso, Hugh Jackman's naked upper torso, John Travolta's weird haircut and heavy-duty special effects do not a satisfying movie make. But this Dominic (\"Kalifornia\") Sena movie is certainly busy. Oh sure, there's a sort-of story, in which Gabriel Shear (Travolta), a shady, all-powerful guy who doesn't even blink when the FBI hisses at him, asks Stanley Jobson (Jackman), a master hacker just out of jail, to help him in a daring billion-dollar scheme. This is for folks just looking for medium-cool action stuff in a medium-range movie. You could do worse and you could do better. Contains sexual activity, violence, obscenity and emotional intensity. Ballston Common, Muvico and Annapolis Mall. \n[sstar]UNDER THE SAND (Unrated) -- On vacation on the southwest coast of France, Marie (Charlotte Rampling), an English teacher, is shocked when her husband Jean (Bruno Cremer) inexplicably disappears. But in this superbly subtle French movie, director Francois Ozon and his three co-writers skillfully manipulate us, so that we wonder not what happened to Jean, but how Marie deals with this potential loss and what this apparent disappearance says about her life and her relationship. Rampling may be older but she's better and more beautiful than ever. It's a great pleasure that, in such a face, we get to ponder one of the most involving psychological mysteries in recent memory. Contains some nudity and sexual scenes. In French with subtitles. Annapolis Harbor, Cineplex Odeon Dupont Circle and the Cinema Arts Theatre at Fair Oaks Mall. \n[sstar]WITH A FRIEND LIKE HARRY (R, 117 minutes) -- French filmmaker Dominik Moll won the Cesar (the French Oscar) last year for his direction of \"With a Friend Like Harry,\" a creepy, darkly comic thriller in the mold of Hitchcock's \"Strangers on a Train\" and the Patricia Highsmith novel, \"The Talented Mr. Ripley.\" Like its illustrious predecessors, \"Harry\" concerns a well-intentioned pal whose initial concern for a friend leads to violence. Said friend is Harry (Sergi Lopez), a smooth talker whose chance meeting with high-school chum Michel (Laurent Lucas) in a highway rest stop leads to unexpected consequences when Michel allows the glib benefactor into his life. Much more than an unconventional whodunit though (we know all along exactly who's doing these atrocious things), \"Harry\" can also be read as a meditation on the creative process, since what he wants more than anything is for Michel to make art. Call Harry, if you will, Michel's avenging muse. Contains murder, partial nudity, sexual subject matter and obscenity. In French and Spanish with subtitles. Cineplex Odeon Foundry.  \n -- Michael O'Sullivan \nRepertoryAIR AND SPACE MUSEUM -- \"Adventures in Wild California,\" daily at 12:15, 2:25, 3:50 and 5:15. \"To Fly!\" daily at 10:20, 11:40, 1:05, 3:15, 4:40 and 6. \"SolarMax,\" daily at 10:55 and 1:40. Seventh and Independence SW. 202\/357-1686. \n\"FILMS ON THE HILL\" -- \"Non-Stop New York\" and \"Charlie Chan at the Olympics,\" Monday at 7. 545 Seventh St. SE. 202\/547-6839. \nLIBRARY OF CONGRESS -- \"Black White Cat,\" Friday at 7. \"Conquest: The Ladder of Life\" and \"A Brief History of Time,\" Tuesday at 7. \"Waiting for Gavrilov,\" Thursday. Free, but reservations recommended. Mary Pickford Theater (64 seats), James Madison Building, First and Independence SE. 202\/707-5677. \nNATIONAL GALLERY OF ART -- \"The Poor and the Noble,\" Friday at 2:30 and Saturday at 3. \"The Fiances,\" Sunday at 4. Free. East Building, Fourth and Constitution NW. 202\/737-4215. \nNAVY MEMORIAL -- \"At Sea,\" Monday-Saturday at 11 and 1. 701 Pennsylvania Ave. NW. 202\/737-2300. \nNEWSEUM -- \"The First Freedom,\" Friday-Sunday and Tuesday-Thursday at 10:10, 11:10, 12:10, 1:15, 2:15, 3:15 and 4:15. Free. 1101 Wilson Blvd., Arlington. 703\/284-3544. \n\"SCREEN ON THE GREEN\" -- \"The Maltese Falcon\" (1941), Monday at dusk. Free. Outdoors on the grounds of the Washington Monument. 877\/262-5866. \nWASHINGTON PSYCHOTRONIC FILM Society -- \"The WPFS Psycho Award Show Film Festival,\" Tuesday at 8. Lucky Bar, 1221 Connecticut Ave. NW. 202\/736-1732. \nWHITE HOUSE VISITORS CENTER -- \"Life and Times of Calvin Coolidge,\" running continuously in the kiosk at the information desk. Free. 1450 Pennsylvania Ave. NW.  \n202\/208-1631. \nNew on Video \nThese movies arrive on video store shelves this week. \n POLLOCK \n (R, 2000, 122 MINUTES, SONY) \n Like \"Henry and June,\" \"Tom and Viv\" and other films about famous artistic couplings, first-time filmmaker Ed Harris's Jackson Pollock bio could -- some might argue should -- be called \"Jackson and Lee.\" Civil War allusions aside, the film is as much the story of Pollock's volatile relationship with his wife, the painter Lee Krasner, as it is about his mercurial life and career as a proto-abstract expressionist. Harris plays Pollock in a performance that goes well beyond the uncanny physical resemblance, while Marcia Gay Harden brings strength, self-sacrifice and her own brand of complementary madness to the role of Pollock's promoter, agent, cheerleader, critic, secretary, ambassador to the art world, nurse, cook and muse. Contains obscenity, non-explicit sexual situations and one extremely volatile artistic temperament. \n -- Michael O'Sullivan \nSWEET NOVEMBER \n(PG-13, 2001 120 MINUTES, WARNER BROS.) \n Starring Keanu Reeves and Charlize Theron, this remake of a 1968 movie is a front-end collision of a romance that is neither charming nor amusing. Reeves plays an arrogant workaholic who begrudgingly agrees to give wild spirit Theron one month to change his life for the better. Unfortunately, Theron's real reasons for taking him on become obvious way before the movie thinks you've figured it out. And we're stuck in cheap poignancy for too long. Director Pat O'Connor's movie should have been called \"Cloying November.\" Contains grotesque cuteness and sexual situations. -- Desson Howe \n","969":"ABSTRACT\nRolfe Winkler Highlights from WSJDLive article reports venture investors Kai-Fu Lee and Jim Breyer say many investors are underestimating Chinese tech sector, including developments in artificial intelligence as well as Web companies such as Alibaba (S)\n","970":"The following lesson activity is based on the article \"A Robot Monk Captivates China, Mixing Spirituality With Artificial Intelligence.\"\u00a0\n Before Reading \nWatch the video above. What questions would you pose to a robot monk? Why?\n After Reading \n Answer the questions, supporting your responses by citing evidence from the text. \n 1. Who is Xian'er, what is his full title, and where does he live?\n 2. Why was this robot created? \n 3. Where did the character first come from?\n 4. How does the robot work?\n 5. What do you think of his responses to the questions and statements visitors posed to him? \n 6. Do you think a robot monk can ever really help people with their problems? Why or why not?\n Going Further \nWhat do you know about Buddhism and its teachings?  Read this, or another basic overview, then scroll through the Times Topics page on Buddhism to see what interesting pieces you might find.\nFor instance, here is a description of a recent talk the Dalai Lama gave to schoolchildren in New Delhi, and here is some Buddhist advice: \"To Be Happier, Start Thinking More About Your Death.\"\n Related \nText to Text | Einstein and 'Where Science and Religion Coexist'\nResources | World Religions\n","971":"Two possible glimpses into the future arrived on Thursday, if you were looking for them.\nIBM, looking to figure out how to change with the times, announced the latest trick for its Watson artificial-intelligence technology. With the single swoop of a $1 billion purchase of Merge Healthcare, Watson would be digesting medical images to help doctors make diagnoses. \u00a0\nAs Steve Lohr reports, images like CAT scans, X-rays and mammograms, IBM researchers estimate, represent about 90 percent of all medical data today. If Watson can help read a patient's image, and that patient's digital medical record, the thinking goes, the technology can lead to better understanding of that patient's condition. That would surely be no small feat.\nIn entirely separate - but also potentially future-casting - news, the Pew Research Center released a study about the friendships teenagers have online. The group found that 57 percent of American teenagers age 13 to 17 say they have made a friend online. As Dino Grandoni writes, the vast majority, 77 percent, of these relationships don't culminate in an actual meeting, the Pew researchers said.\nThat may not come as a surprise to many, but it is a reminder that the web is transforming how we interact with one another. And younger generations are making the jump into the new communications channels first, and with the most enthusiasm.\nThis may not be the world of \"Her,\" and computers aren't now doing the work of doctors. The trend lines, though, sure look pretty clear.\n","972":"More Machines On the Rise\n     O.K., so maybe there isn't a robot more thrilling -- or threatening -- than the shape-shifting, molecular-recombining villainess in \"Terminator 3.\" But reality offers artificial intelligence that is almost as ingenious, and much more benevolent. \n Many of these lean (but never mean) machines can be seen starting tomorrow at \"Robot,\" a free four-day festival at Eyebeam, a Chelsea gallery. The centerpiece is \"Artbots: The Robot Talent Show,\" whose contestants range from BabyBott, which looks like a giant baby bottle and reacts when you cuddle it, to Lemur, an electronic orchestra with a multiarmed robot in the shape of the Hindu god Siva. Its function? To play drums, of course.\u00a0\n\"Kids will probably be most interested in what they can interact with,\" said Douglas Irving Repetto, the show's director. That includes Tribble, which resembles an illuminated soccer ball with whiskers. \"It's kind of like a cyberpet,\" he said.\nChildren can also make music with Lev, a robot that plays the theremin, and meet Neil, a humanoid with a video-monitor face. \"It responds to your physical gestures and tries to emote different moods,\" Mr. Repetto explained. (More examples and artist information are at Artbots.org.) On Sunday the show will culminate in an official prize and an audience choice award. (Some robots will remain on view Monday and Tuesday.)\nThe festival will also offer the Tickle Salon, a robotic massage parlor. But this one is G-rated, said Perry Lowe, Eyebeam's director of marketing. It features the Tickler, a machine that administers a light massage, as well as a device that looks like a small tractor, \"but instead of a continuous chain, it has little rubber nubs,\" he said. Rubdowns are first come, first served.\nSo are workshops from 6 to 8 p.m. on Monday. Participants 12 and over can make their own robots using LEGO Mindstorms (small computers) or build robotic dogs. These creatures, though, are guaranteed to be obedience-trained.\n\"Robot,\" tomorrow through Tuesday from noon to 6 p.m. at Eyebeam, 540 West 21st Street, Chelsea, (212) 252-5193. Donations suggested.The Friendly Skies\nNot counting Spider-Man, the only New Yorkers who can swoop from the top of a skyscraper to the sidewalks below are pigeons, hawks and other winged citizens. But now ordinary residents can have the experience, no bungee cords required.\nThe trip comes courtesy of the New York Skyride, recently reopened at the Empire State Building. This motion-simulator ride creates the illusion of high-velocity air travel by mechanically moving your seat while you watch a film. Now redesigned, it visits 29 city landmarks.\nThe 22-minute trip is narrated, fittingly, by Kevin Bacon, the actor reputed to be only six or so degrees separated from each of us nonfamous types. It starts with gentle revolutions around sites like the Chrysler Building, the Statue of Liberty and the 59th Street and Brooklyn Bridges. But just as you think, \"no big deal,\" he announces that someone has pressed the wrong button. This sends the imaginary air ship into wild soars and dips, including brief visits to outer space, below sea level and, of course, the Cyclone, the Coney Island roller coaster.\nThe illusion of hurtling through places like Grand Central Terminal and F. A. O. Schwarz, sending pedestrians shrieking for cover, seems to thrill riders under 12. (So this is how you release that notorious New York aggression.)\nStill, the trip is not scary, except, perhaps, for the lines. (The two theaters seat only 40 apiece.) But this is also the city that never sleeps: fortunately, the Skyride is open day and evening, 365 days a year.\nThe New York Skyride, second floor, the Empire State Building. Continuous shows daily, 10 a.m. to 10 p.m. Tickets: $14.50; ages 12 to 17 and 62+, $13.50; 5 to 11, $12.50. With observatory admission: $20, $18 and $14. Information: skyride.com.Go Fish\nPoliticians are often obliged to kiss babies. Adrian Benepe, the city parks commissioner, has a less appealing responsibility: to kiss a fish.\nThis morning Mr. Benape will smooch R. H. Macy, a tagged largemouth bass, and release it into Prospect Lake in Brooklyn. This rite signifies the beginning of the annual Macy's Fishing Contest, open to all anglers 15 and under. Any competitor who hooks Macy automatically wins the grand prize, a family trip on a fishing boat in Sheepshead Bay. But it won't be easy. \"The tagged fish hasn't been caught in at least 25 years,\" said Lucy Gentile, Prospect Park's director of marketing.\nIf Macy eludes capture, the trip will go to the youngster who catches the most fish on a single day. There will also be daily awards for the largest number and the longest fish.\nAll equipment is provided, and the only prerequisite is an on-site fishing clinic. The park will also offer drier ways to get into the spirit, including a Fun Fish Fest at the Audubon Center at the Boathouse (with a marionette show tomorrow at 2 p.m.), and old games to improve fishing skills on selected afternoons at the Lefferts Children's Historic House Museum. Even empty-handed fishermen can win prizes in a raffle on July 19, including a bicycle and a boat ride.\nBut the fish have the most to look forward to: all return to the lake.\nMacy's Fishing Contest, today, tomorrow and Tuesday through July 19 from 10 a.m. to 2 p.m. at Prospect Lake, Prospect Park (enter at Ocean and Parkside Avenues), Brooklyn. Free. Information: (718) 965-6975.\n","973":"TheAssociation for Computing Machinery, a leading professional association in computer science, is holding its annual conference this week, focusing on what we're now calling data science - though the ACM still clings to the label adopted when the yearly gatherings began in 1998, Knowledge Discovery and Data Mining. Of course, the field is booming, so the four-day conclave of talks, technical papers and human networking in New York has attracted an estimated 2,200 attendees, double last year's headcount. \nBut in his keynote speech on Monday, Oren Etzioni, a prominent computer scientist and chief executive of the recently createdAllen Institute for Artificial Intelligence, delivered a call to arms to the assembled data mavens. Don't be overly influenced, Mr. Etzioni warned, by the \"big data tidal wave,\" with its emphasis on mining large data sets for correlations, inferences and predictions. The big data approach, he said during his talk and in an interview later, is brimming with short-term commercial opportunity, but he said scientists should set their sights further. \"It might be fine if you want to target ads and generate product recommendations,\" he said, \"but it's not common sense knowledge.\"\u00a0\nIn his presentation, Mr. Etzioni acknowledged the gains made possible by big data methods - identifying patterns and calculating statistical probabilities - in tasks like speech recognition and computer vision. But he then proceeded to underline the limits of the big data approach. He showed the results when one types in \"apple fruit\" into Google and the Knowledge Graph result, the extracted facts that Google presents as a graphic, is mainly a list of nutritional elements of an an apple. The results from other services that assemble knowledge bases, Bing and Wolfram Alpha, were similar.\nBut things that are readily understood by humans - that apples taste sweet and have a crunchy texture in the mouth when chewed, for example - are a challenge for the algorithms that generate digital databases.\n \"Current knowledge bases are full of facts,\" Mr. Etzioni observed, \"but they are surprisingly knowledge poor.\"\nThe \"big\" in big data tends to get all the attention, Mr. Etzioni said, but thorny problems often reside in a seemingly simple sentence or two. He showed the sentence: \"The large ball crashed right through the table because it was made of Styrofoam.\" He asked, What was made of Styrofoam? The large ball? Or the table? The table, humans will invariably answer. But the question is a conundrum for a software program, Mr. Etzioni explained, because the correct answer involves both grammar and background knowledge. And the latter is something humans acquire through experience of the world.\nComputers can't experience the world as humans do. And Mr. Etzioni is skeptical of the progress that will be possible with \"deep learning,\" an artificial intelligence technique that uses the structure of the human brain as metaphorical inspiration for computer systems that process huge amounts of data.\nInstead, at the Allen Institute, financed by Microsoft co-founder Paul Allen, Mr. Etzioni is leading a growing team of 30 researchers that is working on systems that move from data to knowledge to theories, and then can reason. The test, he said, is: \"Does it combine things it knows to draw conclusions?\" This is the step from correlation, probabilities and prediction to a computer system that can understand, in its way. That seems a steep climb of the semantic ladder of meaning. \"We are trying to build these semantic models,\" Mr. Etzioni noted. \nMr. Etzioni's presentation was titled, \"The Battle for the Future of Data Mining.\" But other computer scientists see the big data approach and the quest for understanding described by Mr. Etzioni as less a battle than different yet complementary paths, heading in the same broad direction. The long-range promise, they say, is technology that becomes a layer of data-driven artificial intelligence that resides on top of both the physical and digital worlds, helping people to make faster and smarter decisions as a kind of clever software assistant.\nMr. Etzioni, other scientists say, makes a good point, but the current enthusiasm for big data methods is understandable. \"The dramatic successes of big data have caused everyone to rush over to that side of the boat,\" said Edward Lazowska, a professor at the University of Washington, who is on the board of the Allen Institute.\nAnd the correlation and prediction of data science has certainly been good to Mr. Etzioni, whose talents include being a successful entrepreneur. He was a founder of Farecast, whose software predicted the best time to buy airline tickets. Microsoft bought Farecast in 2008. He was also a founder of Decide, a web site that sifted through historical price data and user recommendations to help consumers make buying decisions. Ebay purchased Decide in 2013.\nThe keynote speaker on Tuesday morning, Eric Horvitz, a computer scientist at Microsoft Research, emphasized all that can be done with big data tools. His talk was titled, \"Data, Predictions, and Decisions in Support of People and Society.\" In his presentation, Mr. Horvitz described several projects he and his team were working on. One involves using patient, treatment and historical data to predict which hospital patients are most at risk of being readmitted within 30 days, and suggest follow-up monitoring. Studies show that 20 percent of Medicare patients return to the hospital within 30 days at an estimated cost of $17.5 billion a year, in addition to the toll in human suffering.\nLater, in a hallway conversation, a university computer scientist asked Mr. Horvitz about whether the software draws conclusions about the causes of hospital re-admissions. You can construct plausible explanations from the data, Mr. Horvitz replied. \"But we don't care,\" he added. \"Of course, we care in general. But it doesn't matter to the effectiveness of the technology.\"\nIn an interview, Mr. Horvitz, who is an academic adviser to the Allen Institute, agreed with Mr. Etzioni that the long-range goal is computer systems that can reason rather than merely recognize patterns and correlations and make predictions. But Mr. Horvitz chose a different emphasis. \"I think we can have a huge impact in so many fields, in the shorter term, along the way to reasoning systems,\" Mr. Horvitz said.\n","974":"Little noticed amid the daily news bulletins about the Islamic State and Syria, the Pentagon has begun a push for exotic new weapons that can deter Russia and China. \nPentagon officials have started talking openly about using the latest tools of artificial intelligence and machine learning to create robot weapons,  \"human-machine teams\" and enhanced, super-powered soldiers. It may sound like science fiction, but Pentagon officials say they have concluded that such high-tech systems are the best way to combat rapid improvements by the Russian and Chinese militaries. \nThese potentially revolutionary U.S. weapons systems were explained in an interview last week by Robert Work, the deputy secretary of defense, and Air Force Gen. Paul Selva, the vice chairman of the Joint Chiefs of Staff. Their comments were the latest in a series of unusual recent disclosures about what, until a few months ago, was some of the military's most secret research. \u00a0\n\"This is how we will make our battle networks more powerful, hopefully, and inject enough uncertainty in the minds of the Russians and the Chinese that, you know, if they ever did come to blows with us, would be able to prevail in a conventional [non-nuclear] way. That, for me, is the definition of conventional deterrence,\" Work explained. \nWithin the Pentagon, this high-tech approach is known by the dull phrase \"third offset strategy,\" emulating two earlier \"offsets\" that checked Russian military advances during the Cold War. The first offset was tactical nuclear weapons; the second was precision-guided conventional weapons. The latest version assumes that smart, robot weapons can help restore deterrence that has been eroded by Russian and Chinese progress. \n             Gen. Joseph F. Dunford, the chairman of the Joint Chiefs of Staff, voiced an early warning during his confirmation hearing in July when he said that Russia posed the greatest \"existential\" threat to the United States. Work said in a recent speech that because the United States has focused on the Middle East since 2001, \"our program has been slow to adapt as these high-end threats have started to re-emerge.\" \nThe Pentagon's 2017 budget includes some money to prime the high-tech pump: $3 billion for advanced weapons to counter, say, a Chinese long-range attack on U.S. naval forces; $3 billion to upgrade undersea systems; $3 billion for human-machine teaming and \"swarming\" operations by unmanned drones; $1.7 billion for cyber and electronic systems that use artificial intelligence; and $500 million for war- gaming and other testing of the new concepts. \nThe Obama administration, sometimes chided for being slow to respond to Russian and Chinese threats, seems to have concluded that America's best strategy is to leverage its biggest advantage, which is technology. The concepts are reminiscent of President Reagan's \"Star Wars\" initiative, but 30 years on.\nThe high-tech resurgence got a boost last year from the blue-ribbon Defense Science Board, which conducted a \"summer study\" of autonomous, robot weapons. \"Imagine if we are unprepared to counter such capabilities in the hands of our adversaries,\" the board warned. \nThe game partly is about messaging the Russians and Chinese. Work has described Russia as \"a resurgent great power\" and China as \"a rising power with impressive latent technological capabilities [that] probably embodies a more enduring strategic challenge.\" In a Feb. 2 budget announcement, Defense Secretary Ashton Carter spoke of Russian \"aggression\" in Europe and said: \"We haven't had to worry about this for 25 years, and while I wish it were otherwise, now we do.\" \nCarter raised some eyebrows in that budget message when he described the Pentagon's \"Strategic Capabilities Office,\" a highly classified initiative that he began in 2012 when he was undersecretary. He noted that the office was working on advanced navigation for smart weapons using micro-cameras and sensors; missile-defense systems using hypervelocity projectiles; and swarming drones that are \"really fast, really resistant.\" \nWork illustrated the new willingness to discuss exotic weaponry. During the interview, he showed off a small \"Perdix\" micro-drone, less than a foot long, which flew with 25 of its mates in a tight grid last summer after being launched from a large plane. These organized drones are part of the Pentagon's vision of future combat. \nThe Ukraine and Syria battlefields have offered sobering demonstrations of Russian capabilities. In the interview and other public comments, Work catalogued Russian military advances that include automated battle networks, advanced sensors, drones, anti-personnel weapons and jamming devices. \n\"Our adversaries, quite frankly, are pursuing enhanced human operations,\" Work warned a gathering at the Center for a New American Security in December. \"And it scares the crap out of us, really.\" \ndavidignatius@washpost.com\n","975":"ABSTRACT\nTed Greenwald Technology article on moves by Nvidia, Intel, Advanced Micro Devices and raft of startups to counter slowdown in sales of PCs and smartphones by crafting new processors for use in artificial intelligence applications; photo (M)\n","976":"ABSTRACT\nJason Anders article in Journal Report: WSJDLive offers edited excerpts of interview with Nvidia\u00a0CEO Jen-Hsun Huang, who comments on self-driving cars and other artificial-intelligence applications; photo (M)\n","977":"ABSTRACT\nBaidu Inc is building $300 million research and development center in Silicon Valley that would employ almost 200 engineers and be led by former Stanford University artificial intelligence lab head Andrew Ng (M)\n","978":" ABSTRACT:Slugs, regarded as bane of gardners, are sold to teachers for biology experiments, as well as to researchers working on such diverse projects as whether slugs can help them develop computerized artificial intelligence; Beaver Biological Supplies, British Columbia, ships thousands of slugs all over world for various experiments (M)\n","979":"To the Editor:\nRe ''How to Make A.I. Human-Friendly'' (Op-Ed, March 8): \n  Fei-fei Li urges that ''human centered'' artificial intelligence ''must be guided by human concerns.''\n  As long as the unholy alliance of tech and business is in charge, without labor at the table, profit will be the only concern.\n  MICHAEL SOBEL, BROOKLYN\n\n\n\u00a0\n","981":"ABSTRACT\nSamsung Electronics' Harman Kardon unit will begin selling Invoke device this fall, which uses Microsoft's Cortana digital assistant to perform various functions, challenging Amazon.com's Echo artificial-intelligence speaker and Alphabet's Google Home; photo (M)\n","982":"ABSTRACT\nGoogle's AlphaGo computer program managed to beat South Korean Go champion Lee Se-dol in March with help of special chip developed secretly over three years to speed up operations and accelerate other artificial-intelligence applications; photo (M)\n","983":" ABSTRACT:International Business Machines Corp announces new software products to aid in design of 'knowledge-based systems,' computer programs that use basic artificial intelligence to make decisions (S)\n","984":"On Aug. 20, the Aloft hotel in Cupertino, Calif., across the street from Apple's corporate campus, will begin testing \"Botlr,\" a robotic bellhop that can shuttle items from the hotel lobby desk to guest rooms. Whether a gimmick or a sign of things to come, Botlr is the latest among a new generation of robots - like Google's self-driving car, Aetheon's Tug hospital supply robot and Caddytrek's electric golf caddy - that are starting to walk or roll around the everyday world, John Markoff reports.\u00a0\nNot surprising, these robotic baby steps toward the mainstream have led to hand-wringing: What are the consequences of smarter-than-us artificial intelligence as seen in movies like \"Her\" and \"Transcendence\"? And will the next stage of machine automation lead to more job elimination?\nAloft Hotels and Savioke (pronounced \"savvy-oak\"), the Silicon Valley start-up that designed Botlr, insist that they are not interested in automation as a labor-saving tool. They say they are simply polishing the small hotel chain's tech-embracing brand while hoping to add some efficiency.\n\"I see this as an enhancement to our customer service,\" said Brian McGuinness, Starwood Hotels' senior vice president for its Specialty Select brands, which include the 100 Aloft hotels expected to be opened in 14 countries by next year. \"It's not going to be a replacement for our human talent.\"                      \n","986":"THE MOST HUMAN HUMAN: What Artificial Intelligence Teaches Us About Being Alive, by Brian Christian. (Anchor, $15.) Each year a tournament is convened to administer the Turing test, pitting humans against artificial-intelligence programs to determine whether machines can ''think.'' In 2009 Christian -- a poet with degrees in computer science and philosophy -- entered the competition, and this lively account investigates the nature of human interactions, the meaning of language and the essence of what sets us apart from machines.\nTHE UNCOUPLING, by Meg Wolitzer. (River\/head, $15.) Wolitzer's surreal comedy of manners explores power, sexuality and the vagaries of female desire. When the high school drama teacher decides to stage ''Lysistrata,'' a spell gradually falls over the women of Stellar Plains, N.J., leading them, like Aristophanes' heroines, to swear off sex.\u00a0\nREADING MY FATHER: A Memoir, by Alexandra Styron. (Scribner, $15.) Styron's story of life with her father, the novelist William Styron (1925-2006), captures the family's vibrant social circle -- it included James Baldwin, Joan Baez, Arthur Miller and Frank Sinatra -- and the shadow cast by her father's depression and paranoia. ''Avoiding my father's wrath was a complicated business,'' the author writes. Our reviewer, James Campbell, called this memoir ''by turns brilliant and shocking.''\nPORTRAITS OF A MARRIAGE, by Sandor Marai. Translated by George Szirtes. (Vintage International, $16.95.) The Hungarian novelist Sandor Marai (1900-89) was driven from his country by Communists in 1948. This novel -- first published in 1941 and told in ''Rashomon''-like monologues set in Budapest, Rome and New York -- follows a love triangle haunted by class differences and misdirected longings.\nBEAUTIFUL AND POINTLESS: A Guide to Modern Poetry, by David Orr. (Harper Perennial, $14.99.) In a book filled with excellent quotations and concepts relevant to contemporary poetry -- ''The Personal,'' ''The Political,'' ''Form,'' ''Ambition'' -- Orr, the poetry columnist for the Times Book Review, argues that poems should be loved, not deciphered.\nWHEN THE KILLING'S DONE, by T. Coraghessan Boyle. (Penguin, $16.) A habitat restorer and an animal-rights activist square off in Boyle's rollicking novel, which concerns a plan to kill invasive species on the Channel Islands of California and features shipwrecks and a disastrous rescue mission. ''Character, science and history co-evolve marvelously here in a tale of fanaticism gone literally overboard,'' Barbara Kingsolver wrote in the Book Review.\nBASEBALL IN THE GARDEN OF EDEN: The Secret History of the Early Game, by John Thorn. (Simon & Schuster, $16.) Thorn, the official historian of Major League Baseball, separates fact from fable to present a thoroughgoing picture of 19th-century baseball and the motives (high and low) of the men who promoted it. In 56: Joe DiMaggio and the Last Magic Number in Sports (Sports Illustrated Books, $16.95), Kostya Kennedy chronicles the 1941 record consecutive-game hit streak in exacting detail, and examines the psychology required to achieve it. ''Baseball's most resonant numbers keep falling,'' Kennedy writes. ''But Joe DiMaggio's is still there.''\n","987":"CHICAGO - Onstage at the Hideout, a small Chicago music club, two performers read passages from Civil War love letters. \"Oh darling wife of the war,\" one begins, \"I shall always be a husband to you and the children and all the folks in our neighborhood.\" He goes on to complain that \"the boys from the army have taken my breakfast.\" The news is worse back home. \"Our horses are sadly on fire,\" his wife laments. But they're ever reunited, she promises, \"I would kiss you as many times as there are stitches in the children.\" \nRest assured, every word from these letters is authentic. It's just that the words have been scrambled up by a computer algorithm and pieced back together, one by one, by writers with an ear for the absurd. At its funniest, this 21st century form of techno-Dadaism is a sublime collaboration \u00adbetween humans and machines, merging the creativity of one with the robotic uncanniness of the other into an original comic voice. No human would write \"I would kiss you as many times as there are stitches in the children,\" but the line is nonetheless flush with romantic longing and the hardships and violence of the period. It's grounded in truth. \u00a0\nWelcome to the world of Botnik Studios, a digital company that specializes in artificial intelligence-assisted interactive comedy. Here's how the Botnik works, according to its co-founder and CEO, Jamie Brew: \"You know that predictive text bar on your phone? You know how it offers you words that are likely words that it thinks you might type next? We make technology that does the same thing, but it's asking, 'What might J.K. Rowling type next?' or 'What might the average Yelp reviewer type next?' What comes out of it is absurd and feels like a kind of comedy writing.\"\nFounded in October 2016 by Brew, a former editor for the Onion's sister comedy site Clickhole, and Bob Mankoff, who served as cartoon editor at the New Yorker for two decades, Botnik has created several viral moments out of its surreal text collages. Scraping the text of all the Harry Potter books yielded a lost chapter of a Rowling bestseller called \"Harry Potter and the Portrait of What Looked Like a Large Pile of Ash\" that got over 46,000 retweets and inspired fan art and T-shirts. Botnik also struck a chord with a fake Coachella lineup composed of bands coughed up by a computer after it analyzed actual names (Fanch, One of Pig, and Lil Hack are the headliners) and deranged new scripts for sitcoms like \"Scrubs\" and \"Seinfeld.\"\nBut here on the Monday night, before a standing-room-only crowd of friends and cult-comedy aficionados, Brew is behind a microphone in the back of the room, emceeing, appropriately, in a voice that's half-human and half-robot, distorted by sound effects. He starts the Botnik Live! show by inviting the audience to create their own predictive-text sentence based on Yelp! reviews of the Hideout. Then come a series of sketches: An airline stewardess reading through mangled safety instructions, a dinner date featuring a character composited from the quirky love interests in \"Garden State\" and \"500 Days of Summer,\" an episode of \"Ebert & Roeper\" (\"not even suffering tropical children will enjoy it,\" goes one pan), and a jaunty singalong that combines classic sea shanties with the lyrics of Bjork and Blue Oyster Cult.\nFinally, Brew climbs onstage with a full band, performing an original song that's in the style of The Smiths but fuses Morrissey's melancholy, world-weary lyrics with the pumped-up rhetoric of Amazon reviews for the P90X home fitness system. \"And I think it's fair to saaaaaay,\" the chorus goes, \"I've gotten bored with this desire to get ripped.\" \nThe rhythms of Botnik take some getting used to, because they have no rhythm at all. The texts from a source - such as the Harry Potter books - are fed into a program called Voicebox, which determines the high-frequency words and incorporates them into a \"predictive writer\" that resembles the text message screen on a phone. There, users can compose sentences one word at a time, selecting each word from a list of nine or more possibilities that the computer thinks could come next. \nBrew and his team of writers and editors use this program to compose their ideas and then pitch them to each other during \"jam\" sessions on the company's Slack chat rooms. For instance, for the Coachella poster, they considered and then rejected Chaos Innards and Mormen Azyra Stove Jazz, but they did pick Project Mayor and Horse Choir. \nThe process is not that dissimilar to the meetings at the Onion, where writers have been pitching satirical news headlines since 1988. Yet the tone of Botnik isn't satirical but silly, and for Mankoff, that encourages a positive shift in our relationship to technology. \"We're already collaborating constantly with machines,\" Mankoff says. \"How might we do so in ways that are playful, creative and joyful?\" \nTo that end, users are free to visit the Botnik website and fiddle around with the app themselves, drawing from dozens of source texts, including the lyrics of Bob Dylan, David Bowie and Beyonc\u00e9, scripts for \"Seinfeld,\" \"Scrubs\" and \"The West Wing,\" poetry by Wordsworth and Coleridge, and miscellaneous keyboards, like one that combines \"Beowulf\" and Maya Angelou with a forklift manual. \nBotnik isn't alone in trying to find the intersection between comedy and AI. In 2016, director Oscar Sharp and Ross Goodwin, an AI researcher at New York University, created an original science-fiction short called \"Sunspring,\" starring Thomas Middleditch (\"Silicon Valley\") - a computer program analyzed dozens of sci-fi movie movies and spit out a script that was hilarious nonsense. Six years before that, Northwestern professor Kristian Hammond tried to create software that generated humor, to see how a computer could mimic human thought, and Sen. John McCain (R-Ariz.) dinged a $700,000 stimulus grant for the \"joke machine.\" What's unique about Botnik, though, is the level of interactivity. This isn't a computer telling jokes, but a collaboration.\nSo how is this a business? \nHunched over beer and popcorn at the Bucktown Pub, Brew and Mankoff make for a peculiar team - one a 27-year-old weaned on Internet comedy, the other a 73-year-old who worked for the most storied institution in the magazine world. Yet Mankoff, who calls himself a comedy \"futurist,\" helped usher the New Yorker's popular cartoon caption contest into the digital age.\nDriven by a mutual interest in comedy and AI, Mankoff and Brew connected through Colin Stokes, who freelanced for Brew while he worked as Mankoff's editorial assistant. Brew had been tinkering with his predictive-text algorithm on a Tumblr blog called Object Dreams, and Mankoff had the connections to incorporate Botnik and help it get off the ground. Now, Brew is the full-time CEO, working with a team of about 30 writers and 20 developers on a freelance or contract basis; Mankoff's day job is cartoon and humor editor at Esquire, though the two are in frequent contact.\nWhile it's those viral comedy bits that spread Botnik's name, the company makes money by selling content to corporations. In mid-2017, it scored a three-month, $100,000 contract to work on Amazon's Alexa Accelerator program, which essentially asked them to use their digital comedy tools to help humanize the voice-based Amazon Echo device. Brew called the experience \"a crash course in entrepreneurship,\" and Botnik has since forged partnerships with CollegeHumor, Funny Or Die and the Onion, with which it's collaborating on a summer ad campaign for Coors. But Brew worries AI-derived comedy writing may be a commercial fad, so he's eying a more ambitious business model, too. \n\"The next thing we want to try out is to make Botnik a Minecraft for text,\" he says, referring to the popular computer game that's like virtual Legos. \"The idea is that [Botnik] is a platform where you can write with any voice, combine multiple voices, save the ones you combine, and share your voices with other people,\" who might buy them from you.\nThe Minecraft analogy is a compelling hook, because it's both \u00aduser-friendly and expansive. As Brew says, in Minecraft, \"you carve out blocks from a hill and you use the blocks to make these structures. From those very simple mechanics, infinite possibilities arise.\" Botnik's founders believe the company will succeed if users are inspired to turn simple mechanics into infinite possibilities, too, and give themselves over to a happier, more creative relationship with machines. \n\"The flour disappears into the batter until you rotate your hands and let it rise like a souffl\u00e9.\" This sentence - never uttered in human history, much less written in a cookbook - was created by me from recipes for cinnamon pancakes.\nstyle@washpost.com\n","988":"Last year Stephen Hawking, one of the world's pre-eminent scientists, suggested: \"Artificial\u00a0intelligence could be a real danger in the not-too-distant future. It could design improvements to itself and outsmart us all.\" Maybe villainous sentient computers like those depicted in \"The Matrix\" and \"2001: A Space Odyssey\" aren't so far-fetched after all. Or, if you are skeptical of killer robots, it's hard to ignore the real possibility that machines might be replacing humans in the work force at a growing rate. \nDo machines represent a threat to humans? Or does the only real threat come from how humans decide to use machines?\u00a0\nIn \"The Machines Are Coming,\" Zeynep Tufekci writes:\nThe machine hums along, quietly scanning the slides, generating Pap smear diagnostics, just the way a college-educated, well-compensated lab technician might.\nA robot with emotion-detection software interviews visitors to the United States at the border. In field tests, this eerily named \"embodied avatar kiosk\" does much better than humans in catching those with invalid documentation. Emotional-processing software has gotten so good that ad companies are looking into \"mood-targeted\" advertising, and the government of Dubai wants to use it to scan all its closed-circuit TV feeds.\nYes, the machines are getting smarter, and they're coming for more and more jobs.\nNot just low-wage jobs, either.\nToday, machines can process regular spoken language and not only recognize human faces, but also read their expressions. They can classify personality types, and have started being able to carry out conversations with appropriate emotional tenor.\nMachines are getting better than humans at figuring out who to hire, who's in a mood to pay a little more for that sweater, and who needs a coupon to nudge them toward a sale. In applications around the world, software is being used to predict whether people are lying, how they feel and whom they'll vote for.\nTo crack these cognitive and emotional puzzles, computers needed not only sophisticated, efficient algorithms, but also vast amounts of human-generated data, which can now be easily harvested from our digitized world. The results are dazzling. Most of what we think of as expertise, knowledge and intuition is being deconstructed and recreated as an algorithmic competency, fueled by big data.\n Students: Read the entire article, then tell us ...\n- Do machines represent a threat to humans? Or does the only real threat come from how humans decide to use machines? Why?\n- Do you agree with Dr. Hawking that artificial\u00a0intelligence may one day equip computers with the tools to outsmart humans? And that this possibility presents a grave danger to humankind? \n- Or are you more concerned with the threat that computers present as competitors for jobs? Or with the threat that Ms. Tufekci describes, that machines more and more shift the balance of power away from workers and in favor of employers?\n- Ms. Tufekci writes: \"It's easy to imagine an alternate future where advanced machine capabilities are used to empower more of us, rather than control most of us. There will potentially be more time, resources and freedom to share, but only if we change how we do things. We don't need to reject or blame technology. This problem is not us versus the machines, but between us, as humans, and how we value one another.\" Do you agree with her assessment? Is the problem not us versus machines, but rather how people value one another? Why?\n Students 13 and older are invited to comment below. Please use only your first name. For privacy policy reasons, we will not publish student comments that include a last name.\n","990":"ABSTRACT\nTimothy W Martin and Dan Strumpf Business Technology article\u00a0 notes hundreds of millions of newly connected consumers are giving Asian tech firms leg up on Silicon Valley in key emerging technologies, especially in artificial intelligence, according to technology talks given at D.Live Asia conference in Hong Kong; photo (M)\n","991":"ABSTRACT\nKeiko Morris Spaces column comments on Building 128 in Brooklyn Navy Yard, century-old space once containing shipbuilding operations that now houses cutting-edge entrepreneurs in sciences of robotics and artificial intelligence; photos (M)\n","992":"ABSTRACT\nChinese Internet giant Baidu apologizes after volunteer computer scientists who administer ImageNet test say it stacked deck to get its top-scoring 4.58% error rate;\u00a0 gaffe highlights intense competition in field of artificial intelligence; photo (M)\n","994":"ABSTRACT\nHeard on the Street article notes study finding that computer model using real-time stock prices and macroeconomic input can do better job than human analysts at forecasting earnings, but best of all is combination of artificial intelligence and humans (S)\n","995":"ABSTRACT\nMatt Ridley Mind & Matter column notes Ray Kurzweil's argument in his new book, How to Create a Mind, that scientists are closing in on full understanding of human brain and how to simulate it through artificial intelligence; drawing (M)\n","996":" ABSTRACT:Boomtown column discusses the University of California (Berkeley) computer-sciences department, which has been vindicated in its decades-long skepticism about the grandiose claims made on behalf of artificial intelligence (M)\n","998":"How to get people interested in art? How to engage millennials? How to expose permanent-collection works that sit in storage? These are questions art museums constantly ponder.\nRecently, Tate Britain asked another one: How can artificial intelligence help?\nIt put the question to anyone who wanted to compete for the 2016 IK Prize, which promotes the use of digital technology in the exploration of art at Tate Britain or on the Tate website.\n\"A.I. was chosen as the theme this year because getting machines to do what humans can do is one of the most exciting frontiers in technology,\" said Tony Guillan, a multimedia producer for Tate who manages the prize, which is named for the philanthropist Irene Kreitman. \"Is there anything more human than looking at art?\"\nThe winning entry -- \"Recognition\" -- came from a team at Fabrica, a communication research center in Treviso, Italy. It features a program that continuously screens about 1,000 news photographs a day, supplied by Reuters, and tries to match them with 30,000 British artworks in the Tate's database, based on similarities in faces, objects, theme and composition.\n\"We asked, 'What if we could link our everyday lives to the collection to illuminate similarities between the present and the past?'\" said Angelo Semeraro, a member of the Fabrica team.\nCoralie Gourguechon, a colleague, added, \"We wanted to see the world through two different lenses -- how the world was represented before and how the world is represented today.\"\nSo, with help from JoliBrain, a crew of French A.I. specialists, and from Microsoft, which partnered in the prize with Tate Britain, the Fabrica team developed \"Recognition.\" Fabrica received about $120,000 for production and about $19,500 in prize money.\nPeople can watch the \"Recognition\" process online: Images in the database rotate past a photo and are given scores, according to the four variables. When a match is made, the pairing is saved in an online gallery and displayed at Tate Britain. Since it began Sept. 2, the program has been twinning images at a rate of one to three an hour. By the time it ends on Nov. 27, \"we expect 2,000 to 3,000 matches,\" Mr. Guillan said.\nSome matches are revealing, but many seem puzzling. For example, there's the match between a ceremony where the Japanese prime minister, Shinzo Abe, greets the prime minister of Singapore, Lee Hsien Loong, and \"Horse Sale at the Barbican\" (1912), by Robert Bevan. There's a Hong Kong pro-democracy demonstration paired with \"Cardinal Bourchier Urges the Widow of Edward IV to Let Her Son Out of Sanctuary,\" by John Zephaniah Bell (circa 1868). And there's a colorful flag fluttering at the United States Embassy in Costa Rica matched with an abstract work called \"Chance, Order, Change 12 (Four Colours)\" (1980), by Kenneth Martin.\n\"Art is not so simple; art is uncharted space for A.I.,\" said Eric Horvitz, director of the Microsoft Research Lab. Mr. Horvitz, who served on the prize's judging panel, said that was one reason Microsoft collaborated with the Tate.\n\"In areas like health care and transportation, we spend a lot of effort characterizing the performance and having a crisp understanding of how A.I. does what it does,\" Mr. Horvitz said. But with art, he added, \"we want A.I. to be creative and make mistakes and meander.\" Something may be gleaned from that whimsy.\nShelley Bernstein, the deputy director for digital initiatives at the Barnes Foundation in Philadelphia, called the project \"pretty awesome.\" She said it would prompt viewers to look at art more closely.\nBut other observers, including Murtha Baca, the head of digital art history at the Getty Research Institute, were more skeptical. \"It may get people to browse the collection,\" Ms. Baca said. \"Often, people don't know what to browse or search for.\" But she sees no potential for it to change art history. She and others also found the site to be user-unfriendly.\nIf visitors do spend time on the site, they will find a link to the data showing why a match was made, which allows them \"to understand more about the machine's thinking,\" Mr. Semeraro said. They may also learn something about the formal qualities of art, like line and composition.\nBut what about those many odd matches? \"One conclusion is that looking at art is very much a creative process,\" Mr. Guillan said.\nHumans, in other words, can judge the subjective, symbolic, ambiguous and narrative qualities of an artwork; machines can't. The final part of \"Recognition\" may start to address that A.I. deficiency. At Tate Britain, visitors to the \"Recognition\" gallery can interrupt the machine and register their own choices. In essence, they will retrain the initial program. \"A.I. will become smarter based on what people say,\" Mr. Horvitz said. These results will be revealed at the end of the experiment.\nMr. Guillan envisions a bigger role for A.I. in museums. It is likely to find its way into helping to create multimedia tours and into technical art history, which uses scientific analytical tools to evaluate artworks.\nJames Cuno, president of the J. Paul Getty Trust and an evangelist for the use of technology by art historians, assessed \"Recognition\" as a \"well-meaning and an interesting experiment.\" Then he added, \"It shows that we are in the early stages of the development of this technology and that there's still a long way to go.\"\nPHOTOS: Matchmaker Images paired by \"Recognition\": Far left, a photo from this year's Afropunk Fest in Brooklyn; left, Stephen McKenna's \"Venus and Adonis.\" (PHOTOGRAPHS BY EDUARDO MUNOZ\/REUTERS; STEPHEN MCKENNA, VIA TATE)Related Articles\n\n","999":"ABSTRACT\nJason L Riley Upward Mobility column on use of live social-network feeds such as\u00a0 Facebook's to air human depravity including murders, rapes and suicides;\u00a0 considers whether artificial intelligence or market pressure is better answer to cleaning up social media\n"},"date":{"0":573436800000,"1":1426550400000,"2":1436572800000,"3":652838400000,"4":977961600000,"5":1414195200000,"6":524275200000,"7":524188800000,"8":524361600000,"9":1504310400000,"10":1418688000000,"11":400723200000,"12":1485993600000,"13":1436486400000,"14":1495929600000,"15":1424822400000,"16":1480896000000,"17":993859200000,"18":493430400000,"19":1453766400000,"20":1422403200000,"21":1415232000000,"22":1367798400000,"23":1440288000000,"25":1464220800000,"26":1459987200000,"27":1421712000000,"28":1508803200000,"29":1475193600000,"30":1129248000000,"31":1427328000000,"32":711590400000,"33":1371600000000,"34":1353888000000,"36":1209772800000,"37":1435708800000,"38":1420416000000,"39":1520380800000,"40":1263081600000,"41":1452384000000,"42":1486252800000,"43":579657600000,"44":901584000000,"45":1446768000000,"46":499046400000,"48":1449878400000,"49":1464652800000,"51":1118102400000,"52":1450137600000,"53":1495584000000,"54":1485907200000,"55":1418688000000,"56":1472774400000,"57":1217548800000,"59":434505600000,"60":1518480000000,"61":1491091200000,"62":1331769600000,"64":1505433600000,"65":1457568000000,"66":1498262400000,"68":855532800000,"69":1465862400000,"71":1319760000000,"73":1305072000000,"74":1263340800000,"75":1369094400000,"76":1509494400000,"78":510278400000,"82":1438041600000,"83":630460800000,"84":742953600000,"87":1421280000000,"88":1511827200000,"90":1354492800000,"91":1313452800000,"92":570585600000,"93":1475107200000,"94":1507161600000,"95":1319587200000,"98":989625600000,"99":1522800000000,"100":863481600000,"101":1424649600000,"102":1248566400000,"103":1507507200000,"105":1430611200000,"106":1496102400000,"107":541728000000,"109":1260230400000,"110":1489968000000,"111":1353715200000,"112":1497571200000,"115":1424822400000,"116":1441411200000,"117":1482192000000,"120":1446768000000,"121":1439769600000,"122":579657600000,"123":1525824000000,"124":1427155200000,"125":633484800000,"126":1418083200000,"129":503452800000,"130":524361600000,"131":879120000000,"132":1493769600000,"134":1206748800000,"135":1243123200000,"136":1111622400000,"137":1281312000000,"141":495158400000,"142":1402444800000,"143":1474243200000,"144":344995200000,"145":1513123200000,"146":1390867200000,"147":475372800000,"148":1476835200000,"149":1228348800000,"150":1515110400000,"151":1153180800000,"152":1496620800000,"153":487123200000,"154":1514332800000,"155":565920000000,"156":1525046400000,"157":1480204800000,"159":1424649600000,"160":1503360000000,"162":1444780800000,"163":1433376000000,"164":1512086400000,"166":1475625600000,"168":1521158400000,"169":1488153600000,"170":480470400000,"171":1510531200000,"172":1323129600000,"173":1502928000000,"175":1040428800000,"176":468374400000,"177":1520208000000,"178":1465776000000,"179":1435622400000,"180":711590400000,"182":1456704000000,"183":1520899200000,"185":1153353600000,"186":1442793600000,"187":477878400000,"188":1510012800000,"189":525312000000,"190":1492128000000,"191":1148947200000,"192":1490572800000,"193":1477872000000,"195":1512259200000,"196":1522972800000,"197":1491782400000,"198":430617600000,"199":1513209600000,"200":1475625600000,"201":1421366400000,"202":449020800000,"203":1442880000000,"204":433296000000,"205":1443916800000,"206":1466812800000,"207":1438819200000,"208":1431475200000,"209":1296950400000,"210":1457395200000,"212":1499731200000,"213":1323129600000,"215":1049068800000,"216":1393718400000,"217":1166313600000,"218":1482192000000,"219":1485129600000,"221":1485129600000,"222":1491091200000,"225":1522627200000,"226":1251590400000,"228":1490227200000,"229":1432166400000,"230":1525737600000,"231":853113600000,"232":579657600000,"233":1456704000000,"236":362534400000,"238":1505952000000,"239":1498348800000,"241":1525737600000,"242":1332547200000,"243":1031270400000,"244":1521676800000,"245":1463011200000,"246":1485302400000,"247":1478044800000,"248":1453852800000,"249":1445731200000,"250":1477267200000,"251":1525478400000,"252":1449792000000,"253":1379548800000,"254":1524528000000,"255":1095292800000,"256":1500595200000,"257":1524787200000,"258":1500163200000,"260":1468972800000,"261":1495670400000,"262":863568000000,"263":1495929600000,"265":1520121600000,"267":1517443200000,"271":1524182400000,"273":1500163200000,"274":1491782400000,"275":1388361600000,"276":1444435200000,"277":1444003200000,"278":1490313600000,"279":1468800000000,"280":1517356800000,"281":988848000000,"282":1483833600000,"283":1451260800000,"284":1508716800000,"285":1381795200000,"286":1477612800000,"287":1217721600000,"288":1465430400000,"289":1477440000000,"290":1458000000000,"292":1519344000000,"294":1277424000000,"295":1427155200000,"296":1036195200000,"297":1521849600000,"298":1164326400000,"300":1510790400000,"302":1447027200000,"303":551836800000,"304":1495065600000,"306":1476576000000,"307":430876800000,"309":1454371200000,"311":1489449600000,"312":1519776000000,"313":647136000000,"314":1519084800000,"315":1240790400000,"316":1457308800000,"317":1486166400000,"318":1480896000000,"319":954374400000,"320":862704000000,"322":1297900800000,"323":1525046400000,"324":1525046400000,"325":1434153600000,"326":1362960000000,"328":1447632000000,"329":1470787200000,"330":1408924800000,"331":1463702400000,"332":1498089600000,"333":1443139200000,"335":778723200000,"336":1481760000000,"337":1430697600000,"338":831254400000,"339":1523491200000,"340":1084406400000,"342":1491523200000,"343":1525046400000,"344":1483920000000,"345":1319587200000,"346":1297641600000,"348":1472083200000,"349":1454544000000,"350":1463616000000,"351":1525478400000,"352":1390780800000,"353":667094400000,"354":1500249600000,"355":1483660800000,"356":1458950400000,"357":1039478400000,"358":1466467200000,"359":642124800000,"360":890870400000,"361":650592000000,"362":1416441600000,"364":1509926400000,"365":1431302400000,"366":1475193600000,"367":641001600000,"368":1483228800000,"370":1025740800000,"371":1430006400000,"372":1432684800000,"373":572572800000,"374":1268870400000,"375":1432598400000,"376":816825600000,"377":1520467200000,"378":1460505600000,"381":1453248000000,"382":1479081600000,"384":430963200000,"385":1329091200000,"386":528854400000,"387":967852800000,"388":656035200000,"389":1465776000000,"390":1479340800000,"392":1463097600000,"393":1525305600000,"394":1487376000000,"395":1508112000000,"397":1453852800000,"398":588384000000,"399":1498608000000,"400":1493078400000,"401":1368662400000,"402":1495497600000,"403":1525737600000,"404":692755200000,"405":996364800000,"406":1068508800000,"407":904176000000,"408":580435200000,"409":1426982400000,"411":1525219200000,"412":1495065600000,"416":1454544000000,"418":1027209600000,"419":1500249600000,"420":1525651200000,"421":1458000000000,"423":1466121600000,"424":1484179200000,"425":1458086400000,"426":1456876800000,"428":1494374400000,"429":1353801600000,"430":1459814400000,"431":1475193600000,"432":1518480000000,"433":981763200000,"434":1510358400000,"435":584064000000,"437":1425513600000,"438":1414368000000,"439":1185148800000,"440":1459123200000,"441":989539200000,"443":1516838400000,"444":1205712000000,"446":1418342400000,"448":1512345600000,"449":1525478400000,"450":1501977600000,"451":973555200000,"454":1476316800000,"455":1483660800000,"457":1475452800000,"458":1454284800000,"460":537062400000,"461":1481500800000,"465":1472860800000,"466":1028764800000,"468":1502668800000,"469":1249344000000,"471":709516800000,"473":1522368000000,"474":1416268800000,"475":1308182400000,"476":1524096000000,"478":1504828800000,"479":1446681600000,"480":1477267200000,"481":1455667200000,"482":968457600000,"483":1475280000000,"485":669168000000,"486":573264000000,"489":1371340800000,"490":744422400000,"493":1317772800000,"494":1506297600000,"495":1507680000000,"496":1414713600000,"497":1446768000000,"498":1409616000000,"499":1500854400000,"502":1442275200000,"503":493516800000,"504":1033948800000,"505":1521763200000,"507":1488412800000,"509":1486080000000,"510":472694400000,"511":1512259200000,"512":1507852800000,"513":1315699200000,"514":1310428800000,"515":1462579200000,"516":1419292800000,"518":1415750400000,"519":539568000000,"520":1518393600000,"521":1508630400000,"522":1485043200000,"523":1184198400000,"524":1017619200000,"525":1512518400000,"526":1515888000000,"527":1509840000000,"529":420595200000,"530":1422316800000,"531":1470614400000,"533":1513209600000,"536":1455062400000,"537":1519689600000,"538":1480896000000,"539":1409184000000,"540":968716800000,"541":476582400000,"542":1516320000000,"544":1509321600000,"545":235267200000,"546":1350259200000,"547":863308800000,"548":1466380800000,"550":1074124800000,"551":1457308800000,"552":1516147200000,"555":1489968000000,"556":1355097600000,"558":1503878400000,"560":1455840000000,"561":1455148800000,"562":1511308800000,"563":1525824000000,"568":933638400000,"569":1242518400000,"570":1377734400000,"571":1207785600000,"572":1488326400000,"573":1520553600000,"574":669427200000,"575":1488672000000,"576":1475193600000,"577":1450569600000,"578":1300492800000,"579":1514937600000,"582":1078704000000,"585":1321228800000,"586":1505606400000,"588":1174780800000,"589":1397779200000,"590":1437004800000,"591":1383436800000,"593":1213747200000,"595":422668800000,"596":1521676800000,"597":1447027200000,"598":1500336000000,"600":1446595200000,"601":1517270400000,"603":1461888000000,"604":1028160000000,"605":1411344000000,"606":1336089600000,"608":1525046400000,"609":1476748800000,"611":1509235200000,"612":1525651200000,"613":562982400000,"614":1011484800000,"615":1506470400000,"617":1503964800000,"619":1516665600000,"622":1477526400000,"623":1516838400000,"624":1495411200000,"625":1489017600000,"626":1455062400000,"627":1469404800000,"628":1407456000000,"632":1522800000000,"633":1191196800000,"634":1490227200000,"635":1407456000000,"636":656467200000,"637":1205366400000,"638":430358400000,"639":994291200000,"640":1397779200000,"641":1164240000000,"642":1113350400000,"643":1459468800000,"644":1089072000000,"645":1462406400000,"647":1490745600000,"648":1462233600000,"649":993340800000,"651":1523404800000,"652":1399852800000,"653":1365120000000,"655":1499212800000,"656":619660800000,"657":1390780800000,"662":1477440000000,"663":1525651200000,"665":1509926400000,"666":545184000000,"667":1424822400000,"668":560908800000,"669":459043200000,"670":690249600000,"671":1472601600000,"672":1226620800000,"673":1491004800000,"674":1453248000000,"675":1452211200000,"678":1494201600000,"679":1507852800000,"681":636163200000,"682":1336003200000,"683":1446422400000,"684":1493078400000,"685":1489968000000,"687":1435795200000,"688":1358640000000,"689":1331942400000,"691":1404864000000,"692":1523404800000,"693":1521072000000,"695":575510400000,"699":1492041600000,"701":1465430400000,"702":1365120000000,"703":1517184000000,"706":1432252800000,"708":1446422400000,"709":1446422400000,"711":1445212800000,"713":668131200000,"714":1493683200000,"715":1502755200000,"716":1019347200000,"717":1457913600000,"718":1446595200000,"719":1347148800000,"721":1521072000000,"722":502243200000,"723":1489968000000,"726":1466121600000,"728":1514937600000,"729":748656000000,"730":1509062400000,"731":1448928000000,"732":1418428800000,"733":1501286400000,"734":1433289600000,"735":1476835200000,"736":1432857600000,"737":1504656000000,"738":1480636800000,"739":1463097600000,"740":1450396800000,"741":1444003200000,"742":1449792000000,"743":1522281600000,"744":1521763200000,"745":1497484800000,"746":1495065600000,"747":1494288000000,"748":1428883200000,"749":531360000000,"750":1510272000000,"751":1492992000000,"752":1478563200000,"753":1462924800000,"754":1440374400000,"755":1495756800000,"756":1493251200000,"757":1420761600000,"758":1386720000000,"759":1286668800000,"760":1448928000000,"761":1525046400000,"762":1525046400000,"763":1525046400000,"764":1525046400000,"765":1510531200000,"766":1509408000000,"767":1504569600000,"768":1500336000000,"769":1484006400000,"770":1436486400000,"771":1520294400000,"772":1514764800000,"773":1474416000000,"774":1030838400000,"775":1477008000000,"776":1523664000000,"777":1511481600000,"778":1492214400000,"779":1486771200000,"780":1480032000000,"781":1477526400000,"782":1477440000000,"783":1463616000000,"784":1458345600000,"785":1454025600000,"786":1411948800000,"787":522979200000,"790":1525046400000,"791":1522800000000,"792":1505347200000,"793":1497225600000,"794":1450828800000,"795":1443571200000,"796":1433289600000,"797":1311379200000,"799":501724800000,"800":1488585600000,"801":1520035200000,"802":1510531200000,"803":1509062400000,"804":1477612800000,"805":1469145600000,"806":1431475200000,"807":588384000000,"808":978566400000,"809":1458000000000,"811":1514419200000,"812":1513209600000,"813":1501718400000,"814":1499644800000,"815":1489363200000,"816":1489363200000,"817":1485129600000,"818":1484179200000,"819":1480550400000,"820":1478217600000,"821":1466121600000,"822":495763200000,"823":1498089600000,"824":1424822400000,"825":1512000000000,"826":1505692800000,"827":1504569600000,"828":1488844800000,"829":1488240000000,"830":1466121600000,"831":1458086400000,"832":1444089600000,"833":1318636800000,"835":1449792000000,"836":1504310400000,"837":1492819200000,"838":1491264000000,"839":1478908800000,"840":1466121600000,"841":1461369600000,"842":1457395200000,"843":1454457600000,"844":1416614400000,"845":890956800000,"847":741571200000,"848":1519603200000,"849":1513036800000,"850":1451952000000,"851":1424390400000,"852":753062400000,"853":742608000000,"854":1516147200000,"855":1408924800000,"856":1508284800000,"857":1480982400000,"858":1440115200000,"859":1513296000000,"861":539136000000,"862":1524096000000,"863":1523923200000,"864":1519603200000,"865":1490227200000,"866":1488844800000,"867":1484697600000,"868":1466121600000,"869":1448668800000,"870":1447027200000,"871":1437782400000,"872":1420848000000,"873":895795200000,"874":728265600000,"875":1213488000000,"876":1422835200000,"877":1444608000000,"878":1525737600000,"879":1525392000000,"880":1508716800000,"881":1475193600000,"882":1457913600000,"883":1128470400000,"884":784771200000,"885":1359244800000,"887":1509580800000,"888":1501113600000,"890":1498435200000,"891":1475539200000,"892":1340928000000,"893":1279065600000,"894":496627200000,"896":1520985600000,"897":1511136000000,"898":1510617600000,"899":1508371200000,"900":1494979200000,"901":1471478400000,"902":1469059200000,"903":1401840000000,"904":1089417600000,"905":1520380800000,"906":1516665600000,"907":1515110400000,"908":1509148800000,"909":1499904000000,"910":1481587200000,"911":1477353600000,"912":1473206400000,"913":1468540800000,"914":1467849600000,"915":1458950400000,"916":669340800000,"917":620524800000,"918":1092268800000,"919":890870400000,"920":1324771200000,"921":1242000000000,"922":1520640000000,"923":1520553600000,"924":1505260800000,"925":1501891200000,"926":1473724800000,"927":1460419200000,"928":701308800000,"929":1486684800000,"930":1438041600000,"931":1461801600000,"932":1508803200000,"933":667094400000,"934":890870400000,"935":1512950400000,"936":1477008000000,"937":1437955200000,"938":1515283200000,"939":1508284800000,"940":1495411200000,"941":1488844800000,"942":493430400000,"943":1522886400000,"947":1524182400000,"948":993772800000,"949":1519084800000,"950":1521417600000,"951":890956800000,"952":1519084800000,"953":1520985600000,"954":1489017600000,"955":1478563200000,"956":1466121600000,"957":1455148800000,"958":1404777600000,"960":1511481600000,"961":1506384000000,"962":1494547200000,"963":1493424000000,"964":1484697600000,"965":1480377600000,"967":1349827200000,"968":996192000000,"969":1477526400000,"970":1461801600000,"971":1438905600000,"972":1057881600000,"973":1409097600000,"974":1456272000000,"975":1510272000000,"976":1477872000000,"977":1400284800000,"978":653011200000,"979":1521504000000,"981":1494288000000,"982":1463616000000,"983":631238400000,"984":1407801600000,"986":1333843200000,"987":1522800000000,"988":1429747200000,"990":1497052800000,"991":1475107200000,"992":1433376000000,"994":1502064000000,"995":1353715200000,"996":1023667200000,"998":1477353600000,"999":1493164800000},"docid":{"0":"1 of 500 DOCUMENTS\n","1":"2 of 500 DOCUMENTS\n","2":"3 of 500 DOCUMENTS\n","3":"4 of 500 DOCUMENTS\n","4":"5 of 500 DOCUMENTS\n","5":"6 of 500 DOCUMENTS\n","6":"7 of 500 DOCUMENTS\n","7":"8 of 500 DOCUMENTS\n","8":"9 of 500 DOCUMENTS\n","9":"10 of 500 DOCUMENTS\n","10":"11 of 500 DOCUMENTS\n","11":"12 of 500 DOCUMENTS\n","12":"13 of 500 DOCUMENTS\n","13":"14 of 500 DOCUMENTS\n","14":"15 of 500 DOCUMENTS\n","15":"16 of 500 DOCUMENTS\n","16":"17 of 500 DOCUMENTS\n","17":"18 of 500 DOCUMENTS\n","18":"19 of 500 DOCUMENTS\n","19":"20 of 500 DOCUMENTS\n","20":"21 of 500 DOCUMENTS\n","21":"22 of 500 DOCUMENTS\n","22":"23 of 500 DOCUMENTS\n","23":"24 of 500 DOCUMENTS\n","25":"26 of 500 DOCUMENTS\n","26":"27 of 500 DOCUMENTS\n","27":"28 of 500 DOCUMENTS\n","28":"29 of 500 DOCUMENTS\n","29":"30 of 500 DOCUMENTS\n","30":"31 of 500 DOCUMENTS\n","31":"32 of 500 DOCUMENTS\n","32":"33 of 500 DOCUMENTS\n","33":"34 of 500 DOCUMENTS\n","34":"35 of 500 DOCUMENTS\n","36":"37 of 500 DOCUMENTS\n","37":"38 of 500 DOCUMENTS\n","38":"39 of 500 DOCUMENTS\n","39":"40 of 500 DOCUMENTS\n","40":"41 of 500 DOCUMENTS\n","41":"42 of 500 DOCUMENTS\n","42":"43 of 500 DOCUMENTS\n","43":"44 of 500 DOCUMENTS\n","44":"45 of 500 DOCUMENTS\n","45":"46 of 500 DOCUMENTS\n","46":"47 of 500 DOCUMENTS\n","48":"49 of 500 DOCUMENTS\n","49":"50 of 500 DOCUMENTS\n","51":"52 of 500 DOCUMENTS\n","52":"53 of 500 DOCUMENTS\n","53":"54 of 500 DOCUMENTS\n","54":"55 of 500 DOCUMENTS\n","55":"56 of 500 DOCUMENTS\n","56":"57 of 500 DOCUMENTS\n","57":"58 of 500 DOCUMENTS\n","59":"60 of 500 DOCUMENTS\n","60":"61 of 500 DOCUMENTS\n","61":"62 of 500 DOCUMENTS\n","62":"63 of 500 DOCUMENTS\n","64":"65 of 500 DOCUMENTS\n","65":"66 of 500 DOCUMENTS\n","66":"67 of 500 DOCUMENTS\n","68":"69 of 500 DOCUMENTS\n","69":"70 of 500 DOCUMENTS\n","71":"72 of 500 DOCUMENTS\n","73":"74 of 500 DOCUMENTS\n","74":"75 of 500 DOCUMENTS\n","75":"76 of 500 DOCUMENTS\n","76":"77 of 500 DOCUMENTS\n","78":"79 of 500 DOCUMENTS\n","82":"83 of 500 DOCUMENTS\n","83":"84 of 500 DOCUMENTS\n","84":"85 of 500 DOCUMENTS\n","87":"88 of 500 DOCUMENTS\n","88":"89 of 500 DOCUMENTS\n","90":"91 of 500 DOCUMENTS\n","91":"92 of 500 DOCUMENTS\n","92":"93 of 500 DOCUMENTS\n","93":"94 of 500 DOCUMENTS\n","94":"95 of 500 DOCUMENTS\n","95":"96 of 500 DOCUMENTS\n","98":"99 of 500 DOCUMENTS\n","99":"100 of 500 DOCUMENTS\n","100":"101 of 500 DOCUMENTS\n","101":"102 of 500 DOCUMENTS\n","102":"103 of 500 DOCUMENTS\n","103":"104 of 500 DOCUMENTS\n","105":"106 of 500 DOCUMENTS\n","106":"107 of 500 DOCUMENTS\n","107":"108 of 500 DOCUMENTS\n","109":"110 of 500 DOCUMENTS\n","110":"111 of 500 DOCUMENTS\n","111":"112 of 500 DOCUMENTS\n","112":"113 of 500 DOCUMENTS\n","115":"116 of 500 DOCUMENTS\n","116":"117 of 500 DOCUMENTS\n","117":"118 of 500 DOCUMENTS\n","120":"121 of 500 DOCUMENTS\n","121":"122 of 500 DOCUMENTS\n","122":"123 of 500 DOCUMENTS\n","123":"124 of 500 DOCUMENTS\n","124":"125 of 500 DOCUMENTS\n","125":"126 of 500 DOCUMENTS\n","126":"127 of 500 DOCUMENTS\n","129":"130 of 500 DOCUMENTS\n","130":"131 of 500 DOCUMENTS\n","131":"132 of 500 DOCUMENTS\n","132":"133 of 500 DOCUMENTS\n","134":"135 of 500 DOCUMENTS\n","135":"136 of 500 DOCUMENTS\n","136":"137 of 500 DOCUMENTS\n","137":"138 of 500 DOCUMENTS\n","141":"142 of 500 DOCUMENTS\n","142":"143 of 500 DOCUMENTS\n","143":"144 of 500 DOCUMENTS\n","144":"145 of 500 DOCUMENTS\n","145":"146 of 500 DOCUMENTS\n","146":"147 of 500 DOCUMENTS\n","147":"148 of 500 DOCUMENTS\n","148":"149 of 500 DOCUMENTS\n","149":"150 of 500 DOCUMENTS\n","150":"151 of 500 DOCUMENTS\n","151":"152 of 500 DOCUMENTS\n","152":"153 of 500 DOCUMENTS\n","153":"154 of 500 DOCUMENTS\n","154":"155 of 500 DOCUMENTS\n","155":"156 of 500 DOCUMENTS\n","156":"157 of 500 DOCUMENTS\n","157":"158 of 500 DOCUMENTS\n","159":"160 of 500 DOCUMENTS\n","160":"161 of 500 DOCUMENTS\n","162":"163 of 500 DOCUMENTS\n","163":"164 of 500 DOCUMENTS\n","164":"165 of 500 DOCUMENTS\n","166":"167 of 500 DOCUMENTS\n","168":"169 of 500 DOCUMENTS\n","169":"170 of 500 DOCUMENTS\n","170":"171 of 500 DOCUMENTS\n","171":"172 of 500 DOCUMENTS\n","172":"173 of 500 DOCUMENTS\n","173":"174 of 500 DOCUMENTS\n","175":"176 of 500 DOCUMENTS\n","176":"177 of 500 DOCUMENTS\n","177":"178 of 500 DOCUMENTS\n","178":"179 of 500 DOCUMENTS\n","179":"180 of 500 DOCUMENTS\n","180":"181 of 500 DOCUMENTS\n","182":"183 of 500 DOCUMENTS\n","183":"184 of 500 DOCUMENTS\n","185":"186 of 500 DOCUMENTS\n","186":"187 of 500 DOCUMENTS\n","187":"188 of 500 DOCUMENTS\n","188":"189 of 500 DOCUMENTS\n","189":"190 of 500 DOCUMENTS\n","190":"191 of 500 DOCUMENTS\n","191":"192 of 500 DOCUMENTS\n","192":"193 of 500 DOCUMENTS\n","193":"194 of 500 DOCUMENTS\n","195":"196 of 500 DOCUMENTS\n","196":"197 of 500 DOCUMENTS\n","197":"198 of 500 DOCUMENTS\n","198":"199 of 500 DOCUMENTS\n","199":"200 of 500 DOCUMENTS\n","200":"201 of 500 DOCUMENTS\n","201":"202 of 500 DOCUMENTS\n","202":"203 of 500 DOCUMENTS\n","203":"204 of 500 DOCUMENTS\n","204":"205 of 500 DOCUMENTS\n","205":"206 of 500 DOCUMENTS\n","206":"207 of 500 DOCUMENTS\n","207":"208 of 500 DOCUMENTS\n","208":"209 of 500 DOCUMENTS\n","209":"210 of 500 DOCUMENTS\n","210":"211 of 500 DOCUMENTS\n","212":"213 of 500 DOCUMENTS\n","213":"214 of 500 DOCUMENTS\n","215":"216 of 500 DOCUMENTS\n","216":"217 of 500 DOCUMENTS\n","217":"218 of 500 DOCUMENTS\n","218":"219 of 500 DOCUMENTS\n","219":"220 of 500 DOCUMENTS\n","221":"222 of 500 DOCUMENTS\n","222":"223 of 500 DOCUMENTS\n","225":"226 of 500 DOCUMENTS\n","226":"227 of 500 DOCUMENTS\n","228":"229 of 500 DOCUMENTS\n","229":"230 of 500 DOCUMENTS\n","230":"231 of 500 DOCUMENTS\n","231":"232 of 500 DOCUMENTS\n","232":"233 of 500 DOCUMENTS\n","233":"234 of 500 DOCUMENTS\n","236":"237 of 500 DOCUMENTS\n","238":"239 of 500 DOCUMENTS\n","239":"240 of 500 DOCUMENTS\n","241":"242 of 500 DOCUMENTS\n","242":"243 of 500 DOCUMENTS\n","243":"244 of 500 DOCUMENTS\n","244":"245 of 500 DOCUMENTS\n","245":"246 of 500 DOCUMENTS\n","246":"247 of 500 DOCUMENTS\n","247":"248 of 500 DOCUMENTS\n","248":"249 of 500 DOCUMENTS\n","249":"250 of 500 DOCUMENTS\n","250":"251 of 500 DOCUMENTS\n","251":"252 of 500 DOCUMENTS\n","252":"253 of 500 DOCUMENTS\n","253":"254 of 500 DOCUMENTS\n","254":"255 of 500 DOCUMENTS\n","255":"256 of 500 DOCUMENTS\n","256":"257 of 500 DOCUMENTS\n","257":"258 of 500 DOCUMENTS\n","258":"259 of 500 DOCUMENTS\n","260":"261 of 500 DOCUMENTS\n","261":"262 of 500 DOCUMENTS\n","262":"263 of 500 DOCUMENTS\n","263":"264 of 500 DOCUMENTS\n","265":"266 of 500 DOCUMENTS\n","267":"268 of 500 DOCUMENTS\n","271":"272 of 500 DOCUMENTS\n","273":"274 of 500 DOCUMENTS\n","274":"275 of 500 DOCUMENTS\n","275":"276 of 500 DOCUMENTS\n","276":"277 of 500 DOCUMENTS\n","277":"278 of 500 DOCUMENTS\n","278":"279 of 500 DOCUMENTS\n","279":"280 of 500 DOCUMENTS\n","280":"281 of 500 DOCUMENTS\n","281":"282 of 500 DOCUMENTS\n","282":"283 of 500 DOCUMENTS\n","283":"284 of 500 DOCUMENTS\n","284":"285 of 500 DOCUMENTS\n","285":"286 of 500 DOCUMENTS\n","286":"287 of 500 DOCUMENTS\n","287":"288 of 500 DOCUMENTS\n","288":"289 of 500 DOCUMENTS\n","289":"290 of 500 DOCUMENTS\n","290":"291 of 500 DOCUMENTS\n","292":"293 of 500 DOCUMENTS\n","294":"295 of 500 DOCUMENTS\n","295":"296 of 500 DOCUMENTS\n","296":"297 of 500 DOCUMENTS\n","297":"298 of 500 DOCUMENTS\n","298":"299 of 500 DOCUMENTS\n","300":"301 of 500 DOCUMENTS\n","302":"303 of 500 DOCUMENTS\n","303":"304 of 500 DOCUMENTS\n","304":"305 of 500 DOCUMENTS\n","306":"307 of 500 DOCUMENTS\n","307":"308 of 500 DOCUMENTS\n","309":"310 of 500 DOCUMENTS\n","311":"312 of 500 DOCUMENTS\n","312":"313 of 500 DOCUMENTS\n","313":"314 of 500 DOCUMENTS\n","314":"315 of 500 DOCUMENTS\n","315":"316 of 500 DOCUMENTS\n","316":"317 of 500 DOCUMENTS\n","317":"318 of 500 DOCUMENTS\n","318":"319 of 500 DOCUMENTS\n","319":"320 of 500 DOCUMENTS\n","320":"321 of 500 DOCUMENTS\n","322":"323 of 500 DOCUMENTS\n","323":"324 of 500 DOCUMENTS\n","324":"325 of 500 DOCUMENTS\n","325":"326 of 500 DOCUMENTS\n","326":"327 of 500 DOCUMENTS\n","328":"329 of 500 DOCUMENTS\n","329":"330 of 500 DOCUMENTS\n","330":"331 of 500 DOCUMENTS\n","331":"332 of 500 DOCUMENTS\n","332":"333 of 500 DOCUMENTS\n","333":"334 of 500 DOCUMENTS\n","335":"336 of 500 DOCUMENTS\n","336":"337 of 500 DOCUMENTS\n","337":"338 of 500 DOCUMENTS\n","338":"339 of 500 DOCUMENTS\n","339":"340 of 500 DOCUMENTS\n","340":"341 of 500 DOCUMENTS\n","342":"343 of 500 DOCUMENTS\n","343":"344 of 500 DOCUMENTS\n","344":"345 of 500 DOCUMENTS\n","345":"346 of 500 DOCUMENTS\n","346":"347 of 500 DOCUMENTS\n","348":"349 of 500 DOCUMENTS\n","349":"350 of 500 DOCUMENTS\n","350":"351 of 500 DOCUMENTS\n","351":"352 of 500 DOCUMENTS\n","352":"353 of 500 DOCUMENTS\n","353":"354 of 500 DOCUMENTS\n","354":"355 of 500 DOCUMENTS\n","355":"356 of 500 DOCUMENTS\n","356":"357 of 500 DOCUMENTS\n","357":"358 of 500 DOCUMENTS\n","358":"359 of 500 DOCUMENTS\n","359":"360 of 500 DOCUMENTS\n","360":"361 of 500 DOCUMENTS\n","361":"362 of 500 DOCUMENTS\n","362":"363 of 500 DOCUMENTS\n","364":"365 of 500 DOCUMENTS\n","365":"366 of 500 DOCUMENTS\n","366":"367 of 500 DOCUMENTS\n","367":"368 of 500 DOCUMENTS\n","368":"369 of 500 DOCUMENTS\n","370":"371 of 500 DOCUMENTS\n","371":"372 of 500 DOCUMENTS\n","372":"373 of 500 DOCUMENTS\n","373":"374 of 500 DOCUMENTS\n","374":"375 of 500 DOCUMENTS\n","375":"376 of 500 DOCUMENTS\n","376":"377 of 500 DOCUMENTS\n","377":"378 of 500 DOCUMENTS\n","378":"379 of 500 DOCUMENTS\n","381":"382 of 500 DOCUMENTS\n","382":"383 of 500 DOCUMENTS\n","384":"385 of 500 DOCUMENTS\n","385":"386 of 500 DOCUMENTS\n","386":"387 of 500 DOCUMENTS\n","387":"388 of 500 DOCUMENTS\n","388":"389 of 500 DOCUMENTS\n","389":"390 of 500 DOCUMENTS\n","390":"391 of 500 DOCUMENTS\n","392":"393 of 500 DOCUMENTS\n","393":"394 of 500 DOCUMENTS\n","394":"395 of 500 DOCUMENTS\n","395":"396 of 500 DOCUMENTS\n","397":"398 of 500 DOCUMENTS\n","398":"399 of 500 DOCUMENTS\n","399":"400 of 500 DOCUMENTS\n","400":"401 of 500 DOCUMENTS\n","401":"402 of 500 DOCUMENTS\n","402":"403 of 500 DOCUMENTS\n","403":"404 of 500 DOCUMENTS\n","404":"405 of 500 DOCUMENTS\n","405":"406 of 500 DOCUMENTS\n","406":"407 of 500 DOCUMENTS\n","407":"408 of 500 DOCUMENTS\n","408":"409 of 500 DOCUMENTS\n","409":"410 of 500 DOCUMENTS\n","411":"412 of 500 DOCUMENTS\n","412":"413 of 500 DOCUMENTS\n","416":"417 of 500 DOCUMENTS\n","418":"419 of 500 DOCUMENTS\n","419":"420 of 500 DOCUMENTS\n","420":"421 of 500 DOCUMENTS\n","421":"422 of 500 DOCUMENTS\n","423":"424 of 500 DOCUMENTS\n","424":"425 of 500 DOCUMENTS\n","425":"426 of 500 DOCUMENTS\n","426":"427 of 500 DOCUMENTS\n","428":"429 of 500 DOCUMENTS\n","429":"430 of 500 DOCUMENTS\n","430":"431 of 500 DOCUMENTS\n","431":"432 of 500 DOCUMENTS\n","432":"433 of 500 DOCUMENTS\n","433":"434 of 500 DOCUMENTS\n","434":"435 of 500 DOCUMENTS\n","435":"436 of 500 DOCUMENTS\n","437":"438 of 500 DOCUMENTS\n","438":"439 of 500 DOCUMENTS\n","439":"440 of 500 DOCUMENTS\n","440":"441 of 500 DOCUMENTS\n","441":"442 of 500 DOCUMENTS\n","443":"444 of 500 DOCUMENTS\n","444":"445 of 500 DOCUMENTS\n","446":"447 of 500 DOCUMENTS\n","448":"449 of 500 DOCUMENTS\n","449":"450 of 500 DOCUMENTS\n","450":"451 of 500 DOCUMENTS\n","451":"452 of 500 DOCUMENTS\n","454":"455 of 500 DOCUMENTS\n","455":"456 of 500 DOCUMENTS\n","457":"458 of 500 DOCUMENTS\n","458":"459 of 500 DOCUMENTS\n","460":"461 of 500 DOCUMENTS\n","461":"462 of 500 DOCUMENTS\n","465":"466 of 500 DOCUMENTS\n","466":"467 of 500 DOCUMENTS\n","468":"469 of 500 DOCUMENTS\n","469":"470 of 500 DOCUMENTS\n","471":"472 of 500 DOCUMENTS\n","473":"474 of 500 DOCUMENTS\n","474":"475 of 500 DOCUMENTS\n","475":"476 of 500 DOCUMENTS\n","476":"477 of 500 DOCUMENTS\n","478":"479 of 500 DOCUMENTS\n","479":"480 of 500 DOCUMENTS\n","480":"481 of 500 DOCUMENTS\n","481":"482 of 500 DOCUMENTS\n","482":"483 of 500 DOCUMENTS\n","483":"484 of 500 DOCUMENTS\n","485":"486 of 500 DOCUMENTS\n","486":"487 of 500 DOCUMENTS\n","489":"490 of 500 DOCUMENTS\n","490":"491 of 500 DOCUMENTS\n","493":"494 of 500 DOCUMENTS\n","494":"495 of 500 DOCUMENTS\n","495":"496 of 500 DOCUMENTS\n","496":"497 of 500 DOCUMENTS\n","497":"498 of 500 DOCUMENTS\n","498":"499 of 500 DOCUMENTS\n","499":"500 of 500 DOCUMENTS\n","502":"3 of 500 DOCUMENTS\n","503":"4 of 500 DOCUMENTS\n","504":"5 of 500 DOCUMENTS\n","505":"6 of 500 DOCUMENTS\n","507":"8 of 500 DOCUMENTS\n","509":"10 of 500 DOCUMENTS\n","510":"11 of 500 DOCUMENTS\n","511":"12 of 500 DOCUMENTS\n","512":"13 of 500 DOCUMENTS\n","513":"14 of 500 DOCUMENTS\n","514":"15 of 500 DOCUMENTS\n","515":"16 of 500 DOCUMENTS\n","516":"17 of 500 DOCUMENTS\n","518":"19 of 500 DOCUMENTS\n","519":"20 of 500 DOCUMENTS\n","520":"21 of 500 DOCUMENTS\n","521":"22 of 500 DOCUMENTS\n","522":"23 of 500 DOCUMENTS\n","523":"24 of 500 DOCUMENTS\n","524":"25 of 500 DOCUMENTS\n","525":"26 of 500 DOCUMENTS\n","526":"27 of 500 DOCUMENTS\n","527":"28 of 500 DOCUMENTS\n","529":"30 of 500 DOCUMENTS\n","530":"31 of 500 DOCUMENTS\n","531":"32 of 500 DOCUMENTS\n","533":"34 of 500 DOCUMENTS\n","536":"37 of 500 DOCUMENTS\n","537":"38 of 500 DOCUMENTS\n","538":"39 of 500 DOCUMENTS\n","539":"40 of 500 DOCUMENTS\n","540":"41 of 500 DOCUMENTS\n","541":"42 of 500 DOCUMENTS\n","542":"43 of 500 DOCUMENTS\n","544":"45 of 500 DOCUMENTS\n","545":"46 of 500 DOCUMENTS\n","546":"47 of 500 DOCUMENTS\n","547":"48 of 500 DOCUMENTS\n","548":"49 of 500 DOCUMENTS\n","550":"51 of 500 DOCUMENTS\n","551":"52 of 500 DOCUMENTS\n","552":"53 of 500 DOCUMENTS\n","555":"56 of 500 DOCUMENTS\n","556":"57 of 500 DOCUMENTS\n","558":"59 of 500 DOCUMENTS\n","560":"61 of 500 DOCUMENTS\n","561":"62 of 500 DOCUMENTS\n","562":"63 of 500 DOCUMENTS\n","563":"64 of 500 DOCUMENTS\n","568":"69 of 500 DOCUMENTS\n","569":"70 of 500 DOCUMENTS\n","570":"71 of 500 DOCUMENTS\n","571":"72 of 500 DOCUMENTS\n","572":"73 of 500 DOCUMENTS\n","573":"74 of 500 DOCUMENTS\n","574":"75 of 500 DOCUMENTS\n","575":"76 of 500 DOCUMENTS\n","576":"77 of 500 DOCUMENTS\n","577":"78 of 500 DOCUMENTS\n","578":"79 of 500 DOCUMENTS\n","579":"80 of 500 DOCUMENTS\n","582":"83 of 500 DOCUMENTS\n","585":"86 of 500 DOCUMENTS\n","586":"87 of 500 DOCUMENTS\n","588":"89 of 500 DOCUMENTS\n","589":"90 of 500 DOCUMENTS\n","590":"91 of 500 DOCUMENTS\n","591":"92 of 500 DOCUMENTS\n","593":"94 of 500 DOCUMENTS\n","595":"96 of 500 DOCUMENTS\n","596":"97 of 500 DOCUMENTS\n","597":"98 of 500 DOCUMENTS\n","598":"99 of 500 DOCUMENTS\n","600":"101 of 500 DOCUMENTS\n","601":"102 of 500 DOCUMENTS\n","603":"104 of 500 DOCUMENTS\n","604":"105 of 500 DOCUMENTS\n","605":"106 of 500 DOCUMENTS\n","606":"107 of 500 DOCUMENTS\n","608":"109 of 500 DOCUMENTS\n","609":"110 of 500 DOCUMENTS\n","611":"112 of 500 DOCUMENTS\n","612":"113 of 500 DOCUMENTS\n","613":"114 of 500 DOCUMENTS\n","614":"115 of 500 DOCUMENTS\n","615":"116 of 500 DOCUMENTS\n","617":"118 of 500 DOCUMENTS\n","619":"120 of 500 DOCUMENTS\n","622":"123 of 500 DOCUMENTS\n","623":"124 of 500 DOCUMENTS\n","624":"125 of 500 DOCUMENTS\n","625":"126 of 500 DOCUMENTS\n","626":"127 of 500 DOCUMENTS\n","627":"128 of 500 DOCUMENTS\n","628":"129 of 500 DOCUMENTS\n","632":"133 of 500 DOCUMENTS\n","633":"134 of 500 DOCUMENTS\n","634":"135 of 500 DOCUMENTS\n","635":"136 of 500 DOCUMENTS\n","636":"137 of 500 DOCUMENTS\n","637":"138 of 500 DOCUMENTS\n","638":"139 of 500 DOCUMENTS\n","639":"140 of 500 DOCUMENTS\n","640":"141 of 500 DOCUMENTS\n","641":"142 of 500 DOCUMENTS\n","642":"143 of 500 DOCUMENTS\n","643":"144 of 500 DOCUMENTS\n","644":"145 of 500 DOCUMENTS\n","645":"146 of 500 DOCUMENTS\n","647":"148 of 500 DOCUMENTS\n","648":"149 of 500 DOCUMENTS\n","649":"150 of 500 DOCUMENTS\n","651":"152 of 500 DOCUMENTS\n","652":"153 of 500 DOCUMENTS\n","653":"154 of 500 DOCUMENTS\n","655":"156 of 500 DOCUMENTS\n","656":"157 of 500 DOCUMENTS\n","657":"158 of 500 DOCUMENTS\n","662":"163 of 500 DOCUMENTS\n","663":"164 of 500 DOCUMENTS\n","665":"166 of 500 DOCUMENTS\n","666":"167 of 500 DOCUMENTS\n","667":"168 of 500 DOCUMENTS\n","668":"169 of 500 DOCUMENTS\n","669":"170 of 500 DOCUMENTS\n","670":"171 of 500 DOCUMENTS\n","671":"172 of 500 DOCUMENTS\n","672":"173 of 500 DOCUMENTS\n","673":"174 of 500 DOCUMENTS\n","674":"175 of 500 DOCUMENTS\n","675":"176 of 500 DOCUMENTS\n","678":"179 of 500 DOCUMENTS\n","679":"180 of 500 DOCUMENTS\n","681":"182 of 500 DOCUMENTS\n","682":"183 of 500 DOCUMENTS\n","683":"184 of 500 DOCUMENTS\n","684":"185 of 500 DOCUMENTS\n","685":"186 of 500 DOCUMENTS\n","687":"188 of 500 DOCUMENTS\n","688":"189 of 500 DOCUMENTS\n","689":"190 of 500 DOCUMENTS\n","691":"192 of 500 DOCUMENTS\n","692":"193 of 500 DOCUMENTS\n","693":"194 of 500 DOCUMENTS\n","695":"196 of 500 DOCUMENTS\n","699":"200 of 500 DOCUMENTS\n","701":"202 of 500 DOCUMENTS\n","702":"203 of 500 DOCUMENTS\n","703":"204 of 500 DOCUMENTS\n","706":"207 of 500 DOCUMENTS\n","708":"209 of 500 DOCUMENTS\n","709":"210 of 500 DOCUMENTS\n","711":"212 of 500 DOCUMENTS\n","713":"214 of 500 DOCUMENTS\n","714":"215 of 500 DOCUMENTS\n","715":"216 of 500 DOCUMENTS\n","716":"217 of 500 DOCUMENTS\n","717":"218 of 500 DOCUMENTS\n","718":"219 of 500 DOCUMENTS\n","719":"220 of 500 DOCUMENTS\n","721":"222 of 500 DOCUMENTS\n","722":"223 of 500 DOCUMENTS\n","723":"224 of 500 DOCUMENTS\n","726":"227 of 500 DOCUMENTS\n","728":"229 of 500 DOCUMENTS\n","729":"230 of 500 DOCUMENTS\n","730":"231 of 500 DOCUMENTS\n","731":"232 of 500 DOCUMENTS\n","732":"233 of 500 DOCUMENTS\n","733":"234 of 500 DOCUMENTS\n","734":"235 of 500 DOCUMENTS\n","735":"236 of 500 DOCUMENTS\n","736":"237 of 500 DOCUMENTS\n","737":"238 of 500 DOCUMENTS\n","738":"239 of 500 DOCUMENTS\n","739":"240 of 500 DOCUMENTS\n","740":"241 of 500 DOCUMENTS\n","741":"242 of 500 DOCUMENTS\n","742":"243 of 500 DOCUMENTS\n","743":"244 of 500 DOCUMENTS\n","744":"245 of 500 DOCUMENTS\n","745":"246 of 500 DOCUMENTS\n","746":"247 of 500 DOCUMENTS\n","747":"248 of 500 DOCUMENTS\n","748":"249 of 500 DOCUMENTS\n","749":"250 of 500 DOCUMENTS\n","750":"251 of 500 DOCUMENTS\n","751":"252 of 500 DOCUMENTS\n","752":"253 of 500 DOCUMENTS\n","753":"254 of 500 DOCUMENTS\n","754":"255 of 500 DOCUMENTS\n","755":"256 of 500 DOCUMENTS\n","756":"257 of 500 DOCUMENTS\n","757":"258 of 500 DOCUMENTS\n","758":"259 of 500 DOCUMENTS\n","759":"260 of 500 DOCUMENTS\n","760":"261 of 500 DOCUMENTS\n","761":"262 of 500 DOCUMENTS\n","762":"263 of 500 DOCUMENTS\n","763":"264 of 500 DOCUMENTS\n","764":"265 of 500 DOCUMENTS\n","765":"266 of 500 DOCUMENTS\n","766":"267 of 500 DOCUMENTS\n","767":"268 of 500 DOCUMENTS\n","768":"269 of 500 DOCUMENTS\n","769":"270 of 500 DOCUMENTS\n","770":"271 of 500 DOCUMENTS\n","771":"272 of 500 DOCUMENTS\n","772":"273 of 500 DOCUMENTS\n","773":"274 of 500 DOCUMENTS\n","774":"275 of 500 DOCUMENTS\n","775":"276 of 500 DOCUMENTS\n","776":"277 of 500 DOCUMENTS\n","777":"278 of 500 DOCUMENTS\n","778":"279 of 500 DOCUMENTS\n","779":"280 of 500 DOCUMENTS\n","780":"281 of 500 DOCUMENTS\n","781":"282 of 500 DOCUMENTS\n","782":"283 of 500 DOCUMENTS\n","783":"284 of 500 DOCUMENTS\n","784":"285 of 500 DOCUMENTS\n","785":"286 of 500 DOCUMENTS\n","786":"287 of 500 DOCUMENTS\n","787":"288 of 500 DOCUMENTS\n","790":"291 of 500 DOCUMENTS\n","791":"292 of 500 DOCUMENTS\n","792":"293 of 500 DOCUMENTS\n","793":"294 of 500 DOCUMENTS\n","794":"295 of 500 DOCUMENTS\n","795":"296 of 500 DOCUMENTS\n","796":"297 of 500 DOCUMENTS\n","797":"298 of 500 DOCUMENTS\n","799":"300 of 500 DOCUMENTS\n","800":"301 of 500 DOCUMENTS\n","801":"302 of 500 DOCUMENTS\n","802":"303 of 500 DOCUMENTS\n","803":"304 of 500 DOCUMENTS\n","804":"305 of 500 DOCUMENTS\n","805":"306 of 500 DOCUMENTS\n","806":"307 of 500 DOCUMENTS\n","807":"308 of 500 DOCUMENTS\n","808":"309 of 500 DOCUMENTS\n","809":"310 of 500 DOCUMENTS\n","811":"312 of 500 DOCUMENTS\n","812":"313 of 500 DOCUMENTS\n","813":"314 of 500 DOCUMENTS\n","814":"315 of 500 DOCUMENTS\n","815":"316 of 500 DOCUMENTS\n","816":"317 of 500 DOCUMENTS\n","817":"318 of 500 DOCUMENTS\n","818":"319 of 500 DOCUMENTS\n","819":"320 of 500 DOCUMENTS\n","820":"321 of 500 DOCUMENTS\n","821":"322 of 500 DOCUMENTS\n","822":"323 of 500 DOCUMENTS\n","823":"324 of 500 DOCUMENTS\n","824":"325 of 500 DOCUMENTS\n","825":"326 of 500 DOCUMENTS\n","826":"327 of 500 DOCUMENTS\n","827":"328 of 500 DOCUMENTS\n","828":"329 of 500 DOCUMENTS\n","829":"330 of 500 DOCUMENTS\n","830":"331 of 500 DOCUMENTS\n","831":"332 of 500 DOCUMENTS\n","832":"333 of 500 DOCUMENTS\n","833":"334 of 500 DOCUMENTS\n","835":"336 of 500 DOCUMENTS\n","836":"337 of 500 DOCUMENTS\n","837":"338 of 500 DOCUMENTS\n","838":"339 of 500 DOCUMENTS\n","839":"340 of 500 DOCUMENTS\n","840":"341 of 500 DOCUMENTS\n","841":"342 of 500 DOCUMENTS\n","842":"343 of 500 DOCUMENTS\n","843":"344 of 500 DOCUMENTS\n","844":"345 of 500 DOCUMENTS\n","845":"346 of 500 DOCUMENTS\n","847":"348 of 500 DOCUMENTS\n","848":"349 of 500 DOCUMENTS\n","849":"350 of 500 DOCUMENTS\n","850":"351 of 500 DOCUMENTS\n","851":"352 of 500 DOCUMENTS\n","852":"353 of 500 DOCUMENTS\n","853":"354 of 500 DOCUMENTS\n","854":"355 of 500 DOCUMENTS\n","855":"356 of 500 DOCUMENTS\n","856":"357 of 500 DOCUMENTS\n","857":"358 of 500 DOCUMENTS\n","858":"359 of 500 DOCUMENTS\n","859":"360 of 500 DOCUMENTS\n","861":"362 of 500 DOCUMENTS\n","862":"363 of 500 DOCUMENTS\n","863":"364 of 500 DOCUMENTS\n","864":"365 of 500 DOCUMENTS\n","865":"366 of 500 DOCUMENTS\n","866":"367 of 500 DOCUMENTS\n","867":"368 of 500 DOCUMENTS\n","868":"369 of 500 DOCUMENTS\n","869":"370 of 500 DOCUMENTS\n","870":"371 of 500 DOCUMENTS\n","871":"372 of 500 DOCUMENTS\n","872":"373 of 500 DOCUMENTS\n","873":"374 of 500 DOCUMENTS\n","874":"375 of 500 DOCUMENTS\n","875":"376 of 500 DOCUMENTS\n","876":"377 of 500 DOCUMENTS\n","877":"378 of 500 DOCUMENTS\n","878":"379 of 500 DOCUMENTS\n","879":"380 of 500 DOCUMENTS\n","880":"381 of 500 DOCUMENTS\n","881":"382 of 500 DOCUMENTS\n","882":"383 of 500 DOCUMENTS\n","883":"384 of 500 DOCUMENTS\n","884":"385 of 500 DOCUMENTS\n","885":"386 of 500 DOCUMENTS\n","887":"388 of 500 DOCUMENTS\n","888":"389 of 500 DOCUMENTS\n","890":"391 of 500 DOCUMENTS\n","891":"392 of 500 DOCUMENTS\n","892":"393 of 500 DOCUMENTS\n","893":"394 of 500 DOCUMENTS\n","894":"395 of 500 DOCUMENTS\n","896":"397 of 500 DOCUMENTS\n","897":"398 of 500 DOCUMENTS\n","898":"399 of 500 DOCUMENTS\n","899":"400 of 500 DOCUMENTS\n","900":"401 of 500 DOCUMENTS\n","901":"402 of 500 DOCUMENTS\n","902":"403 of 500 DOCUMENTS\n","903":"404 of 500 DOCUMENTS\n","904":"405 of 500 DOCUMENTS\n","905":"406 of 500 DOCUMENTS\n","906":"407 of 500 DOCUMENTS\n","907":"408 of 500 DOCUMENTS\n","908":"409 of 500 DOCUMENTS\n","909":"410 of 500 DOCUMENTS\n","910":"411 of 500 DOCUMENTS\n","911":"412 of 500 DOCUMENTS\n","912":"413 of 500 DOCUMENTS\n","913":"414 of 500 DOCUMENTS\n","914":"415 of 500 DOCUMENTS\n","915":"416 of 500 DOCUMENTS\n","916":"417 of 500 DOCUMENTS\n","917":"418 of 500 DOCUMENTS\n","918":"419 of 500 DOCUMENTS\n","919":"420 of 500 DOCUMENTS\n","920":"421 of 500 DOCUMENTS\n","921":"422 of 500 DOCUMENTS\n","922":"423 of 500 DOCUMENTS\n","923":"424 of 500 DOCUMENTS\n","924":"425 of 500 DOCUMENTS\n","925":"426 of 500 DOCUMENTS\n","926":"427 of 500 DOCUMENTS\n","927":"428 of 500 DOCUMENTS\n","928":"429 of 500 DOCUMENTS\n","929":"430 of 500 DOCUMENTS\n","930":"431 of 500 DOCUMENTS\n","931":"432 of 500 DOCUMENTS\n","932":"433 of 500 DOCUMENTS\n","933":"434 of 500 DOCUMENTS\n","934":"435 of 500 DOCUMENTS\n","935":"436 of 500 DOCUMENTS\n","936":"437 of 500 DOCUMENTS\n","937":"438 of 500 DOCUMENTS\n","938":"439 of 500 DOCUMENTS\n","939":"440 of 500 DOCUMENTS\n","940":"441 of 500 DOCUMENTS\n","941":"442 of 500 DOCUMENTS\n","942":"443 of 500 DOCUMENTS\n","943":"444 of 500 DOCUMENTS\n","947":"448 of 500 DOCUMENTS\n","948":"449 of 500 DOCUMENTS\n","949":"450 of 500 DOCUMENTS\n","950":"451 of 500 DOCUMENTS\n","951":"452 of 500 DOCUMENTS\n","952":"453 of 500 DOCUMENTS\n","953":"454 of 500 DOCUMENTS\n","954":"455 of 500 DOCUMENTS\n","955":"456 of 500 DOCUMENTS\n","956":"457 of 500 DOCUMENTS\n","957":"458 of 500 DOCUMENTS\n","958":"459 of 500 DOCUMENTS\n","960":"461 of 500 DOCUMENTS\n","961":"462 of 500 DOCUMENTS\n","962":"463 of 500 DOCUMENTS\n","963":"464 of 500 DOCUMENTS\n","964":"465 of 500 DOCUMENTS\n","965":"466 of 500 DOCUMENTS\n","967":"468 of 500 DOCUMENTS\n","968":"469 of 500 DOCUMENTS\n","969":"470 of 500 DOCUMENTS\n","970":"471 of 500 DOCUMENTS\n","971":"472 of 500 DOCUMENTS\n","972":"473 of 500 DOCUMENTS\n","973":"474 of 500 DOCUMENTS\n","974":"475 of 500 DOCUMENTS\n","975":"476 of 500 DOCUMENTS\n","976":"477 of 500 DOCUMENTS\n","977":"478 of 500 DOCUMENTS\n","978":"479 of 500 DOCUMENTS\n","979":"480 of 500 DOCUMENTS\n","981":"482 of 500 DOCUMENTS\n","982":"483 of 500 DOCUMENTS\n","983":"484 of 500 DOCUMENTS\n","984":"485 of 500 DOCUMENTS\n","986":"487 of 500 DOCUMENTS\n","987":"488 of 500 DOCUMENTS\n","988":"489 of 500 DOCUMENTS\n","990":"491 of 500 DOCUMENTS\n","991":"492 of 500 DOCUMENTS\n","992":"493 of 500 DOCUMENTS\n","994":"495 of 500 DOCUMENTS\n","995":"496 of 500 DOCUMENTS\n","996":"497 of 500 DOCUMENTS\n","998":"499 of 500 DOCUMENTS\n","999":"500 of 500 DOCUMENTS\n"},"source":{"0":"New York Times","1":"Washington Post Blogs","2":"New York Times Blogs","3":"New York Times","4":"New York Times","5":"Washington Post Blogs","6":"Washington Post","7":"Washington Post","8":"New York Times","9":"New York Times","10":"New York Times","11":"New York Times","12":"New York Times","13":"Washington Post Blogs","14":"New York Times","15":"Washington Post Blogs","16":"New York Times","17":"New York Times","18":"Washington Post","19":"New York Times","20":"Washington Post Blogs","21":"New York Times","22":"New York Times Blogs","23":"New York Times","25":"New York Times","26":"New York Times","27":"Washington Post Blogs","28":"New York Times","29":"New York Times","30":"New York Times","31":"New York Times Blogs","32":"New York Times","33":"New York Times Blogs","34":"New York Times Blogs","36":"New York Times","37":"Washington Post Blogs","38":"Washington Post","39":"New York Times","40":"New York Times","41":"Washington Post","42":"New York Times","43":"New York Times","44":"New York Times","45":"New York Times Blogs","46":"New York Times","48":"New York Times","49":"New York Times","51":"New York Times","52":"Washington Post","53":"New York Times","54":"Washington Post Blogs","55":"New York Times","56":"New York Times","57":"Washington Post","59":"New York Times","60":"New York Times","61":"New York Times","62":"New York Times Blogs","64":"New York Times","65":"New York Times","66":"New York Times","68":"New York Times","69":"Washingtonpost","71":"Washingtonpost","73":"Washington Post","74":"New York Times","75":"New York Times Blogs","76":"Washington Post Blogs","78":"New York Times","82":"New York Times","83":"New York Times","84":"New York Times","87":"Washington Post Blogs","88":"Washingtonpost","90":"New York Times Blogs","91":"New York Times","92":"New York Times","93":"New York Times","94":"New York Times","95":"New York Times","98":"New York Times","99":"New York Times","100":"New York Times","101":"New York Times Blogs","102":"New York Times","103":"Washington Post Blogs","105":"Washington Post","106":"New York Times","107":"New York Times","109":"New York Times","110":"New York Times","111":"New York Times","112":"New York Times","115":"Washington Post Blogs","116":"New York Times","117":"Washington Post Blogs","120":"New York Times","121":"New York Times","122":"New York Times","123":"New York Times","124":"Washington Post Blogs","125":"New York Times","126":"Washington Post","129":"New York Times","130":"New York Times","131":"New York Times","132":"New York Times","134":"Washington Post","135":"New York Times","136":"New York Times","137":"New York Times","141":"New York Times","142":"New York Times Blogs","143":"New York Times","144":"New York Times","145":"New York Times","146":"New York Times Blogs","147":"New York Times","148":"Washington Post Blogs","149":"New York Times","150":"New York Times","151":"New York Times","152":"Washington Post Blogs","153":"New York Times","154":"Washington Post Blogs","155":"New York Times","156":"Wall Street Journal Abstracts","157":"New York Times","159":"Washington Post Blogs","160":"Washingtonpost","162":"New York Times Blogs","163":"New York Times","164":"New York Times","166":"Washington Post Blogs","168":"New York Times","169":"Washington Post Blogs","170":"New York Times","171":"New York Times","172":"New York Times Blogs","173":"New York Times","175":"New York Times","176":"Washington Post","177":"Washington Post Blogs","178":"Washington Post Blogs","179":"Washington Post","180":"Washington Post","182":"New York Times","183":"Los Angeles Times","185":"New York Times Blogs","186":"New York Times","187":"New York Times","188":"New York Times","189":"Washington Post","190":"New York Times","191":"New York Times","192":"Washington Post Blogs","193":"New York Times","195":"Washington Post Blogs","196":"Washington Post Blogs","197":"New York Times","198":"Washington Post","199":"New York Times","200":"New York Times","201":"New York Times Blogs","202":"New York Times","203":"Washington Post Blogs","204":"New York Times","205":"New York Times","206":"New York Times","207":"New York Times Blogs","208":"Washington Post Blogs","209":"Washington Post","210":"Washingtonpost","212":"New York Times","213":"New York Times","215":"Washington Post","216":"New York Times Blogs","217":"Washington Post","218":"Washington Post Blogs","219":"Washingtonpost","221":"Washington Post Blogs","222":"Washingtonpost","225":"New York Times","226":"Washington Post","228":"New York Times","229":"New York Times","230":"New York Post","231":"New York Times","232":"New York Times","233":"New York Times","236":"New York Times","238":"New York Times","239":"New York Times","241":"Washington Post Blogs","242":"New York Times","243":"Washington Post","244":"New York Times","245":"New York Times","246":"New York Times","247":"New York Times","248":"New York Times Blogs","249":"New York Times","250":"New York Times","251":"New York Times","252":"New York Times","253":"New York Times Blogs","254":"New York Post","255":"New York Times","256":"New York Times","257":"Washington Post Blogs","258":"Washington Post Blogs","260":"Washington Post Blogs","261":"New York Times","262":"New York Times","263":"Washingtonpost","265":"New York Times","267":"New York Times","271":"New York Times","273":"New York Times","274":"New York Times","275":"New York Times","276":"New York Times","277":"New York Times Blogs","278":"Washington Post Blogs","279":"New York Times","280":"New York Times","281":"New York Times","282":"New York Times","283":"Washington Post","284":"New York Times","285":"New York Times","286":"Washington Post Blogs","287":"Washington Post","288":"Washington Post Blogs","289":"New York Times","290":"Washington Post Blogs","292":"Washington Post Blogs","294":"New York Times","295":"Washington Post Blogs","296":"New York Times","297":"New York Times","298":"New York Times","300":"Washington Post Blogs","302":"New York Times Blogs","303":"Washington Post","304":"Washingtonpost","306":"New York Times","307":"Washington Post","309":"Washingtonpost","311":"New York Times","312":"Washington Post Blogs","313":"Copyright 1990 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","314":"New York Times","315":"New York Times","316":"New York Times","317":"Wall Street Journal Abstracts","318":"Wall Street Journal Abstracts","319":"Copyright 2000 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","320":"New York Times","322":"New York Times","323":"Wall Street Journal Abstracts","324":"Wall Street Journal Abstracts","325":"Wall Street Journal Abstracts","326":"Wall Street Journal Abstracts","328":"New York Times Blogs","329":"Wall Street Journal Abstracts","330":"Wall Street Journal Abstracts","331":"New York Times","332":"Washington Post Blogs","333":"New York Times","335":"New York Times","336":"Wall Street Journal Abstracts","337":"Wall Street Journal Abstracts","338":"New York Times","339":"Washingtonpost","340":"New York Times","342":"Wall Street Journal Abstracts","343":"Wall Street Journal Abstracts","344":"Wall Street Journal Abstracts","345":"Wall Street Journal Abstracts","346":"New York Times","348":"Wall Street Journal Abstracts","349":"Wall Street Journal Abstracts","350":"New York Times","351":"Wall Street Journal Abstracts","352":"New York Times Blogs","353":"New York Times","354":"Wall Street Journal Abstracts","355":"Wall Street Journal Abstracts","356":"New York Times","357":"New York Times","358":"New York Post","359":"Copyright 1990 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","360":"New York Times","361":"Copyright 1990 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","362":"Washingtonpost","364":"Wall Street Journal Abstracts","365":"Wall Street Journal Abstracts","366":"New York Times","367":"Copyright 1990 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","368":"New York Times","370":"New York Times","371":"New York Times","372":"New York Times Blogs","373":"New York Times","374":"Washington Post","375":"New York Times","376":"Copyright 1995 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","377":"New York Times","378":"Washingtonpost","381":"Washington Post Blogs","382":"Washingtonpost","384":"New York Times","385":"New York Times Blogs","386":"New York Times","387":"Washington Post","388":"Copyright 1990 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","389":"Washingtonpost","390":"Washington Post Blogs","392":"New York Times","393":"Washington Post Blogs","394":"Washingtonpost","395":"Washington Post Blogs","397":"Washington Post","398":"New York Times","399":"New York Times","400":"New York Post","401":"New York Times Blogs","402":"New York Times","403":"New York Times","404":"New York Times","405":"New York Times","406":"New York Times","407":"New York Times","408":"New York Times","409":"Washingtonpost","411":"Washington Post Blogs","412":"New York Times","416":"New York Times","418":"New York Times","419":"Washington Post Blogs","420":"New York Times","421":"New York Times","423":"Washington Post Blogs","424":"New York Times","425":"New York Times","426":"New York Times","428":"New York Times","429":"New York Times","430":"New York Times","431":"Washington Post Blogs","432":"New York Times","433":"New York Times","434":"Washingtonpost","435":"New York Times","437":"Washington Post Blogs","438":"Washington Post Blogs","439":"New York Times","440":"New York Times","441":"Washington Post","443":"Washington Post Blogs","444":"Washington Post","446":"Washington Post Blogs","448":"New York Times","449":"Washingtonpost","450":"New York Times","451":"New York Times","454":"Washington Post Blogs","455":"Washington Post Blogs","457":"New York Times Blogs","458":"New York Times Blogs","460":"New York Times","461":"Washingtonpost","465":"New York Times","466":"New York Times","468":"New York Times","469":"New York Times","471":"Washington Post","473":"New York Times","474":"New York Times","475":"New York Times","476":"New York Times","478":"Washington Post Blogs","479":"Washington Post Blogs","480":"Washington Post Blogs","481":"New York Times","482":"Washington Post","483":"New York Times","485":"New York Times","486":"New York Times","489":"New York Post","490":"New York Times","493":"New York Times Blogs","494":"Washington Post Blogs","495":"New York Post","496":"New York Times","497":"New York Times Blogs","498":"New York Times","499":"Washingtonpost","502":"Washington Post Blogs","503":"New York Times","504":"New York Times","505":"Washington Post Blogs","507":"Washington Post Blogs","509":"Washington Post Blogs","510":"New York Times","511":"New York Times","512":"Washington Post Blogs","513":"New York Times","514":"New York Times","515":"New York Times","516":"New York Times","518":"New York Times","519":"New York Times","520":"New York Times","521":"New York Times","522":"New York Times","523":"Washington Post","524":"New York Times","525":"New York Times","526":"New York Times","527":"New York Times","529":"Washington Post","530":"Washington Post Blogs","531":"Washingtonpost","533":"Washington Post Blogs","536":"New York Times","537":"Washington Post Blogs","538":"New York Times","539":"New York Times Blogs","540":"New York Times","541":"New York Times","542":"New York Times","544":"Washington Post Blogs","545":"Washington Post","546":"New York Times Blogs","547":"New York Times","548":"Washingtonpost","550":"New York Times","551":"New York Times","552":"Washingtonpost","555":"New York Times","556":"New York Times","558":"Washington Post Blogs","560":"New York Times","561":"New York Times","562":"Washington Post Blogs","563":"Washingtonpost","568":"New York Times","569":"New York Times","570":"New York Times Blogs","571":"New York Times","572":"Washington Post Blogs","573":"New York Times","574":"Washington Post","575":"New York Post","576":"New York Times","577":"New York Times","578":"New York Times","579":"New York Times","582":"New York Times","585":"New York Times","586":"New York Times","588":"New York Times","589":"Washingtonpost","590":"New York Times","591":"New York Times","593":"Washington Post","595":"Washington Post","596":"New York Times","597":"New York Times","598":"Washington Post Blogs","600":"New York Times","601":"Washington Post Blogs","603":"Washington Post Blogs","604":"New York Times","605":"New York Times Blogs","606":"New York Times Blogs","608":"New York Times","609":"Washington Post Blogs","611":"Washington Post Blogs","612":"New York Times","613":"New York Times","614":"Washington Post","615":"New York Times","617":"Washingtonpost","619":"Washington Post Blogs","622":"Washington Post Blogs","623":"New York Times","624":"New York Times","625":"Washington Post Blogs","626":"Washington Post Blogs","627":"New York Times","628":"Washington Post Blogs","632":"New York Times","633":"Washington Post","634":"New York Times","635":"Washington Post","636":"Washington Post","637":"New York Times","638":"New York Times","639":"New York Times","640":"New York Times","641":"Washington Post","642":"New York Times","643":"Washington Post Blogs","644":"New York Times","645":"Washingtonpost","647":"New York Times","648":"New York Times","649":"New York Times","651":"Washingtonpost","652":"New York Times Blogs","653":"New York Times","655":"Washington Post Blogs","656":"Washington Post","657":"New York Times Blogs","662":"New York Times","663":"New York Times","665":"New York Times","666":"Washington Post","667":"Washington Post Blogs","668":"New York Times","669":"New York Times","670":"New York Times","671":"New York Times","672":"New York Times","673":"Washington Post Blogs","674":"Washingtonpost","675":"New York Times Blogs","678":"Washingtonpost","679":"New York Times","681":"New York Times","682":"New York Times","683":"Washington Post Blogs","684":"New York Post","685":"Washington Post Blogs","687":"Washington Post Blogs","688":"Washingtonpost","689":"New York Times","691":"New York Times","692":"Washington Post Blogs","693":"Washington Post Blogs","695":"Washington Post","699":"New York Times","701":"Washington Post Blogs","702":"New York Times Blogs","703":"Washington Post Blogs","706":"New York Times","708":"Washington Post Blogs","709":"Washington Post Blogs","711":"New York Times Blogs","713":"New York Times","714":"New York Times","715":"Washington Post Blogs","716":"New York Times","717":"Washington Post","718":"Washington Post Blogs","719":"New York Times","721":"New York Times","722":"Washington Post","723":"Washington Post Blogs","726":"New York Times","728":"New York Times","729":"New York Times","730":"Wall Street Journal Abstracts","731":"Wall Street Journal Abstracts","732":"Wall Street Journal Abstracts","733":"New York Times","734":"Wall Street Journal Abstracts","735":"Washington Post BlogsDC Sports Bog","736":"Wall Street Journal Abstracts","737":"Washington Post Blogs","738":"Wall Street Journal Abstracts","739":"Wall Street Journal Abstracts","740":"Wall Street Journal Abstracts","741":"Wall Street Journal Abstracts","742":"New York Times Blogs","743":"Wall Street Journal Abstracts","744":"Wall Street Journal Abstracts","745":"Wall Street Journal Abstracts","746":"Wall Street Journal Abstracts","747":"Wall Street Journal Abstracts","748":"New York Times Blogs","749":"Washington Post","750":"Wall Street Journal Abstracts","751":"Wall Street Journal Abstracts","752":"Wall Street Journal Abstracts","753":"Wall Street Journal Abstracts","754":"Wall Street Journal Abstracts","755":"Wall Street Journal Abstracts","756":"Wall Street Journal Abstracts","757":"Wall Street Journal Abstracts","758":"New York Times","759":"New York Times","760":"Washington Post Blogs","761":"Wall Street Journal Abstracts","762":"Wall Street Journal Abstracts","763":"Wall Street Journal Abstracts","764":"Wall Street Journal Abstracts","765":"Wall Street Journal Abstracts","766":"Wall Street Journal Abstracts","767":"Wall Street Journal Abstracts","768":"Wall Street Journal Abstracts","769":"Wall Street Journal Abstracts","770":"Wall Street Journal Abstracts","771":"New York Times","772":"New York Times","773":"New York Times Blogs","774":"New York Times","775":"Washington Post Blogs","776":"Wall Street Journal Abstracts","777":"Wall Street Journal Abstracts","778":"Wall Street Journal Abstracts","779":"Wall Street Journal Abstracts","780":"Wall Street Journal Abstracts","781":"Wall Street Journal Abstracts","782":"Wall Street Journal Abstracts","783":"Wall Street Journal Abstracts","784":"Wall Street Journal Abstracts","785":"Wall Street Journal Abstracts","786":"Wall Street Journal Abstracts","787":"Copyright 1986 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","790":"Wall Street Journal Abstracts","791":"Wall Street Journal Abstracts","792":"Wall Street Journal Abstracts","793":"Wall Street Journal Abstracts","794":"Wall Street Journal Abstracts","795":"Wall Street Journal Abstracts","796":"Wall Street Journal Abstracts","797":"Wall Street Journal Abstracts","799":"New York Times","800":"Washington Post Blogs","801":"Wall Street Journal Abstracts","802":"Wall Street Journal Abstracts","803":"Wall Street Journal Abstracts","804":"Wall Street Journal Abstracts","805":"Wall Street Journal Abstracts","806":"Wall Street Journal Abstracts","807":"Copyright 1988 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","808":"New York Times","809":"Washington Post","811":"Wall Street Journal Abstracts","812":"Wall Street Journal Abstracts","813":"Wall Street Journal Abstracts","814":"Wall Street Journal Abstracts","815":"Wall Street Journal Abstracts","816":"Wall Street Journal Abstracts","817":"Wall Street Journal Abstracts","818":"Wall Street Journal Abstracts","819":"Wall Street Journal Abstracts","820":"Wall Street Journal Abstracts","821":"Wall Street Journal Abstracts","822":"New York Times","823":"Washington Post Blogs","824":"Washington Post Blogs","825":"Wall Street Journal Abstracts","826":"Wall Street Journal Abstracts","827":"Wall Street Journal Abstracts","828":"Wall Street Journal Abstracts","829":"Wall Street Journal Abstracts","830":"Wall Street Journal Abstracts","831":"Wall Street Journal Abstracts","832":"Wall Street Journal Abstracts","833":"Wall Street Journal Abstracts","835":"Washingtonpost","836":"Wall Street Journal Abstracts","837":"Wall Street Journal Abstracts","838":"Wall Street Journal Abstracts","839":"Wall Street Journal Abstracts","840":"Wall Street Journal Abstracts","841":"Wall Street Journal Abstracts","842":"Wall Street Journal Abstracts","843":"Wall Street Journal Abstracts","844":"Wall Street Journal Abstracts","845":"New York Times","847":"New York Times","848":"Wall Street Journal Abstracts","849":"Wall Street Journal Abstracts","850":"Wall Street Journal Abstracts","851":"Wall Street Journal Abstracts","852":"Copyright 1993 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","853":"Copyright 1993 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","854":"New York Times","855":"New York Times Blogs","856":"Wall Street Journal Abstracts","857":"Wall Street Journal Abstracts","858":"New York Times","859":"Los Angeles Times","861":"New York Times","862":"Wall Street Journal Abstracts","863":"Wall Street Journal Abstracts","864":"Wall Street Journal Abstracts","865":"Wall Street Journal Abstracts","866":"Wall Street Journal Abstracts","867":"Wall Street Journal Abstracts","868":"Wall Street Journal Abstracts","869":"Wall Street Journal Abstracts","870":"Wall Street Journal Abstracts","871":"Wall Street Journal Abstracts","872":"Wall Street Journal Abstracts","873":"Copyright 1998 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","874":"Copyright 1993 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","875":"New York Times","876":"Washington Post Blogs","877":"Washingtonpost","878":"Wall Street Journal Abstracts","879":"Wall Street Journal Abstracts","880":"Wall Street Journal Abstracts","881":"Wall Street Journal Abstracts","882":"Wall Street Journal Abstracts","883":"Copyright","884":"Copyright 1994 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","885":"New York Times Blogs","887":"Wall Street Journal Abstracts","888":"Wall Street Journal Abstracts","890":"Wall Street Journal Abstracts","891":"Wall Street Journal Abstracts","892":"Wall Street Journal Abstracts","893":"WALL STREET JOURNAL ABSTRACTS","894":"Copyright 1985 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","896":"Wall Street Journal Abstracts","897":"Wall Street Journal Abstracts","898":"Wall Street Journal Abstracts","899":"Wall Street Journal Abstracts","900":"Wall Street Journal Abstracts","901":"Wall Street Journal Abstracts","902":"Wall Street Journal Abstracts","903":"Wall Street Journal Abstracts","904":"New York Times","905":"Wall Street Journal Abstracts","906":"Wall Street Journal Abstracts","907":"Wall Street Journal Abstracts","908":"Wall Street Journal Abstracts","909":"Wall Street Journal Abstracts","910":"Wall Street Journal Abstracts","911":"Wall Street Journal Abstracts","912":"Wall Street Journal Abstracts","913":"Wall Street Journal Abstracts","914":"Wall Street Journal Abstracts","915":"Wall Street Journal Abstracts","916":"Copyright 1991 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","917":"Copyright 1989 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","918":"New York Times","919":"New York Times","920":"New York Times Blogs","921":"New York Times","922":"Wall Street Journal Abstracts","923":"Wall Street Journal Abstracts","924":"Wall Street Journal Abstracts","925":"Wall Street Journal Abstracts","926":"Wall Street Journal Abstracts","927":"Wall Street Journal Abstracts","928":"Copyright 1992 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","929":"Washington Post Blogs","930":"Washington Post Blogs","931":"New York Times","932":"Wall Street Journal Abstracts","933":"Copyright 1991 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","934":"New York Times","935":"Washington Post Blogs","936":"Washington Post Blogs","937":"Washington Post Blogs","938":"Los Angeles Times","939":"Wall Street Journal Abstracts","940":"Wall Street Journal Abstracts","941":"Wall Street Journal Abstracts","942":"Copyright 1985 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","943":"Washington Post Blogs","947":"Washington Post Blogs","948":"Washington Post","949":"Wall Street Journal Abstracts","950":"New York Times","951":"New York Times","952":"New York Times","953":"Wall Street Journal Abstracts","954":"Wall Street Journal Abstracts","955":"Wall Street Journal Abstracts","956":"Wall Street Journal Abstracts","957":"Wall Street Journal Abstracts","958":"Wall Street Journal Abstracts","960":"Wall Street Journal Abstracts","961":"Wall Street Journal Abstracts","962":"Wall Street Journal Abstracts","963":"Wall Street Journal Abstracts","964":"Wall Street Journal Abstracts","965":"Wall Street Journal Abstracts","967":"New York Times Blogs","968":"Washington Post","969":"Wall Street Journal Abstracts","970":"New York Times Blogs","971":"New York Times Blogs","972":"New York Times","973":"New York Times Blogs","974":"Washingtonpost","975":"Wall Street Journal Abstracts","976":"Wall Street Journal Abstracts","977":"Wall Street Journal Abstracts","978":"Copyright 1990 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","979":"New York Times","981":"Wall Street Journal Abstracts","982":"Wall Street Journal Abstracts","983":"Copyright 1990 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","984":"New York Times Blogs","986":"New York Times","987":"Washington Post Blogs","988":"New York Times Blogs","990":"Wall Street Journal Abstracts","991":"Wall Street Journal Abstracts","992":"Wall Street Journal Abstracts","994":"Wall Street Journal Abstracts","995":"Wall Street Journal Abstracts","996":"Copyright 2002 New York Times Company: Abstracts\u00a0\u00a0WALL STREET JOURNAL","998":"New York Times","999":"Wall Street Journal Abstracts"},"title":{"0":"Setbacks for Artificial Intelligence\n","1":"Google's Eric Schmidt downplays fears over artificial intelligence; Some say it could destroy civilization. He thinks it'll be one of the largest forces for good ever.\n","2":"The (More) Real Threat Posed by Powerful Computers\n","3":"Machine See, Machine Do\n","4":"Still a Long Way From Checkmate\n","5":"Elon Musk: 'With artificial intelligence we are summoning the demon.'; Tesla's CEO warns that artificial intelligence is likely mankind's biggest threat.\n","6":"Artificial Intelligence Is IBM's Ace in the Hole\n","7":"Computer Conference Finds 'Artificial Intelligence' Hot\n","8":"I.B.M. ENLISTS CARNEGIE-MELLON'S AID ON SOFTWARE\n","9":"How to Regulate Artificial Intelligence\n","10":"Study to Examine Effects of Artificial Intelligence\n","11":"SELLING ARTIFICIAL INTELLIGENCE\n","12":"IBM Gives Watson's Artificial Intelligence a New Challenge: Filing Taxes\n","13":"The tech adviser from 'Minority Report' isn't buying the fears over artificial intelligence; The technology adviser to the film \"Minority Report,\" explains why he thinks concerns are overhyped.\n","14":"Is China Outsmarting America in Artificial Intelligence?\n","15":"Google's artificial intelligence mastermind responds to Elon Musk's fears; Demis Hassabis says we're \"many, many decades\" away from any technology we should worry about.\n","16":"Uber Bets on Artificial Intelligence With Acquisition and New Lab\n","17":"Artificial Intelligence For the New Millennium; A Revolution More Bland Than Kubrick's '2001'\n","18":"Computer Makers Push Artificial Intelligence\n","19":"Marvin Minsky of M.I.T. Is Dead at 88; Early Explorer of Artificial Intelligence\n","20":"Bill Gates on dangers of artificial intelligence: 'I don't understand why some people are not concerned'; During a Reddit AMA, Gates discussed the future of technology and sounded the alarm about AI\n","21":"Artificial Intelligence as a Threat\n","22":"David Ferrucci: Life After Watson\n","23":"Robots and Us\n","25":"Not Close to Human, Artificial Intelligence Still Raises Concerns\n","26":"When Is the Singularity? Probably Not in Your Lifetime; Misconceptions\n","27":"Why the world's most intelligent people shouldn't be so afraid of artificial intelligence; Dystopian views of artificial intelligence focus too much on risk, not enough on reward.\n","28":"Teaching Activities for: 'Tech Giants Are Paying Huge Salaries for Scarce A.I. Talent'; Article of the Day\n","29":"Microsoft Reorganizes Its Research Into A.I.\n","30":"Behind Artificial Intelligence, a Squadron of Bright Real People \n","31":"Facebook's Yann LeCun Discusses Digital Companions and Artificial Intelligence (and Emotions)\n","32":"Allen Newell, 65; Scientist Founded A Computing Field\n","33":"Handicapping the Half-Life of 'Big Data'\n","34":"Great Strides for Deep Learning\n","36":"Pursuing the Next Level Of Artificial Intelligence\n","37":"What the debacle of climate change can teach us about the dangers of artificial intelligence\n","38":"IT Digest: Artificial intelligence studied\n","39":"Most Americans See Artificial Intelligence as a Threat to Jobs (Just Not Theirs)\n","40":"Ray Solomonoff, Pioneer in Artificial Intelligence, Dies at 83\n","41":"Our evolving battle to stay ahead of our intelligent machines\n","42":"China's Intelligent Weaponry Gets Smarter\n","43":"IDEAS AND TRENDS: Can Machines Learn to Think?;The Artificial Intelligence Industry Is Retrenching\n","44":"A CONVERSATION: With Dr. Marvin Minsky;Why Isn't Artificial Intelligence More Like the Real Thing?\n","45":"Daily Report: A $1 Billion Bet on Artificial Intelligence in Silicon Valley\n","46":"FORD ACQUIRES A STAKE IN ARTIFICIAL INTELLIGENCE\n","48":"Artificial-Intelligence Research Center Is Founded by Silicon Valley Investors\n","49":"A.I. May Book Your Next Trip (With a Little Human Assistance)\n","51":"Redefining the Power of the Gamer\n","52":"Big tech names put $1 billion toward making artificial intelligence harmless\n","53":"Daily Report: AlphaGo Shows How Far Artificial Intelligence Has Come\n","54":"The great artificial intelligence gamble that finally paid off\n","55":"Innovators of Intelligence Look to Past\n","56":"Devising Real Ethics for Artificial Intelligence\n","57":"Obituaries\n","59":"WASHINGTON FORMULATES A COMPUTER AGENDA\n","60":"China's Blitz to Dominate A.I.\n","61":"Paid Notice: Deaths BOBROW, DANIEL (DANNY) G.\n","62":"A Turing Award for Helping Make Computers Smarter\n","64":"Artificial Intelligence\n","65":"Machine Masters Man in Complex Game of Go\n","66":"The Man Who Helped Make Canada a High-Tech Hotbed\n","68":"Intelligent Software Finding Niche\n","69":"Artificial Intelligence announcement marks a pretty big shift for Apple\n","71":"Computer wizard credited with coining 'artificial intelligence'\n","73":"How does it know that?\n","74":"Ray Solomonoff, 83; Made Machines Think\n","75":"I.B.M. Puts Watson to Work in Business\n","76":"The AI of science fiction just got one step closer\n","78":"Careers;Philosophy Majors in Demand\n","82":"Elon Musk and Stephen Hawking Among Hundreds to Urge Ban on Military Robots\n","83":"Casting Their Neural Nets\n","84":"IDEAS & TRENDS;A Split in Thinking Among Keepers of Artificial Intelligence\n","87":"Elon Musk's fear of Terminators just netted researchers $10 million; The Tesla and SpaceX chief is worried about malicious artificial intelligence.\n","88":"Facebook is using AI to try to prevent suicides\n","90":"Pentagon's Top Technologist Joins I.B.M.\n","91":"Free Course in Artificial Intelligence Is Offered Online, and 58,000 Sign Up\n","92":"CRITIC'S CHOICES;Theater\n","93":"Protecting Humans and Jobs From Robots Is 5 Tech Giants' Goal\n","94":"Google Unveils Phones and Other Gadgets With a Spotlight on A.I.\n","95":"John McCarthy, 84, Dies; Computer Design Pioneer\n","98":"Kenneth Colby, 81, Psychiatrist Expert in Artificial Intelligence\u00a0\n","99":"Lagging Rivals in A.I., Apple Adds A Top Google Executive to Its Team\n","100":"Deep Blue Can't Triumph in the Game of Life;Artificial Intelligence\n","101":"Outing A.I.: Beyond the Turing Test\n","102":"Ay Robot! Scientists Worry Machines May Outsmart Man\n","103":"Think humans are superior to AI? Don't be a 'carbon chauvinist'; Someday software itself is likely to experience its own 'thereness,' expert says.\n","105":"Our very real fascination with artificial intelligence\n","106":"Daily Report: An Industry's Center of Influence Shifts; Bits\n","107":"Q&A\n","109":"Optimism as Artificial Intelligence Pioneers Reunite\n","110":"Tech Roundup: Will Robots Replace Lawyers?; Bits\n","111":"Learning Curve: No Longer Just A Human Trait\n","112":"Facebook Will Use Artificial Intelligence to Uncover Extremist Posts\n","115":"5 classic Atari games that totally stump Google's artificial intelligence algorithm; Its cutting-edge artificial intelligence still struggles with these five Atari games.\n","116":"Toyota Aims to Make Car a Co-Pilot for Drivers\n","117":"Artificial intelligence could cost millions of jobs. The White House says we need more of it.\n","120":"Toyota Planning an Artificial Intelligence Research Center in California\n","121":"Robot Weapons: What's the Harm?\n","122":"IDEAS AND TRENDS: Can Machines Learn to Think?;Pentagon Plan: The Battle to Mechanize the Military Mind\n","123":"Google Promotes A.I. but Acknowledges Technology's Perils\n","124":"Apple co-founder on artificial intelligence: 'The future is scary and very bad for people'; Joining the likes of Stephen Hawking and Bill Gates, Steve Wozniak has sounded the alarm on A.I.\n","125":"Neural Nets and Human Experts\n","126":"Reliant on a computer, Hawking worries about artificial intelligence\n","129":"SMART MACHINES GET SMARTER\n","130":"Technology;The Computer As Deal Maker\n","131":"A Lawyer Minus the Briefcase\n","132":"Hubert L. Dreyfus, Who Put Computing In Its Place, Dies at 87\n","134":"Now Blooming: Digital Models; 2 Students Offer Futuristic Alternatives To Traditional Peak Blossom Forecasts\n","135":"The Coming Superbrain\n","136":"A New Company to Focus On Artificial Intelligence\n","137":"The First Church of Robotics\n","141":"PERSONAL COMPUTERS;IN SEARCH OF THE THINKING MACHINE-STILL\n","142":"Intelligence Too Big for a Single Machine\n","143":"Why Artificial Intelligence Software Is Booming\n","144":"CREATING COMPUTERS THAT THINK\n","145":"Google, Looking to Tiptoe Back Into China, Announces A.I. Center\n","146":"Google Buys A.I. Company for Search, Not Robots\n","147":"Technology;Chips to Spur Intelligence\n","148":"The Washington Post to use artificial intelligence to cover nearly 500 races on Election Day\n","149":"Oliver Selfridge, an Early Innovator In Artificial Intelligence, Dies at 82\n","150":"Leave Artificial Intelligence Alone\n","151":"Brainy Robots Start Stepping Into Daily Life\n","152":"Why Apple is struggling to become an artificial-intelligence powerhouse\n","153":"THEY CAN THINK, BUT CAN THEY DREAM?\n","154":"The top four global stories of 2017 you might have missed\n","155":"Stage: Comedy Group, 'Artificial Intelligence'\n","156":"WHAT EXACTLY IS ARTIFICIAL INTELLIGENCE, ANYWAY?\n","157":"When A.I. Matures, It May Call J\u00fcrgen Schmidhuber 'Dad'\n","159":"Stephen Hawking says that 'aggression,' humanity's greatest vice, will destroy civilization; Hawking has a long list of things that could wipe out the human race, including aliens, machines, and ourselves.\n","160":"Artificial intelligence can unleash 'revolution' in warfare, tech leaders warn\n","162":"Firms Pit Artificial Intelligence Against Hacking Threats\n","163":"Computer Scientists Are Astir After Baidu Team Is Barred From A.I. Competition\n","164":"A.I. Today May Underwhelm, but Before Long It May Overtake Expectations\n","166":"Samsung takes a big leap and buys artificial intelligence start-up founded by Siri's creators; The creator of Apple's Siri is now joining forces with Apple's rival, Samsung.\n","168":"The Pentagon Looks to Silicon Valley for Help on Artificial Intelligence\n","169":"Artificial intelligence will change America. Here's how.; Our society is headed into a perfect storm where both physical labor and knowledge labor are equally under threat.\n","170":"EMERGING CAREERS;IT'S THE REAL THING: ARTIFICIAL INTELLIGENCE\n","171":"Five Technologies That Will Rock Your World\n","172":"The Future of Computing\n","173":"Review: 'Marjorie Prime' and Jon Hamm: Ceaselessly Into the Past\n","175":"Saul Amarel, 74, an Innovator In the Artificial Intelligence Field \n","176":"Artificial Intelligence: Thinking About Thinking;INTO THE HEART OF MIND, An American Quest for Artificial Intelligence.  By Frank Rose.  Harper & Row.  209 pp.  $15.95.\n","177":"Executives from Microsoft, IBM take part in The Washington Post's Transformers: AI event; Speakers will discuss how AI, machine learning could impact public policy, business and society.\n","178":"Apple's announcement on artificial intelligence is a big shift for the company; The company has never done this before.\n","179":"Facebook can recognize you even if your face isn't visible\n","180":"Allen Newell, Pioneering Researcher, Dies\n","182":"Start-Up Lessons in Speed and Practicality From the Once-Again Hot Field of A.I.\n","183":"Liberal arts aim to give computer science a reboot\n","185":"Our Robots, Ourselves\n","186":"A.I. Answers Questions From SAT Correctly\n","187":"Hewlett to Give To Universities\n","188":"Robots Exit the Classroom And Enter the Real World\n","189":"PERSONAL COMPUTERS;Entrepreneurial Sharks Circling Around Artificial Intelligence\n","190":"New Tools Are Needed to Track Technology's Impact on Jobs, Panel Says\n","191":"Intelligent Beings in Space!\n","192":"Larry Summers: The robots are coming, whether Trump's Treasury secretary admits it or not; \"Secretary Mnuchin's comment about the lack of impact of technology on jobs is to economics approximately what global climate change denial is to atmospheric science or what creationism is to biology.\"\n","193":"Looking Beyond Silicon to Squeeze More Out of Chips\n","195":"Future wars may depend as much on algorithms as on ammunition, report says.; Pentagon boosts spending on artificial intelligence, big data and powerful computers.\n","196":"Elon Musk's nightmarish warning: AI could become 'an immortal dictator from which we would never escape'; The billionaire has compared the adoption of artificial intelligence to \"summoning the devil.\"\n","197":"Canada Tries to Cash In on a Wealth of A.I. Leaders\n","198":"Where the Smarts Start;Narrowing the Gap on Artificial Intelligence\n","199":"Busting the Myths About Artificial Intelligence\n","200":"Moving Into Hardware, Google Introduces Pixel, a Smartphone of Its Own\n","201":"Facebook Offers Artificial Intelligence Tech to Open Source Group\n","202":"'HUMAN' COMPUTER IS COMING OF AGE\n","203":"AI can now muddle its way through the math SAT about as well as you can\n","204":"ARTIFICIAL INTELLIGENCE\n","205":"The 9.20.15 Issue\n","206":"Artificial Intelligence's White Guy Problem; Opinion\n","207":"IBM Adds Medical Images to Watson, Buying Merge Healthcare for $1 Billion\n","208":"Elon Musk's nightmare: A Google robot army annihilating mankind; The Tesla CEO says his friend Larry Page may be building just such an army by accident.\n","209":"IBM prepares for machine-vs.-man 'Jeopardy!' showdown\n","210":"BYLINE: Nancy Szokan\n","212":"Intel Protects Its Lead While Pivoting to A.I.\n","213":"Creating Artificial Intelligence Based on the Real Thing\n","215":"Robert Tannenbaum Management ...\n","216":"The Philosophy of 'Her'\n","217":"Mind Over Matter; Is the human brain a beautifully calibrated computer?\n","218":"Mark Zuckerberg builds an AI assistant to run his house - and entertain his toddler\n","219":"Next step in artificial intelligence: Connecting the car and the home\n","221":"Tariffs alone won't bring back jobs; The sooner we acknowledge the intricacies of our employment reality, the better off we will be.\n","222":"Why Mnuchin should prepare for the robots\n","225":"What '2001' Got Right; Op-Ed Contributor\n","226":"Movie Tips From Your Robot Overlords\n","228":"Coveted A.I. Expert Quits a Chinese Search Giant\n","229":"Ava Is Just Sci-Fi (for Now)\n","230":"Musk, Grimes connect over AI\n","231":"No HAL Yet: Artificial Intelligence Visions Underestimated the Mind\n","232":"IDEAS AND TRENDS: Can Machines Learn to Think?;A Parable of Computers And Brains\n","233":"Fulfilling Watson's Promise\n","236":"IDEAS & TRENDS;A CONCERTO FOR ARTIFICIAL INTELLIGENCE\n","238":"Morning Agenda: Masayoshi Son Warns of the Singularity\n","239":"The Real Threat of Artificial Intelligence\n","241":"Who's that royal? Sky News will use artificial intelligence to ID guests at Prince Harry and Meghan Markle's wedding.; As luminaries make their entrance to the royal wedding, online viewers watching Sky News will learn who's who with on-screen captions powered by facial recognition technology.\n","242":"David L. Waltz, Computer Science Pioneer, Dies at 68\n","243":"Web May Hold the Key to Achieving Artificial Intelligence\n","244":"At Mars, Jeff Bezos Hosted Roboticists, Astronauts, Other Brainiacs and Me\n","245":"Pentagon Courts Tech for Robotic Weaponry\n","246":"How Efficiency Is Wiping Out the Middle Class; Another View\n","247":"New Research Center to Explore Ethics of Artificial Intelligence\n","248":"Alphabet Program Beats the European Human Go Champion\n","249":"Don't Fear the Robots\n","250":"As Artificial Intelligence Evolves, So Does Its Criminal Potential\n","251":"Facebook's A.I. Growth Squeezes Universities\n","252":"An Advance in Artificial Intelligence Rivals Human Vision Abilities\n","253":"Rocket Fuel Prices I.P.O. at Top of Revised Range\n","254":"Box gets a boost AI firm a No. 1 idea at Sohn confab\n","255":"Software Tutors Offer Help and Customized Hints\n","256":"China Sets Goal to Lead In Artificial Intelligence\n","257":"Five myths about artificial intelligence\n","258":"Elon Musk doesn't think we're prepared to face humanity's biggest threat: Artificial intelligence; The Tesla founder has been sounding the alarm for years, but he says most people won't listen until they see \"robots going down the streets killing people.\"\n","260":"Can artificial intelligence help you find an outfit? Macy's is giving it a try.Can artificial intelligence help you find an outfit? Macy's is giving it a try.; The department store chain is teaming up with IBM Watson to provide customer service in its stores.\n","261":"Google's A.I. Program Rattles Chinese Go Master as It Wins Match\n","262":"Yes, Computers Can Think\n","263":"Computer beats world's best Go player - again\n","265":"Paul Allen Wants to Teach Machines Common Sense\n","267":"Building the Hospital of the Future\n","271":"Soaring Salaries In A.I. Research\n","273":"Meet Your Next Travel Agent\n","274":"Tech Roundup: Canada Spends to Keep A.I. Experts Home; Bits\n","275":"Computers Jump to the Head of the Class\n","276":"Researchers Enlist Artificial Intelligence to Fight Tax Evasion\n","277":"Cisco's New C.E.O. Envisions Big Changes\n","278":"The treasury secretary is 'not worried at all' about robots taking jobs. Here's why he could be wrong.; Treasury Secretary Steven Mnuchin said automation is 50 to 100 years away. Many technologists disagree with that assessment.\n","279":"Silicon Valley Swoons Over Artificial Intelligence\n","280":"Is the Populist Revolt Over? Not if Robots Continue Their March\n","281":"GAME THEORY; Tracking an Elusive Film Game Online\n","282":"Data Could Be the Next Tech Hot Button for Regulators\n","283":"A quantum leap for artificial intelligence\n","284":"N.F.L. Salaries for A.I. Talent\n","285":"A Future of Tech Magic\n","286":"Twitter earnings: Three top take-aways for those who tweet\n","287":"Innovator in Artificial Intelligence Created 'Pleasing' Pastel Portraits\n","288":"This is where the real action in artificial intelligence takes place\n","289":"A.I. Inspiration: The Science Fiction That Frames Discussion\n","290":"What AlphaGo's sly move says about machine creativity; Google's machine is leaving the smartest humans in the dust.\n","292":"Why an AI takeover may not be a bad thing\n","294":"Jobs Created And Displaced\n","295":"Elon Musk, Neil deGrasse Tyson laugh about artificial intelligence turning the human race into its pet labrador\n","296":"An Electronic Cop That Plays Hunches \n","297":"Musk, Escalating Feud, Backs #DeleteFacebook By Pulling Companies\n","298":"A Smarter Computer To Pick Stock\n","300":"Stop the rise of the 'killer robots,' warn human rights advocates; It used to be science fiction. These days, it is more science than fiction.\n","302":"Google Offers Free Software in Bid to Gain an Edge in Machine Learning\n","303":"LENGTH: 941 words\n","304":"Google: Artificial intelligence will help personalize its popular tools\n","306":"Sci-Fi That Feels Close to Home\n","307":"Profits in Artificial Intelligence\n","309":"BYLINE: Reuters\n","311":"Ray Kurzweil on How We'll End Up Merging With Our Technology; Nonfiction\n","312":"An alien cyberattack? As if we didn't have enough to worry about.; A pair of astrophysicists warn aliens could hack the planet.\n","313":"LENGTH: 169 words\n","314":"Good News: A.I. Is Getting Cheaper. That's Also Bad News.\n","315":"I.B.M. Computer Program to Take On 'Jeopardy!'\n","316":"Smart Robots Make Strides, but There's No Need to Flee Just Yet\n","317":"ARTIFICIAL INTELLIGENCE SHOWS ITS HAND\n","318":"ARTIFICIAL INTELLIGENCE HAS A WAY TO GO\n","319":"LENGTH: 47 words\n","320":"Smarter Than Us? Who's Us?\n","322":"Computer Wins On 'Jeopardy!': Trivial, It's Not\n","323":"AI TOOLS TARGET MENTAL HEALTH\n","324":"SEVEN JOBS ROBOTS WILL CREATE-OR EXPAND\n","325":"WILL ARTIFICIAL INTELLIGENCE DOOM DUMB FILMS?\n","326":"PROCEEDINGS\n","328":"Artificial Intelligence Draws New Connections for Personalization\n","329":"INTEL DEAL SHOWS PULL OF ARTIFICIAL INTELLIGENCE\n","330":"ARTIFICIAL INTELLIGENCE COMES TO YOUR INBOX\n","331":"Silicon Valley's Artificial Intelligence Marathon Is On\n","332":"New artificial intelligence promises to make travel a little smarter. Does it?; For travel agents, the rise of the machines is real.\n","333":"A Second Home for IBM's Watson, in Silicon Valley\n","335":"BOOKS OF THE TIMES;Making Electronics Mimic Biology\n","336":"CAN ARTIFICIAL INTELLIGENCE BEAT MOM'S COOKING?\n","337":"A TALENT WAR IN ARTIFICIAL INTELLIGENCE\n","338":"Sunday May 5, 1996: BUG-EYED;The Bionic Cockroach\n","339":"Zuckerberg says AI will solve Facebook's problems, but not how or when\n","340":"Putting A Smile On Sober Science\n","342":"ARTIFICIAL INTELLIGENCE IS MONITORING, AND EVALUATING, POTENTIAL BORROWERS\n","343":"WHAT I LEARNED FROM BUILDING MY OWN CHATBOT\n","344":"MORE SPACE TO DISPLAY ARTIFICIAL INTELLIGENCE\n","345":"COMPUTER SCIENTIST COINED 'ARTIFICIAL INTELLIGENCE'\n","346":"The Machine vs. the 'Jeopardy!' Champs\n","348":"CHINA RACES TO TAP ARTIFICIAL INTELLIGENCE\n","349":"GOOGLE TAPS AI CHIEF TO REPLACE DEPARTING SEARCH-ENGINE HEAD\n","350":"A Smarter Phone Becomes a Personal Assistant\n","351":"CURIOSITY IS A NEW POWER IN ARTIFICIAL INTELLIGENCE\n","352":"Google Acquires British Artificial Intelligence Developer\n","353":"COMPANY NEWS;Hewlett Pact With Hitachi\n","354":"TESLA BOSS WARNS ON ARTIFICIAL INTELLIGENCE\n","355":"ARTIFICIAL INTELLIGENCE MASTERS THE GAME OF GO\n","356":"The Race to Control Artificial Intelligence, and Tech's Future\n","357":"Human or Computer? Take This Test \n","358":"A.I. won't turn rogue: Schmidt\n","359":"LENGTH: 65 words\n","360":"Is That Machine After Your Job?\n","361":"LENGTH: 43 words\n","362":"Among a D.C. start-up's investors:Singapore and the Winklevoss twins\n","364":"THE ARTIFICIAL-INTELLIGENCE INVESTOR\n","365":"DOES ARTIFICIAL INTELLIGENCE POSE A THREAT?\n","366":"Daily Report: When Artificial Intelligence Goes to the Dark Side\n","367":"LENGTH: 27 words\n","368":"The 12.18.16 Issue\n","370":"Government Watchdog: Software That Sniffs \n","371":"Robot Overlords? Maybe Not\n","372":"6 Q's About the News | Relax, the Terminator Is Far Away\n","373":"The Stage: 'Tony 'n' Tina's Wedding'\n","374":"The depths of loss, lit by a biting, brilliant humor\n","375":"A Reality Check for A.I.\n","376":"LENGTH: 52 words\n","377":"How to Make A.I. Human-Friendly\n","378":"Why Apple and Google should worry about Facebook's new bot store\n","381":"Why Stephen Hawking believes the next 100 years may be humanity's toughest test; The theoretical physicist outlined more disaster scenarios in a recent lecture.\n","382":"Artificial intelligence goes out to play\n","384":"COMPUTER SYSTEMS APPLYING EXPERTISE\n","385":"The Future of High-Tech Health Care - and the Challenge\n","386":"IN SHORT: SCIENCE & TECHNOLOGY\n","387":"Robotic Menace\n","388":"LENGTH: 36 words\n","389":"Scroll over, Beethoven\n","390":"Stephen Hawking just gave humanity a due date for finding another planet\n","392":"Artificial Intelligence for the Next War\n","393":"AI will spell the end of capitalism\n","394":"Poker's life lessons\n","395":"What Silicon Valley gets wrong about universal basic income\n","397":"MIT professor a key figure in study of artificial intelligence\n","398":"COMPANY NEWS;Xerox Spins Off Intelligence Unit\n","399":"Robocalypse Now? Central Bankers Argue Whether Automation Will Kill Jobs\n","400":"Your brain on Musk\n","401":"Google Buys a Quantum Computer\n","402":"Google's AlphaGo Defeats Chinese Go Master in Win for A.I.\n","403":"Why A.I. and Cryptocurrency Are Making One Type of Computer Chip Scarce\n","404":"The Executive Computer;Hands Off the Keys, Noses Into Books\n","405":"FILM; Mother Love, Too Little Or Too Much\n","406":"Can Robots Become Conscious? \n","407":"NEWS WATCH;Artificial Intelligence Program Tries Its Hand at Cards\n","408":"BUSINESS PEOPLE;A New Chief Executive Is Named at Symbolics\n","409":"Google\n","411":"A Magna Carta for the digital age\n","412":"These Days, Moon Shots Are Domain Of the Valley\n","416":"A Top Engineer for Search Is Set to Exit Google\n","418":"Approximating Life \n","419":"Was this created by a human or computer? See if you can tell the difference.\n","420":"Microsoft Tries a New Role: Moral Leader\n","421":"Former Executive on IBM's Watson to Start Own A.I. Firm\n","423":"The amazing artificial intelligence we were promised is coming, finally\n","424":"Robots Will Take Jobs, but Not as Fast as Some Fear, New Report Says\n","425":"Machine Bests Man in Go Series, Winning 4\n","426":"Cryptography Pioneers Win Turing Award\n","428":"Canada Becomes a Magnet for Tech Talent\n","429":"Fooled You\n","430":"Recognizing the Artifice in Artificial Intelligence\n","431":"Siri answers your questions. This bot will understand your emotions.\n","432":"Google Is Making Its Special A.I. Chips Available to Others via Cloud Computing\n","433":"Herbert A. Simon Dies at 84; Won a Nobel for Economics \n","434":"Rethinking artificial intelligence\n","435":"Washington Talk: Briefing;Taxes and Intelligence\n","437":"'Chappie': Not much intelligence here, artificial or otherwise; South African-set action thriller is a loud and violent video game featuring a wascally widdle wobot.\n","438":"Will Smart Computers Be ALIVE?\n","439":"Donald Michie, 83, and Anne McLaren, 80, Scientists\n","440":"Silicon Valley Looks to Artificial Intelligence for the Next Big Thing\n","441":"Kenneth Colby; Psychiatrist, Computer Scientist\n","443":"Shake-up at Facebook highlights tension in race for AI; Facebook, looking to artificial intelligence to help solve its problem, is racing to keep up with rivals.\n","444":"Computer Programmer Joseph Weizenbaum\n","446":"How Facebook wants to help us avoid drunk-posting embarrassing secrets\n","448":"Pushing A.I. Boundaries in China\n","449":"'Smart' cities: Where problems are detected and solved in real time\n","450":"How a Silicon Valley Firm Is Aiding China's Ambitions\n","451":"A CONVERSATION WITH: ANNE FOERST;Do Androids Dream? M.I.T. Working on It\n","454":"China has now eclipsed us in AI research\n","455":"Your car wants to say hello. And that's only the start.; Toyota and Honda are developing empathetic cars that want to be there for you.\n","457":"The Problem With Google's 'Star Trek' Computer\n","458":"Google Artificial Intelligence Beats Expert at Go Game\n","460":"COMEDY: 4 ACTS IN 5 HOURS\n","461":"Artificial intelligence in credit cards saves you from faux-fraud stupidity\n","465":"Daily Report: Setting the Ethical Rules of A.I.\n","466":"Charmed by Six Feet of Circuitry \n","468":"When Robots Have Minds Of Their Own\n","469":"LENGTH: 494 words\n","471":"NASA Scientist Wm. Gevarter Dies at Age 64\n","473":"Microsoft Reorganizes, And Closes Windows Era\n","474":"Advance Reported in Content-Recognition Software\n","475":"Paid Notice: Deaths HERTZ, DAVID BENDEL\n","476":"Why Silicon Valley Shouldn't Work With the Pentagon; Op-Ed Contributor\n","478":"How Silicon Valley is erasing your individuality; The perils of monopoly.\n","479":"Q&A: Philosopher Nick Bostrom on superintelligence, human enhancement and existential risk; \"Superintelligence is the last invention we will ever need.\"\n","480":"Clinton, Trump, the White House too, terrifyingly transformed by MIT's 'Nightmare Machine'; This new artificial intelligence infuses images with tendrils, as though inky veins or creeping roots had taken over the world's landmarks.\n","481":"Taking Creepiness Out of Computer Voices\n","482":"Robots in La-La Land\n","483":"Morning Agenda: Deutsche Gloom, Och Ziff Bribery, More Tech Reformation\n","485":"Dangerous Thoughts . . . And Machines With Big Ideas\n","486":"COMPANY NEWS;Announcement Of Computer Due\n","489":"Hasn't advanced\n","490":"BUSINESS TECHNOLOGY;Compaq Printer Can Tell You What's Ailing It\n","493":"Siri and Apple's Future\n","494":"Teenage suicide is extremely difficult to predict. That's why some experts are turning to machines for help.; A new app arrives at a time when researchers across the country are developing forms of artificial intelligence that may forever change the way mental health issues are diagnosed and treated.\n","495":"NYPD will use AI to find 'stable' workforce\n","496":"Our Machine Masters\n","497":"Automation Will Change Jobs More Than Kill Them\n","498":"Brainy, Yes, but Far From Handy\n","499":"What is technology leader Musk's great fear? An AI Armageddon.\n","502":"What Eric Schmidt gets right - and wrong - about the future of artificial intelligence\n","503":"Technology;Computers That Reason\n","504":"New Economy; Intriguing possibilities in sensors, an on-ramp for electronics and biotechnology.\n","505":"The Post's Heliograf and ModBot technologies take first place in 2018 Global BIGGIES Awards; Administered by the Big Data & AI for Media Association, the awards recognize Heliograf for \"Excellence in the Use of Bots\" and ModBot for \"Excellence in the Use of Artificial Intelligence.\"\n","507":"Why Nissan's CEO says the human brain still trumps artificial intelligence; He predicts they'll be on the market by 2022, though.\n","509":"How computers were finally able to best poker pros; For years, the card game's bluffing and uncertainties had stymied artificial intelligence.\n","510":"Books of The Times;Hackers as Heroes\n","511":"Will Tech Protect My Kids?\n","512":"These two start-ups say they can remove bias from your next hiring decision\n","513":"In Case You Wondered, A Real Human Being Wrote This Column\n","514":"Race to Build A Robot More Like Us\n","515":"Harold Cohen, a Pioneer of Computer-Generated Art, Dies at 87\n","516":"Artificial Intelligence: Losing Bits of Ourselves\n","518":"Fearing Bombs That Can Pick Whom to Kill\n","519":"A VALENTINE SHOW THAT SHOOTS ARROWS TIPPED WITH SATIRE\n","520":"How Artificial Intelligence Is Edging Its Way Into Our Lives\n","521":"Rebecca Martin, Jared Lander\n","522":"From Jingles to Pop Hits, A.I. Is Music to Some Ears\n","523":"Obituaries\n","524":"Compressed Data; On a Futurists' Forum, Money Backs Up Predictions\n","525":"On Display in China, New Ways to Track Public\n","526":"'I Can Execute That Transaction, Dave'\n","527":"Not the Bots We Were Looking For\n","529":"'PROLOG';Travels of a Computer Code\n","530":"Five ways technology can help us cope with blizzards\n","531":"Early advocate of using computers in education\n","533":"Found: Another star system with eight planets, just like ours\n","536":"Business Briefing; Step Toward Approval for Self-Driving Autos\n","537":"How do you plan to raise your super-intelligent child?\n","538":"Fashion's Future, Printed to Order\n","539":"Questions for IBM's Watson\n","540":"TECHNOLOGY;Web Site Offers Cartoons That Interact, With Feeling\n","541":"TECHNOLOGY;THE NEXT GOAL: COMMON SENSE\n","542":"Support for Education And Immigration Help Toronto Make the Cut\n","544":"Will modern Luddites smash tech's future?; Attacks are coming from many sources.\n","545":"Blind Maryland scientist takes chess title, says study of decision-making helped\n","546":"The Future, as Imagined by Google\n","547":"Computers vs. Humans: Clashing Symbols\n","548":"At MIT, computers can drum up automated audio\n","550":"Kudos for Designer of Gene Experiment in Lab. Well Done, Robot! \n","551":"Taking Baby Steps Toward Software That Reasons Like Humans\n","552":"Computer models beat humans in reading comprehension\n","555":"I, Robot, Esq.? Not Just Yet\n","556":"Robots Are Nearing Reach of Consumers\n","558":"Uber picked a CEO who has an influential network of relatives across Silicon Valley\n","560":"IBM Buys Medical Analytics Company for $2.6 Billion\n","561":"Blurring a Line Between Driver and Computer\n","562":"The fourth industrial revolution is upon us; From Palo Alto to Marrakesh, the world is changing.\n","563":"White House to host tech companies for summit on artificial intelligence\n","568":"A Man, a Plan and a Robot That Makes Eye Contact\n","569":"Olivia Rissland, David Knezevic\n","570":"Nissan Announces Plans to Release Driverless Cars by 2020\n","571":"Microsoft Introduces Tool For Avoiding Traffic Jams\n","572":"Facebook hopes artificial intelligence can curb the 'terribly tragic' trend of suicides; Recent live-streamed suicides \"perhaps could have been prevented if someone had realized what was happening and reported them sooner,\" Mark Zuckerberg said, adding that A.I. \"can help provide a better approach.\"\n","573":"Teaching Activities for: 'Google Researchers Are Learning How Machines Learn'; Article of the Day\n","574":"The Artificial Intelligence Architect and the Machine That Changed Our Lives\n","575":"COULD A ROBOT STEAL YOUR JOB? Doctors, lawyers, teachers: No profession is safe. So how do we survive extinction by cyborg?\n","576":"IBM Moves Deeper Into Financial Consulting With Promontory Purchase\n","577":"Why Do I Have to Call This App 'Julie'?\n","578":"David E. Rumelhart, 68, Who Simulated Perception\n","579":"Chinese App Not as Saucy After Censors Have a Say\n","582":"No Riders: Desert Crossing Is for the Robots Only\n","585":"Google's Lab of Wildest Dreams\n","586":"Chip Off the Old Block\n","588":"Artificial Intelligence, With Help From the Humans\n","589":"A tale that does not compute\n","590":"How Computing Can Help Art Historians\n","591":"Brain Gain\n","593":"Visual Effects Master Stan Winston, 62\n","595":"Vintage Video Whiz;Long Ago; Steve Russel Invented Spacewar!\n","596":"Chatting Up the Google Assistant; Tech Tip\n","597":"Researchers Gaze Into the Tech Crystal Ball\n","598":"Trump is wasting crucial time with his crazy, archaic job ideas\n","600":"IBM Looks to Its Next Century\n","601":"Ford wants to patent a driverless police car that ambushes lawbreakers using artificial intelligence\n","603":"This top scientist offers a solution for the havoc driverless cars may wreck on workers\n","604":"In an Ancient Game, Computing's Future \n","605":"Behind the Groundbreaking Design of Aphex Twin's Record Covers\n","606":"Harvard and M.I.T. Take Their Classes Online\n","608":"What's On Monday\n","609":"This new tool that allows you to easily visualize 2016 campaign finance data\n","611":"Saudi Arabia, which denies women equal rights, makes a robot a citizen\n","612":"After Fatal Uber Crash, a Self-Driving Start-Up Moves Forward\n","613":"BUSINESS TECHNOLOGY: ADVANCES;THE DIFFICULT BIRTH OF THE LOTUS AGENDA\n","614":"Harrison Ford To Be Honored At Golden Globes\n","615":"Montreal Revitalized by Boom in Luxury Building\n","617":"Uber CEO pick's family is a forceful network\n","619":"The future of education is virtual\n","622":"Higher education for the AI age: Let's think about it before the machines do it for us; Northeastern University's president: We need to stop thinking of higher education as a once-in-a-lifetime event and more as a platform for lifelong learning.\n","623":"Trust Issues Cloud Future of $800 Smartphone\n","624":"The Week Ahead; R.B.S. Suit Nears Trial; OPEC Leaders Will Meet\n","625":"In a robot showdown, humanity may happily surrender\n","626":"Why robots and smart thermostats keep America's spy chief up at night; Robots and connected cars are a threat to national security, but also an opportunity for its spies, according to the Director of National Intelligence.\n","627":"Google Races to Catch Up in Cloud Computing\n","628":"Are robots coming for your job?\n","632":"'The Business of War': Google Employees Protest Work for the Pentagon\n","633":"Ryszard Michalski; Shaped How Machines Learn\n","634":"China Enters Delicate Area\n","635":"LENGTH: 734 words\n","636":"Hang-Ups...And the Pentagon's Secret Sharer\n","637":"Joseph Weizenbaum Dies; Computer Pioneer Was 85\n","638":"The Calendar\n","639":"Writing of Intelligence: Academic, Artificial and Amorous \n","640":"I Am My Own Monster (Technology Rules!)\n","641":"Giving Robots More Than a Shred of Humanity\n","642":"A Dance and Its Digitized Echoes\n","643":"Artificial intelligence trumps political experts\n","644":"Virtual Camp Trains Soldiers in Arabic, and More\n","645":"Siri creators change the conversation\n","647":"Evidence That Robots Are Winning the Race for American Jobs\n","648":"Hackers and Tensions Return for a Final Run\n","649":"FILM; A Director's Journey Into A Darkness Of the Heart\n","651":"Remember the human farmworkers\n","652":"Tetsuya Miyamoto's KenKen\u00ae\n","653":"Software Seen Giving Grades On Essay Tests\n","655":"How video games helped give us the self-driving car\n","656":"Software Helps Voyager Engineers Automatically Monitor Spacecraft;Arlington Company Develops 'Expert System'\n","657":"Anger Over Venture Capitalist's Letter\n","662":"At Heart of U.S. Strategy, Weapons That Can Think\n","663":"Nafta Talks Near the End; Fate of a Disney-Fox Deal\n","665":"This A.I. Can Build A.I. Itself\n","666":"Artificial Intelligencend Flexible Time\n","667":"Google's artificial intelligence breakthrough may have a huge impact on self-driving cars and much more; Now it's teaching itself how to beat classic Atari games. One day it may drive your car.\n","668":"Computer Fails As Job-Killer\n","669":"BOOKS OF THE TIMES;\n","670":"It's Alive! (But Must Be Plugged In.)\n","671":"Start-Up Imagines Driverless Vehicles That Can Tell You Where They're Going\n","672":"Google Is Taking Your Questions (Spoken, via the iPhone)\n","673":"There is a jobs crisis brewing that the Trump administration should not ignore\n","674":"Davos report projects 5 million jobs will be lost to new technologies by 2020\n","675":"What New Tech Ideas Does the World Need?\n","678":"Stephen Hawking just moved up humanity's deadline for escaping Earth\n","679":"Google Unveils $1 Billion Job Training Initiative\n","681":"BUSINESS TECHNOLOGY;A Darwinian Creation of Software\n","682":"Harvard And M.I.T. Join to Offer Web Courses\n","683":"Robot intelligence: A primer\n","684":"Nation of robots Non-human CEOs on horizon: Ma\n","685":"Stephen Hawking: 'I fear that I may not be welcome' in Trump's America; Hawking once called Trump \"a demagogue who seems to appeal to the lowest common denominator.\"\n","687":"Robot grabs man, kills him in German car factory\n","688":"A futurist's dream of a machine that can think\n","689":"To Master Crosswords, Computer Fills In SENSE OF HUMOR\n","691":"Astronaut's Re-entry Is Bumpy\n","692":"AI will solve Facebook's most vexing problems, Mark Zuckerberg says. Just don't ask when or how.; Experts say AI is instead more likely to solve the Facebook CEO's issue with \"getting someone else to take responsibility\" for the social network's thorniest problems.\n","693":"Here's who could become one of Trump's top science and tech advisers; The president still has not selected a top science and tech adviser, leaving vacant a post that is supposed to aid the administration on artificial intelligence, climate change and other key matters.\n","695":"Software Lets MS-DOS Owners Use English;Developer Markets Product Through Sak Technologies in Arlington\n","699":"Uber Sees an Executive Exodus as It Faces Questions of Workplace Culture\n","701":"'Press the big red button': Computer experts want kill switch to stop robots from going rogue; Computer experts at Google and the University of Oxford are worried about what happens when robots with boring jobs misbehave or make sub-optimal decisions.\n","702":"How Would You Feel About a Computer Grading Your Essays?\n","703":"Fake video: What do we do when seeing is not believing?\n","706":"New Approach Trains Robots to Match Human Dexterity and Speed\n","708":"We're building superhuman robots. Will they be heroes, or villains?; With great power comes great responsibility. How will we teach that to robots?\n","709":"Paul Allen's AI research group unveils program that aims to shake up how we search scientific knowledge. Give it a try.; The program, known as Semantic Scholar, searches journal articles in a new way.\n","711":"Edward Frenkel: The Age of a Child\n","713":"Computers Gain New Respect as Translators\n","714":"As Europe Heads to the Polls, Tech Tackles Fake News\n","715":"'It knew what you were going to do next': AI learns from pro gamers - then crushes them; It only took the bot a few weeks to go from novice to world class.\n","716":"GRASS-ROOTS BUSINESS; How to Get There? It Counts the Ways\n","717":"Google 'paintbrush' raises some of art's oldest questions\n","718":"Killer robots? Superintelligence? Let's not get ahead of ourselves.; Robots may be coming for our jobs. But we'll see them before they arrive.\n","719":"Tech's New Wave, Driven By Data\n","721":"Lyft to Share Driverless Advances in Deal\n","722":"Artificial Intelligence;Teaching Computers Power Of Creative Stupidity\n","723":"Two more Uber executives call it quits; Jeff Jones and Brian McClendon are leaving the embattled ride-sharing business.\n","726":"Apple Faces an Artificial Intelligence Challenge\n","728":"The Big Tech Trends to Follow at CES 2018; Tech Fix\n","729":"BUSINESS TECHNOLOGY;Fuzzy Thinking Has Merits When It Comes to Elevators\n","730":"ARTIFICIAL INTELLIGENCE-WITH VERY REAL BIASES\n","731":"ARTIFICIAL INTELLIGENCE GETS A SHAKE\n","732":"ARTIFICIAL INTELLIGENCE ISN'T A THREAT - YET\n","733":"Artificial Intelligence Is Stuck. Here's How to Move It Forward.; Gray Matter\n","734":"DIGITS\n","735":"The Capitals have a Facebook Messenger bot, and it has an attitude\n","736":"DIGITS\n","737":"Artificial intelligence update: Siri would already be a better president\n","738":"FACEBOOK AGONIZES OVER USE OF AI\n","739":"ARTIFICIAL-INTELLIGENCE TOOL IS OFFERED FOR FREE\n","740":"TOYOTA INVESTS IN ARTIFICIAL-INTELLIGENCE COMPANY\n","741":"BUSINESS WATCH: APPLE\n","742":"Daily Report: Machines Outdo Humans in Identifying Characters\n","743":"NEW TOOLS TELL BOSSES HOW YOU'RE FEELING\n","744":"BUSINESS WATCH\n","745":"IBM OFFERS WATSON TO HELP FERRET OUT FINANCIAL CRIMES\n","746":"GOOGLE ADDS AI TOUCHES TO BOOST LATEST DEVICES\n","747":"TO SET PRICES, STORES TURN TO ALGORITHMS\n","748":"IBM Creates Watson Health to Analyze Medical Data\n","749":"Lotus Founder Becomes Student of Hot Research Area\n","750":"CHIP MAKERS ZERO IN ON ARTIFICIAL-INTELLIGENCE MARKET\n","751":"BAD INTELLIGENCE BEHIND THE WHEEL\n","752":"SAMSUNG TESTS IMPROVED AI FEATURE\n","753":"YOUR HELPFUL COLLEAGUE THE ROBOT\n","754":"ROBOT MAKER PLACES BET ON ARTIFICIAL INTELLIGENCE\n","755":"BAIDU WHIZ MUST OUTSMART GOOGLE AT ARTIFICIAL INTELLIGENCE\n","756":"AMAZON IS NOW LOOKING AT YOU\n","757":"COMPUTER PROGRAM AS POKER CHAMP? DON'T BET AGAINST IT\n","758":"After Setbacks, Online Courses Are Rethought\n","759":"Guided by Computers and Sensors, a Smooth Ride at 60 Miles Per Hour\n","760":"When artificial intelligence makes a picture worth way more than a thousand words\n","761":"SOLD! AI SYSTEMS HELP COMPANIES FIND CUSTOMERS\n","762":"WHAT KIND OF REGULATION DOES AI NEED?\n","763":"HOW YOU CAN RAISE ROBOT-PROOF CHILDREN\n","764":"DIGITAL ASSISTANTS START TO GET MORE HUMAN\n","765":"AI STILL NEEDS A HUMAN TOUCH\n","766":"TOYOTA WANTS A TALKING CAR TO BE YOUR PAL\n","767":"GOOGLE BUILDS TECH TEAM IN CHINA\n","768":"ON THE RANDOM WALK, DON'T FEAR ROBOTS\n","769":"SIRI, AM I ABOUT TO HAVE A HEART ATTACK?\n","770":"IS YOUR WALL STREET ANALYST A ROBOT?\n","771":"Google Researchers Are Learning How Machines Learn\n","772":"A.I. and Big Data Could Power a New War on Poverty; Op-Ed Contributor\n","773":"Can Bots Fight Bullying?\n","774":"Word for Word\/Wanna Bet?; Taking the Long View And Wagering on What's to Come\n","775":"Every Asian American has been asked this question. A computer gives the best answer.\n","776":"BE A SMART MOUTH\n","777":"NO NEED FOR RADIOLOGISTS TO BE NEGATIVE ON AI\n","778":"SMART MACHINES WILL FREE US ALL\n","779":"FORD TAKES THE WHEEL AT STARTUP\n","780":"TECH COMPANIES TAP NEW TALENT FROM ACADEMIA\n","781":"IBM'S WATSON EXPANDS ITS REACH\n","782":"GM, IBM TEAM UP ON CAR TECH\n","783":"GOOGLE TOUTS NEW AI-POWERED TOOLS\n","784":"MACHINES THAT WILL THINK AND FEEL\n","785":"TECHNOLOGY READS BOJ TEA LEAVES\n","786":"AMELIA, A MACHINE, THINKS LIKE YOU\n","787":"LENGTH: 17 words\n","790":"COMPANIES HAVE YET TO EMBRACE AI\n","791":"POLICE WANT TO SEND AI INTO THE STREET\n","792":"MACHINE LEARNING SOWS SEEDS OF FUTURE\n","793":"THE ROBOT-HUMAN ALLIANCE\n","794":"GOOGLE PLANS NEW, SMARTER MESSAGING APP\n","795":"J&AMP;J, IBM AND APPLE UNITE ON HEALTH APPS\n","796":"FACEBOOK OPENS LAB IN PARIS\n","797":"A COMPUTER THAT PLAYS BY THE RULES (AND READS THEM)\n","799":"CONCERT: BEHRMAN WORKS\n","800":"What students should learn from Uber's recent troubles; We teach our students core technologies but do not give them the vision to better the world.\n","801":"THE AI ARMS RACE\n","802":"ALGORITHMS WITH MINDS OF THEIR OWN\n","803":"TRAINING AI TO PLAY SAFELY (WITH US)\n","804":"THE NATURAL SIDE OF A.I.\n","805":"SOFTBANK TO HOLD ROBOTS, CARS CLOSE TO ITS HEART\n","806":"DIGITS\n","807":"LENGTH: 20 words\n","808":"Radios Aren't So Smart \n","809":"BYLINE: Bloomberg News\n","811":"TECHNOLOGY-DRIVEN BOOM IS FINALLY COMING\n","812":"GOOGLE HIRES CHINA TALENT\n","813":"WE SURVIVED SPREADSHEETS; WE'LL SURVIVE AI\n","814":"CAREER OF THE FUTURE: ROBOT PSYCHOLOGIST\n","815":"HOW AI IS TRANSFORMING THE WORKPLACE\n","816":"HUMAN TOUCH STILL KEY TO DESIGN\n","817":"RETAILERS ARE GETTING PERSONAL\n","818":"AI LOOMS MUCH LARGER FOR CORPORATE WORLD\n","819":"LEARNING HOW TO USE AI\n","820":"LOVE IN THE TIME OF ROBOTS\n","821":"AI: JUST LIKE US?\n","822":"PERSONAL COMPUTERS;ARTIFICIAL INTELLIGENCE: NEW SOFTWARE ARRIVES\n","823":"The Washington Post leverages artificial intelligence in comment moderation\n","824":"The 22 Atari games that Google's artificial intelligence algorithm is better at than a human; Trust us, don't challenge it to a game of Video Pinball.\n","825":"CORRECTIONS AMPLIFICATIONS\n","826":"THE CYBER AGE HAS HARDLY BEGUN\n","827":"APPLE TRIES TO OPEN UP IN AI EFFORTS\n","828":"FOR CIOS, A NEW ERA IN TECH UNFOLDS\n","829":"SOFTBANK ANGLES FOR AI ADVANTAGE\n","830":"HOW DO YOU TEACH A MACHINE?\n","831":"'GO' MATCH GOES BADLY FOR HUMANS\n","832":"DRAGONFLY VISION HELPS TO BUILD BIONIC EYE\n","833":"ARE SMARTPHONES BECOMING SMART ALECKS?\n","835":"Researchers create a computer program that mimics how humans learn\n","836":"'KILLER ROBOTS' CAN MAKE WAR LESS AWFUL\n","837":"AI ISN'T LIKE THE LIBERATING INDUSTRIAL REVOLUTION OF OLD\n","838":"CANADA SETS ITS BEST MINDS ON AI\n","839":"IBM COURTS DEVELOPERS\n","840":"YOU CAN'T TEACH (MACHINES) COMMON SENSE\n","841":"HUMANS STILL RULE IN THIS GAME\n","842":"AUDITORS COUNT ON TECH FOR BACKUP\n","843":"BOJ CONFOUNDS THE ROBOTS\n","844":"AUTOMATION MAKES US DUMB\n","845":"Crafting Genes Is About Ego, Not Science\n","847":"THE MEDIA BUSINESS: Encountering The Digital Age -- An occasional look at computers in everday life.;Potboiler Springs From Computer's Loins\n","848":"AUTHORITARIANISM IS CHINA'S EDGE IN AI\n","849":"LOOKING FOR A PRODUCTIVITY MIRACLE\n","850":"FOR '16, ZUCKERBERG RESOLVED TO CREATE A VIRTUAL ASSISTANT\n","851":"BUILDING A ROBOT FOR THE HOME\n","852":"LENGTH: 50 words\n","853":"LENGTH: 22 words\n","854":"Google Sells A.I. for Building A.I. (Novices Welcome)\n","855":"How the Internet Could Protect Your Memory\n","856":"PRESIDENT USES BIG DATA TO TIGHTEN BIG BROTHER'S GRIP\n","857":"UBER ADDS AI DIVISION FOR SELF-DRIVING CARS\n","858":"Using Artificial Intelligence in New War Weapons\n","859":"SCIENCE FILE; Miniature version of our solar system; Scientists using artificial intelligence find an eighth planet around a sun-like star.\n","861":"THE LIVELY ARTS;COMEDY ACTRESS PARODIES 50's-STYLE TV\n","862":"COMPANY LOOKS INTO DESIGNING CHIPS\n","863":"DON'T BE EVIL, SUPPORT THE TROOPS\n","864":"AI HELPS IDENTIFY THOSE AT RISK FOR SUICIDE\n","865":"CHIEF SCIENTIST RESIGNS AT BAIDU\n","866":"SALESFORCE, IBM LINK AI SERVICES\n","867":"IBM CHIEF SAYS AI WON'T BE JOB KILLER\n","868":"IS IT TIME TO RETHINK YOUR CAREER?\n","869":"IBM SPEEDS ARTIFICIAL INTELLIGENCE\n","870":"MERRILL LYNCH WARNS OF THE COMING ROBOT APOCALYPSE\n","871":"IS IT POSSIBLE TO CREATE AN ETHICAL ROBOT?\n","872":"WHY DIGITAL GURUS GET LOST IN THE 'UNCANNY VALLEY'\n","873":"LENGTH: 63 words\n","874":"LENGTH: 23 words\n","875":"Reading File When Computers Hurt Instead of Help\n","876":"Artificial intelligence is marvelous - unless it's part of a killer robot; Some experts worry that the technology is dangerous as well as marvelous. Think killer robot.\n","877":"Airplane, heal thyself: Artificial intelligence delves into aerospace\n","878":"HM RAMPS UP DATA USE\n","879":"JPMORGAN TAPS PROFESSOR FOR NEW AI ROLE\n","880":"FIRMS LEAVE THE BEAN COUNTING TO THE ROBOTS\n","881":"PROMONTORY AGREES TO AN IBM BUYOUT\n","882":"ON FOURTH TRY, MAN OVER MACHINE\n","883":"LENGTH: 81 words\n","884":"LENGTH: 69 words\n","885":"Cambridge, Cabs and Copenhagen: My Route to Existential Risk\n","887":"SONY WALKS ITS ROBOT DOG BACK TO THE MARKETPLACE\n","888":"MR MUSK'S CONCERNS ABOUT AI ARE MOSTLY UNFOUNDED\n","890":"THE WAYS AI IS TRANSFORMING DRUG DEVELOPMENT\n","891":"SALESFORCE TO ACQUIRE MARKETING SETUP\n","892":"MAN OR MACHINE?\n","893":"LETTING THE MACHINES DECIDE\n","894":"LENGTH: 32 words\n","896":"TECH FIRMS CRAFT AI RULES\n","897":"VOICES FROM THE CONFERENCE\n","898":"THE MAN PLAYING PEACEMAKER BETWEEN TRUMP AND TECH\n","899":"HIGHLIGHTS FROM WSJ D.LIVE CONFERENCE\n","900":"INTEL SHOULD WATCH OUT: NVIDIA IS MOVING IN ON ITS TURF\n","901":"BUSINESS WATCH\n","902":"GOOGLE WOOS CLOUD CUSTOMERS WITH ITS AI\n","903":"THE SPEAKER THAT PREDICTS WHAT YOU'LL WANT TO HEAR\n","904":"LENGTH: 78 words\n","905":"SP DEAL IS LATEST PUSH INTO AI SECTOR\n","906":"U.S. AND CHINESE COMPANIES RACE TO DOMINATE AI\n","907":"BEIJING TAKES CHALLENGE TO AI CHIP LEADERS\n","908":"WE'LL NEED BIGGER BRAINS\n","909":"THE ROBOT IN THE PIN-STRIPED SUIT DISCUSSES FUTURE OF HUMANITY\n","910":"APPLE, SOFTBANK HELD TALKS ON FUND\n","911":"TECH INVESTORS GRAPPLE WITH WHERE TO BET\n","912":"INTEL PLANS TO BUY CHIP STARTUP\n","913":"DENA, STARTUP IN AI DEAL\n","914":"ALIBABA'S CLOUD UNIT TO OFFER DATA ANALYSIS\n","915":"CAREFUL NOT TO HURT YOUR COMPUTER'S FEELINGS\n","916":"LENGTH: 74 words\n","917":"LENGTH: 31 words\n","918":"The Brainy Hearing Aid Extracts Meaning From Noise\n","919":"Is That Machine After Your Job?;Measure of Man\n","920":"The Future of Moral Machines\n","921":"New Search Tool Aims at Answering Tough Queries, but Not at Taking On Google\n","922":"KUBRICK'S '2001' AT 50\n","923":"XI JINPING'S MILITARY MIGHT\n","924":"AI HOLDS PROMISE FOR IMPROVING DIAGNOSES\n","925":"DRONES SPEED UP INSURANCE CLAIMS\n","926":"FINANCE WATCH\n","927":"WHY NVIDIA WON'T RUN OUT OF ROAD\n","928":"LENGTH: 35 words\n","929":"Ford to invest $1 billion in artificial intelligence for your car\n","930":"Elon Musk and Stephen Hawking think we should ban killer robots\n","931":"A Robot Monk Captivates China, Mixing Spirituality With Artificial Intelligence\n","932":"COMPANIES MUST USE AI-OR ELSE\n","933":"LENGTH: 21 words\n","934":"Is That Machine After Your Job?;Institutional Change\n","935":"Drew Harwell to cover artificial intelligence and big data\n","936":"WSJ: Washington Post to Cover Every Major Race on Election Day With Help of Artificial Intelligence\n","937":"Now is the time to ask Stephen Hawking your questions about Artificial Intelligence\n","938":"WORK LIFE; HOW I MADE IT: CHRIS NICHOLSON; Long road leads to an AI start-up; Montana native's path to starting Skymind in S.F. included stops in Europe, Guatemala.\n","939":"FACEBOOK PITCHES IN ON INTEL'S NEW CHIP\n","940":"SOFTBANK, SAUDIS LAUNCH HIGH-TECH INVESTMENT FUND\n","941":"HOW AI WILL CHANGE EVERYTHING\n","942":"LENGTH: 29 words\n","943":"\"Cow Fitbits\" and artificial intelligence are coming to the dairy farm. But some farmers aren't so impressed; Farmers might be tackling a critical question before much of the rest of the workforce: Can new technology ever beat old intuition - even when it comes to a bunch of cows?\n","947":"'Westworld' and 'Ready Player One' show how our relationship to artificial intelligence has changed; They're a lot different from \"Tron\" and \"WarGames,\" because AI is now a bigger part of our lives.\n","948":"'Artificial Intelligence': Almost Brilliant\n","949":"COLLEGE GRADS AREN'T READY FOR AN AI WORLD\n","950":"A Human-Centered Artificial Intelligence?; Letter\n","951":"Crafting Genes Is About Ego, Not Science\n","952":"A Desk Near the Boss's Heart\n","953":"THE CIO AT THE CENTER OF A DIGITAL WORLD\n","954":"HOSPITAL FALTERS IN BID TO USE AI TECH\n","955":"SOFTBANK SEEKS TO EASE ITS DEBT LOAD\n","956":"HOW TO MASTER THE MACHINES\n","957":"BUSINESS WATCH\n","958":"VOICES ON THE FUTURE - EMAIL\n","960":"LET A HUNDRED ORACLES, SAPS BLOOM\n","961":"MICROSOFT UNVEILS VIRTUAL-ASSISTANT BOT\n","962":"FINANCE WATCH\n","963":"CHINESE RIDE-SHARING GIANT'S VALUATION TOPS $50 BILLION\n","964":"STARTUP OFFERS CONCIERGE MEDICINE\n","965":"COGNIZANT EXPANDS INTO DIGITAL SERVICES\n","967":"Decide.com Pays to Get Consumer Reports's Reputation\n","968":"Film CapsulesCapsule reviews ...\n","969":"CHINESE TECH ENTICES VENTURE INVESTORS\n","970":"News Q's | A Robot Monk Captivates China, Mixing Spirituality With Artificial Intelligence\n","971":"Daily Report: A Peek at the Future of Artificial Intelligence and Human Relationships\n","972":"FAMILY FARE \n","973":"Looking to the Future of Data Science\n","974":"Arming ourselves for the next war\n","975":"CHIP FIRMS FLOCK TO AI'S PROMISE\n","976":"INSIDE THE BRAIN OF THE DRIVERLESS CAR\n","977":"CHINA'S BAIDU MOVES INTO SILICON VALLEY FOR R&AMP;D LAB\n","978":"LENGTH: 63 words\n","979":"A Human-Centered A.I.?\n","981":"MICROSOFT RESPONDS TO THE ECHO\n","982":"GOOGLE BUILDS OWN HIGH-SPEED CHIP\n","983":"LENGTH: 33 words\n","984":"Daily Report: Silicon Valley Hotel Puts Robotic Bellhop to the Test\n","986":"Paperback Row\n","987":"Can algorithms be funny? Veterans of Clickhole and the New Yorker team up to find out.; Botnik Studios has spoofed everything from Harry Potter to Coachella posters - with the help of artificial intelligence.\n","988":"Do Machines Represent a Threat to Humans?\n","990":"CONSUMERS GIVE ASIA A LIFT IN AI\n","991":"A 'CATHEDRAL TO MANUFACTURING' IS REBORN\n","992":"BAIDU'S AI FEAT WAS INFLATED\n","994":"OVERHEARD\n","995":"WHY YOU SHOULD BET BIG ON BIONIC BRAINS\n","996":"LENGTH: 38 words\n","998":"Artificial Intelligence as a Bridge for Art and Reality\n","999":"WHO WATCHES A MURDER STREAMED LIVE ON FACEBOOK?\n"}}